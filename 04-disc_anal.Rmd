```{r include=FALSE}
library(tidyverse)
library(mfstylr)
```

# Discrete Variables {#discrete_variables}


## Chi-Square Test

These notes rely on [PSU STAT 500](https://newonlinecourses.science.psu.edu/stat500/node/56/),  [Wikipedia](https://en.wikipedia.org/wiki/Chi-squared_test), and [Disha M](http://www.yourarticlelibrary.com/project-reports/chi-square-test/chi-square-test-meaning-applications-and-uses-statistics/92394)].

The chi-square test compares observed categorical variable frequency counts $O$ with their expected values $E$.  The test statistic $\chi^2 = \sum (O - E)^2 / E$ is distributed $\chi^2$.  $H_0: O = E$ and $H_a$ is at least one pair of frequency counts differ. The chi-square test relies on the central limit theorem, so it is valid for independent, normally distributed samples, typically affirmed with at least 5 successes and failures in each cell.  There a small variations in the chi-square for its various applications.

* The **chi-square goodness-of-fit test** tests whether observed frequency counts $O_j$ of the $j \in (0, 1, \cdots k)$ levels of a single categorical variable differ from expected frequency counts $E_j$. $H_0$ is $O_j = E_j$.

* The **chi-square independence test** tests whether observed joint frequency counts $O_{ij}$ of the $i \in (0, 1, \cdots I)$ levels of categorical variable $Y$ and the $j \in (0, 1, \cdots J)$ levels of categorical variable $Z$ differ from expected frequency counts $E_{ij}$ under the *independence model* where $\pi_{ij} = \pi_{i+} \pi_{+j}$, the joint densities. $H_0$ is $O_{ij} = E_{ij}$.

* The **chi-square homogeneity test** tests whether frequency counts of the $R$ levels of a categorical variable are distributed identically across $C$ different populations.

## Example of Chi-Square Test of Homogeneity
A project studied whether attending physicians order more unnecessary blood transfusions than residents.  The categorical variable frequency of orders has 4 levels: frequently, occasionally, rarely, and never. 
```{r warning=FALSE, message=FALSE}
library(dplyr)
library(ggplot2)
library(stats)

pop <- NULL
pop[1:49] <- 1
pop[50:120] <- 2
lev <- NULL
lev[c(1:2, 50:64)] <- 1
lev[c(3:5, 65:92)] <- 2
lev[c(6:36, 93:115)] <- 3
lev[c(37:49, 116:120)] <- 4
dat <- data.frame(pop, lev)
dat$pop <- factor(dat$pop, levels = c(1, 2), 
                  labels = c("attending", "resident"))
dat$lev <- factor(dat$lev, levels = c(1, 2, 3, 4), 
                  labels = c("frequently", "occasionally", "rarely", "never"))

df <- (2-1)*(4-1)
alpha <- 0.05


(test <- chisq.test(dat$lev, dat$pop))

# Graph of hypothesis test
lrr = -Inf
urr = qchisq(p = alpha, df = df, lower.tail = FALSE)
data.frame(xi = 0:400 / 10) %>%
  mutate(density = dchisq(x = xi, df = df)) %>%
  mutate(rr = ifelse(xi < lrr | xi > urr, density, 0)) %>%
ggplot() +
  geom_line(aes(x = xi, y = density), color = "black") +
  geom_area(aes(x = xi, y = rr), fill = "red", alpha = 0.3) +
  geom_vline(aes(xintercept = test$statistic), color = "blue") +
  labs(title = bquote("Chi-Square Test for Homogeneity"),
       subtitle = bquote("P-value ="~.(test$p.value)),
       x = "xi^2",
       y = "Density") +
  theme(legend.position="none")

```


## One-Way Tables

These notes rely on PSU STATS 504 [course notes](https://online.stat.psu.edu/stat504/node/60/).

A one-way frequency table is a frequency table for a single categorical variable.  You usually construct a one-way table to test whether the frequency counts differ from a hypothesized distribution using the chi-square goodness-of-fit test.

Here is an example.  A researcher crosses tall cut-leaf tomatoes with dwarf potato-leaf tomatoes, then classifies the ($n = 1,611$) offspring phenotypes.

```{r}
o <- c(926, 288, 293, 104)
cell_names <- c("tall cut-leaf", "tall potato-leaf", "dwarf cut-leaf", "dwarf potato-leaf")
names(o) <- cell_names
print(o)
```

The four phenotypes are expected to occur with relative frequencies 9:3:3:1.

```{r}
pi <- c(9, 3, 3, 1) / (9 + 3 + 3 + 1)
print(pi)
```

```{r}
e <- sum(o) * pi
names(e) <- cell_names
print(e)
```


```{r fig.height=4}
data.frame(O = o, E = e) %>%
  rownames_to_column(var = "i") %>%
  pivot_longer(cols = -i, values_to = "freq") %>%
  group_by(name) %>%
  mutate(pct = freq / sum(freq)) %>%
  ungroup() %>%
  ggplot(aes(x = i, y = freq, fill = name, 
             label = paste0(round(freq, 0), "\n", 
                            scales::percent(pct, accuracy = 0.1)))
         ) +
  geom_col(position = position_dodge()) +
  geom_text(position = position_dodge(width = 0.9), size = 2.8) +
  theme_mf() +
  scale_fill_mf() +
  labs(title = "Observed vs Expected", fill = "")
```

Do the observed phenotype counts conform to the expected proportions?  This is a goodness-of-fit question because you are comparing frequencies from a single categorical variable to a set of hypothesized frequencies.


### Chi-Square Goodness-of-Fit Test

The **chi-square goodness-of-fit test** tests whether observed frequency counts $O_j$ of the $j \in (0, 1, \cdots k)$ levels of a categorical variable differ from expected frequency counts $E_j$. $H_0$ is $O_j = E_j$.

There are two test statistics for this test, Pearson $\chi^2$ and deviance $G^2$.  The sampling distribution of each test statistic approach the chi-square distribution as $n$ becomes large with degrees of freedom $df = k - 1$.  It's a good idea to calculate both test statistics.

The Pearson goodness-of-fit statistic is

$$\chi^2 = \sum \frac{(O_j - E_j)^2}{E_j}$$

where $O_j = p_j n$ and $E_j = \pi_j n$.  The deviance statistic is 

$$G^2 = 2 \sum O_j \log \left( \frac{O_j}{E_j} \right)$$

If the saturated model (the observed data represent the fit of the saturated model, the most complex model possible with the data) proportions $p_j$ equal equal the expected proportions $\pi_j$, $\chi^2$ and $G^2$ will equal zero.  Large values indicate the data do not agree well with the proposed model.

As $n \rightarrow \infty$, $G^2$ and $\chi^2$ approach $\chi_{k-1}^2$ where $k$ is the number of n-way table cells.  You can therefore run a chi-square test of significance using the $G^2$ and $\chi^2$ test statistics.  The chi-square test provides reliable results when at least 80% of $E_j >= 5$.  

```{r echo=FALSE}
x2 <- sum((o - e)^2 / e)
g2 <- 2 * sum(o * log(o / e))
dof <- length(o) - 1
```

The $\chi^2$ statistic is `x2 <- sum((o - e)^2 / e) = ` `r  x2` and the $G^2$ statistic is `g2 <- 2 * sum(o * log(o / e)) = ` `r g2`, so nearly identical. The chi-sq test p-values are also nearly identical.

```{r}
pchisq(q = x2, df = dof, lower.tail = FALSE)
pchisq(q = g2, df = dof, lower.tail = FALSE)
```

You can also perform the chi-square test of the Pearson test statistic with `chisq.test()`.

```{r}
chisq.test(o, p = pi)
```

The p-values based on the $\chi^2$ distribution with 3 d.f. are about 0.69, so we fail to reject the null hypothesis - the data are consistent with the genetic theory. The $\chi^2$ value is well outside the $\alpha = 0.05$ level range of regression.

```{r warning=FALSE, message=FALSE, fig.height=4}
alpha <- 0.05
dof <- length(e) - 1
lrr = -Inf
p_val <- pchisq(q = x2, df = length(o) - 1, lower.tail = FALSE)
urr = qchisq(p = alpha, df = dof, lower.tail = FALSE)
 data.frame(chi2 = seq(from = 0, to = 20, by = .1)) %>%
   mutate(density = dchisq(x = chi2, df = dof)) %>%
   mutate(rr = ifelse(chi2 < lrr | chi2 > urr, density, 0)) %>%
 ggplot() +
   geom_line(aes(x = chi2, y = density), color = mf_pal(12)(12)[12], size = 0.8) +
   geom_area(aes(x = chi2, y = rr), fill = mf_pal(12)(12)[2], alpha = 0.8) +
   geom_vline(aes(xintercept = x2), color = mf_pal(12)(12)[11], size = 0.8) +
   labs(title = bquote("Chi-Square Goodness-of-Fit Test"),
        subtitle = paste0("Chisq=", round(x2,2), ", ",
                          "Critical value=", round(urr,2), ", ",
                          "p-value=", round(p_val,3), "."
                          ),
        x = "chisq",
        y = "Density") +
   theme(legend.position="none") +
   theme_mf()
```

Inspect the residuals to learn which differences may lead to rejecting the null hypothesis.  The Pearson and deviance statistics are sums of individual cell comparisons which can be termed the squared "residuals", $\chi^2 = \sum r_j^2$ and $G^2 = \sum G_j^2$.  The expected value of a $\chi^2$ random variable is its d.f., $k-1$, and the typical size of any $\chi^2$ value is be $(k - 1) / k$, so for the typical residual should be within 2 $\sqrt{(k - 1) / k}$.

```{r}
e2_res <- sqrt((o - e)^2 / e)
g2_res <- sign(o - e) * sqrt(abs(2 * o * log(o / e)))
```

```{r fig.height=4}
data.frame(e2_res) %>%
  rownames_to_column() %>%
  # pivot_longer(cols = e2_res:g2_res) %>%
  ggplot(aes(x = rowname, y = e2_res)) +
  geom_point(size = 3, color = mf_pal(12)(12)[2], alpha = 0.8) +
  theme_mf() +
  labs(title = "X^2 Residuals by Cell", color = "", x = "", y = "")
```

If you do not have a theoretical value, but instead want to test whether the data conform to a distribution, the test is nearly the same.  Your first step is the estimate the distribution parameter(s).  Then you can perform the goodness of fit test, but with degrees of freedom reduced for each estimated parameter.

For example, suppose sample $n = 100$ families and count the number of children $J$, and you want to test whether the counts $O$ follow a Poisson distribution, $X \sim  Pois(\lambda)$.  

```{r}
dat <- data.frame(j = 0:5, o = c(19, 26, 29, 13, 10, 3))
print(dat)
```

The ML estimate for $\lambda$ is

$$\hat{\lambda} = \frac{j_0 O_0 + j_1 O_1, + \cdots j_k O_k}{O}$$

```{r}
lambda_hat <- sum(dat$j * dat$o) / sum(dat$o)
print(lambda_hat)
```

Then the expected values are 

$$f(j; \lambda) = \frac{e^{-\hat{\lambda}} \hat{\lambda}^j}{j!}.$$

```{r}
E <- exp(-lambda_hat) * lambda_hat^dat$j / factorial(dat$j) * sum(dat$o)
dat <- cbind(dat, e = E)
```

```{r fig.height=4}
dat %>%
  rename(pois = e) %>%
  pivot_longer(cols = -j, values_to = "freq") %>%
  group_by(name) %>%
  mutate(pct = freq / sum(freq)) %>%
  ungroup() %>%
  ggplot(aes(x = fct_inseq(as.factor(j)), y = freq, fill = name, 
             label = paste0(round(freq, 0), "\n", 
                            scales::percent(pct, accuracy = 0.1)))
         ) +
  geom_col(position = position_dodge()) +
  geom_text(position = position_dodge(width = 0.9), size = 2.8) +
  theme_mf() +
  scale_fill_mf() +
  labs(title = "Observed vs Expected", fill = "", x = "children in family")
```

Compare the expected values to the observed values with the $\chi^2$ goodness of fit test.  In this case, $k = 6 - 1 - 1$, reducing the degrees of freedom by $1$ for the estimated paramater $\lambda$.

```{r}
(X2 <- sum((dat$o - dat$e)^2 / dat$e))
(dof <- nrow(dat) - 1 - 1)
pchisq(q = X2, df = dof)
```

Be careful of this consideration, because `chisq.test()` does not take this into account, and you cannot override the degrees of freedom.

```{r}
chisq.test(dat$o, p = dat$e / sum(dat$e))
```


### One-Sample Proportion Test

Suppose a student population is hypothesized to be 60 percent female $\pi_F=0.60$.  A random sample of $n=100$ students yields 53 percent females $p_F=0.53$.  Is the sample representative of the population at an $\alpha=0.05$ level of significance?

```{r}
O <- c(53, 47)
E <- c(.60, .40) * sum(O)
n <- sum(O)

p <- O[1] / sum(O)
v <- p * (1 - p) / n
```

The one-sample proportion test statistic is 

$$Z = \frac{p_F - \pi_F}{se} = -1.423$$

```{r}
Z <- (p - 0.60) / sqrt(v)
```

```{r}
pnorm(Z)
```

. $P(Z>|-1.43|) = 0.153$.  The advantage of the one-sample proportion test is that it also calculates a $1 - \alpha)\%$ CI.
```{r}
alpha <- .05
prop.test(x = observed[1], 
          n = n, 
          p = expected[1] / n, 
          alternative = "two.sided", 
          conf.level = 1 - alpha, 
          correct = FALSE)

```

When $R = 2$, especially when entries are "small", the $\chi^2$ distribution is not continuous.  Adjust for this by applying the Yates continuity correction.  The Yates continuity correction adds/subtracts 0.5 to the observed counts to bring their fractional limit closer to the expected values.  
```{r}
observed2 <- c(53+.5, 47-.5)
(chisq <- sum((observed2 - expected)^2 / expected))
(p.value <- pchisq(q = chisq, df = r - 1, lower.tail = FALSE))
```

In `prop.test` and `chisq.test`, specify `correct = TRUE` to apply the Yates continuity correction.
```{r}
prop.test(x = observed[1], 
          n = n, 
          p = expected[1] / n, 
          alternative = "two.sided", 
          conf.level = 1 - alpha, 
          correct = TRUE)

chisq.test(x = observed, 
           p = expected / n,
           correct = TRUE)
```

```{r warning=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)

lrr = -Inf
urr = qchisq(p = alpha, df = r - 1, lower.tail = FALSE)
p_value <- chisq.test.result$p.value
data.frame(chi2 = 100:1000 / 100) %>%
  mutate(density = dchisq(x = chi2, df = r - 1)) %>%
  mutate(rr = ifelse(chi2 < lrr | chi2 > urr, density, 0)) %>%
ggplot() +
  geom_line(aes(x = chi2, y = density)) +
  geom_area(aes(x = chi2, y = rr, fill = "red"), alpha = 0.3) +
#  geom_vline(aes(xintercept = pi_0), color = "black") +
  geom_vline(aes(xintercept = chisq), color = "red") +
  labs(title = bquote("Chi-Squared Goodness-of-Fit Test"),
       subtitle = bquote("Chisq ="~.(round(chisq,2))~", n ="~.(n)~", alpha ="~.(alpha)~", chisq_crit ="~.(round(urr,2))~", p-value ="~.(round(p_value,3))),
       x = "chisq",
       y = "Density") +
  theme(legend.position="none")
```




The **one-sample proportion test** tests whether observed frequency counts $O_j$ of the $j \in (0, 1)$ levels of a categorical variable differ from expected frequency counts $E_j$. $H_0$ is $O_j = E_j$.  



Define a $(1 - \alpha)\%$ confidence interval as $\hat{d} \pm z_{\alpha {/} 2} se$  where $z_{\alpha {/} 2} se = \epsilon$ is the margin of error. $se = \sqrt{\frac{p_X (1 - p_X)}{n_X} + \frac{p_Y (1 - p_Y)}{n_Y}} =  \sqrt{\frac{.752 (1 - .752)}{125} + \frac{.646 (1 - .646)}{175}} = .0529$ and $z_{\alpha {/} 2} = 1.96$.  

```{r}
(se <- sqrt(p_x * (1 - p_x) / n_x + p_y * (1 - p_y) / n_y))
(z_crit <- qnorm(p = alpha / 2, mean = 0, sd = 1, lower.tail = FALSE))

lcl <- d_hat - z_crit * se
ucl <- d_hat + z_crit * se
cat("95% CI: ", round(d_hat, 4), "+/-", round(se * z_crit, 4), "= (", round(lcl, 4), ", ", round(ucl, 4), ")")

```

R function `prop.test` can calculate a confidence interval around a proportion.  In this case, use a two-tail distribution.
```{r}
(prop.test.result <- prop.test(x = c(x, y), 
                               n = c(n_x, n_y), 
                               conf.level = 1 - alpha, 
                               correct = FALSE))
```

Graph the 95% CI.
```{r}
lcl <- round(prop.test.result$conf.int[1], 3)
ucl <- round(prop.test.result$conf.int[2], 3)
data.frame(d = -300:300 / 1000) %>%
  mutate(density = dnorm(x = d, mean = d_hat, sd = se)) %>%
  mutate(rr = ifelse(d < lcl | d > ucl, density, 0)) %>%
ggplot() +
  geom_line(aes(x = d, y = density)) +
  geom_area(aes(x = d, y = rr), fill = "red", alpha = 0.3) +
  geom_vline(aes(xintercept = d_hat), color = "blue") +
  labs(title = bquote("Difference in Proportions Confidence Interval"),
       subtitle = bquote('p_X ='~.(round(p_x,3))~'p_Y ='~.(round(p_y,3))),
       x = "d",
       y = "Density") +
  theme(legend.position="none")

```


## Two-Way Tables

These notes rely on PSU STATS 504 [course notes](https://online.stat.psu.edu/stat504/node/69/).

A two-way frequency table is a frequency table for *two* categorical variables.  You usually construct a two-way table to test whether the frequency counts in one categorical variable differ from the other categorical variable using the chi-square independence test.  If there is a significant difference, then you characterize the effect sizes with measures of association (difference in proportions, relative risk, or odds ratio). 

Here is an example.  A double blind study investigated the therapeutic value of vitamin C $(trt \in [Placebo, VitaminC])$ for treating common colds $(result \in [Cold, No Cold])$ on a sample of $n = 279$ persons. There are two discrete variables each with two levels, hence a two way table.

```{r}
vitc_dat <- matrix(
  c(31, 17, 109, 122), 
  ncol = 2, 
  dimnames = list(
    treat = c("Placebo", "VitaminC"), 
    resp = c("Cold", "NoCold")
  )
)
print(vitc_dat)
prop.table(vitc_dat)
prop.table(vitc_dat, margin = 1)  # row pct
prop.table(vitc_dat, margin = 2)  # col pct
```

Do the observed frequency counts differ from the expected frequency counts under the independence model?  (The *independence model* is a subset of the *saturated model* where the two explanatory variables are independent.)


### Chi-Square Independence Test

The **chi-square independence test** tests whether observed joint frequency counts $O_{ij}$ of the $i \in (0, 1, \cdots I)$ levels of categorical variable $Y$ and the $j \in (0, 1, \cdots J)$ levels of categorical variable $Z$ differ from expected frequency counts $E_{ij}$ under the *independence model* where $\pi_{ij} = \pi_{i+} \pi_{+j}$. $H_0$ is $O_{ij} = E_{ij}$.

There are two test statistics for this test, Pearson $\chi^2$ and deviance $G^2$.  The sampling distribution of each test statistic approach the chi-square distribution as $n$ becomes large.  It's a good idea to calculate both test statistics.

The Pearson goodness-of-fit statistic is

$$\chi^2 = \sum_{i,j} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$$

where $O_{ij}$ is the observed count, and $E_{ij}$ equals the marginal probability of row $i$ multiplied by the marginal probability of column $j$.  

```{r}
E <- sum(vitc_dat) * prop.table(vitc_dat, 1) * prop.table(vitc_dat, 2)
X2 <- sum((vitc_dat - E)^2 / E)
print(X2)
```

The deviance statistic is 

$$G^2 = 2 \sum_{ij} O_{ij} \log \left( \frac{O_{ij}}{E_{ij}} \right)$$

```{r}
G2 <- - 2 * sum(vitc_dat * log(vitc_dat / E))
print(G2)
```

If the saturated model proportions $p_{ij}$ equal equal the independence model proportions $\pi_{ij}$, $\chi^2$ and $G^2$ will equal zero.  Large values indicate the data do not agree well with the proposed model.

As $n \rightarrow \infty$, $G^2$ and $\chi^2$ approach $\chi_{k-1}^2$ where $k$ is the number of n-way table cells.  You can therefore run a chi-square test of significance using the $G^2$ and $\chi^2$ test statistics.  The degrees of freedom for $E^2$ and $G^2$ equals the degrees of freedom for the saturated model, $I \times J - 1$, minus the degrees of freedom from the independence model, $(I - 1) + (J - 1)$, which you can algebraically solve for $(I - 1)(J - 1)$.

```{r}
dof <- (nrow(vitc_dat) - 1) * (ncol(vitc_dat) - 1)
print(dof)
```

The associated p-values are
```{r}
pchisq(q = G2, df = dof, lower.tail = FALSE)
pchisq(q = X2, df = dof, lower.tail = FALSE)
```

The `chisq.test()` function applies the Yates's continuity correcton by default to correct for situations with small cell counts by subtracting 0.5 from the observed - expected differences.

```{r}
chisq.test(vitc_dat, correct = FALSE)
```

The correction yields more conservative p-values.

```{r}
chisq.test(vitc_dat)
```

The p-values indicate strong evidence for rejecting the independence model.


### Difference in Proportions

The difference in proportions measure is the difference in the relative frequency of a characteristic $Z$ between two groups $Y = 1$ and $Y = 2$: $\delta = \pi_{1|1} - \pi_{1|2}$.  In social sciences and epidemiology $\pi_{1|1}$ and $\pi_{1|2}$ are sometimes referred to as "risk" values. The point estimate for $\delta$ is $d = p_{1|1} - p_{1|2}$.  

Under the normal approximation method, the sampling distribution of the difference in population proportions has a normal distribution centered at $d$ with variance $Var(\delta)$. The point estimate for $Var(\delta)$ is $Var(d)$.

$$Var(d) = \frac{p_{1|1} (1 - p_{1|1})}{n_{1+}} + \frac{p_{1|2} (1 - p_{1|2})}{n_{2+}}$$

In the vitamin C acid example, $\delta$ is the difference in the row conditional frequencies.

```{r}
p <- prop.table(vitc_dat, margin = 1)
d <- p[2, 1] - p[1, 1]
print(d)
```

The variance is

```{r}
var_d <- (p[2, 1])*(1 - p[2, 1]) / sum(vitc_dat[2, ]) +
  (p[1, 1])*(1 - p[1, 1]) / sum(vitc_dat[1, ])
```

The 95% CI is

```{r}
d + c(-1, 1) * qnorm(.975) * sqrt(var_d)
```

This is how `prop.test()` without the continuity correction calculates the confidence interval.

```{r}
(prop.test.result <- prop.test(vitc_dat, correct = FALSE))
```

```{r fig.height=3.5}
lcl <- -round(prop.test.result$conf.int[2], 3)
ucl <- -round(prop.test.result$conf.int[1], 3)
data.frame(d_i = -300:300 / 1000) %>%
  mutate(density = dnorm(x = d_i, mean = d, sd = sqrt(var_d))) %>%
  mutate(rr = ifelse(d_i < lcl | d_i > ucl, density, 0)) %>%
ggplot() +
  geom_line(aes(x = d_i, y = density)) +
  geom_area(aes(x = d_i, y = rr, fill = mf_pal(12)(12)[2]), alpha = 0.8) +
  geom_vline(aes(xintercept = d), color = "blue") +
  theme_mf() +
  labs(title = bquote("Difference in Proportions Confidence Interval"),
       subtitle = paste0(
         "d = ", round(d, 3) 
        ),
       x = "d",
       y = "Density") +
  theme(legend.position="none")

```


The normal approximation method applies when the central limit theorem conditions hold:

* the sample is independently drawn (random sampling without replacement from $n < 10\%$ of the population in observational studies, or random assignment in experiments), 
* there are at least $n_i p_i >= 5$ successes and $n_i (1 - p_i) >= 5$ failures for each group, 
* the sample sizes are both $>=30$, and 
* the probability of success for each group is not extreme, $(0.2, 0.8)$.   

Test $H_0: d = \delta_0$ for some hypothesized population $\delta$ (usually 0) with test statistic 

$$Z = \frac{d - \delta_0}{se_{d}}$$ 

where 

$$se_{d} = \sqrt{p (1 - p) \left( \frac{1}{n_{1+}} + \frac{1}{n_{2+}} \right)}$$ 

approximates $se_{\delta_0}$ where $p$ is the pooled proportion 

$$p = \frac{n_{11} + n_{21}}{n_{1+} + n_{2+}}.$$

```{r}
p_pool <- (vitc_dat[1, 1] + vitc_dat[2, 1]) / sum(vitc_dat)
se_d <- sqrt(p_pool * (1 - p_pool) * (1 / sum(vitc_dat[1, ]) + 1 / sum(vitc_dat[2, ])))
z <- (d - 0) / se_d
pnorm(z) * 2
```

```{r message=FALSE, warning=FALSE, fig.height=3.5}
lrr = qnorm(p = .05/2, mean = 0, sd = se_d, lower.tail = TRUE)
urr = qnorm(p = .05/2, mean = 0, sd = se_d, lower.tail = FALSE)
data.frame(d_i = -300:300 / 1000) %>%
  mutate(density = dnorm(x = d_i, mean = 0, sd = se_d)) %>%
  mutate(rr = ifelse(d_i < lrr | d_i > urr, density, 0)) %>%
ggplot() +
  geom_line(aes(x = d_i, y = density)) +
  geom_area(aes(x = d_i, y = rr, fill = mf_pal(12)(12)[2]), alpha = 0.8) +
  geom_vline(aes(xintercept = d), color = "blue") +
  geom_vline(aes(xintercept = 0), color = "black") +
  theme_mf() +
  labs(title = "Hypothesis Test of Difference in Proportions",
       subtitle = paste0(
         "d = ", round(d, 3), 
         " (Z = ", round(z, 2), 
         ", p = ", round(pnorm(z) * 2, 4), ")."
        ),
       x = "d",
       y = "Density") +
  theme(legend.position="none") 
```


The null hypothesis $H_0: \delta_0 = 0$ is equivalent to saying that two variables are independent, $\pi_{1|1} = \pi_{1|2}$, so you can also use the $\chi^2$ or $G^2$ test for independence in a 2 × 2.  That's what `prop.test()` is doing.  The square of the z-statistic is algebraically equal to $\chi^2$.  The two-sided test comparing $Z$ to a $N(0, 1)$ is identical to comparing $\chi^2$ to a chi-square distribution with df = 1.  Compare the $Z^2$ to the output from `prop.test()`.

```{r}
z^2
```

Even though difference of two proportions is very easy to interpret, one problem with using δ is that when Z = 1 is a rare event, the individual probabilities P(Z = 1 | Y = 1) and P(Z = 1 | Y = 2) are both small, i.e., close to zero. Absolute value of δ will be close to zero even when the effect is strong. In the following sections we study two other  common measures of association which compare the relative value of the proportions, rather than the absolute values.

One problem with evaluating $\delta$ is that when $Z = 1$ is a rare event, the individual probabilities $\pi_{1|1}$ and $\pi_{1|2}$ are both close to zero, and so $\delta$ is also close to zero even when the effect is strong.

More notes online at [PSU STAT 500](https://newonlinecourses.science.psu.edu/stat500/node/55/), [PSU STAT 504](https://online.stat.psu.edu/stat504/node/77/), and [Stat Trek](https://www.stattrek.com/estimation/difference-in-proportions.aspx?Tutorial=AP).]


### Relative Risk

The difference in proportions measure is the ratio of the probabilities of characteristic $Z$ conditioned on two groups $Y = 1$ and $Y = 2$: $\rho = \pi_{1|1} / \pi_{1|2}$.  In social sciences and epidemiology $\rho$ is sometimes referred to as the "relative risk". The point estimate for $\rho$ is $r = p_{1|1} / p_{1|2}$.  

Because $\rho$ is non-negative, a normal approximation for $\log \rho$ has a less skewed distribution than $\rho$. The approximate variance of $\log \rho$ is

$$Var(\log \rho) = \frac{1 - \pi_{11}/\pi_{1+}}{n_{1+}\pi_{11}/\pi_{1+}} + \frac{1 - \pi_{21}/\pi_{2+}}{n_{2+}\pi_{21}/\pi_{2+}}$$

which is estimated by 

$$Var(\log r) = \left( \frac{1}{n_{11}} - \frac{1}{n_{1+}} \right) + \left( \frac{1}{n_{21}} - \frac{1}{n_{2+}} \right)$$

In the vitamin C acid example, $r$ is the ratio of the row conditional frequencies.

```{r}
p <- prop.table(vitc_dat, margin = 1)
r <- p[2, 1] / p[1, 1]
print(r)
```

The variance is

```{r}
var_r <- 1 / vitc_dat[1, 1] - 1 / sum(vitc_dat[1, ]) + 
  1 / vitc_dat[2, 1] - 1 / sum(vitc_dat[2, ])
```

The 95% CI is

```{r}
r + c(-1, 1) * qnorm(.975) * sqrt(var_r)
```


The relative risk of catching a cold while taking vitamin c compared to not taking vitamin c is

```{r}

```


### Odds Ratio


