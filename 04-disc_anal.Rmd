```{r include=FALSE}
library(tidyverse)
library(mfstylr)
```

# Discrete Analysis {#discrete_anal}


## Goodness-of-Fit Test

These notes rely heavily upon PSU STATS 504 [course notes](https://online.stat.psu.edu/stat504/node/60/).

The **chi-square goodness-of-fit test**, tests whether observed frequency counts $O_j$ of the $j \in (0, 1, \cdots k)$ levels of a categorical variable differ from expected frequency counts $E_j$. $H_0$ is $O_j = E_j$.

There are two test statistics for this test, Pearson $X^2$ and deviance $G^2$.  The sampling distribution of each test statistic approach the chi-squared distribution as $n$ becomes large.  It's a good idea to calculate both test statistics.

The Pearson goodness-of-fit statistic is

$$X^2 = \sum \frac{(O_j - E_j)^2}{E_j}$$

where $O_j = p_j n$ and $E_j = \pi_j n$.  The deviance statistic is 

$$G^2 = 2 \sum O_j \log \left( \frac{O_j}{E_j} \right)$$

If the saturated model (the observed data represent the fit of the saturated model, the most complex model possible with the data) proportions $p_j$ equal equal the expected proportions $\pi_j$, $X^2$ and $G^2$ will equal zero.  Large values indicate the data do not agree well with the proposed model.

As $n \rightarrow \infty$, $G^2$ and $X^2$ approach $\chi_{k-1}^2$ where $k$ is the number of n-way table cells.  You can therefore run a chi-square test of significance using the $G^2$ and $X^2$ test statistics.  The chi-square test provides reliable results when at least 80% of $E_j >= 5$.  


### One-Way Tables

A one-way frequency table is a frequency table for a single categorical table.

Here is a simple chi-square test example.  A researcher crosses tall cut-leaf tomatoes with dwarf potato-leaf tomatoes, then classifies the ($n = 1,611$) offspring phenotypes.

```{r}
o <- c(926, 288, 293, 104)
cell_names <- c("tall cut-leaf", "tall potato-leaf", "dwarf cut-leaf", "dwarf potato-leaf")
names(o) <- cell_names
print(o)
```

The four phenotypes are expected to occur with relative frequencies 9:3:3:1.

```{r}
pi <- c(9, 3, 3, 1) / (9 + 3 + 3 + 1)
e <- sum(o) * pi
names(e) <- cell_names
print(e)
```

```{r fig.width=6, fig.height=3.5}
data.frame(o, e) %>%
  rownames_to_column() %>%
  pivot_longer(cols = -rowname) %>%
  ggplot(aes(x = as.factor(rowname), y = value, fill = name)) +
  geom_col(position = position_dodge()) +
  geom_text(aes(label = round(value, 0)), position = position_dodge(width = 0.9)) +
  theme_mf() +
  scale_fill_mf() +
  labs(title = "Observed vs Expected", fill = "", x = "", y = "")
```

```{r echo=FALSE}
x2 <- sum((o - e)^2 / e)
g2 <- 2 * sum(o * log(o / e))
```

The $X^2$ statistic is `x2 <- sum((o - e)^2 / e) = ` `r  x2` and the $G^2$ statistic is `g2 <- 2 * sum(o * log(o / e)) = ` `r g2`, so nearly identical. The chi-sq test p-values are also nearly identical.

```{r}
pchisq(q = x2, df = length(o) - 1, lower.tail = FALSE)
pchisq(q = g2, df = length(o) - 1, lower.tail = FALSE)
```

You can also perform the chi-square test of the Pearson test statistic with `chisq.test()`.

```{r}
chisq.test(o, p = pi)
```

The p-values based on the $\chi^2$ distribution with 3 d.f. are about 0.69, so we fail to reject the null hypothesis - the data are consistent with the genetic theory. The $\chi^2$ value is well outside the $\alpha = 0.05$ level range of regression.

```{r warning=FALSE, message=FALSE, fig.height=4}
alpha <- 0.05
dof <- length(e) - 1
lrr = -Inf
p_val <- pchisq(q = x2, df = length(o) - 1, lower.tail = FALSE)
urr = qchisq(p = alpha, df = dof, lower.tail = FALSE)
 data.frame(chi2 = seq(from = 0, to = 20, by = .1)) %>%
   mutate(density = dchisq(x = chi2, df = dof)) %>%
   mutate(rr = ifelse(chi2 < lrr | chi2 > urr, density, 0)) %>%
 ggplot() +
   geom_line(aes(x = chi2, y = density), color = mf_pal(12)(12)[12], size = 0.8) +
   geom_area(aes(x = chi2, y = rr), fill = mf_pal(12)(12)[2], alpha = 0.8) +
   geom_vline(aes(xintercept = x2), color = mf_pal(12)(12)[11], size = 0.8) +
   labs(title = bquote("Chi-Squared Goodness-of-Fit Test"),
        subtitle = paste0("Chisq=", round(x2,2), ", ",
                          "Critical value=", round(urr,2), ", ",
                          "p-value=", round(p_val,3), "."
                          ),
        x = "chisq",
        y = "Density") +
   theme(legend.position="none") +
   theme_mf()
```

Inspect the residuals to learn which differences may lead to rejecting the null hypothesis.  The Pearson and deviance statistics are sums of individual cell comparisons which can be termed the squared "residuals", $X^2 = \sum r_j^2$ and $G^2 = \sum G_j^2$.  The expected value of a $\chi^2$ random variable is its d.f., $k-1$, and the typical size of any $\chi^2$ value is be $(k - 1) / k$, so for the typical residual should be within 2 $\sqrt{(k - 1) / k}$.

```{r}
e2_res <- sqrt((o - e)^2 / e)
g2_res <- sign(o - e) * sqrt(abs(2 * o * log(o / e)))
```

```{r}
data.frame(e2_res, g2_res, obs = 1:length(e2_res)) %>%
  rownames_to_column() %>%
  pivot_longer(cols = e2_res:g2_res) %>%
  ggplot(aes(x = obs, y = value, color = name)) +
  geom_point() +
  scale_color_mf()
```


Here is a summary of the analysis.

```{r}
data.frame(o, e, e2_res, g2_res)
```

If you do not have a theoretical value, but instead want to test whether the data conform to a distribution, the test is nearly the same.  Your first step is the estimate the distribution parameter(s).  Then you can perform the goodness of fit test, but with degrees of freedom reduced for each estimated parameter.

For example, suppose sample $n = 100$ families and count the number of children, and you want to test whether the counts follow a Poisson distribution, $X \sim  Pois(\lambda)$.  

```{r}
dat <- data.frame(children = 0:5, cnt = c(19, 26, 29, 13, 10, 3))
print(dat)
```

The ML estimate for $\lambda$ is

$$\hat{\lambda} = \frac{children_0 cnt_0 + childred_1 cnt_1, + \cdots children_5 n_5}{cnt}$$

```{r}
lambda_hat <- sum(dat$children * dat$cnt) / sum(dat$cnt)
```


Then the expected values are 

$$f(x; \lambda) = \frac{e^{-\hat{\lambda}} \hat{\lambda}^x}{x!}.$$

```{r}
e = e^(-lambda_hat) * lambda_hat^o / factorial(o)
```


$\chi^2$ test