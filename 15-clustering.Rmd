# Cluster Analysis

These notes are primarily taken from studying DataCamp courses [Cluster Analysis in R](https://www.datacamp.com/courses/cluster-analysis-in-r) and [Unsupervised Learning in R](https://campus.datacamp.com/courses/unsupervised-learning-in-r),  [AIHR](https://www.analyticsinhr.com/blog/people-analytics-r-clustering), UC Business Analytics R Programming Guide for [K-Means](https://uc-r.github.io/kmeans_clustering) and [HCA](https://uc-r.github.io/hc_clustering), and [PSU STAT-505](https://online.stat.psu.edu/stat505). 

Cluster analysis is a data mining tool for dividing features into clusters, distinct populations with no a priori defining characteristics. The goal is to describe those populations with the observed data. Popular uses of clustering are audience segmentation, creating personas, detecting anomalies, and pattern recognition in images.

There are two common approaches to cluster analysis.

* Agglomerative **hierarchical** algorithms start by defining each data point as a cluster, then repeatedly combine the two closest clusters into a new cluster until all data points are merged into a single cluster.

* **Non-hierarchical** methods such as K-means initially randomly partitions the data into a set of K clusters, then iteratively moves observations into different clusters until there is no sensible reassignment possible.

#### Setup {-}

I will learn by example, using the [IBM HR Analytics Employee Attrition & Performance](https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset) data set from Kaggle to discover which factors are associated with employee turnover and whether distinct clusters of employees are more susceptible to turnover. The clusters can help personalize [employee experience](https://www.digitalhrtech.com/employee-experience-guide/?_ga=2.197559791.13240713.1594804554-1792658053.1594804554) (AIHR). This data set includes 1,470 employee records consisting of the `EmployeeNumber`, a flag for `Attrition` during some time frame, and 32 other descriptive variables.

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(plotly)            # interactive graphing
library(cluster)           # daisy and pam
library(Rtsne)             # dimensionality reduction and visualization
library(dendextend)        # color_branches

set.seed(1234)  # reproducibility

dat <- read_csv("./input/WA_Fn-UseC_-HR-Employee-Attrition.csv")
dat <- dat %>%
  mutate_if(is.character, as_factor) %>%
  mutate(
    EnvironmentSatisfaction = factor(EnvironmentSatisfaction, ordered = TRUE),
    StockOptionLevel = factor(StockOptionLevel, ordered = TRUE),
    JobLevel = factor(JobLevel, ordered = TRUE),
    JobInvolvement = factor(JobInvolvement, ordered = TRUE)
  ) %>%
  select(EmployeeNumber, Attrition, everything())
my_skim <- skimr::skim_with(numeric = skimr::sfl(p25 = NULL, p50 = NULL, p75 = NULL, hist = NULL))
my_skim(dat)
```

You would normally start a cluster analysis with an exploration of the data to determine which variables are interesting and relevant to your goal. I'll bypass that rigor and just use a binary correlation analysis. Binary correlation analysis converts features into binary format by binning the continuous features and one-hot encoding the binary features. `correlate()` calculates the correlation coefficient for each binary feature to the response variable. A Correlation Funnel is an tornado plot that lists the highest correlation features (based on absolute magnitude) at the top of the and the lowest correlation features at the bottom. Read more on the [correlationfunel](https://github.com/business-science/correlationfunnel) GitHub README.

Using binary correlation, I'll include just the variables with a correlation coefficient of at least 0.10. For our employee attrition data set, `OverTime` (Y|N) has the largest correlation, `JobLevel = 1`, `MonthlyIncome <= 2,695.80`, etc.

```{r warning=FALSE, fig.height=8, fig.width=8}
dat %>%
  select(-EmployeeNumber) %>%
  correlationfunnel::binarize(n_bins = 5, thresh_infreq = 0.01) %>%
  correlationfunnel::correlate(Attrition__Yes) %>%
  correlationfunnel::plot_correlation_funnel(interactive = FALSE) %>%
  ggplotly()  # Makes prettier, but drops the labels
```

Using the cutoff of 0.1 leaves 14 features for the analysis. 

```{r}
vars <- c(
  "EmployeeNumber", "Attrition", 
  "OverTime", "JobLevel", "MonthlyIncome", "YearsAtCompany", "StockOptionLevel",
  "YearsWithCurrManager", "TotalWorkingYears", "MaritalStatus", "Age", 
  "YearsInCurrentRole", "JobRole", "EnvironmentSatisfaction", "JobInvolvement", 
  "BusinessTravel"
)

dat_2 <- dat %>% select(one_of(vars))
```


#### Data Preparation {-}

The concept of distance is central to clustering. Two observations are similar if the distance between their features is relatively small. To compare feature distances, they should be on a similar scale. There are many ways to define distance (see options in `?dist`), but the two most common are Euclidean, $d = \sqrt{\sum{(x_i - y_i)^2}}$, and binary, 1 minus the proportion of shared features ([Wikipedia](https://en.wikipedia.org/wiki/Jaccard_index), [PSU-505](https://online.stat.psu.edu/stat505/lesson/14/14.3)). If you have a mix a feature types, use the *Gower* distance ([Analytics Vidhya](https://medium.com/analytics-vidhya/concept-of-gowers-distance-and-it-s-application-using-python-b08cf6139ac2)) range-normalizes the quantitative variables, one-hot encodes the nominal variables, and ranks the ordinal variables. Then it calculates distances using the Manhattan distance for quantitative and ordinal variables, and the Dice coefficient for nominal variables. 

Gower's distance is computationally expensive, so you could one-hot encode the data and standardize the variables as $(x - \bar{x}) / sd(x)$ so that each feature has a mean of 0 and standard deviation of 1, like this:

```{r eval=FALSE}
# cannot one-hot encode ordered factors, so change to unordered
x <- dat_2 %>% mutate_if(is.ordered, ~factor(., ordered = FALSE))
dat_2_mtrx <- mltools::one_hot(data.table::as.data.table(x[, 2:16])) %>% 
  as.matrix() 
row.names(dat_2_mtrx) <- dat_2$EmployeeNumber
dat_2_mtrx <- na.omit(dat_2_mtrx)
dat_2_mtrx <- scale(dat_2_mtrx)
dat_2_dist <- dist(dat_2_mtrx)
```

But normally you would go ahead and calculate Gower's distance using `daisy()`.

```{r}
dat_2_gwr <- cluster::daisy(dat_2[, 2:16], metric = "gower")
```

As a sanity check, let's see the most similar and dissimilar pairs of employees according to their Gower distance. Here are the most similar employees.

```{r}
x <- as.matrix(dat_2_gwr)
dat_2[which(x == min(x[x != 0]), arr.ind = TRUE)[1, ], ] %>% 
  t() %>% as.data.frame() %>% rownames_to_column() %>% 
  flextable::flextable() %>% flextable::autofit()
```

They are identical except for `MonthlyIncome`. Here are the most dissimilar employees.

```{r}
dat_2[which(x == max(x), arr.ind = TRUE)[1, ], ] %>% 
  t() %>% as.data.frame() %>% rownames_to_column() %>% 
  flextable::flextable() %>% flextable::autofit()
```

These two employees have nothing in common. With the data preparation complete, we can finally perform our cluster analysis. I'll try K-means and HCA.


## K-Means

The k-means clustering algorithm randomly assigns all observations to one of $k$ clusters. K-means then iteratively calculates the cluster centroids and reassigns the observations to their nearest centroid.  The centroid is the mean of the points in the cluster (Hence the name "k-means"). The iterations continue until either the centroids stabilize or the iterations reach a set maximum, `iter.max` (typically 50). The result is `k` clusters with the minimum total intra-cluster variation.

The centroid of cluster $c_i \in C$ is the mean of the cluster observations $S_i$: $c_i = \frac{1}{|S_i|} \sum_{x_i \in S_i}{x_i}$.  The nearest centroid is the minimum squared euclidean distance, $\underset{c_i \in C}{\operatorname{arg min}} dist(c_i, x)^2$. A more robust version of k-means is partitioning around medoids (pam), which minimizes the sum of dissimilarities instead of a sum of squared euclidean distances. That's what I'll use.

The algorithm will converge to a result, but the result may only be a local optimum. Other random starting centroids may yield a different local optimum. Common practice is to run the k-means algorithm `nstart` times and select the lowest within-cluster sum of squared distances among the cluster members.  A typical number of runs is `nstart = 20`.

#### Choosing K {-}

What is the best number of clusters? You may have a preference in advance, but more likely you will use a *scree plot* or use the *silhouette method*. The scree plot is a plot of the total within-cluster sum of squared distances as a function of *k*. The sum of squares always decreases as *k* increases, but at a declining rate. The optimal *k* is at the "elbow" in the curve - the point at which the curve flattens. `kmeans()` returns an object of class `kmeans`, a list in which one of the components is the model sum of squares `tot.withinss`.  In the scree plot below, the elbow may be *k = 5*.

```{r}
wss <- map_dbl(2:10, ~ kmeans(dat_2_gwr, centers = .x)$tot.withinss)
wss_tbl <- tibble(k = 2:10, wss)
ggplot(wss_tbl, aes(x = k, y = wss)) +
  geom_point(size = 2) +
  geom_line() +
  scale_x_continuous(breaks = 2:10) +
  labs(title = "Scree Plot")
```

The silhouette method calculates the within-cluster distance $C(i)$ for each observation, and its distance to the nearest cluster $N(i)$. The silhouette width is $S = 1 - C(i) / N(i)$ for $C(i) < N(i)$ and $S = N(i) / C(i) - 1$ for $C(i) > N(i)$. A value close to 1 means the observation is well-matched to its current cluster; A value near 0 means the observation is on the border between the two clusters; and a value near -1 means the observation is better-matched to the other cluster.  The optimal number of clusters is the number that maximizes the total silhouette width. `cluster::pam()` returns a list in which one of the components is the average width `silinfo$avg.width`. In the silhouette plot below, the maximum width is at *k = 6*.

```{r warning=FALSE, message=FALSE}
sil <- map_dbl(2:10, ~ pam(dat_2_gwr, k = .x)$silinfo$avg.width)
sil_tbl <- tibble(k = 2:10, sil)
ggplot(sil_tbl, aes(x = k, y = sil)) +
  geom_point(size = 2) +
  geom_line() +
  scale_x_continuous(breaks = 2:10) +
  labs(title = "Silhouette Plot")
```

#### Summarize Results {-}

Run `pam()` again and attach the results to the original table for visualization and summary statistics.  

```{r}
mdl <- pam(dat_2_gwr, k = 6)
dat_3 <- dat_2 %>% mutate(cluster = as.factor(mdl$clustering))
```

Here are the six medoids from our data set.

```{r}
dat_2[mdl$medoids, ] %>%
  t() %>% as.data.frame() %>% rownames_to_column() %>% 
  flextable::flextable() %>% flextable::autofit()
```

We're most concerned about attrition. Do high-attrition employees fall into a particular cluster? Yes! 79.7% of cluster 3 left the company - that's 59.5% of all turnover in the company.

```{r}
dat_3_smry <- dat_3 %>%
  count(cluster, Attrition) %>%
  group_by(cluster) %>%
  mutate(cluster_n = sum(n),
         turnover_rate = scales::percent(n / sum(n), accuracy = 0.1)) %>%
  ungroup() %>%
  filter(Attrition == "Yes") %>%
  mutate(pct_of_turnover = scales::percent(n / sum(n), accuracy = 0.1)) %>%
  select(cluster, cluster_n, turnover_n = n, turnover_rate, pct_of_turnover)
dat_3_smry %>% flextable::flextable() %>% flextable::autofit()
```

You can get some sense of the quality of clustering by constructing the Barnes-Hut t-Distributed Stochastic Neighbor Embedding (t-SNE).

```{r warning=FALSE}
dat_4 <- dat_3 %>%
  left_join(dat_3_smry, by = "cluster") %>%
  rename(Cluster = cluster) %>%
  mutate(
    MonthlyIncome = MonthlyIncome %>% scales::dollar(),
    description = str_glue("Turnover = {Attrition}
                                  MaritalDesc = {MaritalStatus}
                                  Age = {Age}
                                  Job Role = {JobRole}
                                  Job Level {JobLevel}
                                  Overtime = {OverTime}
                                  Current Role Tenure = {YearsInCurrentRole}
                                  Professional Tenure = {TotalWorkingYears}
                                  Monthly Income = {MonthlyIncome}
                                 
                                  Cluster: {Cluster}
                                  Cluster Size: {cluster_n}
                                  Cluster Turnover Rate: {turnover_rate}
                                  Cluster Turnover Count: {turnover_n}
                                  "))

tsne_obj <- Rtsne(dat_2_gwr, is_distance = TRUE)

tsne_tbl <- tsne_obj$Y %>%
  as_tibble() %>%
  setNames(c("X", "Y")) %>%
  bind_cols(dat_4) %>%
  mutate(Cluster = as_factor(Cluster))

g <- tsne_tbl %>%
  ggplot(aes(x = X, y = Y, colour = Cluster, text = description)) +
  geom_point()

ggplotly(g)
  
```

Another common approach is to take summary statistics and draw your own conclusions. You might start by asking which attributes differ among the clusters. The box plots below show the distribution of the numeric variables. All of the numeric variable distributions appear to vary among the clusters. 

```{r}
my_boxplot <- function(y_var){
  dat_3 %>%
    ggplot(aes(x = cluster, y = !!sym(y_var))) +
    geom_boxplot() +
    geom_jitter(aes(color = Attrition), alpha = 0.2, height = 0.10) +
    theme_minimal() +
    theme(legend.position = "none") +
    labs(x = "", y = "", title = y_var)
}
vars_numeric <- dat_3 %>% select(-EmployeeNumber) %>% select_if(is.numeric) %>% colnames()
g <- map(vars_numeric, my_boxplot)
gridExtra::marrangeGrob(g, nrow=1, ncol = 2)
```

You can perform an analysis of variance to confirm. The table below collects the ANOVA results for each of the numeric variables. The results indicate that there are significant differences among clusters at the .01 level for all of the numeric variables.

```{r}
km_aov <- vars_numeric %>% 
  map(~ aov(rlang::eval_tidy(expr(!!sym(.x) ~ cluster)), data = dat_3))
km_aov %>% 
  map(anova) %>%
  map(~ data.frame(F = .x$`F value`[[1]], p = .x$`Pr(>F)`[[1]])) %>%
  bind_rows() %>%
  bind_cols(Attribute = vars_numeric) %>%
  select(Attribute, everything()) %>%
  flextable::flextable() %>%
  flextable::autofit() %>%
  flextable::colformat_num(j = 2, digits = 2) %>%
  flextable::colformat_num(j = 3, digits = 4)
```

We're particularly interested in cluster 3, the high attrition cluster, and what sets it apart from the others. Right away, we can see that clusters 3 and 4 similar in almost all of theses attributes. They tend to have the smaller incomes, company and job tenure, years with their current manager, total working experience, and age. I.e., they tend to be lower on the career ladder.

To drill into cluster differences to determine which clusters differ from others, use the Tukey HSD post hoc test with Bonferroni method applied to control the experiment-wise error rate. That is, only reject the null hypothesis of equal means among clusters if the p-value is less than $\alpha / p$, or $.05 / 6 = 0.0083$. The significantly different cluster combinations are shown in bold. Clusters 3 and 4 differ from the others on all six measures. However, there are no significant differences between c3 and c4 (highlighted green) for the numeric variables.

```{r message=FALSE}
km_hsd <- map(km_aov, TukeyHSD)

map(km_hsd, ~ .x$cluster %>% 
      data.frame() %>% 
      rownames_to_column() %>% 
      filter(str_detect(rowname, "-"))) %>% 
  map2(vars_numeric, bind_cols) %>%
  bind_rows() %>% 
  select(predictor = `...6`, everything()) %>%
  mutate(cluster_a = str_sub(rowname, start = 1, end = 1),
         cluster = paste0("c", str_sub(rowname, start = 3, end = 3))) %>%
  pivot_wider(c(predictor, cluster_a, cluster, p.adj), 
              names_from = cluster_a, values_from = p.adj,
              names_prefix = "c") %>%
  flextable::flextable() %>%
  flextable::colformat_num(j = c(3:7), digits = 4) %>%
  flextable::bold(i = ~ c2 < .05 / length(vars_numeric), j = ~ c2, bold = TRUE) %>%
  flextable::bold(i = ~ c3 < .05 / length(vars_numeric), j = ~ c3, bold = TRUE) %>%
  flextable::bold(i = ~ c4 < .05 / length(vars_numeric), j = ~ c4, bold = TRUE) %>%
  flextable::bold(i = ~ c5 < .05 / length(vars_numeric), j = ~ c5, bold = TRUE) %>%
  flextable::bold(i = ~ c6 < .05 / length(vars_numeric), j = ~ c6, bold = TRUE) %>%
  flextable::bg(i = ~ cluster == "c3", j = ~ c4, bg = "#B6E2D3") %>%
  flextable::autofit() 
```

So at this point we have kind of an incomplete picture. We know the high-attrition employees are low on the career ladder, but cluster 4 is also low on the career ladder and they are not high-attrition.

How about the factor variables? The tile plots below show that clusters 3 and 4 are lab technicians and research scientists. They are both at the lowest job level. But three factors distinguish these clusters from each other: cluster 3 is far more likely to work overtime, have no stock options, and be single.

```{r}
my_tileplot <- function(y_var){
  dat_3 %>%
    count(cluster, !!sym(y_var)) %>%
    ungroup() %>%
    group_by(cluster) %>%
    mutate(pct = n / sum(n)) %>%
    ggplot(aes(y = !!sym(y_var), x = cluster, fill = pct)) +
    geom_tile() +
    scale_fill_gradient(low = "#E9EAEC", high = "#FAD02C") +
    geom_text(aes(label = scales::percent(pct, accuracy = 1.0)), size = 3) +
    theme_minimal() + 
    theme(legend.position = "none")
}
vars_factor <- dat_3 %>% select(-cluster) %>% select_if(is.factor) %>% colnames()

g <- map(vars_factor, my_tileplot)
gridExtra::marrangeGrob(g, nrow=1, ncol = 2) 
```

You can perform a chi-squared independence test to confirm. The table below collects the Chis-Sq test results for each of the factor variables. The results indicate that there are significant differences among clusters at the .05 level for all of the factor variables except `EnvironmentSatisfaction` and `JobInvolvement`.

```{r}
km_chisq <- vars_factor %>%
  map(~ janitor::tabyl(dat_3, cluster, !!sym(.x))) %>%
  map(janitor::chisq.test)

km_chisq %>% 
  map(~ data.frame(ChiSq = .x$statistic[[1]], df = .x$parameter[[1]], p = .x$p.value[[1]])) %>%
  bind_rows() %>%
  bind_cols(Attribute = vars_factor) %>%
  select(Attribute, everything()) %>%
  flextable::flextable() %>%
  flextable::autofit() %>%
  flextable::colformat_num(j = ~ ChiSq, digits = 1) %>%
  flextable::colformat_num(j = ~ p, digits = 4)
```

We're particularly interested in cluster 3, the high attrition cluster, and what sets it apart from the others, and cluster 4 in particular. We think it is that cluster 3 is far more likely to work overtime, have no stock options, and be single, but let's perform a residuals analysis on the the chi-sq test to check. The residuals with absolute value >2 are driving the differences among the clusters.

```{r warning=FALSE}
km_chisq %>%
  map(~ data.frame(.x$residuals)) %>%
  map(data.frame) %>%
  map(t) %>%
  map(data.frame) %>%
  map(rownames_to_column, var = "Predictor") %>%
  map(~ filter(.x, Predictor != "cluster")) %>%
  map(~ select(.x, Predictor, c1 = X1, c2 = X2, c3 = X3, c4 = X4, c5 = X5, c6 = X6)) %>%
  map(~ mutate_at(.x, vars(2:7), as.numeric)) %>%
  map(flextable::flextable) %>%
  map(~ flextable::colformat_num(.x, j = ~ c1 + c2 + c3 + c4 + c5 + c6, digits = 1)) %>%
  map(~ flextable::bold(.x, i = ~ abs(c1) > 2, j = ~ c1, bold = TRUE)) %>%
  map(~ flextable::bold(.x, i = ~ abs(c2) > 2, j = ~ c2, bold = TRUE)) %>%
  map(~ flextable::bold(.x, i = ~ abs(c3) > 2, j = ~ c3, bold = TRUE)) %>%
  map(~ flextable::bold(.x, i = ~ abs(c4) > 2, j = ~ c4, bold = TRUE)) %>%
  map(~ flextable::bold(.x, i = ~ abs(c5) > 2, j = ~ c5, bold = TRUE)) %>%
  map(~ flextable::bold(.x, i = ~ abs(c6) > 2, j = ~ c6, bold = TRUE)) %>%
#  map(~ flextable::bg(i = ~ cluster == "c3", j = ~ c4, bg = "#B6E2D3") %>%
map2(vars_factor, ~ flextable::set_caption(.x, caption = .y))
```

Cluster 3 are significantly more likely to work overtime while cluster 4 (and 1) are significantly less likely. Cluster 3 (and 1) are significantly more likely to have no stock options and be single.

## HCA

Hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which builds a hierarchy of clusters (usually presented in a dendrogram). The HCA process is:

1. Calculate the distance between each observation with `dist()` or `daisy()`. We did that above when we created `dat_2_gwr`.

2. Cluster the two closest observations into a cluster with `hclust()`. Then calculate the cluster's distance to the remaining observations.  If the shortest distance is between two observations, define a second cluster, otherwise adds the observation as a new level to the cluster. The process repeats until all observations belong to a single cluster. The "distance" to a cluster can be defined as:

* complete: distance to the furthest member of the cluster,
* single: distance to the closest member of the cluster,
* average: average distance to all members of the cluster, or
* centroid: distance between the centroids of each cluster.

Complete and average distances tend to produce more balanced trees and are most common. Pruning an unbalanced tree can result in most observations assigned to one cluster and only a few observations assigned to other clusters.  This is useful for identifying outliers.

```{r}
mdl_hc <- hclust(dat_2_gwr, method = "complete")
```

3. Evaluate the `hclust` tree with a dendogram, principal component analysis (PCA), and/or summary statistics. The vertical lines in a dendogram indicate the distance between nodes and their associated cluster. Choose the number of clusters to keep by identifying a cut point that creates a reasonable number of clusters with a substantial number of observations per cluster (I know, "reasonable" and "substantial" are squishy terms). Below, cutting at height 0.65 to create 7 clusters seems good.

```{r warning=FALSE, message=FALSE}
# Inspect the tree to choose a size.
plot(color_branches(as.dendrogram(mdl_hc), k = 7))
abline(h = .65, col = "red")
```

4. "Cut" the hierarchical tree into the desired number of clusters (`k`) or height `h` with `cutree(hclust, k = NULL, h = NULL)`.  `cutree()` returns a vector of cluster memberships.  Attach this vector back to the original dataframe for visualization and summary statistics.

```{r}
dat_2_clstr_hca <- dat_2 %>% mutate(cluster = cutree(mdl_hc, k = 7))
```

5. Calculate summary statistics and draw conclusions.  Useful summary statistics are typically membership count, and feature averages (or proportions).

```{r warning=FALSE, message=FALSE}
dat_2_clstr_hca %>% 
  group_by(cluster) %>% 
  summarise_if(is.numeric, funs(mean(.)))
```

#### K-Means vs HCA

Hierarchical clustering has some advantages over k-means.  It can use any distance method - not just euclidean.  The results are stable - k-means can produce different results each time.  While they can both be evaluated with the silhouette and elbow plots, hierachical clustering can also be evaluated with a dendogram.  But hierarchical clusters has one significant drawback: it is computationally complex compared to k-means.  For this last reason, k-means is more common.


