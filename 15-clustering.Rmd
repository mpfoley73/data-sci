# Cluster Analysis

These note are primarily taken from the DataCamp courses [Cluster Analysis in R](https://www.datacamp.com/courses/cluster-analysis-in-r) and [Unsupervised Learning in R](https://campus.datacamp.com/courses/unsupervised-learning-in-r),  [AIHR](https://www.analyticsinhr.com/blog/people-analytics-r-clustering), and the [UC Business Analytics R Programming Guide](https://uc-r.github.io/kmeans_clustering). 


Unsupervised machine learning searches for structure in unlabeled data (data without a response variable).  The goal of unsupervised learning is clustering into homogenous subgroups, and dimensionality reduction.  Examples of cluster analysis are k-means clustering and hierarchical cluster analysis (HCA) (others [here](https://theappsolutions.com/blog/development/unsupervised-machine-learning/#:~:text=Unsupervised%20learning%20is%20a%20type%20of%20machine%20learning,in%20the%20dataset.%20The%20term%20%E2%80%9Cunsupervised%E2%80%9D%20refers%20to)). Clustering is used for audience segmentation, creating personas, detecting anomalies, and pattern recognition in images.

I will learn by example, using the [IBM HR Analytics Employee Attrition & Performance](https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset) data set from Kaggle to discover what factors are associated with employee turnover and whether distinct clusters of employees are more susceptible to turnover. The clusters can help personalize [employee experience](https://www.digitalhrtech.com/employee-experience-guide/?_ga=2.197559791.13240713.1594804554-1792658053.1594804554) (AIHR). This data set includes 1,470 employee records consisting of the `EmployeeNumber`, a flag for `Attrition` during some timeframe, and 32 other descriptive variables.

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(correlationfunnel) # binary correlation analysis
library(cluster)           # daisy and pam
library(Rtsne)             # dimensionality reduction and visualization
library(plotly)            # interactive graphing
library(dendextend)        # color_branches

set.seed(1234)  # reproducibility

dat <- read_csv("./input/WA_Fn-UseC_-HR-Employee-Attrition.csv")
dat <- dat %>%
  mutate_if(is.character, as_factor) %>%
  select(EmployeeNumber, Attrition, everything())
my_skim <- skimr::skim_with(numeric = skimr::sfl(p25 = NULL, p50 = NULL, p75 = NULL, hist = NULL))
my_skim(dat)
```

You would normally start a cluster analysis with an exploration of the data to determine which variables are interesting and relevant to your goal. I'll bypass that rigor with a simple correlation funnel [GitHub README](https://github.com/business-science/correlationfunnel) using binary correlation and include just the variables with a correlation coefficient of at least 0.10.

Binary correlation analysis converts features into binary format by binning the continuous features and one-hot encoding the binary features. `correlate()` calculates the correlation coefficient for each binary feature to the response variable. A Correlation Funnel is an tornado plot that lists the highest correlation features (based on absolute magnitude) at the top of the and the lowest correlation features at the bottom. For our data set, `OverTime` (Y|N) has the largest correlation, `JobLevel = 1`, `MonthlyIncome <= 2,695.80`, etc.

```{r warning=FALSE, fig.height=8, fig.width=8}
dat %>%
  select(-EmployeeNumber) %>%
  binarize(n_bins = 5, thresh_infreq = 0.01) %>%
  correlate(Attrition__Yes) %>%
  plot_correlation_funnel(interactive = FALSE) #%>%
#  ggplotly()  # Makes prettier, but drops the labels
```

Using the cutoff of 0.1, we get 14 features to use in the analysis.

```{r}
vars <- c(
  "EmployeeNumber", "Attrition", 
  "OverTime", "JobLevel", "MonthlyIncome", "YearsAtCompany", "StockOptionLevel",
  "YearsWithCurrManager", "TotalWorkingYears", "MaritalStatus", "Age", 
  "YearsInCurrentRole", "JobRole", "EnvironmentSatisfaction", "JobInvolvement", 
  "BusinessTravel"
)

dat_2 <- dat %>%
  select(one_of(vars))
```

Central to clustering is the concept of distance. Two observations are similar if the distance between their features is relatively small. There are many ways to define distance (see options in `?dist`), but the two most common are Euclidean, $d = \sqrt{\sum{(x_i - y_i)^2}}$, and binary, 1 minus the proportion of shared features ([Wikipedia](https://en.wikipedia.org/wiki/Jaccard_index)). If a data set contains both numerica and categorical data, there is a third distance measure, the Gower Distance.

When calculating a Euclidean distance, the features should be on similar scales. Standardize their values as $(x - \bar{x})) / sd(x)$ so that each feature has a mean of 0 and standard deviation of 1. Check if scaling is necessary with `colmeans()` function and `apply(df, 2, sd)`. The `scale()` function is a generic function that scales the columns of a matrix.  When calculating a binary distance, the categorical features should be binary.  Create dummy variables with `dummies::dummy.data.frame()`. The clustering algorithm may calculate the distances, but standardizing is up to you.

R function `dist()` calculates distances between observations. `daisy()` is an alternative that calculates the Gower distance after standardizing.

```{r}
gower_dist <- daisy(dat_2[, 2:16], metric = "gower")
```

As a sanity check, we can check the most similar and dissimilar pair of employees according to their Gower Distance score. Here are the most similar employees.

```{r}
gower_mat <- as.matrix(gower_dist)
dat_2[which(gower_mat == min(gower_mat[gower_mat != 0]), arr.ind = TRUE)[1, ], ] %>% glimpse()
```

And here are the most dissimilar employees.

```{r}
dat_2[which(gower_mat == max(gower_mat), arr.ind = TRUE)[1, ], ] %>% glimpse()
```


## K-Means

The K-means clustering algorithm randomly assigns all observations to one of `k` clusters. K-means then iteratively calculates the cluster centroids and reassigns the observations to their nearest centroid.  The centroid is the mean of the points in the cluster (Hence the name "k-means"). The iterations continue until either the centroid values stabilize or the iterations reach a set maximum, `iter.max` (typically 50). The result is `k` clusters with the minimum total intra-cluster variation.

The centroid of cluster $c_i \in C$ is the mean of the cluster observations $S_i$: $c_i = \frac{1}{|S_i|} \sum_{x_i \in S_i}{x_i}$.  The nearest centroid is the minimum squared Euclidean distance, $\underset{c_i \in C}{\operatorname{arg min}} dist(c_i, x)^2$.^[Euclidean distances are appropriate for quantitative variables.  What about categorical variables?  [This discussion](https://datascience.stackexchange.com/questions/22/k-means-clustering-for-mixed-numeric-and-categorical-data) at StackExchange explains that **k-modes** is suitable for categorical data.  It may be okay to convert categorical variables into binary values and treating them as numeric.]

The algorithm will converge to a result, but the result may only be a local optimum. Other random starting centroids may yield a different local optimum. Common practice is to run the k-means algorithm `nstart` times and select the lowest within-cluster sum of squared distances among the cluster members.  A typical number of runs is `nstart = 20`.

The general methodology is to 1) prepare the data, 2) choose the optimal number of clusters, 3) run the k-means algorithm, and 4) interpret the results.  A good way to learn the methodology is by example.  The `WisconsinCancer` data set contains 30 features of tumors (10 attributes with mean, se, and "worst" values) in `n = 569` patients as well as the `diagnosis` (`B` = benign, `M` = malignant). 

```{r warning=FALSE, message=FALSE}
wisc.df <- read_csv(file = "http://s3.amazonaws.com/assets.datacamp.com/production/course_1903/datasets/WisconsinCancer.csv")
glimpse(wisc.df)
```

Exclude the `diagnosis` column from the cluster analysis because it is more of a response variable than a feature.  Set it aside for reference though.  The data set contains 357 benign tumors (`diagnosis = B`) and 212 malignant tumors (`diagnosis = M`).
```{r}
table(wisc.df$diagnosis)
diagnosis <- as.numeric(wisc.df$diagnosis == "M")
```


 Step 1: Prepare Data
It is possible to perform k-means on a dataframe, but it is a little easier to extract the feature columns into a matrix.

```{r}
wisc.data <- as.matrix(wisc.df[, c(3:32)])
row.names(wisc.data) <- wisc.df$id
```

Before conducting k-means, check whether any preprocessing is required.  If there are NAs, drop the observations or impute values.  There are no `NA`s in this data set.
```{r}
wisc.data <- na.omit(wisc.data)
```

If not all of the features are of comparable scale, standardize the variables as $(x - \bar{x}) / sd(x)$ so that each feature has a mean of 0 and standard deviation of 1.  The `scale()` function scales columns of a matrix.  
```{r}
paste("Means")
summary(colMeans(wisc.data))
paste("Std Dev")
summary(apply(X = wisc.data, MARGIN = 2, FUN = sd))

wisc.data.scaled <- scale(wisc.data)
```

If any of the features multi-nomial, create binary variables.  K-means is designed for quantitative data, so it may be better to just not include categorical variables.  There are no categorical variables here.

 Step 2: Choose K
Perform k-means clustering with base function `kmeans(df, centers, nstart, iter.max)`.  `centers` is the defined number of clusters.  K-means is a random process, so it may produce a different set of clusters each time it runs.  `nstart` sets the number of times to cluster the observations.  `kmeans()` chooses the best set of clusters from the `nstart` runs.  A good value for `nstart` is 20.  It is possible that the clustering iterations do not completely stabilize.  If so, you will probably want to cap the number of iterations.  `iter.max` sets the maximum number of times to iterate through the re-assignment process.  A good value for `iter.max` is 50.

What is the right number of clusters (`centers`)?  You may have a preference in advance.  Or more likely, you need to use judgement from observing the results.  There are two common methods for choosing `centers`: constructing a **scree plot** or using the **silhouette method**.  The scree plot is a plot of the total within-cluster sum of squared distances resulting from candidate values of `centers`.  The sum of squares decreases as `k` increases, but at a declining rate.  The optimal number of clusters is at the "elbow" in the curve - the point at which the curve flattens.^[Finding the elbow is a matter of judgement.]  `kmeans()` returns an object of class `kmeans`, a list in which one of the components is the model sum of squares `tot.withinss`.  In the scree plot below, the elbow may be at `k = 2` or `k = 3`.      

```{r warning=FALSE, message=FALSE}
set.seed(1)

wss <- 0
for (k in 1:15) {
  # within sum of squares
  wss[k] <- kmeans(wisc.data.scaled, 
                   centers = k, 
                   nstart = 20, 
                   iter.max = 50)$tot.withinss
}

data.frame(k = 1:15,
           wss = wss) %>%
  ggplot(aes(x = k, y = wss)) +
  geom_line() +
  geom_point(shape = 21) +
  scale_x_continuous(breaks = 1:15) +
  labs(title = "K-means Scree Plot",
       x = "Number of Clusters", 
       y = "Within groups sum of squares")
```

The silhouette method calculates the within-cluster distance $C(i)$ for each observation, and its distance to the nearest cluster $N(i)$.  The silhouette width is $S = 1 - C(i) / N(i)$ for $C(i) < N(i)$ and $S = N(i) / C(i) - 1$ for $C(i) > N(i)$.  A value close to 1 means the observation is well-matched to its current cluster; A value near 0 means the observation is on the border between the two clusters; and a value near -1 means the observation is better-matched to the other cluster.  The optimal number of clusters is the number that maximizes the total silhouette width.  The `pam()` function from the `cluster` package returns an object of class `pam`, a list in which one of the components is the average width `silinfo$avg.width`.  In the silhoette plot below, the maximum silhoette width is at `k = 2` with `k = 3` a close second.

```{r warning=FALSE, message=FALSE}
set.seed(1)

sil_width <- 0
for (k in 2:15) {
    sil_width[k] <- pam(wisc.data.scaled, 
                   k = k)$silinfo$avg.width
}

data.frame(k = 1:15,
           sil_width = sil_width) %>%
  ggplot(aes(x = k, y = sil_width)) +
  geom_col() +
  scale_x_continuous(breaks = 1:15) +
  labs(title = "K-means Silhoette Plot",
       x = "Number of Clusters", 
       y = "Average Silhoette Width")

```

The initial cluster assignment is random, so the process may yield different results each time.  Use r function `set.seed(seed)`to create consistent reproducible results.   
```{r}
set.seed(1)
wisc.k2 <- kmeans(wisc.data.scaled, 
                  centers = 2, 
                  nstart = 20, 
                  iter.max = 50)
wisc.df$cluster <- wisc.k2$cluster

```


An intuitive way to interpret the results of k-means models is by plotting the data as a scatter plot and using color to label the samples' cluster membership.  Function `fviz_cluster()` from the `factoextra` package illustrates the clusters.  Of course, this only works if there are only two features. More likely, create summary statistics of the features grouped by cluster.
```{r}
# wisc.df %>%
#   gather(key = "Feature", 
#          value = "Value", 
#          c(HitPoints, Attack, Defense, 
#            SpecialAttack, SpecialDefense, Speed)) %>%
#   ggplot(aes(x = factor(Ability), y = Score, color = factor(cluster))) + 
#   geom_point(aes(group = Number))
```

```{r}
factoextra::fviz_cluster(wisc.k2, data = wisc.data)
```


Attach the cluster assignment vector back to the original dataframe for visualization and/or summary statistics.  Draw conclusions about the clusters by calculating summary statistics of the resulting cluster assigments, typically membership count, and feature averages (or proportions).
```{r}

# View the resulting model
knitr::kable(round(wisc.k2$size, 0),
             caption = "Cluster Size")
knitr::kable(round(wisc.k2$centers, 0),
             caption = "Cluster Centers")

```



```{r warning=FALSE, message=FALSE}
library(dplyr)
wisc.df %>%
  group_by(diagnosis, cluster) %>%
  summarise(n = n())
wisc.df %>%
  group_by(diagnosis, cluster) %>%
  summarise_all("mean")
```



## HCA

Hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which builds a hierarchy of clusters.  One usually presents the HCA results in a dendrogram.  The HCA process is:

1. Calculate the distance between each observation with `dist(df, method = c("euclidean", "binary")`.  `dist()` returns an object of class `dist`.

2. Cluster the distances with `hclust(dist, method = c("complete", "single", "average", "centroid")`.  `hclust` groups the two closest observations into a cluster.  `hclust` then calculates the cluster distance to the remaining observations.  If the shortest distance is between two observations, `hclust` defines a second cluster, otherwise `hclust` adds the observation as a new level to the cluster.  The process repeats until all observations belong to a single cluster.  The "distance" to a cluster requires definition.  The "complete" distance is the distance to the furthest member of the cluster.  The "single" distance is the distance to the closest member of the cluster.  The "average" distance is the average distance to all members of the cluster.  The "centroid" distance is the distance between the centroids of each cluster.^[As a rule of thumb, "complete" and "average" tend to produce more balanced trees and are most common.  Pruning an unbalanced tree can result in most observations assigned to one cluster and only a few observations assigned to other clusters.  This is useful for identifying outliers. `hclust()` returns a value of class `hclust`.

3. Evaluate the `hclust` tree with a dendogram, principal component analysis (PCA), and/or summary statistics. The vertical lines in a dendogram indicate the distance between nodes and their associated cluster.  Dendograms are difficult to visualize When the number of features is greater than two.^[One work-around is to plot just two dimensions at a time.]

4. "Cut" the hierarchical tree into the desired number of clusters (`k`) or height `h` with `cutree(hclust, k = NULL, h = NULL)`.  `cutree()` returns a vector of cluster memberships.  Attach this vector back to the original dataframe for visualization and summary statistics.

5. Calculate summary statistics and draw conclusions.  Useful summary statistics are typically membership count, and feature averages (or proportions).


 Example
The `pokemon` dataset contains observations of 800 pokemons^[More information on the dataset at [https://www.kaggle.com/abcsds/pokemon](https://www.kaggle.com/abcsds/pokemon)] on 6 dimensions.  The data is unlabeled, meaning there is no response variable, just features.  The features here are six pokeon ability measures.
```{r warning=FALSE, message=FALSE}
pokemon <- read_csv(url("https://assets.datacamp.com/production/course_1815/datasets/Pokemon.csv"))
pokemon$Name <- NULL
pokemon$Type1 <- NULL
pokemon$Type2 <- NULL
pokemon$Total <- NULL
pokemon$Generation <- NULL
pokemon$Legendary <- NULL

head(pokemon)
```

Before conducting k-means, check whether any preprocessing is required:  Are there any NAs?  If so, drop these observations, or impute values.  Are all of the features comparable?  If not, standardize the variables.  Are the features multi-nomial?  If so, create binary variables.  In this case, the means and standard deviations are similar, but I am scaling anyway for the exercise.

```{r warning=FALSE, message=FALSE}
# Means and SDs
colMeans(pokemon[, -c(1)])
apply(pokemon[, -c(1)], MARGIN = 2, FUN = sd)

# Scale the data
pokemon.scaled <- scale(pokemon)

# Create the full tree
hc_model <- hclust(dist(pokemon.scaled), method = "complete")

# Inspect the tree to choose a size.
plot(color_branches(as.dendrogram(hc_model), 
                    k = 7))
abline(h = 7, col = "red")
```

The dendogram suggests the optimal number of clusters is seven.  Build a cluster with `k = 7` means.  Attach the cluster assignment vector back to the original dataframe for visualization and/or summary statistics.
```{r warning=FALSE, message=FALSE}
pokemon <- mutate(pokemon, cluster = cutree(hc_model, k = 7))

# View the resulting model
pokemon %>% 
  group_by(cluster) %>% 
  summarise_all(funs(mean(.))) %>%
  select(-c(2)) %>%
  knitr::kable(caption = "Cluster Centers")

pokemon %>%
  gather(key = "Ability", 
         value = "Score", 
         c(HitPoints, Attack, Defense, 
           SpecialAttack, SpecialDefense, Speed)) %>%
  ggplot(aes(x = factor(Ability), y = Score, color = factor(cluster))) + 
  geom_point(aes(group = Number))
```

Cluster 1 has the lowest values in all features.  Cluster 5 has very high HitPoints.  Cluster 3 has very high Special Attack.

## Cluster Analysis with Time-Series Data

Cluster analysis is useful for spacial data, qualitative data, and time-series data.  With time series data, the time periods are the features.  Typically, this requires the transposition of the data set so that the dates are columns.  Otherwise, the same rules apply.

## K-Means vs HCA
Hierarchical clustering has some advantages over k-means.  It can use any distance method - not just euclidean.  The results are stable - k-means can produce different results each time.  While they can both be evaluated with the silhouette and elbow plots, hierachical clustering can also be evaluated with a dendogram.  But hierarchical clusters has one significant drawback: it is computationally complex compared to k-means.  For this last reason, k-means is more common.


