```{r include=FALSE}
library(tidyverse)
library(mfstylr)
```

# Continuous Variable Analysis {#continuous_analysis}


### Correlation

Correlation measures the strength and direction of association between two variables.  There are three common correlation tests: the Pearson product moment (Pearson's r), Spearman's rank-order (Spearman's rho), and Kendall's tau (Kendall's tau). 

Use the **Pearson's r** if both variables are quantitative (interval or ratio), normally distributed, and the relationship is linear with homoscedastic residuals.

The **Spearman's rho** and **Kendal's tao** correlations are [non-parametric](https://www.statisticshowto.datasciencecentral.com/parametric-and-non-parametric-data/) measures, so they are valid for both quantitative and ordinal variables and do not carry the normality and homoscedasticity conditions.  However, non-parametric tests have less statistical power than parametric tests, so only use these correlations if Pearson does not apply.


#### Pearson's r

Pearson's $r$ 

$$r = \frac{\sum{(X_i - \bar{X})(Y_i - \bar{Y})}}{\sqrt{\sum{(X_i - \bar{X})^2 \sum{(Y_i - \bar{Y})^2}}}} = \frac{cov(X,Y)}{s_X s_Y}$$

estimates the population correlation $\rho$.  Pearson's $r$ ranges from $-1$ (perfect negative linear relationship) to $+1$ (perfect positive linear relationship, and $r = 0$ when there is no linear relationship.  A correlation in the range $(.1, .3)$ is condidered small, $(.3, .5)$ medium, and $(.5, 1.0)$ large.

Pearson's $r$ only applies if the variables are interval or ratio, normally distributed, linearly related, there are minimal outliers, and the residuals are homoscedastic.

Test $H_0: \rho = 0$ with test statistic 

$$T = r \sqrt{\frac{n-2}{1-r^2}}.$$


The `nascard` data set consists of $n = 898$ races from 1975 - 2003.  

```{r message=FALSE, warning=FALSE, cache=TRUE}
nascard <- read.fwf(
  file = url("http://jse.amstat.org/datasets/nascard.dat.txt"),
  widths = c(5, 6, 4, 4, 4, 5, 9, 4, 11, 30),
  col.names = c('series_race', 'year', 'race_year', 'finish_pos', 'start_pos',
                'laps_comp', 'winnings', 'num_cars','car_make', 'driver')
)
nascard_sr1 <- nascard[nascard$series_race == 1,]
glimpse(nascard_sr1)
```

In race 1 of 1975 $(n = 35)$, what was the correlation between the driver's finishing position and prize?

Explore the relationship with a scatterplot.  As expected, there is a negative relationship between finish position and winnings, but it is non-linear.  However, 1/winnings may be linearly related to finish position. 

```{r message=FALSE, warning=FALSE, fig.height=3.5, fig.width=6.5}
p1 <- ggplot(data = nascard_sr1, aes(x = finish_pos, y = winnings)) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  theme_mf() +
  labs(title = "Pearson's Rho", subtitle = "")
p2 <- ggplot(data = nascard_sr1, aes(x = finish_pos, y = 1/winnings)) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  theme_mf() +
  labs(title = "Pearson's Rho", subtitle = "y = 1 / winnings")
gridExtra::grid.arrange(p1, p2, nrow = 1)
```

(*I tried several [variable tranformations](https://stattrek.com/regression/linear-transformation.aspx) and chose the one producing the highest $R^2$ that also had a normal distribution in each variable.)

```{r}
#summary(lm(winnings ~ finish_pos, data = nascard_sr1)) # r2 = .5474
#summary(lm(log(winnings) ~ finish_pos, data = nascard_sr1)) # r2 = .9015
nascard_lm <- lm(1/winnings ~ finish_pos, data = nascard_sr1) # r2 = .9509
#summary(lm(winnings ~ log(finish_pos), data = nascard_sr1)) # r2 = .883
#summary(lm(log(winnings) ~ log(finish_pos), data = nascard_sr1)) # r2 = .9766
#summary(lm(winnings ~ log(finish_pos), data = nascard_sr1)) # r2 = .883
coef(nascard_lm)
```

Finish position and prize are ratio variables, so the Pearson's r applies.  Check whether each variable is normally distributed.  

```{r warning=FALSE, message=FALSE, fig.height=3.5, fig.width=6.5}
p1 <- ggplot(nascard_sr1, aes(sample = finish_pos)) +
  stat_qq() +
  stat_qq_line() +
  theme_mf() +
  labs(title = "Q-Q Plots", subtitle = "Finish Position")
p2 <- ggplot(nascard_sr1, aes(sample = winnings)) +
  stat_qq() +
  stat_qq_line() +
  theme_mf() +
  labs(title = "", subtitle = "Winnings")
gridExtra::grid.arrange(p1, p2, nrow = 1)
```

The normal distribution plots look good.  You can also use the Anderson-Darling statistical test.
```{r}
nortest::ad.test(nascard_sr1$finish_pos)
nortest::ad.test(1/nascard_sr1$winnings)
```

Both fail to reject the normality null hypothesis.

Pearson's $r$ is 

```{r}
x <- nascard_sr1$finish_pos
y <- 1/nascard_sr1$winnings
(r = sum((x - mean(x)) * (y - mean(y))) /
   sqrt(sum((x - mean(x))^2) * sum((y - mean(y))^2)))
```

Test $H_0: \rho = 0$ with test statistic $T$.

```{r}
n <- nrow(nascard_sr1)
(t = r * sqrt((n - 2) / (1 - r^2)))
```

```{r}
pt(q = t, df = n - 2, lower.tail = FALSE) * 2
```

$P(T>.9752) < .0001$, so reject $H_0$ that the correlation is zero.  `cor.test()` performs these calculations.  Specify `method = "pearson"`.

```{r}
cor.test(
  x = nascard_sr1$finish_pos,
  y = 1/nascard_sr1$winnings,
  alternative = "two.sided",
  method = "pearson"
)
```


#### Spearman's Rho

Spearman's $\rho$ is the Pearson's r applied to the sample variable *ranks*.  Let $(X_i, Y_i)$ be the ranks of the $n$ sample pairs with mean ranks $\bar{X} = \bar{Y} = (n+1)/2$.  Spearman's rho is   

$$\hat{\rho} = \frac{\sum{(X_i - \bar{X})(Y_i - \bar{Y})}}{\sqrt{\sum{(X_i - \bar{X})^2 \sum{(Y_i - \bar{Y})^2}}}}$$

Spearman's rho is a non-parametric test, so there is no associated confidence interval.

From the `nascard` study, what was the correlation between the driver's starting position `start_pos` and finishing position `finish_pos`?  From the scatterplot, there does not appear to be much of a relationship.

```{r fig.height=3.5, fig.width=3.75}
nascard_sr1 %>%
  ggplot(aes(x = start_pos, y = finish_pos)) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  theme_mf() +
  labs(title = "Spearman's Rho")
```

Both `star_pos` and `finish_pos` are ordinal variables, so use Spearman's rho instead of Pearson. Normally, you replace the variable values with their ranks, but in this case, the values *are* their ranks.  After that, Spearman's $\rho$ is the same as Pearson's r.

```{r}
x <- rank(nascard_sr1$start_pos)
y <- rank(nascard_sr1$finish_pos)

(rho = sum((x - mean(x)) * (y - mean(y))) /
   sqrt(sum((x - mean(x))^2) * sum((y - mean(y))^2)))
```

Test $H_0: \rho = 0$ with test statistic $T$.

```{r}
n <- nrow(nascard_sr1)
(t = rho * sqrt((n - 2) / (1 - rho^2)))
```

```{r}
pt(q = abs(t), df = n - 2, lower.tail = FALSE) * 2
```

$P(T>.2206) = .8268$, so *do not* reject $H_0$ that the correlation is zero.  `cor.test()` performs these calculations.  Specify `method = "spearman"`.

```{r}
cor.test(
  x = nascard_sr1$start_pos,
  y = nascard_sr1$finish_pos,
  alternative = "two.sided",
  method = "spearman"
)
```


#### Kendall's Tau

The Kendall Rank correlation coefficient (Kendall's tau) measures the relationship between ranked variables. (see also [Statistics How To](https://www.statisticshowto.datasciencecentral.com/kendalls-tau/)).  Let $(X_i, Y_i)$ be the ranks of the $n$ sample pairs.  For each $X_i$, count $Y > X_i$. The total count is $k$.   Kendall's tau is

$$\hat{\tau} = \frac{4k}{n(n-1)} -1$$

If $X$ and $Y$ have the same rank orders, $\hat{\tau} = 1$, and if they have opposite rank orders $\hat{\tau} = -1$.  Test $H_0: \tau = 0$ with test statistic 

$$Z = \hat{\tau} \sqrt{\frac{9n(n - 1)}{2(2n + 5)}}.$$

Kendall's tau is a non-parametric test, so there is no associated confidence interval.

From the `nascard` study, what was the correlation between the driver's starting position `start_pos` and finishing position `finish_pos`?  Spearman's rho was $\hat{\rho} = -0.038$.  For Kendall's tau, for each observation, count the number of other observations where both `start_pos` and  `finish_pos` is larger.  The sum is $k = 293$.  Then calculate Kendall's $\tau$ is $\hat{\tau} = \frac{4 * 293}{35(35-1)} -1 = -.0151$.

```{r}
n <- nrow(nascard_sr1)
k <- 0
for (i in 1:n) {
  for(j in i:n) {
    k = k + if_else(
      nascard_sr1[j, ]$start_pos > nascard_sr1[i, ]$start_pos
      & nascard_sr1[j, ]$finish_pos > nascard_sr1[i, ]$finish_pos, 1, 0)
  }
}
(k)
(tau = 4.0*k / (n * (n - 1)) - 1)
```

Test $H_0: \tau = 0$ with test statistic $T$.

```{r}
n <- nrow(nascard_sr1)
(t = tau * sqrt(9 * n * (n-1) / (2 * (2*n + 5))))
```

```{r}
pt(q = abs(t), df = n - 2, lower.tail = FALSE) * 2
```

$P(|T| > 0.128) = .8991$, so *do not* reject $H_0$ that the correlation is zero.  `cor.test()` performs these calculations.  Specify `method = "kendall"`.

```{r}
cor.test(
  x = nascard_sr1$start_pos,
  y = nascard_sr1$finish_pos,
  alternative = "two.sided",
  method = "kendall"
)
```

Here is a fun way to tie Pearson, Spearman, and Kendal together: Calculate Spearman's rho and Kendall's tau for all 898 races in the `nascard` data set and calculate the Pearson correlation of their values!

```{r message=FALSE, warning=FALSE}
nascard_smry <- nest(nascard, -c(series_race, year)) %>%
  mutate(spearman = map(~ cor.test(.x$start_pos, .x$finish_pos,
                                  alternative = "two.sided", 
                                  method = "spearman")))
series_race <- nascard %>% distinct(nascard$series_race, nascard$year)
spearman <- sapply(series_race[,1], function(series_race) {
  cor.test(x = nascard[nascard$series_race == series_race, 'start_pos'],
         y = nascard[nascard$series_race == series_race, 'finish_pos'],
         alternative = "two.sided",
         method = "spearman")$estimate
})
spearman.p <- sapply(series_race[,1], function(series_race) {
  cor.test(x = nascard[nascard$series_race == series_race, 'start_pos'],
         y = nascard[nascard$series_race == series_race, 'finish_pos'],
         alternative = "two.sided",
         method = "spearman")$p.value
})
kendall <- sapply(series_race[,1], function(series_race) {
  cor.test(x = nascard[nascard$series_race == series_race, 'start_pos'],
         y = nascard[nascard$series_race == series_race, 'finish_pos'],
         alternative = "two.sided",
         method = "kendall")$estimate
})
kendall.p <- sapply(series_race[,1], function(series_race) {
  cor.test(x = nascard[nascard$series_race == series_race, 'start_pos'],
         y = nascard[nascard$series_race == series_race, 'finish_pos'],
         alternative = "two.sided",
         method = "kendall")$p.value
})
nascard_smry <- data.frame(series_race = series_race$`nascard$series_race`, 
                           year = series_race$`nascard$year`,
                           spearman, spearman.p,
                           kendall, kendall.p)
```

```{r message=FALSE, warning=FALSE}
library(tidyr)
nascard_smry2 <- nascard_smry %>% 
  group_by(year) %>%
  summarize(spearman_avg = mean(spearman),
            kendall_avg = mean(kendall)) %>%
  gather(key = method, value = estimate, c(spearman_avg, kendall_avg))
ggplot(data = nascard_smry2, aes(x = year, color = method)) +
  geom_line(aes(y = estimate)) +
  geom_line(aes(y = estimate)) +
  expand_limits(y = 0) +
  labs(title = "Average Rank Correlations vs Year")
```

Summarize the results in a contingency table.  The two tests usually return the same results in terms of significance.  
```{r}
nascard_smry$spearman_assoc <- 
  ifelse(nascard_smry$spearman.p < .05, "sig", "insig")
nascard_smry$kendall_assoc <- 
  ifelse(nascard_smry$kendall.p < .05, "sig", "insig")
table(nascard_smry$spearman_assoc, nascard_smry$kendall_assoc)
```

We can calculate the Pearson coefficient to measure the relationship between Spearman's rho and Kendall's tau.  
```{r}
ggplot(data = nascard_smry, aes(x = kendall, y = spearman)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  labs(title = "Spearman's rho vs Kendall's tau")
```

The scatterplot with fitted line has r-squared equal to the Pearson coefficient squared.

```{r}
cor.test(x = nascard_smry$spearman,
         y = nascard_smry$kendall,
         alternative = "two.sided",
         method = "pearson")

summary(lm(kendall~spearman, data = nascard_smry))
```

