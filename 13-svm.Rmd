# Support Vector Machines

These notes rely on [@James2013], [@Hastie2017], [@Kuhn2016], [PSU STAT 508](https://online.stat.psu.edu/stat508/), and the [e1071 SVM vignette](https://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf).

Support Vector Machines (SVM) is a classification model that maps observations as points in space so that the categories are divided by as wide a gap as possible. New observations can then be mapped into the space for prediction. The SVM algorithm finds the optimal separating hyperplane using a nonlinear mapping to a sufficiently high dimension. The hyperplane is defined by the observations that lie within a margin optimized by a cost hyperparameter. These observations are called the *support vectors*.

SVM is an extension of the *support vector classifier* which in turn is a generalization of the simple and intuitive *maximal margin classifier*.  The maximal margin classifier is defined for cases where the data can be separated by a linear boundary (uncommon). The support vector classifier generalizes the maximal margin classifier by introducing a margin around that hyperplane which permits some observations to land on wrong side of the hyperplane. The support vector machine generalizes still more by introducing non-linear hyperplanes. The best way to understand the SVM is to start with the maximal margin classifier and work up.


## Maximal Margin Classifier

The maximal margin classifier is the optimal hyperplane defined in the (rare) case where two classes are *linearly separable*. Given an $n \times p$ predictor matrix $X$ with a binary response variable $y \in \{-1, 1\}$ it *might* be possible to define a *p*-dimensional hyperplane $h(x) = \beta_0 + \beta_1x_1 + \beta_2x_2 \dots + \beta_px_p = X_i^{'} \beta + \beta_0 = 0$ such that all of the $y_i = -1$ observations fall on the negative side of the hyperplane and the $y_i = +1$ observations fall on the positive side:

$$y_i \left(x_i^{'} \beta + \beta_0 \right) > 0$$

This *separating hyperplane* is a simple classifier, and the magnitude of $\left(x_i^{'} \beta + \beta_0 \right)$ is an indicator of confidence in the predicted classification.

If you constrain $\beta$ to be a unit vector, $||\beta|| = \sum\beta^2 = 1$, then the products of the hyperplane and response variables, $\left(x_i^{'} \beta + \beta_0 \right)$, are the positive perpendicular distances from the hyperplane. If a separating hyperplane exists, there are probably an infinite number of possible hyperplanes. One way to evaluate a hyperplane is to measure its *margin*, $M$, the perpendicular distance to the closest observation. 

$$M = \min \left\{y_i (x_i^{'} \beta + \beta_0) \right\}.$$

The *maximal margin classifier* is the hyperplane that maximizes $M.$ The figure below (figure 9.3 from [@James2013]) shows a maximal marginal classifier. The three vectors shown in the figure anchor the hyperplane and are called the *support vectors*. Interestingly, it is only these three observations that factor into the determination of the maximal marginal classifier.

![FIGURE 9.3 from An Introduction to Statistical Learning](./images/svm_mmc.png)

So, to put it all together, if a separating hyperplane exists, one could calculate it by maximizing $M$  subject to $||\beta|| = 1$ and $y_i (x_i^{'} \beta + \beta_0) \ge M$ for all $i$. However, a separating hyperplane rarely exists. In fact, even if a separating hyperplane does exist, its maximal margin classifier is probably undesirably narrow. A maximal margin classifier is sensitive to outliers so it tends to overfit data.


## Support Vector Classifier

The maximal margin classifier can be generalized to non-separable cases using a so-called *soft margin*.  The generalization is called the *support vector classifier*.  The soft margin allows some misclassification in the interest of greater robustness to individual observations. 

The support vector classifier maximizes $M$ subject to $||\beta|| = 1$ and $y_i (x_i^{'} \beta + \beta_0) \ge M(1 - \xi_i)$ and $\sum \xi_i \le \Xi$ for all $i$. The $\xi_i$ are *slack variables* whose sum is bounded by some constant tuning parameter $\Xi$. The slack variable values indicate where the observation lies:  $\xi_i = 0$ observations lie on the correct side of the margin;  $\xi_i > 0$ observation lie on the wrong side of the margin;  $\xi_i > 1$ observations lie on the wrong side of the hyperplane.  $\Xi$ sets the tolerance for margin violation.  If $\Xi = 0$, then all observations must reside on the correct side of the margin, as in the maximal margin classifier.  $\Xi$ controls the bias-variance trade-off: as $\Xi$ increases, the margin widens and allows more violations, increasing bias and decreasing variance. Similar to the maximal margin classifier, only the observations that are on the margin or that violate the margin factor into the determination of the support vector classifier. These observations are the support vectors.

The figure below (figure 9.7 from [@James2013]) shows two support vector classifiers. The one on the left uses a large $\Xi$ and as a result includes many support vectors. The one on the right uses a smaller $\Xi.$

![FIGURE 9.7 from An Introduction to Statistical Learning](./images/svm_svc.png)

As $\Xi$ increases, the number of violating observations increase, and thus the number of support vectors increases. This property makes the algorithm robust to the extreme observations far away from the hyperplane. The only shortcoming with the algorithm is that it presumes a linear decision boundary.


## Support Vector Machines

Enlarging the feature space of the support vector classifier accommodates nonlinear relationships.  Support vector machines do this in a specific way, using *kernels*. Before you dive into kernels, you need to understand (somewhat) the solution to the support vector classifier optimization problem.

The linear support vector classifier can be represented as 

$$f(x) = \beta_0 + \sum_i^n \alpha_i \langle x, x_i \rangle.$$

That is, the classification of test observation $x$ is the sum of the dot products of $x$ with all the $n$ observations in the training set, multiplied by the vector $\alpha$ (plus the constant $\beta_0$). The $\alpha$ vector is calculated from the $n \choose 2$ dot products of the training data set. Actually, the classification is simpler than that because $\alpha_i = 0$ for all observation that are not support vectors, so you can actually represent the solution as

$$f(x) = \beta_0 + \sum_{i \in S} \alpha_i \langle x, x_i \rangle$$
where $S$ is the set of support vector indices.

Now, you can generalize the inner dot product with a wrapper function, called a *kernel*, $K(x_i, x_{i^{'}})$. 

$$f(x) = \beta_0 + \sum_{i \in S} \alpha_i K(x, x_i)$$

To get the the support vector, you'd defined $K$ to be a *linear* kernel: 

$$K(x_i, x_i^{'}) = \langle x, x_i \rangle$$

But you could also use other kernels, like the polynomial of degree $d$, 

$$K(x, x') = (1 + \langle x, x' \rangle)^d$$ 

or radial 

$$K(x, x') = \exp\{-\gamma ||x - x'||^2\}.$$

The figure below (figure 9.9 from [@James2013]) shows two support vector classifiers. The one on the left uses a polynomial kernel and the one on the right uses a radial kernel.

![FIGURE 9.9 from An Introduction to Statistical Learning](./images/svm_svm.png)

The SVM model can be expressed in the familiar "loss + penalty" minimization structure, $\min_{\beta} \left \{ L(X,y,\beta) + \lambda P(\beta) \right \}$ as

$$\min_\beta \left \{ \sum_{i=1}^n \max [0, 1-y_i f(x_i)] + \lambda \sum_{j=1}^p \beta_j^2 \right \}$$

Increasing $\lambda$, shrinks $\beta$ and more violations to the margin are tolerated, resulting in a lower-variance/higher-bias model. The loss function above is known as a *hinge loss*.


## SVM Example

Here is a contrived data set of two classes $y \in [-1, 1]$ described by two features $X1$ and $X2$. From the plot below you can see the classes are not linearly separable.

```{r message=FALSE, warning=FALSE}
library(tidyverse)

set.seed(1)
dat <- data.frame(
  X1 = c(rnorm(20), rnorm(20) + 1),
  X2 = c(rnorm(20), rnorm(20) + 1),
  y = factor(c(rep(-1, 20), rep(1, 20)))
)
train_idx <- createDataPartition(dat$y, p = 0.60, list = FALSE)[, 1]
dat_train <- dat[train_idx, ]
dat_test <- dat[-train_idx, ]

ggplot(training, aes(x = X1, y = X2, color = y)) +
  geom_point(size = 2) +
  labs(title = "Binary response with two features") +
  theme(legend.position = "top")
```

Fit a support vector classifier with `e1071::svm(kernel="linear")`. (Change the kernel to `c("polynomial", "radial")` for SVM). `svm` requires the response variable be a factor (which we've already done). The tuning parameter for the support vector classifier is $\Xi$. `svm()` uses something similar, `\cost`, which the documentation notes is the "*cost of constraints violation (default: 1)—it is the ‘C’-constant of the regularization term in the Lagrange formulation.*" Try a `cost = 10`.

```{r}
library(e1071)
m <- svm(
  y ~ ., 
  data = dat_train,
  kernel = "linear",
  type = "C-classification",  # (default) for classification
  cost = 10,  # default is 1
  scale = FALSE  # do not standardize features
)
plot(m, dat_train)
```

There are `r length(m$index)` support vectors, plotted as "x's". 

```{r}
m$index
```

The summary shows adds additional information, including the distribution of the support vector classes.

```{r}
summary(m)
```

10 of the 19 support vectors are one class, and 9 are in the other. What if we lower the cost of margin violations? This will increase bias and lower variance.

```{r}
m <- svm(
  y ~ ., 
  data = dat_train,
  kernel = "linear",
  type = "C-classification",  
  cost = 0.1,
  scale = FALSE
)
plot(m, train_data)
```

There are more support vectors now (`r length(m$index)`). Which cost level yields the *best* predictive performance on holdout data?  Use cross validation to find out. SVM defaults to 10-fold CV.  Try seven candidate values for `cost`.

```{r}
set.seed(1)
m_tune <- tune(
  svm,
  y ~ .,
  data = dat_train,
  kernel ="linear",
  ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100))
)
summary(m_tune)
```

The lowest cross-validation error rate is 0.32 with cost = 1.  `tune()` saves the best tuning parameter value.
```{r}
m_best <- m_tune$best.model
summary(m_best)
```

There are 19 support vectors, 10 in one class, 9 in the other.  This is a pretty wide margin.

```{r}
plot(m_best, train_data)
```

How does the model perform on the holdout data set? 87.5% accuracy.

```{r}
ypred <- predict(m_best, dat_test)
caret::confusionMatrix(data = ypred, reference  = dat_test$y)
```

To fit an SVM, use a different kernel.  You can use `kernal = c("polynomial", "radial", "sigmoid")`.  For a polynomial model, also specify the polynomial degree. For a radial model, include the gamma value.

```{r}
set.seed(1)
m3_tune <- tune(
  svm,
  y ~ .,
  data = dat_train,
  kernel ="polynomial",
  ranges = list(
    cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100),
    degree = c(1, 2, 3)
  )
)
summary(m3_tune)
```


Oh, that's funny, it chose a linear kernel anyway. The lowest cross-validation error rate is 0.35 with cost = 5, polynomial degree 1.

```{r}
m3_best <- m3_tune$best.model
summary(m3_best)
```

There are 19 support vectors. How did the SVM perform?

```{r}
ypred <- predict(m3_best, dat_test)
caret::confusionMatrix(data = ypred, reference  = dat_test$y)
```


The model can also be fit using **caret**.  I'll used LOOCV since the data set is so small, and standardize the variables to make their scale comparable (they are already since the data is from a random number generator, but I'm practicing good form). Also, I need to change the level names from -1, 1 to something that will translate into valid variable names.

```{r svm-caret, message=FALSE, warning=FALSE}
library(caret)
library(kernlab)

#train_data_3 <- train_data %>%
#  mutate(y = factor(y, labels = c("A", "B")))
dat_train$y <- factor(dat_train$y, labels = c("A", "B"))
dat_test$y <- factor(dat_test$y, labels = c("A", "B"))

m4 <- train(
  y ~ .,
  data = dat_train,
  method = "svmPoly",
  preProcess = c("center", "scale"),
  trControl = trainControl(
    method = "LOOCV",
#    number = 5,
    summaryFunction = twoClassSummary,	# Use AUC to pick the best model
    classProbs=TRUE
  )
)

m4$bestTune
```

Caret optimized to a degree 3 polynomial and cost 1.0. How did the SVM perform?

```{r}
ypred <- predict(m4, dat_test)
caret::confusionMatrix(data = ypred, reference  = dat_test$y)
```

## Full SVM Example

I'll learn by example, using the `ISLR::Default` data set to predict which customers will default on their credit card deb from its 3 predictor variables. I'm using this  [Dataaspirant](https://dataaspirant.com/support-vector-machine-classifier-implementation-r-caret-package/#:~:text=For%20machine%20learning%2C%20caret%20package%20is%20a%20nice,build%20a%20hyperplane%20separating%20data%20for%20different%20classes.) tutorial for guidance.

```{r warning=FALSE, message=FALSE}
library(tidyverse)
library(caret)

dat <- ISLR::Default
skimr::skim(dat)
```

For an SVM model, caret will expect the response variable to be a factor with labels that can double as R variable names. We're okay: `default` is a factor with labels "No" and "Yes". You also want the predictor variables to be of comparable scale, and that is *not* the case here: `student` is binary, `balance` has a range of \$0 - \$2654, and `income` has a range of \$772 - \$73,554. I'll standardized the features in the `train(preProcess)` argument.

Split `dat` (n = 10,000) into `dat_train` (80%, n = 800) to fit models, and `dat_test` (20%, n = 200) to compare performance on new data.

```{r}
set.seed(202008)
train_index <- createDataPartition(y = dat$default, p = 0.8, list = FALSE)
dat_train <- dat[train_index, ]
dat_test <- dat[-train_index, ]
```

I'll build the model using 10-fold cross-validation to optimize the hyperparameter CP. 

```{r}
mdl_ctrl = trainControl(
   method = "cv",
   number = 5,
   savePredictions = "final"       # save predictions for the optimal tuning parameter
)
```

Train an SVM model with polynomial kernel by specifying `method = "svmPoly"`. `svmPoly` has the following tuneable hyperparameters (see `modelLookup("svmPoly")`).

* `degree`: polynomial degree
* `scale`: scale
* `C`: cost

```{r}
caret::modelLookup("svmPoly")
```

```{r svm-1, cache=TRUE}
set.seed(1234)
#garbage <- capture.output(
mdl_svmpoly <- train(
  default ~ ., 
  data = dat_train, 
  method = "svmPoly",
#   metric = "ROC",
  tuneLength = 3,
  trControl = mdl_ctrl
#  preProcess = c("center", "scale"),
)
mdl_svmpoly
```
