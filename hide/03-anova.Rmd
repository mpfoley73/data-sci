# Analysis of Variance

```{r include=FALSE}
library(tidyverse)
library(broom)
library(flextable)
```

These notes are primarily taken from studying [PSU STAT-502](https://online.stat.psu.edu/stat502) and [Laerd Statistics](https://statistics.laerd.com/premium/spss/owa/one-way-anova-in-spss.php).

Classic analysis of variance (ANOVA) compares the mean responses from *experimental* studies. However, ANOVA also compares the mean responses from *observational* studies, but conclusions are just less rigorous. 

## One-Way ANOVA

Use the one-way ANOVA test to compare the mean response of a continuous dependent variable among the levels of a factor variable (if you only have two levels, use the independent-samples t-test). The observations must be independent, meaning the data generators cannot influence each other (e.g., same participant in different groups, or participants interact with each other to produce the observed outcome).

The ANOVA method decomposes the deviation of observation $Y_{ij}$ around the overall mean $\bar{Y}_{..}$ into two parts, the deviation of the observations around their treatment means, $SSE$, and the deviation of the treatment means around the overall mean, $SSR$. Their ratio, $F = SSR/SSE$ follows an *F*-distribution with $k-1$ numerator dof and $N-k$ denominator dof. The more observation variance captured by the treatments, the large is $F$, and the less likely that the null hypothesis, $H_0 = \mu_1 = \mu_2 = \cdots = \mu_k$ is true.

```{r echo=FALSE}
tmp <- tribble(
  ~Source, ~SS, ~df, ~MS, ~F,
  "SSR", "$\\sum{n_i(\\bar{Y}_{i.} - \\bar{Y}_{..})^2}$", "$k - 1$", "${SSR}/{(k - 1)}$", "${MSR}/{MSE}$", 
  "SSE", "$\\sum(Y_{ij} - \\bar{Y}_{i.})^2$", "$N - k$", "${SSE}/{(N - k)}$", "",
  "SST", "$\\sum(Y_{ij} - \\bar{Y}_{..})^2$", "$N - 1$", "", ""
)

tmp %>% 
  knitr::kable(tmp, format = "html", caption = "ANOVA Table") %>%
  kableExtra::kable_styling(full_width = TRUE) %>%
  kableExtra::row_spec(row = 0, align = "c") %>%
  kableExtra::footnote(
    general_title = "Note.",
    general = "Compare the *F*-statistic to the *F*-distribution with $k-1$ numerator degrees of freedom and $N-k$ denominator degrees of freedom",
   footnote_as_chunk = TRUE
    )
```

The *F* test does not indicate which populations cause the rejection of $H_0$. For this, use one of the post-hoc tests: Tukey, Fisher's Least Significant Difference (LSD), Bonferroni, Scheffe, or Dunnett.

ANOVA returns reliable results if the following conditions hold: (a) there should be no significant outliers within the factor levels; (b) the response variable should be approximately normally distributed within each factor level; and (c) the the response variable variances within the factor levels should be equal.

Here is an example where you might use an ANOVA test. A study compares the growth of plants using one of three fertilizers and a control group. Data set `greenhouse` contains 6 observations per each of the *k* = 4 treatment levels (*N* = 24) - a balanced design.

```{r include=FALSE}
greenhouse <- tribble(
  ~group, ~growth,
"Control",      21,
"Control",      19.5,
"Control",      22.5,
"Control",      21.5,
"Control",      20.5,
"Control",      21,
"F1",      32,
"F1",      30.5,
"F1",      25,
"F1",      27.5,
"F1",      28,
"F1",      28.6,
"F2",      22.5,
"F2",      26,
"F2",      28,
"F2",      27,
"F2",      26.5,
'F2',      25.2,
"F3",      28,
"F3",      27.5,
"F3",      31,
"F3",      29.5,
"F3",      30,
"F3",      29.2
) %>%
  mutate(group = factor(group), id = row_number())
skimr::skim(greenhouse)
```

Start with a column plot with 95% confidence intervals. All three fertilizers produced more growth than the control group.  Fertilizers *F1* and *F3* appear to be about tied for most growth, but it is unclear if the fertilizers are significantly different from each other.  

```{r echo=FALSE, fig.height=2.5}
greenhouse %>%
  group_by(group) %>%
  summarize(.groups = "drop",
            mean_growth = mean(growth),
            cl_025 = mean_growth + qnorm(.025) * sd(growth) / sqrt(n()),
            cl_975 = mean_growth + qnorm(.975) * sd(growth) / sqrt(n())) %>%
  ggplot(aes(x = group, y = mean_growth)) +
  geom_col(fill = "snow3", color = "snow3", alpha = 0.6, width = 0.5) +
  geom_errorbar(aes(ymin = cl_025, ymax = cl_975, width = 0.3)) +
  theme_minimal() +
  labs(title = "Mean Growth by Group",
       x = NULL, y = "centimeters",
       caption = "Error Bars: 95% CI")
```

```{r echo=FALSE}
greenhouse_desc <- greenhouse %>%
  group_by(group) %>%
  summarise(.groups = "drop",
            n = n(),
            mean = mean(growth),
            sd = sd(growth),
            se = sd / sqrt(n),
            ci_l = mean - qnorm(0.975) * se,
            ci_u = mean + qnorm(0.975) * se,
            min = min(growth),
            max = max(growth))

greenhouse_desc %>%
  flextable() %>% 
  set_table_properties(width = 0.8, layout = "autofit") %>%
  set_caption("Descriptive Statistics")
```

Data is presented as mean ± standard deviation. The ability to cope with workplace-related stress (CWWS score) increased from the sedentary (n = 7, 4.2 ± 0.8), to low (n = 9, 5.9 ± 1.7), to moderate (n = 8, 7.1 ± 1.6) to high (n = 7, 7.5 ± 1.2) physical activity groups, in that order.

The one-way ANOVA indicates amount of growth was statistically significantly different for different levels of fertilizer group, *F*(3, 20) = 27.5, *p* < .0001.

```{r}
greenhouse_aov <- aov(growth ~ group, data = greenhouse)
greenhouse_anova <- anova(greenhouse_aov)

greenhouse_anova %>% 
  tidy() %>%
  flextable() %>%
  set_table_properties(width = 0.8, layout = "autofit") %>%
  colformat_num(j = c(3, 4, 5), digits = 1) %>%
  colformat_num(j = 6, digits = 4) %>%
  set_caption("Results of ANOVA for Growth vs Fertilizer Group")
```

### Checking Conditions {-}

The ANOVA test applies when the dependent variable is continuous, the independent variable is categorical, and the observations are independent *within* groups. Independent means the observations should be from a random sample, or from an experiment using random assignment.  Each group's size should be less than 10% of its population size. The groups must also be independent of each other (non-paired, and non-repeated measures). Additionally, there are three conditions related to the data distribution.

1. **No outliers**. There should be no significant outliers in the groups. Outliers exert a large influence on the mean and standard deviation. Test with a box plot. If it fails this condition, you might be able to drop the outliers or transform the data. Otherwise you'll need to switch to a non-parametric test.
2. **Normality**.  Each group's values should be *nearly* normally distributed ("nearly" because ANOVA is considered robust to the normality assumption). This condition is especially important with small sample sizes. Test with the Q-Q plots or the Shapiro-Wilk test for normality. If the data is very non-normal, you might be able to transform your response variable, or use a nonparametric test such as Kruskal-Wallis.
3. **Equal Variances**.  The group variances should be roughly equal. This condition is especially important when sample sizes differ. The IQR of the box plot is a good way to visually assess this condition. A rule of thumb that the largest sample standard deviation should be less than twice the size of the smallest. More formal [homogeneity of variance](http://www.cookbook-r.com/Statistical_analysis/Homogeneity_of_variance/) tests include the Bartlett test, and Levene test. If the variances are very different, use a Games-Howell post hoc test for multiple comparisons instead of the Tukey post hoc test.

#### Outliers {-}

Assess outliers with a box plot. The whiskers extend no further than 1.5\*IQR from the upper and lower hinges. Any observations beyond the whiskers are outliers and are plotted individually. Our example includes an outlier in fertilizer group *F2*.

```{r echo=FALSE}
greenhouse %>%
  ggplot(aes(x = group, y = growth)) +
  geom_boxplot(fill = "snow3", color = "snow4", alpha = 0.6, width = 0.5, 
               outlier.color = "goldenrod") +
  theme_minimal() +
  labs(title = "Boxplot of Growth vs Fertilizer Group",
       y = "Growth (cm)", x = "Fertilizer Group")
```

Had the data passed this test, you would report

> There were no outliers in the data, as assessed by inspection of a boxplot.

The data did not pass the test, so what do you do? There are generally three reasons for outliers: data entry errors, measurement errors, and genuinely unusual values. If the problem's data entry - fix it! If it's measurement, throw it out. If it is genuine, you have some options.

* Kruskal-Wallace *H* test. It is a non-parametric test. Be careful here, because it is not quite testing the same $H_0$.
* Transform the dependent variable. Don't do this unless the data is also non-normal. It also has the downside of making interpretation more difficult.
* Leave it in if it doesn't affect the conclusion (compared to taking it out).

Here are the ANOVA results with the *F2* outlier removed. The conclusion is the same.

```{r}
greenhouse_aov <- aov(growth ~ group, data = greenhouse %>% filter(!id == 13))
greenhouse_anova <- anova(greenhouse_aov)

greenhouse_anova %>% 
  tidy() %>%
  flextable() %>%
  set_table_properties(width = 0.8, layout = "autofit") %>%
  colformat_num(j = c(3, 4, 5), digits = 1) %>%
  colformat_num(j = 6, digits = 4) %>%
  set_caption("Results of ANOVA for Growth vs Fertilizer Group") %>%
  footnote(i = 1, j = 1,
           value = as_paragraph("Note: One outlier in group F2 removed."),
           ref_symbols = c(""),
           part = "header", inline = TRUE)
```

#### Normality {-}

You can assume the populations are normally distributed if $n_j >= 30$. Otherwise, try the Q-Q plot, or skewness and kurtosis values, or histograms. If you still don't feel confident about normality, run a [Shapiro-Wilk Test]. If $n_j >= 50$, stick with graphical methods because at larger sample sizes Shapiro-Wilk flags even minor deviations from normality.

The QQ plots below appear to be approximately normal.

```{r}
greenhouse %>%
  ggplot(aes(sample = growth)) +
  stat_qq() +
  stat_qq_line(col = "goldenrod") +
  facet_wrap(~group) +
  theme_minimal() +
  labs(title = "Normal Q-Q Plot")
```

And the Shapiro-Wilk test fails to reject the normality null hypothesis.

```{r}
x <- by(greenhouse, greenhouse$group, function(x) shapiro.test(x$growth) %>% tidy())

x[1:4] %>%
  bind_rows() %>%
  mutate(group = names(x)) %>%
  dplyr::select(group, everything(), - method) %>%
  flextable() %>% 
  set_table_properties(width = 0.6, layout = "autofit") %>%
  set_caption("Shapiro-Wilk Normality Test")
```

If the data passes the test, report 

> Growth was normally distributed, as assessed by inspection of a Q-Q plot.

or

> Growth was normally distributed, as assessed by Shapiro-Wilk's test (p > .05).

Had the data not been normally distributed, you would have three options: (a) transform the dependent variable; (b) use a non-parametric test such as Kruskal-Wallis; or (c) carry on regardless.

Transformations will generally only work when the distribution of scores in all groups are the same shape. They also have the drawback of making the data less interpratable.

You can also choose to carry on regardless. ANOVA is considered "robust" to normality violations.

#### Equal Variances {-}

The equality of sample variances condition is less critical when sample sizes are similar among the groups. One rule of thumb is that no group's standard deviation should be more than double that of any other.  In this case `F1` is more than double `Control`.

```{r}
greenhouse %>% 
  group_by(group) %>% 
  summarize(.groups = "drop", sd = sd(growth)) %>%
  flextable() %>%
  set_table_properties(width = 0.5, layout = "autofit")
```

```{r}

```

Bartlett's test of homogeneity fails to reject $H_0$ of homogeity (*p* = 0.2494).  

```{r}
bartlett.test(growth ~ group, data = greenhouse)
```

Now that the conditions are checked, conduct a post-hoc test to see which groups differ.  Here is the Tukey test.  As expected, all three fertilizer factor levels differ from the control.  `F3` differed from `F2`, but `F1` was not significantly different from either `F2` or `F3`.

```{r}
plot(TukeyHSD(greenhouse_aov))
```

## Handling Non-Constant Variance

The statistical tests for the model conditions (e.g. Bartlett's test for homogeneity) are often too sensitive. ANOVA is robust to small violations of the conditions.  However, heterogeneity is a common problem in ANOVA.  Tranforming the response variable can often remove the heterogeneity.  Finding the correct transformation can be challenging, but the Box-Cox procedure can help. The MASS::boxcox() function calculates the profile log-likelihoods for a power transformation of the response variable $Y^\lambda$.

|$\lambda$ | $Y^\lambda$ | Transformation |
|---|---|---|
|2 | $Y^2$ | Square |
|1 | $Y^1$ | (no transformation) |
|.5 | $Y^{.5}$ | Square Root |
|0 | $\ln(Y)$ | Log |
|-.5 | $Y^{-.5}$ | Inverse Square Root |
|-1 | $Y^{-1}$ | Inverse|

The Box-Cox procedure does not recommend any particular transformation of the data in this case.

```{r message=FALSE}
library(MASS)
boxcox(greenhouse_aov, plotit = TRUE)
```


### Example
*In a completely randomized design experiment, 20 young pigs are assigned at random among 4 experimental groups, and each group is fed a different diet. The response variable is the pig's weight in kg after consuming the diet for 10 months.  Are the mean pig weights the same for all 4 diets?*

```{r}
pig <- read.delim(file = "input/pig_weight.txt", header = TRUE, sep = ",")
pig <- pig %>%
  gather(key = diet, value = weight) %>%
  mutate(diet = factor(diet))

glimpse(pig)

ggplot(data = pig, aes(x = diet, y = weight)) +
  geom_boxplot()
```

The measurements are independent because this is a completely randomized experiment.  The individual populations could be assumed normally distributed if $n >= 30$, but $n = 20$, so we need to check for normality.  The sample sizes are similar (5 per each of the 4 factor levels), so the equality of sample variances is less critical, but we can check anyway.

First a check of the normality condition.  Test for normality by starting with the assumption that the distribution are normal, $H_0: normal$, then falsifying the assumption if sufficient evidence exists.  In these normal Q-Q plots, look for substantial deviations from a straight line.  These plots looks good.
```{r}
layout(rbind(c(1, 2), c(3, 4)))
qqnorm(pig[pig$diet == "Feed.1",]$weight)
qqline(pig[pig$diet == "Feed.1",]$weight)
qqnorm(pig[pig$diet == "Feed.2",]$weight)
qqline(pig[pig$diet == "Feed.2",]$weight)
qqnorm(pig[pig$diet == "Feed.3",]$weight)
qqline(pig[pig$diet == "Feed.3",]$weight)
qqnorm(pig[pig$diet == "Feed.4",]$weight)
qqline(pig[pig$diet == "Feed.4",]$weight)
```

There are statistical tests for that provide a quantitative evaluation, but the sample sizes are two small for them to be useful.

Now check for equal variances with Bartlett's test of homogeneity of variances.  The p-value is >>.05, so do not reject $H_0$ of equal variances.
```{r}
bartlett.test(weight ~ diet, data = pig)
```

Now we are ready for the one-way ANOVA test.  The null hypothesis is that all means are equal.  The p-value is <.0001, so we reject $H_0$.
```{r}
summary(pig.aov <- aov(weight ~ diet, data = pig))
```

Perform a post-hoc test to see which of the groups differ.  Here we use Tukey's test.  All pairs differed from each other.
```{r}
TukeyHSD(pig.aov)
plot(TukeyHSD(pig.aov))
```









In a multi-factor experiment each level of m
Multi-factor ANOVA (MANOVA) is a method to compare mean responses by treatment factor level of two or more treatments applied in combination. The null hypotheses are $H_0: \mu_{1.} = \mu_{2.} = \dots = \mu_{a.}$ for the $a$ levels of factor 1, $H_0: \mu_{.1} = \mu_{.2} = \dots = \mu_{.b}$ for the $b$ levels of factor 2, etc. for all the factors in the experiment, and $H_0: $ no interaction for all the factor interactions.

There are two equivalent ways to state the MANOVA model:

$$Y_{ijk} = \mu_{ij} + \epsilon_{ijk}$$

In this notation $Y_{ijk}$ refers to the $k^{th}$ observation in the $j^{th}$ level of factor two and the $i^{th}$ level of factor 1.  Potentially there could be additional factors.  This model formulation decomposes the response into a cell mean and an error term.  The second makes the factor effect more explicit and is thus more common:

$$Y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha\beta)_{ij} +  \epsilon_{ijk}$$

$\alpha_i 


### Multiple Variance Comparison F Test


### Example
*A study investigates the relationship between oxygen update and two explanatory variables: smoking, and type of stress test.  A sample of* $n = 27$ *persons, 9 non-smoking, 9 moderately-smoking, and 9 heavy-smoking are divided into three stress tests, bicycle, treadmill, and steps and their oxygen uptake was measured.  Is oxygen uptake related to smoking status and type of stress test?  Is there an interaction effect between smoking status and type of stress test?*
```{r message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(nortest)  # for Anderson-Darling test
library(stats)  # for anova

smoker <- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 
            2, 2, 2, 2, 2, 2, 2, 2, 2, 
            3, 3, 3, 3, 3, 3, 3, 3, 3)
stress <- c(1, 1, 1, 2, 2, 2, 3, 3, 3,
            1, 1, 1, 2, 2, 2, 3, 3, 3,
            1, 1, 1, 2, 2, 2, 3, 3, 3)
oxytime <- c(12.8, 13.5, 11.2, 16.2, 18.1, 17.8, 22.6, 19.3, 18.9,
             10.9, 11.1, 9.8, 15.5, 13.8, 16.2, 20.1, 21.0, 15.9,
             8.7, 9.2, 7.5, 14.7, 13.2, 8.1, 16.2, 16.1, 17.8)
oxy <- data.frame(oxytime, smoker, stress)
oxy$smoker <- ordered(oxy$smoker,
                      levels = c(1, 2, 3),
                      labels = c("non-smoker", "moderate", "heavy"))
oxy$stress <- factor(oxy$stress,
                     labels = c("bicycle", "treadmill", "steps"))

lm_oxy <- lm(oxytime~smoker+stress+smoker*stress, data = oxy)
anova(lm_oxy)
```


## References

[PSU STAT502](https://newonlinecourses.science.psu.edu/stat502/node/152/)

[SFU BIO710](http://online.sfsu.edu/efc/classes/biol710/manova/MANOVAnewest.pdf)

