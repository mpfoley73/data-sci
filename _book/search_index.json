[
["index.html", "My Data Science Notes Intro", " My Data Science Notes Michael Foley 2020-03-10 Intro These notes are pulled from various classes, tutorials, books, etc. and are intended for my own consumption. If you are finding this on the internet, I hope it is useful to you, but you should know that I am just a student and there’s a good chance whatever you’re reading here is mistaken. In fact, that should probably be your null hypothesis… or your prior. Whatever. "],
["probability.html", "Chapter 1 Probability ", " Chapter 1 Probability "],
["principles.html", "1.1 Principles", " 1.1 Principles Here are three rules that come up all the time. \\(Pr(A \\cup B) = Pr(A)+Pr(B) - Pr(AB)\\). This rule generalizes to \\(Pr(A \\cup B \\cup C)=Pr(A)+Pr(B)+Pr(C)-Pr(AB)-Pr(AC)-Pr(BC)+Pr(ABC)\\). \\(Pr(A|B) = \\frac{P(AB)}{P(B)}\\) If A and B are independent, \\(Pr(A \\cap B) = Pr(A)Pr(B)\\), and \\(Pr(A|B)=Pr(A)\\). Uniform distributions on finite sample spaces often reduce to counting the elements of A and the sample space S, a process called combinatorics. Here are three important combinatorial rules. Multiplication Rule. \\(|S|=|S_1 |⋯|S_k|\\). How many outcomes are possible from a sequence of 4 coin flips and 2 rolls of a die? \\(|S|=|S_1| \\cdot |S_2| \\dots |S_6| = 2 \\cdot 2 \\cdot 2 \\cdot 2 \\cdot 6 \\cdot 6 = 288\\). How many subsets are possible from a set of n=10 elements? In each subset, each element is either included or not, so there are \\(2^n = 1024\\) subsets. How many subsets are possible from a set of n=10 elements taken k at a time with replacement? Each experiment has \\(n\\) possible outcomes and is repeated \\(k\\) times, so there are \\(n^k\\) subsets. Permutations. The number of ordered arrangements (permutations) of a set of \\(|S|=n\\) items taken \\(k\\) at a time without replacement has \\(n(n-1) \\dots (n-k+1)\\) subsets because each draw is one of k experiments with decreasing number of possible outcomes. \\[_nP_k = \\frac{n!}{(n-k)!}\\] Notice that if \\(k=0\\) then there is 1 permutation; if \\(k=1\\) then there are \\(n\\) permutations; if \\(k=n\\) then there are \\(n!\\) permutations. How many ways can you distribute 4 jackets among 4 people? \\(_nP_k = \\frac{4!}{(4-4)!} = 4! = 24\\) How many ways can you distribute 4 jackets among 2 people? \\(_nP_k = \\frac{4!}{(4-2)!} = 12\\) Subsets. The number of unordered arrangements (combinations) of a set of \\(|S|=n\\) items taken \\(k\\) at a time without replacement has \\[_nC_k = {n \\choose k} = \\frac{n!}{k!(n-k)!}\\] combinations and is called the binomial coefficient. The binomial coefficient is the number of different subsets. Notice that if k=0 then there is 1 subset; if k=1 then there are n subsets; if k=n then there is 1 subset. The connection with the permutation rule is that there are \\(n!/(n-k)!\\) permutations and each permutation has \\(k!\\) permutations. How many subsets of 7 people can be taken from a set of 12 persons? \\(_{12}C_7 = {12 \\choose 7} = \\frac{12!}{7!(12-7)!} = 792\\) If you are dealt five cards, what is the probability of getting a “full-house” hand containing three kings and two aces (KKKAA)? \\[P(F) = \\frac{{4 \\choose 3} {4 \\choose 2}}{{52 \\choose 5}}\\] Distinguishable permutations. The number of unordered arrangements (distinguishable permutations) of a set of \\(|S|=n\\) items in which \\(n_1\\) are of one type, \\(n_2\\) are of another type, etc., is \\[{n \\choose {n_1, n_2, \\dots, n_k}} = \\frac{n!}{n_{1}! n_{2}! \\dots n_{k}!}\\] How many ordered arrangements are there of the letters in the word PHILIPPINES? There are n=11 objects. \\(|P|=n_1=3\\); \\(|H|=n_2=1\\); \\(|I|=n_3=3\\); \\(|L|=n_4=1\\); \\(|N|=n_5=1\\); \\(|E|=n_6=1\\); \\(|S|=n_7=1\\). \\[{n \\choose {n_1, n_2, \\dots, n_k}} = \\frac{11!}{3! 1! 3! 1! 1! 1! 1!} = 1,108,800\\] How many ways can a research pool of 15 subjects be divided into three equally sized test groups? \\[{n \\choose {n_1, n_2, \\dots, n_k}} = \\frac{15!}{5! 5! 5!} = 756,756\\] "],
["disc-dist.html", "Chapter 2 Discrete Distributions", " Chapter 2 Discrete Distributions These notes rely heavily on PSU STATS 504 course notes. The most important discrete distributions are the Binomial, Poisson, and Multinomial. Sometimes useful are the related Bernoulli, negative binomial, geometric, and hypergeometric distributions. A discrete random variable \\(X\\) is described by its probability mass function \\(f(x) = P(X = x)\\). The set of \\(x\\) values for which \\(f(x) &gt; 0\\) is called the support. If the distribution depends on unknown parameter(s) \\(\\theta\\) we write it as \\(f(x; \\theta)\\) (frequentist) or \\(f(x | \\theta)\\) (Bayesian). "],
["bernoulli.html", "2.1 Bernoulli", " 2.1 Bernoulli If \\(X\\) is the result of a trial with two outcomes of probability \\(P(X = 1) = \\pi\\) and \\(P(X = 0) = 1 - \\pi\\), then \\(X\\) is a random variable with a Bernoulli distribution \\[f(x) = \\pi^x (1 - \\pi)^{1 - x}, \\hspace{1cm} x \\in (0, 1)\\] with \\(E(X) = \\pi\\) and \\(Var(X) = \\pi(1 - \\pi)\\). "],
["binomial.html", "2.2 Binomial", " 2.2 Binomial If \\(X\\) is the count of successful events in \\(n\\) identical and independent Bernoulli trials of success probability \\(\\pi\\), then \\(X\\) is a random variable with a binomial distribution \\(X \\sim Bin(n,\\pi)\\) \\[f(x;n, \\pi) = \\frac{n!}{x!(n-x)!} \\pi^x (1-\\pi)^{n-x} \\hspace{1cm} x \\in (0, 1, ..., n), \\hspace{2mm} \\pi \\in [0, 1]\\] with \\(E(X)=n\\pi\\) and \\(Var(X) = n\\pi(1-\\pi)\\). Binomial sampling is used to model counts of one level of a categorical variable over a fixed sample size. Here is a simple analysis of data from a Binomial process. Data set dat contains frequencies of high-risk drinkers vs non-high-risk drinkers in a college survey. ## ## No Yes ## 685 630 The MLE of \\(\\pi\\) from the Binomial distribution is the sample mean. x &lt;- sum(dat$high_risk == &quot;Yes&quot;) n &lt;- nrow(dat) p &lt;- x / n print(p) ## [1] 0.4790875 Here is the binomial distribution \\(f(x; \\pi), \\hspace{5mm} x \\in [550, 700]\\). events &lt;- round(seq(from = 550, to = 700, length = 20), 0) density &lt;- dbinom(x = events, prob = p, size = n) prob &lt;- pbinom(q = events, prob = p, size = n, lower.tail = TRUE) df &lt;- data.frame(events, density, prob) ggplot(df, aes(x = factor(events))) + # geom_col(aes(y = density)) + geom_col(aes(y = density), fill = mf_pal()(1), alpha = 0.8) + geom_text( aes(label = round(density, 3), y = density + 0.001), position = position_dodge(0.9), size = 3, vjust = 0 ) + geom_line( data = df, aes(x = as.numeric(factor(events)), y = prob/40), color = mf_pal()(1), size = 1) + scale_y_continuous(sec.axis = sec_axis(~.*40, name = &quot;Cum Prob&quot;)) + theme_mf() + labs(title = &quot;PMF and CDF of Binomial Distribution&quot;, subtitle = &quot;Bin(1315, 0.479).&quot;, x = &quot;Events (x)&quot;, y = &quot;Density&quot;) There are several ways to calculate a confidence interval for \\(\\pi\\). One method is the normal approximation (Wald) interval. \\[\\pi = p \\pm z_{\\alpha /2} \\sqrt{\\frac{p (1 - p)}{n}}\\] alpha &lt;- .05 z &lt;- qnorm(1 - alpha / 2) se &lt;- sqrt(p * (1 - p) / n) p + c(-z*se, z*se) ## [1] 0.4520868 0.5060882 This method is easy to understand and calculate by hand, but its accuracy suffers when \\(np&lt;5\\) or \\(n(1-p)&lt;5\\) and it does not work at all when \\(p = 0\\) or \\(p = 1\\). Option two is the Wilson method. \\[\\frac{p + \\frac{z^2}{2n}}{1 + \\frac{z^2}{n}} \\pm \\frac{z}{1 + \\frac{z^2}{n}} \\sqrt{\\frac{p(1 - p)}{n} + \\frac{z^2}{4n^2}}\\] est &lt;- (p + (z^2)/(2*n)) / (1 + (z^2) / n) pm &lt;- z / (1 + (z^2)/n) * sqrt(p*(1-p)/n + (z^2) / (4*(n^2))) est + c(-pm, pm) ## [1] 0.4521869 0.5061098 This is what prop.test() does when you set correct = FALSE. prop.test(x = x, n = n, correct = FALSE) ## ## 1-sample proportions test without continuity correction ## ## data: x out of n, null probability 0.5 ## X-squared = 2.3004, df = 1, p-value = 0.1293 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.4521869 0.5061098 ## sample estimates: ## p ## 0.4790875 There is a second version of the Wilson interval that applies a “continuity correction” that aligns the “minimum coverage probability”, rather than the “average probability”, with the nominal value. I’ll need to learn what’s inside those quotations at some point. prop.test(x = x, n = n) ## ## 1-sample proportions test with continuity correction ## ## data: x out of n, null probability 0.5 ## X-squared = 2.2175, df = 1, p-value = 0.1365 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.4518087 0.5064898 ## sample estimates: ## p ## 0.4790875 Finally, there is the Clopper-Pearson exact confidence interval. Clopper-Pearson inverts two single-tailed binomial tests at the desired alpha. This is a non-trivial calculation, so there is no easy formula to crank through. Just use the binom.test() function and pray no one asks for an explanation. binom.test(x = x, n = n) ## ## Exact binomial test ## ## data: x and n ## number of successes = 630, number of trials = 1315, p-value = 0.1364 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.4517790 0.5064896 ## sample estimates: ## probability of success ## 0.4790875 The expected probability of no one being a high-risk drinker is \\(f(0;0.479) = \\frac{1315!}{0!(1315-0)!} 0.479^0 (1-0.479)^{1315-0} = 0\\). dbinom(x = 0, size = n, p = p) ## [1] 0 The expected probability of half the population being a high-risk drinker, \\(f(658, 0.479)\\), is impossible to write out, and slow to calculate. pbinom(q = .5*n, size = n, prob = p, lower.tail = FALSE) ## [1] 0.06455096 As n increases for fixed \\(\\pi\\), the binomial distribution approaches normal distribution \\(N(n\\pi, n\\pi(1−\\pi))\\). The normal distribution is a good approximation when \\(n\\) is large. pnorm(q = 0.5, mean = p, sd = se, lower.tail = FALSE) ## [1] 0.06450357 Here are some more examples using smaller sample sizes. The probability 2 out of 10 coin flips are heads if the probability of heads is 0.3: dbinom(x = 2, size = 10, prob = 0.3) ## [1] 0.2334744 Here is a simulation from n = 10,000 random samples of size 10. rbinom() generates a random sample of numbers from the binomial distribution. data.frame(cnt = rbinom(n = 10000, size = 10, prob = 0.3)) %&gt;% count(cnt) %&gt;% ungroup() %&gt;% mutate(pct = n / sum(n), X_eq_x = cnt == 2) %&gt;% ggplot(aes(x = as.factor(cnt), y = n, fill = X_eq_x, label = pct)) + geom_col(alpha = 0.8) + scale_fill_mf() + geom_label(aes(label = round(pct, 2)), size = 3, alpha = .6) + theme_mf() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Binomial Distribution&quot;, subtitle = paste0( &quot;P(X=2) successes in 10 trials when p = 0.3 is &quot;, round(dbinom(2, 10, 0.3), 4), &quot;.&quot; ), x = &quot;Successes&quot;, y = &quot;Count&quot;, caption = &quot;Simulation from n = 10,000 binomial random samples.&quot;) What is the probability of &lt;=2 heads in 10 coin flips where probability of heads is 0.3? The cumulative probability is the sum of the first three bars in the simulation above. Function pbinom() calculates the cumulative binomial probability. pbinom(q = 2, size = 10, prob = 0.3, lower.tail = TRUE) ## [1] 0.3827828 What is the expected number of heads in 25 coin flips if the probability of heads is 0.3? The expected value, \\(\\mu = np\\), is 7.5. Here’s an empirical test from 10,000 samples. mean(rbinom(n = 10000, size = 25, prob = .3)) ## [1] 7.4736 The variance, \\(\\sigma^2 = np (1 - p)\\), is 5.25. Here’s an empirical test. var(rbinom(n = 10000, size = 25, prob = .3)) ## [1] 5.176393 Suppose X and Y are independent random variables distributed \\(X \\sim Bin(10, .6)\\) and \\(Y \\sim Bin(10, .7)\\). What is the probability that either variable is &lt;=4? Let \\(P(A) = P(X&lt;=4)\\) and \\(P(B) = P(Y&lt;=4)\\). Then \\(P(A|B) = P(A) + P(B) - P(AB)\\), and because the events are independent, \\(P(AB) = P(A)P(B)\\). p_a &lt;- pbinom(q = 4, size = 10, prob = 0.6, lower.tail = TRUE) p_b &lt;- pbinom(q = 4, size = 10, prob = 0.7, lower.tail = TRUE) p_a + p_b - (p_a * p_b) ## [1] 0.2057164 Here’s an empirical test. df &lt;- data.frame( x = rbinom(10000, 10, 0.6), y = rbinom(10000, 10, 0.7) ) mean(if_else(df$x &lt;= 4 | df$y &lt;= 4, 1, 0)) ## [1] 0.2058 A couple other points to remember: The Bernoulli distribution is a special case of the binomial with \\(n = 1\\). The binomial distribution assumes independent trials. If you sample without replacement from a finite population, use the hypergeometric distribution. "],
["poission.html", "2.3 Poission", " 2.3 Poission If \\(X\\) is the number of successes in \\(n\\) (many) trials when the probability of success \\(\\lambda / n\\) is small, then \\(X\\) is a random variable with a Poisson distribution \\(X \\sim Poisson(\\lambda)\\) \\[f(x;\\lambda) = \\frac{e^{-\\lambda} \\lambda^x}{x!} \\hspace{1cm} x \\in (0, 1, ...), \\hspace{2mm} \\lambda &gt; 0\\] with \\(E(X)=\\lambda\\) and \\(Var(X) = \\lambda\\). The Poisson likelihood function is \\[L(\\lambda; x) = \\prod_{i=1}^N f(x_i; \\lambda) = \\prod_{i=1}^N \\frac{e^{-\\lambda} \\lambda^x_i}{x_i !} = \\frac{e^{-n \\lambda} \\lambda^{\\sum x_i}}{\\prod x_i}.\\] The Poisson loglikelihood function is \\[l(\\lambda; x) = \\sum_{i=1}^N x_i \\log \\lambda - n \\lambda.\\] One can show that the loglikelihood function is maximized at \\[\\hat{\\lambda} = \\sum_{i=1}^N x_i / n.\\] Thus, for a Poisson sample, the MLE for \\(\\lambda\\) is just the sample mean. Poisson sampling is used to model counts of events that occur randomly over a fixed period of time. Here is a simple analysis of data from a Poisson process. Data set dat contains frequencies of goal counts during the first round matches of the 2002 World Cup. ## goals freq ## 1 0 23 ## 2 1 37 ## 3 2 20 ## 4 3 11 ## 5 4 2 ## 6 5 1 ## 7 6 0 ## 8 7 0 ## 9 8 1 The MLE of \\(\\lambda\\) from the Poisson distribution is the sample mean. lambda &lt;- weighted.mean(dat$goals, dat$freq) print(lambda) ## [1] 1.378947 The 0.95 CI is \\(\\lambda \\pm z_{.05/2} \\sqrt{\\lambda / n}\\) n &lt;- sum(dat$freq) z &lt;- qnorm(0.975) se &lt;- sqrt(lambda / n) paste0(&quot;[&quot;, round(lambda - z*se, 2), &quot;, &quot;, round(lambda + z*se, 2),&quot;]&quot;) ## [1] &quot;[1.14, 1.62]&quot; The expected probability of scoring 2 goals in a match is \\(\\frac{e^{-1.38} 1.38^2}{2!} = 0.239\\). dpois(x = 2, lambda = lambda) ## [1] 0.2394397 events &lt;- 0:10 density &lt;- dpois(x = events, lambda = 3) prob &lt;- ppois(q = events, lambda = 3, lower.tail = TRUE) df &lt;- data.frame(events, density, prob) ggplot(df, aes(x = factor(events), y = density)) + geom_col() + geom_text( aes(label = round(density, 3), y = density + 0.01), position = position_dodge(0.9), size = 3, vjust = 0 ) + geom_line( data = df, aes(x = events, y = prob/4), size = 1) + scale_y_continuous(sec.axis = sec_axis(~.*4, name = &quot;Cum Prob&quot;)) + theme_mf() + scale_fill_mf() + labs(title = &quot;PMF and CDF of Poisson Distribution&quot;, subtitle = &quot;Poisson(3).&quot;, x = &quot;Events (x)&quot;, y = &quot;Density&quot;) The expected probability of scoring 2 to 4 goals in a match is sum(dpois(x = c(2:4), lambda = lambda)) ## [1] 0.3874391 Or, using the cumulative probability distribution, ppois(q = 4, lambda = lambda) - ppois(q = 1, lambda = lambda) ## [1] 0.3874391 How well does the Poisson distribution fit the 2002 World Cup data? dat %&gt;% mutate(pred = n * dpois(x = goals, lambda = lambda)) %&gt;% rename(obs = freq) %&gt;% pivot_longer(cols = -goals) %&gt;% ggplot(aes(x = goals, y = value, color = name)) + geom_point() + theme_mf() + scale_color_mf() + geom_smooth(se = FALSE) + labs( title = &quot;Poisson Dist: Observed vs Expected&quot;, color = &quot;&quot;, y = &quot;frequencey&quot; ) It fits the data pretty good! \\(Poison(\\lambda) \\rightarrow Bin(n, \\pi)\\) when \\(n\\pi = \\lambda\\) and \\(n \\rightarrow \\infty\\) and \\(\\pi \\rightarrow 0\\). Because the Poisson is limit of the \\(Bin(n, \\pi)\\), it is useful as an approximation to the binomial when \\(n\\) is large (\\(n&gt;=20\\)) and \\(\\pi\\) small (\\(p&lt;=0.05\\)). For example, suppose a baseball player has a p=.03 chance of hitting a homerun. What is the probability of X&gt;=20 homeruns in 500 at-bats? This is a binomial process because the sample size is fixed. pbinom(q = 20, size = 500, prob = 0.03, lower.tail = FALSE) ## [1] 0.07979678 But \\(n\\) is large and \\(\\pi\\) is small, so the Poission distribution will work well too. ppois(q = 20, lambda = 0.03 * 500, lower.tail = FALSE) ## [1] 0.08297091 What is the distribution of successes from a sample of n = 50 when the probability of success is p = .03? n = 500 p = 0.03 x = 0:30 data.frame( events = x, Poisson = dpois(x = x, lambda = p * n), Binomial = dbinom(x = x, size = n, p = p) ) %&gt;% pivot_longer(cols = -events) %&gt;% ggplot(aes(x = events, y = value, color = name)) + geom_point() + theme_mf() + scale_color_mf() + labs(title = &quot;Poisson(15) vs. Bin(500, .03)&quot;, subtitle = &quot;Poisson approximation to binomial.&quot;, x = &quot;Events&quot;, y = &quot;Density&quot;, color = &quot;&quot;) When the observed variance is greater than \\(\\lambda\\) (overdispersion), the Negative Binomial distribution can be used instead of Poisson. Suppose the probability that a drug produces a certain side effect is p = = 0.1% and n = 1,000 patients in a clinical trial receive the drug. What is the probability 0 people experience the side effect? The expected value is np, 1. The probability of measuring 0 when the expected value is 1 is dpois(x = 0, lambda = 1000 * .001) = 0.3678794. "],
["multinomial.html", "2.4 Multinomial", " 2.4 Multinomial If \\(X = (X_1, X_2, \\cdots, X_k)\\) are the counts of successful events in \\(n\\) identical and independent trials of success probabilities \\(\\pi = (\\pi_1, \\pi_2, \\cdots, \\pi_k)\\), then \\(X\\) is a random variable with a multinomial distribution \\(X \\sim Mult(n,\\pi)\\) \\[f(x; n, \\pi) = \\frac{n!}{x_{1}! x_{2}! \\cdots x_{k}!} \\pi^{x_1} \\pi^{x_2} \\cdots \\pi^{x_k} \\hspace{1cm} x \\in \\{0, 1, ..., n \\}, \\hspace{2mm} \\pi \\in [0, 1]\\] with expected values vector \\(E(X_j) = n\\pi_j\\) and covariance matrix \\[Var(X) = \\begin{bmatrix}n\\pi_{1}(1-\\pi_{1}) &amp; -n\\pi_{1}\\pi_{2} &amp; \\cdots &amp; -n\\pi_{1}\\pi_{k}\\\\ -n\\pi_{1}\\pi_{2} &amp; n\\pi_{2}(1-\\pi_{2}) &amp; \\cdots &amp; -n\\pi_{2}\\pi_{k}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ -n\\pi_{1}\\pi_{k} &amp; -n\\pi_{2}\\pi_{k} &amp; \\cdots &amp; n\\pi_{k}(1-\\pi_{k}) \\end{bmatrix}\\] so \\(Var(X_j) = n \\pi_j (1 - \\pi_j)\\) and \\(cov(X_j, X_k) = -n \\pi_j \\pi_k\\). The individual components of a multinomial random vector are binomial and have a binomial distribution, \\(X_i = Bin(n, \\pi_i)\\). Binomial is a special case of multinomial for k = 2. Suppose a city population is 20% black, 15% Hispanic, and 65% other. From a random sample of \\(n = 12\\) persons, what is the probability of 4 black and 8 other? \\[f(x;\\pi) = \\frac{12!}{4! 0! 8!} (0.20)^4 (0.15)^0 (0.65)^8 = 0.0252\\] Function dmultinom() calculates the multinomial probability. dmultinom(x = c(4, 0, 8), prob = c(0.20, 0.15, 0.65)) ## [1] 0.025 To calculate the probability of &lt;= 1 black, combine Hispanic and other, then sum the probability of black = 1 and black = 2. \\[f(x;\\pi) = \\frac{12!}{0! 12!} (0.20)^0 (0.80)^{12} + \\frac{12!}{1! 11!} (0.20)^1 (0.80)^{11} = 0.2748\\] dmultinom(x = c(0, 12), prob = c(0.20, 0.80)) + dmultinom(x = c(1, 11), prob = c(0.20, 0.80)) ## [1] 0.27 "],
["negative-binomial.html", "2.5 Negative-Binomial", " 2.5 Negative-Binomial If \\(X\\) is the count of failure events ocurring prior to reaching \\(r\\) successful events in a sequence of Bernouli trias of success probability \\(p\\), then \\(X\\) is a random variable with a negative-binomial distribution \\(X \\sim NB(r, p)\\). The probability of \\(X = x\\) failures prior to \\(r\\) successes is \\[f(x;r, p) = {{x + r - 1} \\choose {r - 1}} p^r (1-p)^{x}.\\] with \\(E(X) = r (1 - p) / p\\) and \\(Var(X) = r (1-p) / p^2\\). When the data has overdispersion, model the data with the negative-binomial distribution instead of Poission. Examples An oil company has a \\(p = 0.20\\) chance of striking oil when drilling a well. What is the probability the company drills \\(x + r = 7\\) wells to strike oil \\(r = 3\\) times? Note that the question is formulated as counting total events, \\(x + r = 7\\), so translate it to total failed events, \\(x = 4\\). \\[f(x;r, p) = {{4 + 3 - 1} \\choose {3 - 1}} (0.20)^3 (1 - 0.20)^4 = 0.049.\\] Function dnbinom() calculates the negative-binomial probability. Parameter x equals the number of failures, \\(x - r\\). dnbinom(x = 4, size = 3, prob = 0.2) ## [1] 0.049 The expected number of failures prior to 3 successes is \\(E(X) = 3 (1 - 0.20) / 0.20 = 12\\) with variance \\(Var(X) = 3 (1 - 0.20) / 0.20^2 = 60\\). Confirm this with a simulation from n = 10,000 random samples using rnbinom(). my_dat &lt;- rnbinom(n = 10000, size = 3, prob = 0.20) mean(my_dat) ## [1] 12 var(my_dat) ## [1] 61 "],
["geometric.html", "2.6 Geometric", " 2.6 Geometric If \\(X\\) is the count of Bernoulli trials of success probability \\(p\\) required to achieve the first successful event, then \\(X\\) is a random variable with a geometric distribution \\(X \\sim G(p)\\). The probability of \\(X = x\\) trials is \\[f(x; p) = p(1-p)^{x-1}.\\] with \\(E(X)=\\frac{{n}}{{p}}\\) and \\(Var(X) = \\frac{(1-p)}{p^2}\\). The probability of \\(X&lt;=n\\) trials is \\[F(X=n) = 1 - (1-p)^n.\\] Examples What is the probability a marketer encounters x = 3 people on the street who did not attend a sporting event before the first success if the population probability is p = 0.20? \\[f(4; 0.20) = 0.20(1-0.20)^{4-1} = 0.102.\\] Function dgeom() calculates the geometric distribution probability. Parameter x is the number of failures, not the number of trials. dgeom(x = 3, prob = 0.20) ## [1] 0.1 data.frame(cnt = rgeom(n = 10000, prob = 0.20)) %&gt;% count(cnt) %&gt;% top_n(n = 15, wt = n) %&gt;% ungroup() %&gt;% mutate(pct = round(n / sum(n), 3), X_eq_x = cnt == 3) %&gt;% ggplot(aes(x = as.factor(cnt), y = n, fill = X_eq_x, label = pct)) + geom_col(alpha = 0.8) + scale_fill_mf() + geom_text(size = 3) + theme_mf() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Distribution of trials prior to first success&quot;, subtitle = paste(&quot;P(X = 3) | X ~ G(.2) = &quot;, round(dgeom(2, .2), 3)), x = &quot;Unsuccessful trials&quot;, y = &quot;Count&quot;, caption = &quot;simulation of n = 10,000 samples from geometric dist.&quot;) What is the probability the marketer fails to find someone who attended a game in x &lt;= 5 trials before finding someone who attended a game on the sixth trial when the population probability is p = 0.20? p = 0.20 n = 5 # exact pgeom(q = n, prob = p, lower.tail = TRUE) ## [1] 0.74 # simulated mean(rgeom(n = 10000, prob = p) &lt;= n) ## [1] 0.74 What is the probability the marketer fails to find someone who attended a game on x &gt;= 5 trials before finding someone who attended a game on the next trial? p = 0.20 n = 5 # exact pgeom(q = n, prob = p, lower.tail = FALSE) ## [1] 0.26 # simulated mean(rgeom(n = 10000, prob = p) &gt; n) ## [1] 0.26 The expected number of trials to achieve the first success is 1 / 0.20 = 5, Var(X) = (1 - 0.20) / 0.20^2 = 20? p = 0.20 # mean # exact 1 / p ## [1] 5 # simulated mean(rgeom(n = 10000, prob = p)) + 1 ## [1] 5 # Variance # exact (1 - p) / p^2 ## [1] 20 # simulated var(rgeom(n = 100000, prob = p)) ## [1] 20 "],
["hypergeometric.html", "2.7 Hypergeometric", " 2.7 Hypergeometric If \\(X\\) is the count of successful events in a sample of size \\(n\\) without replacement from a population of size \\(N\\) containing \\(K\\) successes and \\(N-K\\) non-successes, then \\(X\\) is a random variable with a hypergeometric distribution \\[f(x|N,K,n) = \\frac{{{K}\\choose{k}}{{N-K}\\choose{n-k}}}{{N}\\choose{n}}.\\] with \\(E(X) = n\\frac{K}{N}\\) and \\(Var(X) = n \\frac{K}{N} \\cdot \\frac{N-n}{N} \\cdot \\frac{N-K}{N-1}\\). The formula follows from the frequency table of the possible outcomes. Sampled Not Sampled Total success k K-k K non-success n-k (N-K)-(n-k) N-K Total n N-n N If \\(X\\) is the count of successful events in a sample of size \\(k\\) without replacement from a population containing \\(M\\) successes and \\(N\\) non-successes, then \\(X\\) is a random variable with a hypergeometric distribution \\[f(x|m,n,k) = \\frac{{{m}\\choose{x}}{{n}\\choose{k-x}}}{{m+n}\\choose{k}}.\\] with \\(E(X)=k\\frac{m}{m+n}\\) and \\(Var(X) = k\\frac{m}{m+n}\\cdot\\frac{m+n-k}{m+n}\\cdot\\frac{n}{m+n-1}\\). phyper returns the cumulative probability (percentile) p at the specified value (quantile) q. qhyper returns the value (quantile) q at the specified cumulative probability (percentile) p. Example What is the probability of selecting \\(X = 14\\) red marbles from a sample of \\(k = 20\\) taken from an urn containing \\(m = 70\\) red marbles and \\(n = 30\\) green marbles? Function dhyper() calculates the hypergeometric probability. x = 14 m = 70 n = 30 k = 20 dhyper(x = x, m = m, n = n, k = k) ## [1] 0.21 The expected value is 14 and variance is 3.39. The hypergeometric random variable is similar to the binomial random variable except that it applies to situations of sampling without replacement from a small population. As the population size increases, sampling without replacement converges to sampling with replacement, and the hypergeometric distribution converges to the binomial. What if the total population size is 250? 500? 1000? "],
["gamma.html", "2.8 Gamma", " 2.8 Gamma If \\(X\\) is the interval until the \\(\\alpha^{th}\\) successful event when the average interval is \\(\\theta\\), then \\(X\\) is a random variable with a gamma distribution \\(X \\sim \\Gamma(\\alpha, \\theta)\\). The probability of an interval of \\(X = x\\) is \\[f(x; \\alpha, \\theta) = \\frac{1}{\\Gamma(\\alpha)\\theta^\\alpha}x^{\\alpha-1}e^{-x/\\theta}.\\] where \\(\\Gamma(\\alpha) = (1 - \\alpha)!\\) with \\(E(X) = \\alpha \\theta\\) and \\(Var(X) = \\alpha \\theta^2\\). Examples On average, someone sends a money order once per 15 minutes (\\(\\theta = .25\\)). What is the probability someone sends \\(\\alpha = 10\\) money orders in less than \\(x = 3\\) hours?* theta = 0.25 alpha = 10 pgamma(q = 3, shape = alpha, scale = 0.25) ## [1] 0.76 data.frame(x = 0:1000 / 100, prob = pgamma(q = 0:1000 / 100, shape = alpha, scale = theta, lower.tail = TRUE)) %&gt;% mutate(Interval = ifelse(x &gt;= 0 &amp; x &lt;= 3, &quot;0 to 3&quot;, &quot;other&quot;)) %&gt;% ggplot(aes(x = x, y = prob, fill = Interval)) + geom_area(alpha = 0.9) + theme_mf() + scale_fill_mf() + labs(title = &quot;X ~ Gam(alpha = 10, theta = .25)&quot;, subtitle = &quot;Probability of 10 events in X hours when the mean time to an event is .25 hours.&quot;, x = &quot;Interval (x)&quot;, y = &quot;pgamma&quot;) "],
["cont-dist.html", "Chapter 3 Continuous Distributions ", " Chapter 3 Continuous Distributions "],
["normal.html", "3.1 Normal", " 3.1 Normal Random variable \\(X\\) is distributed \\(X \\sim N(\\mu, \\sigma^2)\\) if \\[f(X)=\\frac{{1}}{{\\sigma \\sqrt{{2\\pi}}}}e^{-.5(\\frac{{x-\\mu}}{{\\sigma}})^2}\\]. Example IQ scores are distributed \\(X \\sim N(100, 16^2\\). What is the probability a randomly selected person’s IQ is &lt;90? my_mean = 100 my_sd = 16 my_x = 90 # exact pnorm(q = my_x, mean = my_mean, sd = my_sd, lower.tail = TRUE) ## [1] 0.27 # simulated mean(rnorm(n = 10000, mean = my_mean, sd = my_sd) &lt;= my_x) ## [1] 0.27 library(dplyr) library(ggplot2) data.frame(x = 0:1500 / 10, prob = pnorm(q = 0:1500 / 10, mean = my_mean, sd = my_sd, lower.tail = TRUE)) %&gt;% mutate(cdf = ifelse(x &gt; 0 &amp; x &lt;= my_x, prob, 0)) %&gt;% ggplot() + geom_line(aes(x = x, y = prob)) + geom_area(aes(x = x, y = cdf), alpha = 0.3) + labs(title = bquote(&#39;X~N(&#39;~mu==.(my_mean)~&#39;,&#39;~sigma^{2}==.(my_sd)^{2}~&#39;)&#39;), subtitle = bquote(&#39;P(X&lt;=&#39;~.(my_x)~&#39;) when mean is&#39;~.(my_mean)~&#39; and variance is&#39;~.(my_sd)^{2}~&#39;.&#39;), x = &quot;x&quot;, y = &quot;Probability&quot;) 3.1.1 Example IQ scores are distributed \\(X \\sim N(100, 16^2\\). What is the probability a randomly selected person’s IQ is &gt;140? my_mean = 100 my_sd = 16 my_x = 140 # exact pnorm(q = my_x, mean = my_mean, sd = my_sd, lower.tail = FALSE) ## [1] 0.0062 # simulated mean(rnorm(n = 10000, mean = my_mean, sd = my_sd) &gt; my_x) ## [1] 0.0072 library(dplyr) library(ggplot2) data.frame(x = 0:1500 / 10, prob = pnorm(q = 0:1500 / 10, mean = my_mean, sd = my_sd, lower.tail = TRUE)) %&gt;% mutate(cdf = ifelse(x &gt; my_x &amp; x &lt; 1000, prob, 0)) %&gt;% ggplot() + geom_line(aes(x = x, y = prob)) + geom_area(aes(x = x, y = cdf), alpha = 0.3) + labs(title = bquote(&#39;X~N(&#39;~mu==.(my_mean)~&#39;,&#39;~sigma^{2}==.(my_sd)^{2}~&#39;)&#39;), subtitle = bquote(&#39;P(X&lt;=&#39;~.(my_x)~&#39;) when mean is&#39;~.(my_mean)~&#39; and variance is&#39;~.(my_sd)^{2}~&#39;.&#39;), x = &quot;x&quot;, y = &quot;Probability&quot;) 3.1.2 Example IQ scores are distributed \\(X \\sim N(100, 16^2\\). What is the probability a randomly selected person’s IQ is between 92 and 114? my_mean = 100 my_sd = 16 my_x_l = 92 my_x_h = 114 # exact pnorm(q = my_x_h, mean = my_mean, sd = my_sd, lower.tail = TRUE) - pnorm(q = my_x_l, mean = my_mean, sd = my_sd, lower.tail = TRUE) ## [1] 0.5 library(dplyr) library(ggplot2) data.frame(x = 0:1500 / 10, prob = pnorm(q = 0:1500 / 10, mean = my_mean, sd = my_sd, lower.tail = TRUE)) %&gt;% mutate(cdf = ifelse(x &gt; my_x_l &amp; x &lt;= my_x_h, prob, 0)) %&gt;% ggplot() + geom_line(aes(x = x, y = prob)) + geom_area(aes(x = x, y = cdf), alpha = 0.3) + labs(title = bquote(&#39;X~N(&#39;~mu==.(my_mean)~&#39;,&#39;~sigma^{2}==.(my_sd)^{2}~&#39;)&#39;), subtitle = bquote(&#39;P(X&lt;=&#39;~.(my_x)~&#39;) when mean is&#39;~.(my_mean)~&#39; and variance is&#39;~.(my_sd)^{2}~&#39;.&#39;), x = &quot;x&quot;, y = &quot;Probability&quot;) 3.1.3 Example Class scores are distributed \\(X \\sim N(70, 10^2\\). If the instructor wants to give A’s to &gt;=85th percentile and B’s to 75th-85th percentile, what are the cutoffs? my_mean = 70 my_sd = 10 my_pct_l = .75 my_pct_h = .85 qnorm(p = my_pct_l, mean = my_mean, sd = my_sd, lower.tail = TRUE) ## [1] 77 qnorm(p = my_pct_h, mean = my_mean, sd = my_sd, lower.tail = TRUE) ## [1] 80 library(dplyr) library(ggplot2) data.frame(x = 0:1000 / 10, prob = pnorm(q = 0:1000 / 10, mean = my_mean, sd = my_sd, lower.tail = TRUE)) %&gt;% mutate(cdf = ifelse(prob &gt; my_pct_l &amp; prob &lt;= my_pct_h, prob, 0)) %&gt;% ggplot() + geom_line(aes(x = x, y = prob)) + geom_area(aes(x = x, y = cdf), alpha = 0.3) + labs(title = bquote(&#39;X~N(&#39;~mu==.(my_mean)~&#39;,&#39;~sigma^{2}==.(my_sd)^{2}~&#39;)&#39;), subtitle = bquote(&#39;P(X&lt;=x) = [&#39;~.(my_pct_l)~&#39;,&#39;~.(my_pct_h)~&#39;] when mean is&#39;~.(my_mean)~&#39; and variance is&#39;~.(my_sd)^{2}~&#39;.&#39;), x = &quot;x&quot;, y = &quot;Probability&quot;) 3.1.4 Normal Approximation to Binomial The CLT implies that certain distributions can be approximated by the normal distribution. The binomial distribution \\(X \\sim B(n,p)\\) is approximately normal with mean \\(\\mu = n p\\) and variance \\(\\sigma^2=np(1-p)\\). The approximation is useful when the expected number of successes and failures is at least 5: \\(np&gt;=5\\) and \\(n(1-p)&gt;=5\\). 3.1.5 Example A measure requires p&gt;=50% popular to pass. A sample of n=1,000 yields x=460 approvals. What is the probability that the overall population approves, P(X)&gt;0.5? my_x = 460 my_p = 0.50 my_n = 1000 my_mean = my_p * my_n my_sd = round(sqrt(my_n * my_p * (1 - my_p)), 1) # Exact binomial pbinom(q = my_x, size = my_n, prob = my_p, lower.tail = TRUE) ## [1] 0.0062 # Normal approximation pnorm(q = my_x, mean = my_p * my_n, sd = sqrt(my_n * my_p * (1 - my_p)), lower.tail = TRUE) ## [1] 0.0057 library(dplyr) library(ggplot2) library(tidyr) data.frame(x = 400:600, Normal = pnorm(q = 400:600, mean = my_p * my_n, sd = sqrt(my_n * my_p * (1 - my_p)), lower.tail = TRUE), Binomial = pbinom(q = 400:600, size = my_n, prob = my_p, lower.tail = TRUE)) %&gt;% gather(key = &quot;Distribution&quot;, value = &quot;cdf&quot;, c(-x)) %&gt;% ggplot(aes(x = x, y = cdf, color = Distribution)) + geom_line() + labs(title = bquote(&#39;X~B(n=&#39;~.(my_n)~&#39;, p=&#39;~.(my_p)~&#39;), &#39;~&#39;X~N(&#39;~mu==.(my_mean)~&#39;,&#39;~sigma^{2}==.(my_sd)^{2}~&#39;)&#39;), subtitle = &quot;Normal approximation to the binomial&quot;, x = &quot;x&quot;, y = &quot;Probability&quot;) The Poisson distribution \\(x~P(\\lambda)\\) is approximately normal with mean \\(\\mu = \\lambda\\) and variance \\(\\sigma^2 = \\lambda\\), for large values of \\(\\lambda\\). 3.1.6 Example The annual number of earthquakes registering at least 2.5 on the Richter Scale and having an epicenter within 40 miles of downtown Memphis follows a Poisson distribution with mean \\(\\lambda=6.5\\). What is the probability that at least \\(x&gt;=9\\)* such earthquakes will strike next year?* my_x = 9 my_lambda = 6.5 my_sd = round(sqrt(my_lambda), 2) # Exact Poisson ppois(q = my_x - 1, lambda = my_lambda, lower.tail = FALSE) ## [1] 0.21 # Normal approximation pnorm(q = my_x - 0.5, mean = my_lambda, sd = my_sd, lower.tail = FALSE) ## [1] 0.22 library(dplyr) library(ggplot2) library(tidyr) data.frame(x = 0:200 / 10, Normal = pnorm(q = 0:200 / 10, mean = my_lambda, sd = my_sd, lower.tail = TRUE), Poisson = ppois(q = 0:200 / 10, lambda = my_lambda, lower.tail = TRUE)) %&gt;% gather(key = &quot;Distribution&quot;, value = &quot;cdf&quot;, c(-x)) %&gt;% ggplot(aes(x = x, y = cdf, color = Distribution)) + geom_line() + labs(title = bquote(&#39;X~P(&#39;~lambda~&#39;=&#39;~.(my_lambda)~&#39;), &#39;~&#39;X~N(&#39;~mu==.(my_lambda)~&#39;,&#39;~sigma^{2}==.(my_lambda)~&#39;)&#39;), subtitle = &quot;Normal approximation to the Poisson&quot;, x = &quot;x&quot;, y = &quot;Probability&quot;) 3.1.7 From Sample to Population Suppose a person’s blood pressure typically measures 160?20 mm. If one takes n=5 blood pressure readings, what is the probability the average will be &lt;=150? my_mu = 160 my_sigma = 20 my_n = 5 my_x = 150 my_se = round(my_sigma / sqrt(my_n), 1) pnorm(q = my_x, mean = my_mu, sd = my_sigma / sqrt(my_n), lower.tail = TRUE) ## [1] 0.13 library(dplyr) library(ggplot2) data.frame(x = 1000:2000 / 10, prob = pnorm(q = 1000:2000 / 10, mean = my_mu, sd = my_sigma / sqrt(my_n), lower.tail = TRUE)) %&gt;% mutate(cdf = ifelse(x &gt; 0 &amp; x &lt;= my_x, prob, 0)) %&gt;% ggplot() + geom_line(aes(x = x, y = prob)) + geom_area(aes(x = x, y = cdf), alpha = 0.3) + labs(title = bquote(&#39;X~N(&#39;~mu==.(my_mu)~&#39;,&#39;~sigma^{2}==.(my_se)^{2}~&#39;)&#39;), subtitle = bquote(&#39;P(X&lt;=&#39;~.(my_x)~&#39;) when mean is&#39;~.(my_mu)~&#39; and variance is&#39;~sigma~&#39;/sqrt(n)&#39;~.(my_se)^{2}~&#39;.&#39;), x = &quot;x&quot;, y = &quot;Probability&quot;) knitr::include_app(&quot;https://mpfoley73.shinyapps.io/shiny_dist/&quot;, height = &quot;600px&quot;) "],
["join-distributions.html", "3.2 Join Distributions", " 3.2 Join Distributions "],
["likelihood.html", "3.3 Likelihood", " 3.3 Likelihood The likelihood function is the likelihood of a parameter \\(\\theta\\) given an observed value of the random variable \\(X\\). The likelihood function is identical to the probability distribution function, except that it reverses which variable is considered fixed. E.g., the binomial probability distribution expresses the probability that \\(X = x\\) given the success probability \\(\\theta = \\pi\\). \\[f(x|\\pi) = \\frac{n!}{x!(n-x)!} \\pi^x (1-\\pi)^{n-x}.\\] The corresponding likelihood function expresses the probability that \\(\\pi = p\\) given the observed value \\(x\\). \\[L(p|x) = \\frac{n!}{x!(n-x)!} p^x (1-p)^{n-x}.\\] You usually want to know the value of \\(\\theta\\) at the maximum of the likelihood function. When taking derivatives, any multiplicative constant is irrevelant and can be discarded. So for the binomial distribution, the likelihood function for \\(\\pi\\) may instead be expressed as \\[L(p|x) \\propto p^x (1-p)^{n-x}\\] Calculating the maximum is usually simplified using the log-likelihood, \\(l(\\theta|x) = \\log L(\\theta|x)\\). For the binomial distribution, \\(l(p|x) = x \\log p + (n - x) \\log (1 - p)\\). Frequently you derive loglikelihood from a sample. The overall likelihood is the product of the individual likelihoods, and the overall loglikelihood is the log of the overall likelihood. \\[l(\\theta|x) = \\log \\prod_{i=1}^n f(x_i|\\theta)\\] Here are plots of the binomial log-likelihood of \\(pi\\) for several values of \\(X\\) from a sample of size \\(n = 5\\). As the total sample size \\(n\\) grows, the loglikelihood function becomes more sharply peaked around its maximum, and becomes nearly quadratic (i.e. a parabola, if there is a single parameter). Here is the same plot with \\(n = 500\\). The value of \\(\\theta\\) that maximizes \\(l\\) (and \\(L\\)) is the maximum-likelihood estimator (MLE) of \\(\\theta\\), \\(\\hat{\\theta}\\). E.g., suppose you have an experiment of \\(n = 5\\) Bernoulli trials \\(\\left(X \\sim Bin(5, \\pi) \\right)\\) with and \\(X = 3\\) successful events. A plot of \\(L(p|x) = p^3(1 - p)^2\\) shows the MLE is at \\(p = 0.6\\). This approach is called maximum-likelihood estimation. MLE usually involves setting the derivatives to zero and solving for \\(theta\\). "],
["discrete-variables.html", "Chapter 4 Discrete Variables ", " Chapter 4 Discrete Variables "],
["chi-square-test.html", "4.1 Chi-Square Test", " 4.1 Chi-Square Test These notes rely on PSU STAT 500, Wikipedia, and Disha M]. The chi-square test compares observed categorical variable frequency counts \\(O\\) with their expected values \\(E\\). The test statistic \\(X^2 = \\sum (O - E)^2 / E\\) is distributed \\(\\chi^2\\). \\(H_0: O = E\\) and \\(H_a\\) is at least one pair of frequency counts differ. The chi-square test relies on the central limit theorem, so it is valid for independent, normally distributed samples, typically affirmed with at least 5 successes and failures in each cell. There a small variations in the chi-square for its various applications. The chi-square goodness-of-fit test tests whether observed frequency counts \\(O_j\\) of the \\(j \\in (0, 1, \\cdots k)\\) levels of a single categorical variable differ from expected frequency counts \\(E_j\\). \\(H_0\\) is \\(O_j = E_j\\). The chi-square independence test tests whether observed joint frequency counts \\(O_{ij}\\) of the \\(i \\in (0, 1, \\cdots I)\\) levels of categorical variable \\(Y\\) and the \\(j \\in (0, 1, \\cdots J)\\) levels of categorical variable \\(Z\\) differ from expected frequency counts \\(E_{ij}\\) under the independence model where \\(\\pi_{ij} = \\pi_{i+} \\pi_{+j}\\), the joint densities. \\(H_0\\) is \\(O_{ij} = E_{ij}\\). The chi-square homogeneity test tests whether frequency counts of the \\(R\\) levels of a categorical variable are distributed identically across \\(C\\) different populations. "],
["one-way-tables.html", "4.2 One-Way Tables", " 4.2 One-Way Tables These notes rely on PSU STATS 504 course notes. A one-way table is a frequency table for a single categorical variable. You usually construct a one-way table to test whether the frequency counts differ from a hypothesized distribution using the chi-square goodness-of-fit test. You may also simply want to construct a confidence interval around a proportion. Here is an example. A researcher crosses tall cut-leaf tomatoes with dwarf potato-leaf tomatoes, then classifies the (\\(n = 1,611\\)) offspring phenotypes. o &lt;- c(926, 288, 293, 104) cell_names &lt;- c(&quot;tall cut-leaf&quot;, &quot;tall potato-leaf&quot;, &quot;dwarf cut-leaf&quot;, &quot;dwarf potato-leaf&quot;) names(o) &lt;- cell_names print(o) ## tall cut-leaf tall potato-leaf dwarf cut-leaf dwarf potato-leaf ## 926 288 293 104 The four phenotypes are expected to occur with relative frequencies 9:3:3:1. pi &lt;- c(9, 3, 3, 1) / (9 + 3 + 3 + 1) print(pi) ## [1] 0.562 0.188 0.188 0.062 e &lt;- sum(o) * pi names(e) &lt;- cell_names print(e) ## tall cut-leaf tall potato-leaf dwarf cut-leaf dwarf potato-leaf ## 906 302 302 101 data.frame(O = o, E = e) %&gt;% rownames_to_column(var = &quot;i&quot;) %&gt;% pivot_longer(cols = -i, values_to = &quot;freq&quot;) %&gt;% group_by(name) %&gt;% mutate(pct = freq / sum(freq)) %&gt;% ungroup() %&gt;% ggplot(aes(x = i, y = freq, fill = name, label = paste0(round(freq, 0), &quot;\\n&quot;, scales::percent(pct, accuracy = 0.1))) ) + geom_col(position = position_dodge()) + geom_text(position = position_dodge(width = 0.9), size = 2.8) + theme_mf() + scale_fill_mf() + labs(title = &quot;Observed vs Expected&quot;, fill = &quot;&quot;) Do the observed phenotype counts conform to the expected proportions? This is a goodness-of-fit question because you are comparing frequencies from a single categorical variable to a set of hypothesized frequencies. 4.2.1 Chi-Square Goodness-of-Fit Test The chi-square goodness-of-fit test tests whether observed frequency counts \\(O_j\\) of the \\(J\\) levels of a categorical variable differ from expected frequency counts \\(E_j\\) in a sample. \\(H_0\\) is \\(O_j = E_j\\). There are two possible test statistics for this test, Pearson \\(X^2\\) and deviance \\(G^2\\). The sampling distributions of \\(X^2\\) and \\(G^2\\) approach the \\(\\chi_{J-1}^2\\) as the sample size \\(n \\rightarrow \\infty\\). It’s a good idea to calculate both test statistics. The Pearson goodness-of-fit statistic is \\[X^2 = \\sum \\frac{(O_j - E_j)^2}{E_j}\\] where \\(O_j = p_j n\\) and \\(E_j = \\pi_j n\\). The deviance statistic is \\[G^2 = 2 \\sum O_j \\log \\left[ \\frac{O_j}{E_j} \\right]\\] If the saturated model (the observed data represent the fit of the saturated model, the most complex model possible with the data) proportions \\(p_j\\) equal equal the expected proportions \\(\\pi_j\\), \\(X^2\\) and \\(G^2\\) will equal zero. Large values indicate the data do not agree well with the proposed model. You can perform a chi-square test of significance with the \\(G^2\\) and \\(X^2\\) test statistics with \\(dof\\) degrees of freedom (d.f.). The chi-square test is reliable when at least 80% of \\(E_j &gt;= 5\\). Calculate \\(X^2\\) as x2 &lt;- sum((o - e)^2 / e) = 1.47 and the \\(G^2\\) as g2 &lt;- 2 * sum(o * log(o / e)) = 1.48. The degrees of freedom are length(o) - 1 = 3. The chi-sq test p-values are nearly identical. pchisq(q = x2, df = dof, lower.tail = FALSE) ## [1] 0.69 pchisq(q = g2, df = dof, lower.tail = FALSE) ## [1] 0.69 chisq.test() performs the chi-square test of the Pearson test statistic. chisq.test(o, p = pi) ## ## Chi-squared test for given probabilities ## ## data: o ## X-squared = 1, df = 3, p-value = 0.7 The p-values based on the \\(\\chi^2\\) distribution with 3 d.f. are about 0.69, so the test fails to reject the null hypothesis that the observed frequencies are consistent with the theory. The plot of the chi-squared distribution shows \\(X^2\\) well outside the \\(\\alpha = 0.05\\) range of rejection. alpha &lt;- 0.05 dof &lt;- length(e) - 1 lrr = -Inf p_val &lt;- pchisq(q = x2, df = length(o) - 1, lower.tail = FALSE) urr = qchisq(p = alpha, df = dof, lower.tail = FALSE) data.frame(chi2 = seq(from = 0, to = 20, by = .1)) %&gt;% mutate(density = dchisq(x = chi2, df = dof)) %&gt;% mutate(rr = ifelse(chi2 &lt; lrr | chi2 &gt; urr, density, 0)) %&gt;% ggplot() + geom_line(aes(x = chi2, y = density), color = mf_pal(12)(12)[12], size = 0.8) + geom_area(aes(x = chi2, y = rr), fill = mf_pal(12)(12)[2], alpha = 0.8) + geom_vline(aes(xintercept = x2), color = mf_pal(12)(12)[11], size = 0.8) + labs(title = bquote(&quot;Chi-Square Goodness-of-Fit Test&quot;), subtitle = paste0(&quot;X^2=&quot;, round(x2,2), &quot;, &quot;, &quot;Critical value=&quot;, round(urr,2), &quot;, &quot;, &quot;p-value=&quot;, round(p_val,3), &quot;.&quot; ), x = &quot;chisq&quot;, y = &quot;Density&quot;) + theme(legend.position=&quot;none&quot;) + theme_mf() If you reject \\(H_0\\), you can inspect the residuals to learn which differences may have lead to rejecting the rejection. \\(X^2\\) and \\(G^2\\) are sums of squared cell comparisons, or “residuals”. The expected value of a \\(\\chi^2\\) random variable is its d.f., \\(k - 1\\), so the average residual size is \\((k - 1) / k\\). The typical residual should be within 2 \\(\\sqrt{(k - 1) / k}\\). e2_res &lt;- sqrt((o - e)^2 / e) g2_res &lt;- sign(o - e) * sqrt(abs(2 * o * log(o / e))) data.frame(e2_res) %&gt;% rownames_to_column() %&gt;% # pivot_longer(cols = e2_res:g2_res) %&gt;% ggplot(aes(x = rowname, y = e2_res)) + geom_point(size = 3, color = mf_pal(12)(12)[2], alpha = 0.8) + theme_mf() + labs(title = &quot;X^2 Residuals by Cell&quot;, color = &quot;&quot;, x = &quot;&quot;, y = &quot;&quot;) If you want to test whether the data conform to a particular distribution instead of some set of theoretical values, the test is nearly the same except for an adjustment to the d.f. Your first step is the estimate the distribution’s parameter(s). Then you perform the goodness of fit test, but with degrees of freedom reduced for each estimated parameter. For example, suppose you sample \\(n = 100\\) families and count the number of children. The count of children should be a Poisson random variable, \\(J \\sim Pois(\\lambda)\\). dat &lt;- data.frame(j = 0:5, o = c(19, 26, 29, 13, 10, 3)) The ML estimate for \\(\\lambda\\) is \\[\\hat{\\lambda} = \\frac{j_0 O_0 + j_1 O_1, + \\cdots j_k O_k}{O}\\] lambda_hat &lt;- sum(dat$j * dat$o) / sum(dat$o) print(lambda_hat) ## [1] 1.8 The probabilities for each possible count are \\[f(j; \\lambda) = \\frac{e^{-\\hat{\\lambda}} \\hat{\\lambda}^j}{j!}.\\] f &lt;- exp(-lambda_hat) * lambda_hat^dat$j / factorial(dat$j) E &lt;- f * sum(dat$o) dat &lt;- cbind(dat, e = E) dat %&gt;% rename(pois = e) %&gt;% pivot_longer(cols = -j, values_to = &quot;freq&quot;) %&gt;% group_by(name) %&gt;% mutate(pct = freq / sum(freq)) %&gt;% ungroup() %&gt;% ggplot(aes(x = fct_inseq(as.factor(j)), y = freq, fill = name, label = paste0(round(freq, 0), &quot;\\n&quot;, scales::percent(pct, accuracy = 0.1))) ) + geom_col(position = position_dodge()) + geom_text(position = position_dodge(width = 0.9), size = 2.8) + theme_mf() + scale_fill_mf() + labs(title = &quot;Observed vs Expected&quot;, fill = &quot;&quot;, x = &quot;children in family&quot;) Compare the expected values to the observed values with the \\(\\chi^2\\) goodness of fit test. In this case, \\(df = 6 - 1 - 1\\) because the estimated paramater \\(\\lambda\\) reduces d.f. by 1. (X2 &lt;- sum((dat$o - dat$e)^2 / dat$e)) ## [1] 2.8 (dof &lt;- nrow(dat) - 1 - 1) ## [1] 4 pchisq(q = X2, df = dof) ## [1] 0.42 Be careful of this adjustment to the d.f. because chisq.test() does not take this into account, and you cannot override the d.f.. chisq.test(dat$o, p = dat$e / sum(dat$e)) ## Warning in chisq.test(dat$o, p = dat$e/sum(dat$e)): Chi-squared approximation ## may be incorrect ## ## Chi-squared test for given probabilities ## ## data: dat$o ## X-squared = 3, df = 5, p-value = 0.7 4.2.2 Proportion Test A special case of the one-way table is the \\(2 \\times 1\\) table for a binomial random variable. When you calculate a single proportion \\(p\\), you can compare it to a hypothesized \\(\\pi_0\\), or create a confidence interval around the estimate. Suppose a company claims to resolve at least 70% of maintenance requests within 24 hours. In a random sample of \\(n = 50\\) repair requests, the company resolves \\(O_1 = 33\\) (\\(p_1 = 66\\%)\\) within 24 hours. At a 5% level of significance, is the maintenance company’s claim valid? o &lt;- c(33, 17) n &lt;- sum(o) cell_names &lt;- c(&quot;resolved&quot;, &quot;not resolved&quot;) names(o) &lt;- cell_names print(o) ## resolved not resolved ## 33 17 The null hypothesis is that the maintenance company resolves \\(\\pi_0 = 0.70\\) of requests within 24 hours, \\(H_0: \\pi = \\pi_0\\) with alternative hypothesis \\(H_a: \\pi &lt; \\pi_0\\). This is a left-tailed test with an \\(\\alpha = 0.05\\) level of significance. pi_0 &lt;- 0.70 alpha &lt;- 0.05 The sample is independently drawn without replacement from &lt;10% of the population (by assumption) and there were &gt;=5 successes, so you can use the Clopper-Pearson exact binomial test. Clopper-Pearson inverts two single-tailed binomial tests at the desired alpha. binom.test(x = o, p = pi_0, alternative = &quot;less&quot;, conf.level = 1 - alpha) ## ## Exact binomial test ## ## data: o ## number of successes = 33, number of trials = 50, p-value = 0.3 ## alternative hypothesis: true probability of success is less than 0.7 ## 95 percent confidence interval: ## 0.00 0.77 ## sample estimates: ## probability of success ## 0.66 There is insufficient evidence (p = 0.3161) to reject \\(H_0\\) that true probability of success is less than 0.7. x &lt;- c(0:50) p_x &lt;- dbinom(x = x, size = n, prob = pi_0) observed &lt;- factor(if_else(x == o[1], 1, 0)) data.frame(x, p_x, observed) %&gt;% ggplot(aes(x = x, y = p_x, fill = observed)) + geom_col() + theme_mf() + scale_fill_mf() + labs(title = &quot;Exact Binomial&quot;) There were &gt;=5 failures, &gt;=30 observations, and the measured probability of success was within (.2,.80), so you can also use the Wald normal approximation method where \\(\\pi = p \\pm z_{\\alpha/2} SE\\) and \\(Z = (p - \\pi_0) / SE\\) where \\(SE = \\sqrt{\\pi_0 (1 - \\pi_0) / n}\\). p &lt;- o[1] / sum(o) se &lt;- sqrt(pi_0 * (1 - pi_0) / sum(o)) z &lt;- (p - pi_0) / se pnorm(q = p, mean = pi_0, sd = se, lower.tail = TRUE) ## resolved ## 0.27 Again, there is insufficient evidence (p = 0.2685) to reject \\(H_0\\) that true probability of success is less than 0.7. The 95% CI around the measured p = 0.66 is z_alpha &lt;- qnorm(0.95, mean = p, sd = se, lower.tail = FALSE) c(0, p + z_alpha * se) ## resolved ## 0.0 0.7 "],
["two-way-tables.html", "4.3 Two-Way Tables", " 4.3 Two-Way Tables These notes rely on PSU STATS 504 course notes. A two-way frequency table is a frequency table for two categorical variables. You usually construct a two-way table to test whether the frequency counts in one categorical variable differ from the other categorical variable using the chi-square independence test. If there is a significant difference (i.e., the variables are related), then describe the relationship with an analysis of the residuals, calculations of measures of association (difference in proportions, relative risk, or odds ratio), and partition tests. Here are three case studies that illustrate the concepts. The first is a simple 2x2 table. The second is a 3x2 table that extends some of the concepts. The third is a 2x4 table where one factor is ordinal. Study 1: “Vitamin C” 2x2 Table. A double blind study investigated whether vitamin C prevents common colds on a sample of n = 279 persons. This study has two categorical variables each with two levels, a 2x2 two way table. vitc_dat &lt;- matrix( c(31, 17, 109, 122), ncol = 2, dimnames = list( treat = c(&quot;Placebo&quot;, &quot;VitaminC&quot;), resp = c(&quot;Cold&quot;, &quot;NoCold&quot;) ) ) vitc_dat %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot; &quot;) %&gt;% janitor::adorn_totals(where = c(&quot;row&quot;, &quot;col&quot;)) ## Cold NoCold Total ## Placebo 31 109 140 ## VitaminC 17 122 139 ## Total 48 231 279 Study 2: “Smoking” 3x2 Table. An analysis classifies n = 5375 high school students by their smoking behavior and the smoking behavior of their parents. smoke_dat &lt;- matrix( c(400, 416, 188, 1380, 1823, 1168), ncol = 2, dimnames = list( parents = c(&quot;Both&quot;, &quot;One&quot;, &quot;Neither&quot;), student = c(&quot;Smoker&quot;, &quot;Non-smoker&quot;)) ) smoke_dat %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot; &quot;) %&gt;% janitor::adorn_totals(where = c(&quot;row&quot;, &quot;col&quot;)) ## Smoker Non.smoker Total ## Both 400 1380 1780 ## One 416 1823 2239 ## Neither 188 1168 1356 ## Total 1004 4371 5375 Study 3: “Coronary Heart Disease” Ordinal Table. A study of classified n = 1329 patients by cholesterol level and whether they had been diagnosed with coronary heart disease (CHD). # tribble() is a little easier. chd_dat &lt;- tribble( ~L_0_199, ~L_200_219, ~L_220_259, ~L_260p, 12, 8, 31, 41, 307, 246, 439, 245 ) %&gt;% as.matrix() rownames(chd_dat) &lt;- c(&quot;CHD&quot;, &quot;No CHD&quot;) chd_dat %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot; &quot;) %&gt;% janitor::adorn_totals(where = c(&quot;row&quot;, &quot;col&quot;)) ## L_0_199 L_200_219 L_220_259 L_260p Total ## CHD 12 8 31 41 92 ## No CHD 307 246 439 245 1237 ## Total 319 254 470 286 1329 4.3.1 Chi-Square Independence Test The chi-square independence test tests whether observed joint frequency counts \\(O_{ij}\\) differ from expected frequency counts \\(E_{ij}\\) under the independence model (the model of independent explanatory variables, \\(\\pi_{ij} = \\pi_{i+} \\pi_{+j}\\). \\(H_0\\) is \\(O_{ij} = E_{ij}\\). (The independence model is a subset of the saturated model). There are two possible test statistics for this test, Pearson \\(X^2\\), and deviance \\(G^2\\). The sampling distribution of each test statistic approach \\(\\chi^2\\) as \\(n \\rightarrow \\infty\\). The Pearson goodness-of-fit statistic is \\[X^2 = \\sum \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\] where \\(O_{ij}\\) is the observed count, and \\(E_{ij}\\) is the product of the row and column marginal probabilities. For the Vitamin C study, \\(X^2\\) is E &lt;- sum(vitc_dat) * prop.table(vitc_dat, 1) * prop.table(vitc_dat, 2) X2 &lt;- sum((vitc_dat - E)^2 / E) print(X2) ## [1] 4.8 and the deviance statistic is \\[G^2 = 2 \\sum_{ij} O_{ij} \\log \\left( \\frac{O_{ij}}{E_{ij}} \\right)\\] G2 &lt;- - 2 * sum(vitc_dat * log(vitc_dat / E)) print(G2) ## [1] 4.9 \\(\\chi^2\\) and \\(G^2\\) increase with the disagreement between the saturated model proportions \\(p_{ij}\\) and the independence model proportions \\(\\pi_{ij}\\). As \\(n \\rightarrow \\infty\\), \\(G^2\\) and \\(\\chi^2\\) approach \\(\\chi_{k-1}^2\\) where \\(k\\) is the number of n-way table cells. You can therefore run a chi-square test of significance using the \\(G^2\\) and \\(\\chi^2\\) test statistics. The degrees of freedom for \\(E^2\\) and \\(G^2\\) equals the degrees of freedom for the saturated model, \\(I \\times J - 1\\), minus the degrees of freedom from the independence model, \\((I - 1) + (J - 1)\\), which you can algebraically solve for \\((I - 1)(J - 1)\\). dof &lt;- (nrow(vitc_dat) - 1) * (ncol(vitc_dat) - 1) print(dof) ## [1] 1 The associated p-values are pchisq(q = G2, df = dof, lower.tail = FALSE) ## [1] 0.027 pchisq(q = X2, df = dof, lower.tail = FALSE) ## [1] 0.028 The chisq.test() function applies the Yates’s continuity correcton by default to correct for situations with small cell counts by subtracting 0.5 from the observed - expected differences. chisq.test(vitc_dat, correct = FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: vitc_dat ## X-squared = 5, df = 1, p-value = 0.03 The correction yields more conservative p-values. vitc_chisq_test &lt;- chisq.test(vitc_dat) print(vitc_chisq_test) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: vitc_dat ## X-squared = 4, df = 1, p-value = 0.04 The p-values indicate strong evidence for rejecting the independence model. 4.3.2 Residuals Analysis The residuals in a Chi-squared test indicate which cells drive the lack of fit. The Pearson residuals in a two-way table are \\[r_{ij} = \\frac{O_{ij} - E_{ij}}{\\sqrt{E_{ij}}}\\] and the \\(r_{ij}\\) values have a normal distribution with mean 0, but with unequal variances. The standardized Pearson residual for a two-way table is \\[r_{ij} = \\frac{O_{ij} - E_{ij}}{\\sqrt{E_{ij}(1 - p_{i+})(1 - p_{+j})}}\\] and the \\(r_{ij}\\) values do have a \\(\\sim N(0, 1)\\) distribution. The chissq.test() object includes residuals that match the manual calculation. (r &lt;- (vitc_dat - E) / sqrt(E)) ## resp ## treat Cold NoCold ## Placebo -1.4 0.64 ## VitaminC 1.4 -0.64 vitc_chisq_test$residuals ## resp ## treat Cold NoCold ## Placebo 1.4 -0.64 ## VitaminC -1.4 0.64 It also includes stdres that match the manual standardized calculation. (well, no it doesn’t, but I don’t know what my mistake is.) p &lt;- prop.table(vitc_dat, margin = 1) (r_std &lt;- ( E - vitc_dat) / sqrt(E * (1 - prop.table(vitc_dat, margin = 1)) * (1 - prop.table(vitc_dat, margin = 2)) )) ## resp ## treat Cold NoCold ## Placebo 2.7 -1.9 ## VitaminC -1.9 2.7 vitc_chisq_test$stdres ## resp ## treat Cold NoCold ## Placebo 2.2 -2.2 ## VitaminC -2.2 2.2 4.3.3 Difference in Proportions The difference in proportions measure is the difference in the relative frequency of a characteristic \\(Z\\) between two groups \\(Y = 1\\) and \\(Y = 2\\): \\(\\delta = \\pi_{1|1} - \\pi_{1|2}\\). In social sciences and epidemiology \\(\\pi_{1|1}\\) and \\(\\pi_{1|2}\\) are sometimes referred to as “risk” values. The point estimate for \\(\\delta\\) is \\(d = p_{1|1} - p_{1|2}\\). Under the normal approximation method, the sampling distribution of the difference in population proportions has a normal distribution centered at \\(d\\) with variance \\(Var(\\delta)\\). The point estimate for \\(Var(\\delta)\\) is \\(Var(d)\\). \\[Var(d) = \\frac{p_{1|1} (1 - p_{1|1})}{n_{1+}} + \\frac{p_{1|2} (1 - p_{1|2})}{n_{2+}}\\] In the vitamin C acid example, \\(\\delta\\) is the difference in the row conditional frequencies. p &lt;- prop.table(vitc_dat, margin = 1) d &lt;- p[2, 1] - p[1, 1] print(d) ## [1] -0.099 The variance is var_d &lt;- (p[2, 1])*(1 - p[2, 1]) / sum(vitc_dat[2, ]) + (p[1, 1])*(1 - p[1, 1]) / sum(vitc_dat[1, ]) The 95% CI is d + c(-1, 1) * qnorm(.975) * sqrt(var_d) ## [1] -0.187 -0.011 This is how prop.test() without the continuity correction calculates the confidence interval. (prop.test.result &lt;- prop.test(vitc_dat, correct = FALSE)) ## ## 2-sample test for equality of proportions without continuity ## correction ## ## data: vitc_dat ## X-squared = 5, df = 1, p-value = 0.03 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## 0.011 0.187 ## sample estimates: ## prop 1 prop 2 ## 0.22 0.12 lcl &lt;- -round(prop.test.result$conf.int[2], 3) ucl &lt;- -round(prop.test.result$conf.int[1], 3) data.frame(d_i = -300:300 / 1000) %&gt;% mutate(density = dnorm(x = d_i, mean = d, sd = sqrt(var_d))) %&gt;% mutate(rr = ifelse(d_i &lt; lcl | d_i &gt; ucl, density, 0)) %&gt;% ggplot() + geom_line(aes(x = d_i, y = density)) + geom_area(aes(x = d_i, y = rr, fill = mf_pal(12)(12)[2]), alpha = 0.8) + geom_vline(aes(xintercept = d), color = &quot;blue&quot;) + theme_mf() + labs(title = bquote(&quot;Difference in Proportions Confidence Interval&quot;), subtitle = paste0( &quot;d = &quot;, round(d, 3) ), x = &quot;d&quot;, y = &quot;Density&quot;) + theme(legend.position=&quot;none&quot;) The normal approximation method applies when the central limit theorem conditions hold: the sample is independently drawn (random sampling without replacement from \\(n &lt; 10\\%\\) of the population in observational studies, or random assignment in experiments), there are at least \\(n_i p_i &gt;= 5\\) successes and \\(n_i (1 - p_i) &gt;= 5\\) failures for each group, the sample sizes are both \\(&gt;=30\\), and the probability of success for each group is not extreme, \\((0.2, 0.8)\\). Test \\(H_0: d = \\delta_0\\) for some hypothesized population \\(\\delta\\) (usually 0) with test statistic \\[Z = \\frac{d - \\delta_0}{se_{d}}\\] where \\[se_{d} = \\sqrt{p (1 - p) \\left( \\frac{1}{n_{1+}} + \\frac{1}{n_{2+}} \\right)}\\] approximates \\(se_{\\delta_0}\\) where \\(p\\) is the pooled proportion \\[p = \\frac{n_{11} + n_{21}}{n_{1+} + n_{2+}}.\\] p_pool &lt;- (vitc_dat[1, 1] + vitc_dat[2, 1]) / sum(vitc_dat) se_d &lt;- sqrt(p_pool * (1 - p_pool) * (1 / sum(vitc_dat[1, ]) + 1 / sum(vitc_dat[2, ]))) z &lt;- (d - 0) / se_d pnorm(z) * 2 ## [1] 0.028 lrr = qnorm(p = .05/2, mean = 0, sd = se_d, lower.tail = TRUE) urr = qnorm(p = .05/2, mean = 0, sd = se_d, lower.tail = FALSE) data.frame(d_i = -300:300 / 1000) %&gt;% mutate(density = dnorm(x = d_i, mean = 0, sd = se_d)) %&gt;% mutate(rr = ifelse(d_i &lt; lrr | d_i &gt; urr, density, 0)) %&gt;% ggplot() + geom_line(aes(x = d_i, y = density)) + geom_area(aes(x = d_i, y = rr, fill = mf_pal(12)(12)[2]), alpha = 0.8) + geom_vline(aes(xintercept = d), color = &quot;blue&quot;) + geom_vline(aes(xintercept = 0), color = &quot;black&quot;) + theme_mf() + labs(title = &quot;Hypothesis Test of Difference in Proportions&quot;, subtitle = paste0( &quot;d = &quot;, round(d, 3), &quot; (Z = &quot;, round(z, 2), &quot;, p = &quot;, round(pnorm(z) * 2, 4), &quot;).&quot; ), x = &quot;d&quot;, y = &quot;Density&quot;) + theme(legend.position=&quot;none&quot;) The null hypothesis \\(H_0: \\delta_0 = 0\\) is equivalent to saying that two variables are independent, \\(\\pi_{1|1} = \\pi_{1|2}\\), so you can also use the \\(\\chi^2\\) or \\(G^2\\) test for independence in a 2 × 2. That’s what prop.test() is doing. The square of the z-statistic is algebraically equal to \\(\\chi^2\\). The two-sided test comparing \\(Z\\) to a \\(N(0, 1)\\) is identical to comparing \\(\\chi^2\\) to a chi-square distribution with df = 1. Compare the \\(Z^2\\) to the output from prop.test(). z^2 ## [1] 4.8 Even though difference of two proportions is very easy to interpret, one problem with using δ is that when Z = 1 is a rare event, the individual probabilities P(Z = 1 | Y = 1) and P(Z = 1 | Y = 2) are both small, i.e., close to zero. Absolute value of δ will be close to zero even when the effect is strong. In the following sections we study two other common measures of association which compare the relative value of the proportions, rather than the absolute values. One problem with evaluating \\(\\delta\\) is that when \\(Z = 1\\) is a rare event, the individual probabilities \\(\\pi_{1|1}\\) and \\(\\pi_{1|2}\\) are both close to zero, and so \\(\\delta\\) is also close to zero even when the effect is strong. More notes online at PSU STAT 500, PSU STAT 504, and Stat Trek.] 4.3.4 Relative Risk The relative risk measure is the ratio of the probabilities of characteristic \\(Z\\) conditioned on two groups \\(Y = 1\\) and \\(Y = 2\\): \\(\\rho = \\pi_{1|1} / \\pi_{1|2}\\). In social sciences and epidemiology \\(\\rho\\) is sometimes referred to as the “relative risk”. The point estimate for \\(\\rho\\) is \\(r = p_{1|1} / p_{1|2}\\). Because \\(\\rho\\) is non-negative, a normal approximation for \\(\\log \\rho\\) has a less skewed distribution than \\(\\rho\\). The approximate variance of \\(\\log \\rho\\) is \\[Var(\\log \\rho) = \\frac{1 - \\pi_{11}/\\pi_{1+}}{n_{1+}\\pi_{11}/\\pi_{1+}} + \\frac{1 - \\pi_{21}/\\pi_{2+}}{n_{2+}\\pi_{21}/\\pi_{2+}}\\] which is estimated by \\[Var(\\log r) = \\left( \\frac{1}{n_{11}} - \\frac{1}{n_{1+}} \\right) + \\left( \\frac{1}{n_{21}} - \\frac{1}{n_{2+}} \\right)\\] In the vitamin C acid example, \\(r\\) is the ratio of the row conditional frequencies. p &lt;- prop.table(vitc_dat, margin = 1) r &lt;- p[2, 1] / p[1, 1] print(r) ## [1] 0.55 The variance is var_r &lt;- 1 / vitc_dat[1, 1] - 1 / sum(vitc_dat[1, ]) + 1 / vitc_dat[2, 1] - 1 / sum(vitc_dat[2, ]) The 95% CI is exp(log(r) + c(-1, 1) * qnorm(.975) * sqrt(var_r)) ## [1] 0.32 0.95 Thus, at 0.05 level, you can reject the independence model. People taking vitamin C are half as likely to catch a cold. 4.3.5 Odds Ratio The odds ratio is the most commonly used measure of association. It is also a natural parameter for many of the log-linear and logistic models. The odds is the ratio of probabilities of “success” and “failure”. When conditioned on a variable, the odds ratio is \\[\\theta = \\frac{\\pi_{1|1} / \\pi_{2|1}} {\\pi_{1|2} / \\pi_{2|2}}\\] and is estimated by the sample frequencies \\[\\hat{\\theta} = \\frac{n_{11} n_{22}} {n_{12} n_{21}}\\] The log-odds ratio has a better normal approximation than the odds ratio, so define a confidence interval on the log scale. \\[Var(\\log \\hat{\\theta}) = \\frac{1}{n_{11}} + \\frac{1}{n_{12}} + \\frac{1}{n_{21}} + \\frac{1}{n_{22}}\\] For the Vitamin C example, the odds of getting a cold given a skier took vitamin C, are The odds of getting a cold after taking a placebo pill are \\(0.22 / 0.78 = 0.28\\) and the odds of getting a cold after taking Vitamin C are \\(0.12 / 0.88 = 0.14\\). odds &lt;- p[, 1] / p[, 2] The odds of getting a cold given vitamin C are \\(0.14 / 0.28 = 0.49\\) times the odds of getting cold given a placebo. theta_hat &lt;- odds[2] / odds[1] print(theta_hat) ## VitaminC ## 0.49 with variance var_theta_hat &lt;- sum(1 / vitc_dat) The 95% CI is z_alpha &lt;- qnorm(p = 0.975) exp(log(theta_hat) + c(-1, 1) * z_alpha * sqrt(var_theta_hat)) ## [1] 0.26 0.93 Keep in mind the following properties of odds ratios. You can convert an odds pack to probabilities by solving \\(\\pi / (1 - \\pi)\\) for \\(\\pi = odds / (1 + odds)\\). If two variables are independent, then the conditional probabilities \\(\\pi_{1|1}\\) and \\(\\pi_{1|2}\\) will be equal and therefore the odds ratio will equal 1. If \\(\\pi_{1|1} &gt; \\pi_{1|2}\\) then the odds ratio will be \\(1 &lt; \\theta &lt; \\infty\\). If \\(\\pi_{1|1} &lt; \\pi_{1|2}\\) then the odds ratio will be \\(0 &lt; \\theta &lt; 1\\). the sample odds ratio will equal \\(0\\) or \\(\\infty\\) if any \\(n_{ij} = 0\\). If you have any empty cells add 1/2 to each cell count. "],
["example-of-chi-square-test-of-homogeneity.html", "4.4 Example of Chi-Square Test of Homogeneity", " 4.4 Example of Chi-Square Test of Homogeneity A project studied whether attending physicians order more unnecessary blood transfusions than residents. The categorical variable frequency of orders has 4 levels: frequently, occasionally, rarely, and never. library(dplyr) library(ggplot2) library(stats) pop &lt;- NULL pop[1:49] &lt;- 1 pop[50:120] &lt;- 2 lev &lt;- NULL lev[c(1:2, 50:64)] &lt;- 1 lev[c(3:5, 65:92)] &lt;- 2 lev[c(6:36, 93:115)] &lt;- 3 lev[c(37:49, 116:120)] &lt;- 4 dat &lt;- data.frame(pop, lev) dat$pop &lt;- factor(dat$pop, levels = c(1, 2), labels = c(&quot;attending&quot;, &quot;resident&quot;)) dat$lev &lt;- factor(dat$lev, levels = c(1, 2, 3, 4), labels = c(&quot;frequently&quot;, &quot;occasionally&quot;, &quot;rarely&quot;, &quot;never&quot;)) df &lt;- (2-1)*(4-1) alpha &lt;- 0.05 (test &lt;- chisq.test(dat$lev, dat$pop)) ## ## Pearson&#39;s Chi-squared test ## ## data: dat$lev and dat$pop ## X-squared = 32, df = 3, p-value = 0.0000006 # Graph of hypothesis test lrr = -Inf urr = qchisq(p = alpha, df = df, lower.tail = FALSE) data.frame(xi = 0:400 / 10) %&gt;% mutate(density = dchisq(x = xi, df = df)) %&gt;% mutate(rr = ifelse(xi &lt; lrr | xi &gt; urr, density, 0)) %&gt;% ggplot() + geom_line(aes(x = xi, y = density), color = &quot;black&quot;) + geom_area(aes(x = xi, y = rr), fill = &quot;red&quot;, alpha = 0.3) + geom_vline(aes(xintercept = test$statistic), color = &quot;blue&quot;) + labs(title = bquote(&quot;Chi-Square Test for Homogeneity&quot;), subtitle = bquote(&quot;P-value =&quot;~.(test$p.value)), x = &quot;xi^2&quot;, y = &quot;Density&quot;) + theme(legend.position=&quot;none&quot;) "],
["inference.html", "Chapter 5 Inference", " Chapter 5 Inference Categorical data is usually generated from unrestricted sampling (Poisson), fixed sample size sampling (binomial or multinomial) Poisson sampling assumes the random data generating mechanism can be described by a Poisson distribution. It is useful for modeling events occuring over a fixed period of time or space. It is useful when the event probability is very small and the event count is very large. "],
["regression.html", "Chapter 6 Regression", " Chapter 6 Regression "],
["generalized-linear-models.html", "Chapter 7 Generalized Linear Models", " Chapter 7 Generalized Linear Models These notes are primarily from PSU Online course Analysis of Discrete Data which uses Alan Agresti’s Categorical Data Analysis (which I have not yet purchased) (Agresti 2013). I also reviewed: Penn State University, STAT 501, “Lesson 15: Logistic, Poisson &amp; Nonlinear Regression”. https://newonlinecourses.science.psu.edu/stat501/lesson/15 “Generalized Linar Models in R”. DataCamp. https://www.datacamp.com/courses/generalized-linear-models-in-r. “Multiple and Logistic Regression”. DataCamp. https://www.datacamp.com/courses/multiple-and-logistic-regression. Molnar, Christoph. “Interpretable machine learning. A Guide for Making Black Box Models Explainable”, 2019. https://christophm.github.io/interpretable-ml-book/. In generalized linear models (GLMs), the modeled response is a function of the mean of \\(y\\). There are three components to a GLM. The random component is the probability distribution of the response variable (normal, binomial, Poisson, etc.). The systematic component is the explanatory variables \\(X\\beta\\). The link function \\(\\eta\\) specifies the link between random and systematic components. The link function converts the response value from a range \\([0,1]\\) in logistic and probit and \\([0,+\\infty]\\) for Poisson to a value ranging from \\([-\\infty, +\\infty]\\), and creates a linear relationship with the predictor variables. For a standard linear regression, the link function is the identity function, \\[f(\\mu_Y) = \\mu_Y.\\] The standard linear regression is thus a special case of the GLM. For a logistic regression, the link function is \\[f(\\mu_Y) = \\ln(\\frac{\\pi}{1-\\pi})\\] where \\(\\pi\\) is the event probability. For a probit regression, the link function is \\[f(\\mu_Y) = \\Phi^{-1}(\\pi).\\] The difference between logistic and probit link function is theoretical - the practical significance is slight. Logistic regression has the advantage that it can be back-transformed from log odds to odds ratios. For a Poisson regression, the link function is \\[f(\\mu_Y) = \\ln(\\lambda)\\] where \\(\\lambda\\) is the expected event rate. GLM uses maximum likelihood estimation (MLE) rather than ordinary least squares (OLS) to estimate the parameters, and thus relies on large-sample approximations. In R, specify a GLM just like an linear model, but with the glm() function, specifying the distribution with the family parameter. family = \"gaussian\": linear regression family = \"binomial\": logistic regression family = binomial(link = \"probit\"): probit regression family = \"poisson\": Poisson regression References "],
["logistic-regression.html", "7.1 Logistic Regression", " 7.1 Logistic Regression Logistic regression estimates the probability of a particular level of a categorical response variable given a set of predictors. The response levels can be binary, nominal (multiple categories), or ordinal (multiple levels). The binary logistic regression model is \\[y_i = logit(\\pi_i) = \\log\\left(\\frac{\\pi_i}{1-\\pi_i}\\right) = X_i\\beta\\] where \\(\\pi_i\\) is the “success probability” that observation \\(i\\) is in a specified category of the binary y variable. You can solve for \\(\\pi\\) to get \\[\\pi = \\frac{\\exp(X \\beta)}{1 + \\exp(X \\beta)}.\\] The model predicts the log odds of the response variable. The maximum likelihood estimator maximizes the the likelihood function \\[L(\\beta; y, X) = \\prod_{i=1}^n \\pi_i^{y_i}(1 - \\pi_i)^{(1-y_i)} = \\prod_{i=1}^n\\frac{\\exp(y_i X_i \\beta)}{1 + \\exp(X_i \\beta)}.\\] There is no closed-form solution, so GLM estimates coefficients with interatively reweighted least squares. Example Dataset leuk contains response variable REMISS indicating whether leukemia remission occurred (1|0) and several explanatory variables. data_dir &lt;- &quot;C:/Users/mpfol/OneDrive/Documents/Data Science/Data/&quot; leuk &lt;- read_tsv(paste(data_dir, &quot;leukemia_remission.txt&quot;, sep = &quot;/&quot;)) ## Parsed with column specification: ## cols( ## REMISS = col_double(), ## CELL = col_double(), ## SMEAR = col_double(), ## INFIL = col_double(), ## LI = col_double(), ## BLAST = col_double(), ## TEMP = col_double() ## ) str(leuk) ## Classes &#39;spec_tbl_df&#39;, &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 27 obs. of 7 variables: ## $ REMISS: num 1 1 0 0 1 0 1 0 0 0 ... ## $ CELL : num 0.8 0.9 0.8 1 0.9 1 0.95 0.95 1 0.95 ... ## $ SMEAR : num 0.83 0.36 0.88 0.87 0.75 0.65 0.97 0.87 0.45 0.36 ... ## $ INFIL : num 0.66 0.32 0.7 0.87 0.68 0.65 0.92 0.83 0.45 0.34 ... ## $ LI : num 1.9 1.4 0.8 0.7 1.3 0.6 1 1.9 0.8 0.5 ... ## $ BLAST : num 1.1 0.74 0.18 1.05 0.52 0.52 1.23 1.35 0.32 0 ... ## $ TEMP : num 1 0.99 0.98 0.99 0.98 0.98 0.99 1.02 1 1.04 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. REMISS = col_double(), ## .. CELL = col_double(), ## .. SMEAR = col_double(), ## .. INFIL = col_double(), ## .. LI = col_double(), ## .. BLAST = col_double(), ## .. TEMP = col_double() ## .. ) Fit a logistic regression in R using glm(formula, data, family = binomial) where family = binomial specifies a binomial error distribution. m1 &lt;- glm(REMISS ~ ., family = binomial, data = leuk) m1 ## ## Call: glm(formula = REMISS ~ ., family = binomial, data = leuk) ## ## Coefficients: ## (Intercept) CELL SMEAR INFIL LI BLAST ## 64.2581 30.8301 24.6863 -24.9745 4.3605 -0.0115 ## TEMP ## -100.1734 ## ## Degrees of Freedom: 26 Total (i.e. Null); 20 Residual ## Null Deviance: 34 ## Residual Deviance: 22 AIC: 36 summary(m1) ## ## Call: ## glm(formula = REMISS ~ ., family = binomial, data = leuk) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9540 -0.6626 -0.0252 0.7818 1.5747 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 64.2581 74.9648 0.86 0.39 ## CELL 30.8301 52.1352 0.59 0.55 ## SMEAR 24.6863 61.5260 0.40 0.69 ## INFIL -24.9745 65.2809 -0.38 0.70 ## LI 4.3605 2.6580 1.64 0.10 ## BLAST -0.0115 2.2663 -0.01 1.00 ## TEMP -100.1734 77.7529 -1.29 0.20 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 34.372 on 26 degrees of freedom ## Residual deviance: 21.594 on 20 degrees of freedom ## AIC: 35.59 ## ## Number of Fisher Scoring iterations: 8 The predicted value \\(\\hat{y}\\) is the estimated log odds of the response variable, \\[\\hat{y} = X \\hat{\\beta} = \\ln (\\frac{\\pi}{1 - \\pi}).\\] Suppose each predictor equals its mean value, then the log odds of REMISS is \\(-2.684\\). pred &lt;- predict(m1, newdata = data.frame(CELL = mean(leuk$CELL), SMEAR = mean(leuk$SMEAR), INFIL = mean(leuk$INFIL), LI = mean(leuk$LI), BLAST = mean(leuk$BLAST), TEMP = mean(leuk$TEMP))) Log odds are not easy to interpet, but it is convenient for updating prior probabilities in Bayesian analyses. See this article in Statistics How To. Exponentiate the log odds to get the more intuitive odds. \\[\\exp (\\hat{y}) = \\exp (X \\hat{\\beta}) = \\frac{\\pi}{1 - \\pi}.\\] The odds of having achieved remission when each predictor equals its mean value is \\(\\exp(\\hat{y}) = 0.068\\). exp(pred) ## 1 ## 0.068 You might express that more commonly as 1 / 0.068 = 15:1. So a person with average values of the predictors has an odds of “15 to 1” of having achieved remission. 1/exp(pred) ## 1 ## 15 Or, solve for \\(\\pi\\) to get the probability. \\[\\pi = \\frac{\\exp (X \\beta)}{1 + \\exp (X \\beta)}\\] The probability of having achieved remission when each predictor equals its mean value is \\(\\pi = 0.064\\). The predict() function for a logistic model returns log-odds, but can also return \\(\\pi\\) by specifying parameter type = \"response\". exp(pred) / (1 + exp(pred)) ## 1 ## 0.064 prob &lt;- predict(m1, newdata = data.frame(CELL = mean(leuk$CELL), SMEAR = mean(leuk$SMEAR), INFIL = mean(leuk$INFIL), LI = mean(leuk$LI), BLAST = mean(leuk$BLAST), TEMP = mean(leuk$TEMP)), type = &quot;response&quot;) It is common to express the results in terms of the odds ratio. The odds ratio is the ratio of the odds before and after an increment to the predictors. It tells you how much the odds would be multiplied after a \\(X_1 - X_0\\) unit increase in \\(X\\). \\[\\theta = \\frac{\\pi / (1 - \\pi) |_{X = X_1}}{\\pi / (1 - \\pi) |_{X = X_0}} = \\frac{\\exp (X_1 \\hat{\\beta})}{\\exp (X_0 \\hat{\\beta})} = \\exp ((X_1-X_0) \\hat{\\beta}) = \\exp (\\delta \\hat{\\beta})\\] For example, increasing LI by .01 increases the odds of remission by a factor of \\(\\exp(0.1 \\cdot 4.36) = 1.547\\) (from 15:1 to 23:1). exp(.1 * m1$coefficients) ## (Intercept) CELL SMEAR INFIL LI BLAST ## 617.579877 21.823908 11.806280 0.082295 1.546579 0.998848 ## TEMP ## 0.000045 You can calculate an odds ratio using oddsratio::or_glm(). library(oddsratio) or_glm(data = leuk, model = m1, incr = list(CELL = 0.01, SMEAR = 0.01, INFIL = 5, LI = 0.1, BLAST = 1.0, TEMP = 0.3)) ## # A tibble: 6 x 5 ## predictor oddsratio `CI_low (2.5)` `CI_high (97.5)` increment ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 CELL 1.36 0.747 4.64e 0 0.01 ## 2 SMEAR 1.28 0.537 5.15e 0 0.01 ## 3 INFIL 0 0 1.09e152 5 ## 4 LI 1.55 1.04 2.99e 0 0.1 ## 5 BLAST 0.989 0.009 9.10e 1 1 ## 6 TEMP 0 0 1.10e 3 0.3 The predicted values can also be expressed as the probabilities \\(\\pi\\). This produces the familiar signmoidal shape of the binary relationship. augment(m1, type.predict = &quot;response&quot;) %&gt;% ggplot(aes(x = LI, y = REMISS)) + geom_point() + geom_line(aes(y = .fitted), color = &quot;red&quot;) + labs(x = &quot;LI&quot;, y = &quot;Probability of Event&quot;, title = &quot;Binary Fitted Line Plot&quot;) Whereas in linear regression the the coefficient p-values use the t test (t statistic), logistic regression coefficient p-values use the Wald test **Z*-statistic). \\[Z = \\frac{\\hat{\\beta_i}}{SE(\\hat{\\beta}_i)}\\] round((z &lt;- m1$coefficients / summary(m1)$coefficients[,&quot;Std. Error&quot;]), 3) ## (Intercept) CELL SMEAR INFIL LI BLAST ## 0.857 0.591 0.401 -0.383 1.641 -0.005 ## TEMP ## -1.288 round(pnorm(abs(z), lower.tail = FALSE) * 2, 3) ## (Intercept) CELL SMEAR INFIL LI BLAST ## 0.39 0.55 0.69 0.70 0.10 1.00 ## TEMP ## 0.20 Evaluate a logistic model fit with an analysis of deviance. Deviance is defined as -2 times the log-likelihood \\(-2l(\\beta)\\). The null deviance is the deviance of the null model and is analagous to SST in ANOVA. The residual deviance is analagous to SSE in ANOVA. logLik(glm(REMISS ~ ., data = leuk, family = &quot;binomial&quot;)) * (-2) ## &#39;log Lik.&#39; 22 (df=7) anova(m1) ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: REMISS ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev ## NULL 26 34.4 ## CELL 1 2.58 25 31.8 ## SMEAR 1 0.52 24 31.3 ## INFIL 1 0.29 23 31.0 ## LI 1 6.78 22 24.2 ## BLAST 1 0.33 21 23.9 ## TEMP 1 2.28 20 21.6 m1 ## ## Call: glm(formula = REMISS ~ ., family = binomial, data = leuk) ## ## Coefficients: ## (Intercept) CELL SMEAR INFIL LI BLAST ## 64.2581 30.8301 24.6863 -24.9745 4.3605 -0.0115 ## TEMP ## -100.1734 ## ## Degrees of Freedom: 26 Total (i.e. Null); 20 Residual ## Null Deviance: 34 ## Residual Deviance: 22 AIC: 36 summary(m1) ## ## Call: ## glm(formula = REMISS ~ ., family = binomial, data = leuk) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9540 -0.6626 -0.0252 0.7818 1.5747 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 64.2581 74.9648 0.86 0.39 ## CELL 30.8301 52.1352 0.59 0.55 ## SMEAR 24.6863 61.5260 0.40 0.69 ## INFIL -24.9745 65.2809 -0.38 0.70 ## LI 4.3605 2.6580 1.64 0.10 ## BLAST -0.0115 2.2663 -0.01 1.00 ## TEMP -100.1734 77.7529 -1.29 0.20 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 34.372 on 26 degrees of freedom ## Residual deviance: 21.594 on 20 degrees of freedom ## AIC: 35.59 ## ## Number of Fisher Scoring iterations: 8 The deviance of the null model (no regressors) is 34.372. The deviance of the full model is 26.073. glance(m1) ## # A tibble: 1 x 7 ## null.deviance df.null logLik AIC BIC deviance df.residual ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 34.4 26 -10.8 35.6 44.7 21.6 20 Use the GainCurvePlot() function to plot the gain curve (background on gain curve at Data Science Central from the model predictions. The x-axis is the fraction of items seen when sorted by the predicted value, and the y-axis is the cumulative summed true outcome. The “wizard” curve is the gain curve when the data is sorted by the true outcome. If the model’s gain curve is close to the wizard gain curve, then the model sorted the response variable well. The grey area is the gain over a random sorting. augment(m1) %&gt;% data.frame() %&gt;% GainCurvePlot(xvar = &quot;.fitted&quot;, truthVar = &quot;REMISS&quot;, title = &quot;Logistic Model&quot;) REMISS equals 1 in 9 of the 27 responses. The wizard curve shows that after sorting the responses it encounters all 9 1s (100%) after looking at 9 of the 27 response (33%). The bottom of the grey diagonal shows that after making random predictions and sorting the predictions, it encounters only 3 1s (33%) after looking at 9 of the 27 responses (33%). It has to look at all 27 responses (100%) to encounter all 9 1s (100%). The gain curve encounters 5 1s (55%) after looking at 9 of the 27 responses (33%). It has to look at 14 responses to encounter all 9 1s (100%). Another way to evaluate the predictive model is the ROC curve. It evaluates all possible thresholds for splitting predicted probabilities into predicted classes. This is often a much more useful metric than simply ranking models by their accuracy at a set threshold, as different models might require different calibration steps (looking at a confusion matrix at each step) to find the optimal classification threshold for that model. library(caTools) ## Warning: package &#39;caTools&#39; was built under R version 3.6.2 colAUC(m1$fitted.values, m1$data$REMISS, plotROC = TRUE) ## [,1] ## 0 vs. 1 0.9 "],
["multinomial-logistic-regression.html", "7.2 Multinomial Logistic Regression", " 7.2 Multinomial Logistic Regression The following notes rely heavily on the [PSU STAT 504 course notes](https://online.stat.psu.edu/stat504/node/171/. Multinomial logistic regression models the odds the multinomial response variable \\(Y \\sim Mult(n, \\pi)\\) is in level \\(j\\) relative to baseline category \\(j^*\\) for all pairs of categories as a function of \\(k\\) explanatory variables, \\(X = (X_1, X_2, ... X_k)\\). \\[\\log \\left( \\frac{\\pi_{ij}}{\\pi_{ij^*}} \\right) = x_i^T \\beta_j, \\hspace{5mm} j \\ne j^2\\] Interpet the \\(k^{th}\\) element of \\(\\beta_j\\) as the increase in log-odds of falling a response in category \\(j\\) relative to category \\(j^*\\) resulting from a one-unit increase in the \\(k^{th}\\) predictor term, holding the other terms constant. Multinomial model is a type of GLM. Here is an example using multinomial logistic regression. A researcher classified the stomach contents of \\(n = 219\\) alligators according to \\(r = 5\\) categories (fish, Inv., Rept, Bird, Other) as a function of covariates Lake, Sex, and Size.. gator_dat &lt;- tribble( ~profile, ~Gender, ~Size, ~Lake, ~Fish, ~Invertebrate, ~Reptile, ~Bird, ~Other, &quot;1&quot;, &quot;f&quot;, &quot;&lt;2.3&quot;, &quot;george&quot;, 3, 9, 1, 0, 1, &quot;2&quot;, &quot;m&quot;, &quot;&lt;2.3&quot;, &quot;george&quot;, 13, 10, 0, 2, 2, &quot;3&quot;, &quot;f&quot;, &quot;&gt;2.3&quot;, &quot;george&quot;, 8, 1, 0, 0, 1, &quot;4&quot;, &quot;m&quot;, &quot;&gt;2.3&quot;, &quot;george&quot;, 9, 0, 0, 1, 2, &quot;5&quot;, &quot;f&quot;, &quot;&lt;2.3&quot;, &quot;hancock&quot;, 16, 3, 2, 2, 3, &quot;6&quot;, &quot;m&quot;, &quot;&lt;2.3&quot;, &quot;hancock&quot;, 7, 1, 0, 0, 5, &quot;7&quot;, &quot;f&quot;, &quot;&gt;2.3&quot;, &quot;hancock&quot;, 3, 0, 1, 2, 3, &quot;8&quot;, &quot;m&quot;, &quot;&gt;2.3&quot;, &quot;hancock&quot;, 4, 0, 0, 1, 2, &quot;9&quot;, &quot;f&quot;, &quot;&lt;2.3&quot;, &quot;oklawaha&quot;, 3, 9, 1, 0, 2, &quot;10&quot;, &quot;m&quot;, &quot;&lt;2.3&quot;, &quot;oklawaha&quot;, 2, 2, 0, 0, 1, &quot;11&quot;, &quot;f&quot;, &quot;&gt;2.3&quot;, &quot;oklawaha&quot;, 0, 1, 0, 1, 0, &quot;12&quot;, &quot;m&quot;, &quot;&gt;2.3&quot;, &quot;oklawaha&quot;, 13, 7, 6, 0, 0, &quot;13&quot;, &quot;f&quot;, &quot;&lt;2.3&quot;, &quot;trafford&quot;, 2, 4, 1, 1, 4, &quot;14&quot;, &quot;m&quot;, &quot;&lt;2.3&quot;, &quot;trafford&quot;, 3, 7, 1, 0, 1, &quot;15&quot;, &quot;f&quot;, &quot;&gt;2.3&quot;, &quot;trafford&quot;, 0, 1, 0, 0, 0, &quot;16&quot;, &quot;m&quot;, &quot;&gt;2.3&quot;, &quot;trafford&quot;, 8, 6, 6, 3, 5 ) gator_dat &lt;- gator_dat %&gt;% mutate( Gender = as_factor(Gender), Lake = fct_relevel(Lake, &quot;hancock&quot;), Size = as_factor(Size) ) There are 4 equations to estimate: \\[\\log \\left( \\frac{\\pi_j} {\\pi_{j^*}} \\right) = \\beta X\\] where \\(\\pi_{j^*}\\) is the probability of fish, the baseline category. Run a multivariate logistic regression model with VGAM::vglm(). library(VGAM) ## Warning: package &#39;VGAM&#39; was built under R version 3.6.2 vglm() fits 4 logit models. gator_vglm &lt;- vglm( cbind(Bird,Invertebrate,Reptile,Other,Fish) ~ Lake + Size + Gender, data = gator_dat, family = multinomial ) summary(gator_vglm) ## ## Call: ## vglm(formula = cbind(Bird, Invertebrate, Reptile, Other, Fish) ~ ## Lake + Size + Gender, family = multinomial, data = gator_dat) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## log(mu[,1]/mu[,5]) -1.199 -0.548 -0.2242 0.368 3.48 ## log(mu[,2]/mu[,5]) -1.322 -0.461 0.0105 0.381 1.87 ## log(mu[,3]/mu[,5]) -0.703 -0.575 -0.3551 0.261 2.06 ## log(mu[,4]/mu[,5]) -1.694 -0.289 -0.1081 1.124 1.37 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept):1 -1.857 0.581 -3.19 0.0014 ** ## (Intercept):2 -1.611 0.551 -2.93 0.0034 ** ## (Intercept):3 -2.287 0.657 -3.48 0.0005 *** ## (Intercept):4 -0.664 0.380 -1.75 0.0806 . ## Lakegeorge:1 -0.575 0.795 -0.72 0.4694 ## Lakegeorge:2 1.781 0.623 2.86 0.0043 ** ## Lakegeorge:3 -1.129 1.193 -0.95 0.3437 ## Lakegeorge:4 -0.767 0.569 -1.35 0.1776 ## Lakeoklawaha:1 -1.126 1.192 -0.94 0.3451 ## Lakeoklawaha:2 2.694 0.669 4.02 0.000057 *** ## Lakeoklawaha:3 1.401 0.810 1.73 0.0839 . ## Lakeoklawaha:4 -0.741 0.742 -1.00 0.3184 ## Laketrafford:1 0.662 0.846 0.78 0.4341 ## Laketrafford:2 2.936 0.687 4.27 0.000019 *** ## Laketrafford:3 1.932 0.825 2.34 0.0193 * ## Laketrafford:4 0.791 0.588 1.35 0.1784 ## Size&gt;2.3:1 0.730 0.652 1.12 0.2629 ## Size&gt;2.3:2 -1.336 0.411 -3.25 0.0012 ** ## Size&gt;2.3:3 0.557 0.647 0.86 0.3890 ## Size&gt;2.3:4 -0.291 0.460 -0.63 0.5275 ## Genderm:1 -0.606 0.689 -0.88 0.3787 ## Genderm:2 -0.463 0.396 -1.17 0.2418 ## Genderm:3 -0.628 0.685 -0.92 0.3598 ## Genderm:4 -0.253 0.466 -0.54 0.5881 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Names of linear predictors: log(mu[,1]/mu[,5]), log(mu[,2]/mu[,5]), ## log(mu[,3]/mu[,5]), log(mu[,4]/mu[,5]) ## ## Residual deviance: 50 on 40 degrees of freedom ## ## Log-likelihood: -73 on 40 degrees of freedom ## ## Number of Fisher scoring iterations: 5 ## ## No Hauck-Donner effect found in any of the estimates ## ## ## Reference group is level 5 of the response The residual deviance is 50.2637 on 40 degrees of freedom. Residual deviance tests the current model fit versus the saturated model. The saturated model, which fits a separate multinomial distribution to each of the 16 profiles (unique combinations of lake, sex and size), has 16 × 4 = 64 parameters. The current model has an intercept, three lake coefficients, one sex coefficient and one size coefficient for each of the four logit equations, for a total of 24 parameters. Therefore, the overall fit statistics have 64 − 24 = 40 degrees of freedom. E &lt;- data.frame(fitted(gator_vglm) * rowSums(gator_dat[, 5:9])) O &lt;- gator_dat %&gt;% select(Bird, Invertebrate, Reptile, Other, Fish) + .000001 (g2 &lt;- 2 * sum(O * log(O / E))) ## [1] 50 indicates the model fits okay, but not great. The Residual Deviance of 50.26 with 40 df from the table above output is reasonable, with p-value of 0.1282 and the statistics/df is close to 1 that is 1.256. "],
["ordinal-logistic-regression.html", "7.3 Ordinal Logistic Regression", " 7.3 Ordinal Logistic Regression The following notes rely heavily on this UVA web page and [PSU STAT 504 class notes](https://online.stat.psu.edu/stat504/node/171/. The ordinal logistic regression model is \\[logit[P(Y \\le j)] = \\alpha_j - \\beta X, \\hspace{5mm} j \\in [1, J-1]\\] where \\(j \\in [1, J-1]\\) are the levels of the ordinal outcome variable \\(Y\\). The proportional odds model assumes there is a common set of slope parameters \\(\\beta\\) for the predictors. The ordinal outcomes are distinguished by the \\(J-1\\) intercepts \\(\\alpha_j\\). The benchmark level is \\(J\\). “Logit” means log-odds, so \\(logit[P(Y \\le j)] = \\log[P(Y \\le j) / P(Y \\gt j)]\\). Suppose we want to model the probability a respondent holds a political ideology [“Very Liberal”, “Slightly Liberal”, “Moderate”, “Slightly Conservative”, “Very Conservative”] given their party affiliation [“Republican”, “Democrat”]. ideo &lt;- c( &quot;Very Liberal&quot;, &quot;Slightly Liberal&quot;, &quot;Moderate&quot;, &quot;Slightly Conservative&quot;, &quot;Very Conservative&quot; ) ideo_cnt_rep &lt;- c(30, 46, 148, 84, 99) ideo_cnt_dem &lt;- c(80, 81, 171, 41, 55) dat &lt;- data.frame( party = factor(rep(c(&quot;Rep&quot;, &quot;Dem&quot;), c(407, 428)), levels = c(&quot;Rep&quot;, &quot;Dem&quot;)), ideo = factor( c(rep(ideo, ideo_cnt_rep), rep(ideo, ideo_cnt_dem)), levels = ideo ) ) table(dat) ## ideo ## party Very Liberal Slightly Liberal Moderate Slightly Conservative ## Rep 30 46 148 84 ## Dem 80 81 171 41 ## ideo ## party Very Conservative ## Rep 99 ## Dem 55 Fit a proportional odds model with the MASS::polr() function (polr stands for proportional odds linear regression). pom &lt;- MASS::polr(ideo ~ party, data = dat) summary(pom) ## ## Re-fitting to get Hessian ## Call: ## MASS::polr(formula = ideo ~ party, data = dat) ## ## Coefficients: ## Value Std. Error t value ## partyDem -0.975 0.129 -7.54 ## ## Intercepts: ## Value Std. Error t value ## Very Liberal|Slightly Liberal -2.469 0.132 -18.736 ## Slightly Liberal|Moderate -1.475 0.109 -13.531 ## Moderate|Slightly Conservative 0.237 0.094 2.516 ## Slightly Conservative|Very Conservative 1.070 0.104 10.292 ## ## Residual Deviance: 2474.98 ## AIC: 2484.98 The log-odds a Democrat identifies as “Very Liberal” or lower is \\[logit[P(Y \\lt 1)] = -2.4690 - (-0.9745)(1) = -1.4945.\\] Solve \\(logit[P(Y \\le 1)] = \\log[P(Y \\le 1) / P(Y \\gt 1)]\\) for \\(P(Y \\le 1)\\) to get the probability a Democrat identifies as “Very Liberal” or lower as \\(P(Y \\le 1) = exp(-1.4945) / (1 + exp(-1.4945)) = 0.183\\). The log odds a Democrat identifies as “Slightly Liberal” or lower is \\[logit[P(Y \\lt 2)] = -1.4745 - (-0.9745)(1) = -0.5.\\] The corresponding probability is \\(P(Y \\le 2) = exp(-0.5) / (1 + exp(-0.5)) = 0.378\\). To get the probability a Democrat identifies as “Slightly Liberal”, just subtract the adjacent cumulative probabilities, \\(P(Y \\le 2) - P(Y \\le 1) = 0.378 = 0.183 = 0.194\\). That’s what’s happening when you use the model to predict the level probabilities. predict(pom, newdata = data.frame(party = &quot;Dem&quot;), type = &quot;probs&quot;) ## Very Liberal Slightly Liberal Moderate ## 0.18 0.19 0.39 ## Slightly Conservative Very Conservative ## 0.11 0.11 The baseline for the model was “Republican”, so the log-odds a Republicn identifies as “Very Liberal” or lower is \\[logit[P(Y \\lt 1)] = -2.4690 - (-0.9745)(0) = -2.4690\\] with corresponding probability \\(P(Y \\le 1) = exp(-2.4690) / (1 + exp(-2.4690)) = 0.078\\). And so on. predict(pom, newdata = data.frame(party = &quot;Rep&quot;), type = &quot;probs&quot;) ## Very Liberal Slightly Liberal Moderate ## 0.078 0.108 0.373 ## Slightly Conservative Very Conservative ## 0.186 0.255 Here is a manual calculation of the probabilities. Note the zeta variable instead of beta because the model fits \\(\\alpha_j - \\beta X\\) instead of \\(\\alpha_j + \\beta X\\), and so it uses a new variable \\(\\zeta = -\\beta\\). Technically, the model could be expressed \\(logit[P(Y \\le j)] = \\alpha_j + \\zeta X\\). library(stringr) # cum log odds, dems an reps dclo &lt;- c(pom$zeta - pom$coefficients) rclo &lt;- c(pom$zeta - 0) # cum probs, dems and reps dcp &lt;- exp(dclo) / (1 + exp(dclo)) rcp &lt;- exp(rclo) / (1 + exp(rclo)) # fix the names and add 1 for &quot;Very...&quot; names(dcp) &lt;- str_sub(names(dcp), start = 1, end = str_locate(names(dcp), &quot;\\\\|&quot;)[, 1] - 1) names(rcp) &lt;- str_sub(names(rcp), start = 1, end = str_locate(names(rcp), &quot;\\\\|&quot;)[, 1] - 1) dcp &lt;- c(dcp, 1) rcp &lt;- c(rcp, 1) names(dcp) &lt;- c(names(dcp)[1:4], &quot;Very Conservative&quot;) names(rcp) &lt;- c(names(rcp)[1:4], &quot;Very Conservative&quot;) # Democrat probs (dp &lt;- dcp - lag(dcp, 1, 0)) ## Very Liberal Slightly Liberal Moderate ## 0.18 0.19 0.39 ## Slightly Conservative Very Conservative ## 0.11 0.11 (rp &lt;- rcp - lag(rcp, 1, 0)) ## Very Liberal Slightly Liberal Moderate ## 0.078 0.108 0.373 ## Slightly Conservative Very Conservative ## 0.186 0.255 The “proportional odds” part of the proportional odds model is that the ratios of the \\(J - 1\\) odds-ratios are identical for the different levels of the predictors. Here we have a single predictor, party. The odds ratios for party = Dem are (dcp / (1 - dcp)) / (rcp / (1 - rcp)) ## Very Liberal Slightly Liberal Moderate ## 2.6 2.6 2.6 ## Slightly Conservative Very Conservative ## 2.6 NaN Which is to say, any level of ideology \\(j\\), the estimated odds that a Democrat’s ideology is more liberal \\((\\lt j)\\) rather than more conservative \\((\\ge j)\\) is about 2.65 times a Republicans odds. The log of these odds ratios is the coefficient estimator for party. log((dcp / (1 - dcp)) / (rcp / (1 - rcp))) ## Very Liberal Slightly Liberal Moderate ## 0.97 0.97 0.97 ## Slightly Conservative Very Conservative ## 0.97 NaN Always check the assumption of proportional odds. One way to do this is by comparing the proportional odds model with a multinomial logit model, also called an unconstrained baseline logit model. The multinomial logit model models unordered responses and fits a slope to each level of the \\(J – 1\\) responses. The proportional odds model is nested in the multinomial model, so can use a likelihood ratio test to see if the models are statistically different. mlm &lt;- nnet::multinom(ideo ~ party, data = dat) ## # weights: 15 (8 variable) ## initial value 1343.880657 ## iter 10 value 1239.866743 ## final value 1235.648615 ## converged Calculate the deviance test statistic \\(D = -2 loglik(\\beta)\\). G &lt;- -2 * (logLik(pom)[1] - logLik(mlm)[1]) pchisq(G, df = length(pom$zeta) - 1, lower.tail = FALSE) ## [1] 0.3 The p-value is high, so the proportional odds model fits as well as the more complex multinomial logit model. "],
["poisson-regression.html", "7.4 Poisson Regression", " 7.4 Poisson Regression Poisson models count data, like “traffic tickets per day”, or “website hits per day”. The response is an expected rate or intensity. For count data, specify the generalized model, this time with family = poisson or family = quasipoisson. Recall that the probability of achieving a count \\(y\\) when the expected rate is \\(\\lambda\\) is distributed \\[P(Y = y|\\lambda) = \\frac{e^{-\\lambda} \\lambda^y}{y!}.\\] The poisson regression model is \\[\\lambda = \\exp(X \\beta).\\] You can solve this for \\(y\\) to get \\[y = X\\beta = \\ln(\\lambda).\\] That is, the model predicts the log of the response rate. For a sample of size n, the likelihood function is \\[L(\\beta; y, X) = \\prod_{i=1}^n \\frac{e^{-\\exp({X_i\\beta})}\\exp({X_i\\beta})^{y_i}}{y_i!}.\\] The log-likelihood is \\[l(\\beta) = \\sum_{i=1}^n (y_i X_i \\beta - \\sum_{i=1}^n\\exp(X_i\\beta) - \\sum_{i=1}^n\\log(y_i!).\\] Maximizing the log-likelihood has no closed-form solution, so the coefficient estimates are found through interatively reweighted least squares. Poisson processes assume the variance of the response variable equals its mean. “Equals” means the mean and variance are of a similar order of magnitude. If that assumption does not hold, use the quasi-poisson. Use Poisson regression for large datasets. If the predicted counts are much greater than zero (&gt;30), the linear regression will work fine. Whereas RMSE is not useful for logistic models, it is a good metric in Poisson. Example Dataset fire contains response variable injuries counting the number of injuries during the month and one explanatory variable, the month mo. fire &lt;- read_csv(file = &quot;C:/Users/mpfol/OneDrive/Documents/Data Science/Data/CivilInjury_0.csv&quot;) ## Parsed with column specification: ## cols( ## ID = col_double(), ## `Injury Date` = col_datetime(format = &quot;&quot;), ## `Total Injuries` = col_double() ## ) fire &lt;- fire %&gt;% mutate(mo = as.POSIXlt(`Injury Date`)$mon + 1) %&gt;% rename(dt = `Injury Date`, injuries = `Total Injuries`) str(fire) ## Classes &#39;spec_tbl_df&#39;, &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 300 obs. of 4 variables: ## $ ID : num 1 2 3 4 5 6 7 8 9 10 ... ## $ dt : POSIXct, format: &quot;2005-01-10&quot; &quot;2005-01-11&quot; ... ## $ injuries: num 1 1 1 5 2 1 1 1 1 1 ... ## $ mo : num 1 1 1 1 1 1 2 2 2 4 ... In a situation like this where there the relationship is bivariate, start with a visualization. ggplot(fire, aes(x = mo, y = injuries)) + geom_jitter() + geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;poisson&quot;)) + labs(title = &quot;Injuries by Month&quot;) Fit a poisson regression in R using glm(formula, data, family = poisson). But first, check whether the mean and variance of injuries are the same magnitude? If not, then use family = quasipoisson. mean(fire$injuries) ## [1] 1.4 var(fire$injuries) ## [1] 1 They are of the same magnitude, so fit the regression with family = poisson. m2 &lt;- glm(injuries ~ mo, family = poisson, data = fire) summary(m2) ## ## Call: ## glm(formula = injuries ~ mo, family = poisson, data = fire) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.399 -0.347 -0.303 -0.250 4.318 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.2280 0.1048 2.18 0.03 * ## mo 0.0122 0.0140 0.87 0.38 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 139.87 on 299 degrees of freedom ## Residual deviance: 139.11 on 298 degrees of freedom ## AIC: 792.1 ## ## Number of Fisher Scoring iterations: 5 The predicted value \\(\\hat{y}\\) is the estimated log of the response variable, \\[\\hat{y} = X \\hat{\\beta} = \\ln (\\lambda).\\] Suppose mo is January (mo = ), then the log ofinjuries` is \\(\\hat{y} = 0.323787\\). Or, more intuitively, the expected count of injuries is \\(\\exp(0.323787) = 1.38\\) predict(m2, newdata = data.frame(mo=1)) ## 1 ## 0.24 predict(m2, newdata = data.frame(mo=1), type = &quot;response&quot;) ## 1 ## 1.3 Here is a plot of the predicted counts in red. augment(m2, type.predict = &quot;response&quot;) %&gt;% ggplot(aes(x = mo, y = injuries)) + geom_point() + geom_point(aes(y = .fitted), color = &quot;red&quot;) + scale_y_continuous(limits = c(0, NA)) + labs(x = &quot;Month&quot;, y = &quot;Injuries&quot;, title = &quot;Poisson Fitted Line Plot&quot;) Evaluate a logistic model fit with an analysis of deviance. (perf &lt;- glance(m2)) ## # A tibble: 1 x 7 ## null.deviance df.null logLik AIC BIC deviance df.residual ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 140. 299 -394. 792. 799. 139. 298 (pseudoR2 &lt;- 1 - perf$deviance / perf$null.deviance) ## [1] 0.0054 The deviance of the null model (no regressors) is 139.9. The deviance of the full model is 132.2. The psuedo-R2 is very low at .05. How about the RMSE? RMSE(pred = predict(m2, type = &quot;response&quot;), obs = fire$injuries) ## [1] 1 The average prediction error is about 0.99. That’s almost as much as the variance of injuries - i.e., just predicting the mean of injuries would be almost as good! Use the GainCurvePlot() function to plot the gain curve. augment(m2, type.predict = &quot;response&quot;) %&gt;% ggplot(aes(x = injuries, y = .fitted)) + geom_point() + geom_smooth(method =&quot;lm&quot;) + labs(x = &quot;Actual&quot;, y = &quot;Predicted&quot;, title = &quot;Poisson Fitted vs Actual&quot;) augment(m2) %&gt;% data.frame() %&gt;% GainCurvePlot(xvar = &quot;.fitted&quot;, truthVar = &quot;injuries&quot;, title = &quot;Poisson Model&quot;) It seems that mo was a poor predictor of injuries. "],
["classification.html", "Chapter 8 Classification", " Chapter 8 Classification "],
["classification-1.html", "Chapter 9 Classification", " Chapter 9 Classification "],
["decision-trees.html", "Chapter 10 Decision Trees", " Chapter 10 Decision Trees Decision trees, also known as classification and regression tree (CART) models, are tree-based methods for supervised machine learning. Simple classification trees and regression trees are easy to use and interpret, but are not competitive with the best machine learning methods. However, they form the foundation for bagged trees, random forests, and boosted trees models, which although less interpretable, are very accurate. CART models segment the predictor space into \\(K\\) non-overlapping terminal nodes (leaves), \\(A_1, A_2, \\dots, A_K\\). Each node is described by a set of rules which can be used to predict new responses. The predicted value \\(\\hat{y}\\) for each node is the mode (classification), or mean (regression). CART models define the nodes through a top-down greedy process called recursive binary splitting. The process is top-down because it begins at the top of the tree with all observations in a single region and successively splits the predictor space. It is greedy because at each splitting step, the best split is made at that particular step without consideration to subsequent splits. The best split is the predictor variable and cutpoint that minimizes a cost function. For a regression tree, the most common cost function is the sum of squared residuals, \\[RSS = \\sum_{k=1}^K\\sum_{i \\in A_k}{\\left(y_i - \\hat{y}_{A_k} \\right)^2}.\\] For a classification tree, the most common cost functions are the Gini index, \\[G = \\sum_{c=1}^C{\\hat{p}_{kc}(1 - \\hat{p}_{kc})},\\] or the entropy \\[D = - \\sum_{c=1}^C{\\hat{p}_{kc} \\log \\hat{p}_{kc}}\\] where \\(\\hat{p}_{kc}\\) is the proportion of training observations in node \\(k\\) node that are class \\(c\\). A completely pure node in a binary tree will have \\(\\hat{p} \\in [0, 1]\\) and \\(G = D = 0\\). A completely impure node in a binary tree will have \\(\\hat{p} = 0.5\\) and \\(G = 0.5^2 \\cdot 2 = 0.25\\) and \\(D = -(0.5 \\log(0.5)) \\cdot 2 = 0.69\\). CART repeats the splitting process for each of the child nodes until a stopping criterion is satisfied, usually when no node size surpasses a predefined maximum, or continued splitting does not improve the model significantly. CART may also impose a minimum number of observations in each node. The resulting tree likely over-fits the training data and therefore does not generalize well to test data, so CART prunes the tree, minimizing the cross-validated prediction error. Rather than cross-validating every possible subtree to find the one with minimum error, CART uses cost-complexity pruning. Cost-complexity is the tradeoff between error (cost) and tree size (complexity) where the tradeoff is quantified with cost-complexity parameter \\(c_p\\). In the equation below, the cost complexity of the tree \\(R_{c_p}(T)\\) is the sum of its risk (error) plus a “cost complexity” factor \\(c_p\\) multiple of the tree size \\(|T|\\). \\[R_{c_p}(T) = R(T) + c_p|T|\\] \\(c_p\\) can take on any value from \\([0..\\infty]\\), but it turns out there is an optimal tree for ranges of \\(c_p\\) values, so there are only a finite set of interesting values for \\(c_p\\) (James et al. 2013) (Therneau and Atkinson 2019) (Kuhn and Johnson 2016). A parametric algorithm identifies the interesting \\(c_p\\) values and their associated pruned trees, \\(T_{c_p}\\). CART uses cross-validation to determine which \\(c_p\\) is optimal. References "],
["classification-tree.html", "10.1 Classification Tree", " 10.1 Classification Tree A simple classification tree is rarely performed on its own; the bagged, random forest, and gradient boosting methods build on this logic. However, it is good to start here to build understanding. I’ll learn by example. Using the ISLR::OJ data set, I will predict which brand of orange juice, Citrus Hill (CH) or Minute Maid = (MM), customers Purchase using from the 17 feature variables. Load the libraries and data. library(ISLR) # OJ dataset library(rpart) # classification and regression trees library(caret) # modeling workflow library(rpart.plot) # better formatted plots than the ones in rpart library(plotROC) # ROC curves library(ROCR) library(tidyverse) library(skimr) # neat alternative to glance &amp; summary oj_dat &lt;- OJ #skim_with(numeric = list(p0 = NULL, p25 = NULL, p50 = NULL, p75 = NULL, # p100 = NULL, hist = NULL)) #skim(oj_dat) I’ll split oj_dat (n = 1,070) into oj_train (80%, n = 857) and oj_test (20%, n = 213). I’ll fit a simple decision tree with oj_train, then later a bagged tree, a random forest, and a gradient boosting tree. I’ll compare their predictive performance with oj_test. set.seed(12345) partition &lt;- createDataPartition(y = oj_dat$Purchase, p = 0.8, list = FALSE) oj_train &lt;- oj_dat[partition, ] oj_test &lt;- oj_dat[-partition, ] Function rpart::rpart() builds a full tree, minimizing the Gini index \\(G\\) by default (parms = list(split = \"gini\")), until the stopping criterion is satisfied. The default stopping criterion is only attempt a split if the current node as at least minsplit = 20 observations, only accept a split if each of the two resulting nodes have at least minbucket = round(minsplit/3) observations, and only accept a split if the resulting overall fit improves by cp = 0.01 (i.e., \\(\\Delta G &lt;= 0.01\\)). set.seed(123) oj_model_1 &lt;- rpart( formula = Purchase ~ ., data = oj_train, method = &quot;class&quot; # &quot;class&quot; for classification, &quot;anova&quot; for regression ) print(oj_model_1) ## n= 857 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 857 330 CH (0.610 0.390) ## 2) LoyalCH&gt;=0.48 537 94 CH (0.825 0.175) ## 4) LoyalCH&gt;=0.76 271 13 CH (0.952 0.048) * ## 5) LoyalCH&lt; 0.76 266 81 CH (0.695 0.305) ## 10) PriceDiff&gt;=-0.16 226 50 CH (0.779 0.221) * ## 11) PriceDiff&lt; -0.16 40 9 MM (0.225 0.775) * ## 3) LoyalCH&lt; 0.48 320 80 MM (0.250 0.750) ## 6) LoyalCH&gt;=0.28 146 58 MM (0.397 0.603) ## 12) SalePriceMM&gt;=2 71 31 CH (0.563 0.437) * ## 13) SalePriceMM&lt; 2 75 18 MM (0.240 0.760) * ## 7) LoyalCH&lt; 0.28 174 22 MM (0.126 0.874) * The output starts with the root node. The predicted class at the root is CH and this prediction produces 334 errors on the 857 observations for a success rate of 0.61026838 and an error rate of 0.38973162. The child nodes of node “x” are labeled 2x) and 2x+1), so the child nodes of 1) are 2) and 3), and the child nodes of 2) are 4) and 5). Terminal nodes are labeled with an asterisk (*). Surprisingly, only 3 of the 17 features were used the in full tree: LoyalCH (Customer brand loyalty for CH), PriceDiff (relative price of MM over CH), and SalePriceMM (absolute price of MM). The first split is at LoyalCH = 0.48285. Here is what the full (unpruned) tree looks like. rpart.plot(oj_model_1, yesno = TRUE) The boxes show the node classification (based on mode), the proportion of observations that are not CH, and the proportion of observations included in the node. rpart() not only grew the full tree, it identified the set of cost complexity parameters, and measured the model performance of each corresponding tree using cross-validation. printcp() displays the candidate \\(c_p\\) values. You can use this table to decide how to prune the tree. printcp(oj_model_1) ## ## Classification tree: ## rpart(formula = Purchase ~ ., data = oj_train, method = &quot;class&quot;) ## ## Variables actually used in tree construction: ## [1] LoyalCH PriceDiff SalePriceMM ## ## Root node error: 334/857 = 0 ## ## n= 857 ## ## CP nsplit rel error xerror xstd ## 1 0 0 1 1 0 ## 2 0 1 1 1 0 ## 3 0 3 0 0 0 ## 4 0 5 0 0 0 There are 4 \\(c_p\\) values in this model. The model with the smallest complexity parameter allows the most splits (nsplit). The highest complexity parameter corresponds to a tree with just a root node. rel error is the error rate relative to the root node. The root node absolute error is 0.38973162 (the proportion of MM), so its rel error is 0.38973162/0.38973162 = 1.0. That means the absolute error of the full tree (at CP = 0.01) is 0.42814 * 0.38973162 = 0.1669. You can verify that by calculating the error rate of the predicted values: data.frame(pred = predict(oj_model_1, newdata = oj_train, type = &quot;class&quot;)) %&gt;% mutate(obs = oj_train$Purchase, err = if_else(pred != obs, 1, 0)) %&gt;% summarize(mean_err = mean(err)) ## mean_err ## 1 0.17 Finishing the CP table tour, xerror is the relative cross-validated error rate and xstd is its standard error. If you want the lowest possible error, then prune to the tree with the smallest relative CV error (xerror) (\\(c_p\\) = 0.01, CV error = 0.18). If you want to balance predictive power with simplicity, prune to the smallest tree within 1 SE of the one with the smallest relative error. The CP table is not super-helpful for finding that tree. I’ll add a column to find it. oj_model_1$cptable %&gt;% data.frame() %&gt;% mutate(min_xerror_idx = which.min(oj_model_1$cptable[, &quot;xerror&quot;]), rownum = row_number(), xerror_cap = oj_model_1$cptable[min_xerror_idx, &quot;xerror&quot;] + oj_model_1$cptable[min_xerror_idx, &quot;xstd&quot;], eval = case_when(rownum == min_xerror_idx ~ &quot;min xerror&quot;, xerror &lt; xerror_cap ~ &quot;under cap&quot;, TRUE ~ &quot;&quot;)) %&gt;% select(-rownum, -min_xerror_idx) ## CP nsplit rel.error xerror xstd xerror_cap eval ## 1 0.479 0 1.00 1.00 0.043 0.5 ## 2 0.033 1 0.52 0.54 0.036 0.5 ## 3 0.013 3 0.46 0.47 0.034 0.5 under cap ## 4 0.010 5 0.43 0.46 0.034 0.5 min xerror The simplest tree using the 1-SE rule is $c_p = 0.01347305, CV error = 0.18). Fortunately, plotcp() presents a nice graphical representation of the relationship between xerror and cp. plotcp(oj_model_1, upper = &quot;splits&quot;) The dashed line is set at the minimum xerror + xstd. The top axis shows the number of splits in the tree. I’m not sure why the CP values are not the same as in the table (they are close, but not the same). The figure suggests I should prune to 5 or 3 splits. I see this curve never really hits a minimum - it is still decreasing at 5 splits. The default tuning parameter value cp = 0.01 may be too small, so I’ll set it to cp = 0.001 and start over. set.seed(123) oj_model_1b &lt;- rpart( formula = Purchase ~ ., data = oj_train, method = &quot;class&quot;, cp = 0.001 ) print(oj_model_1b) ## n= 857 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 857 330 CH (0.610 0.390) ## 2) LoyalCH&gt;=0.48 537 94 CH (0.825 0.175) ## 4) LoyalCH&gt;=0.76 271 13 CH (0.952 0.048) * ## 5) LoyalCH&lt; 0.76 266 81 CH (0.695 0.305) ## 10) PriceDiff&gt;=-0.16 226 50 CH (0.779 0.221) ## 20) ListPriceDiff&gt;=0.26 115 11 CH (0.904 0.096) * ## 21) ListPriceDiff&lt; 0.26 111 39 CH (0.649 0.351) ## 42) PriceMM&gt;=2.2 19 2 CH (0.895 0.105) * ## 43) PriceMM&lt; 2.2 92 37 CH (0.598 0.402) ## 86) DiscCH&gt;=0.12 7 0 CH (1.000 0.000) * ## 87) DiscCH&lt; 0.12 85 37 CH (0.565 0.435) ## 174) ListPriceDiff&gt;=0.22 45 15 CH (0.667 0.333) * ## 175) ListPriceDiff&lt; 0.22 40 18 MM (0.450 0.550) ## 350) LoyalCH&gt;=0.53 28 13 CH (0.536 0.464) ## 700) WeekofPurchase&lt; 2.7e+02 21 8 CH (0.619 0.381) * ## 701) WeekofPurchase&gt;=2.7e+02 7 2 MM (0.286 0.714) * ## 351) LoyalCH&lt; 0.53 12 3 MM (0.250 0.750) * ## 11) PriceDiff&lt; -0.16 40 9 MM (0.225 0.775) * ## 3) LoyalCH&lt; 0.48 320 80 MM (0.250 0.750) ## 6) LoyalCH&gt;=0.28 146 58 MM (0.397 0.603) ## 12) SalePriceMM&gt;=2 71 31 CH (0.563 0.437) ## 24) LoyalCH&lt; 0.3 7 0 CH (1.000 0.000) * ## 25) LoyalCH&gt;=0.3 64 31 CH (0.516 0.484) ## 50) WeekofPurchase&gt;=2.5e+02 52 22 CH (0.577 0.423) ## 100) PriceCH&lt; 1.9 35 11 CH (0.686 0.314) ## 200) StoreID&lt; 1.5 9 1 CH (0.889 0.111) * ## 201) StoreID&gt;=1.5 26 10 CH (0.615 0.385) ## 402) LoyalCH&lt; 0.41 17 4 CH (0.765 0.235) * ## 403) LoyalCH&gt;=0.41 9 3 MM (0.333 0.667) * ## 101) PriceCH&gt;=1.9 17 6 MM (0.353 0.647) * ## 51) WeekofPurchase&lt; 2.5e+02 12 3 MM (0.250 0.750) * ## 13) SalePriceMM&lt; 2 75 18 MM (0.240 0.760) ## 26) SpecialCH&gt;=0.5 14 6 CH (0.571 0.429) * ## 27) SpecialCH&lt; 0.5 61 10 MM (0.164 0.836) * ## 7) LoyalCH&lt; 0.28 174 22 MM (0.126 0.874) ## 14) LoyalCH&gt;=0.035 117 21 MM (0.179 0.821) ## 28) WeekofPurchase&lt; 2.7e+02 104 21 MM (0.202 0.798) ## 56) PriceCH&gt;=1.9 20 9 MM (0.450 0.550) ## 112) WeekofPurchase&gt;=2.5e+02 12 5 CH (0.583 0.417) * ## 113) WeekofPurchase&lt; 2.5e+02 8 2 MM (0.250 0.750) * ## 57) PriceCH&lt; 1.9 84 12 MM (0.143 0.857) * ## 29) WeekofPurchase&gt;=2.7e+02 13 0 MM (0.000 1.000) * ## 15) LoyalCH&lt; 0.035 57 1 MM (0.018 0.982) * This is a much larger tree. Did I find a cp value that produces a local min? plotcp(oj_model_1b, upper = &quot;splits&quot;) Yes, the min is at CP = 0.011 with 5 splits. The min + 1 SE is at CP = 0.021 with 3 splits. I’ll prune the tree to 3 splits. oj_model_1b_pruned &lt;- prune( oj_model_1b, cp = oj_model_1b$cptable[oj_model_1b$cptable[, 2] == 3, &quot;CP&quot;] ) rpart.plot(oj_model_1b_pruned, yesno = TRUE) The most “important” indicator of Purchase appears to be LoyalCH. From the rpart vignette (page 12), “An overall measure of variable importance is the sum of the goodness of split measures for each split for which it was the primary variable, plus goodness (adjusted agreement) for all splits in which it was a surrogate.” Surrogates refer to alternative features for a node to handle missing data. For each split, CART evaluates a variety of alternative “surrogate” splits to use when the feature value for the primary split is NA. Surrogate splits are splits that produce results similar to the original split. A variable’s importance is the sum of the improvement in the overall Gini (or RMSE) measure produced by the nodes in which it appears. Here is the variable importance for this model. oj_model_1b_pruned$variable.importance ## LoyalCH PriceDiff SalePriceMM StoreID WeekofPurchase ## 150.2 20.8 11.6 10.0 8.4 ## DiscMM PriceMM PctDiscMM PriceCH SalePriceCH ## 7.1 7.1 6.3 3.1 1.0 oj_model_1b_pruned$variable.importance %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;Feature&quot;) %&gt;% rename(Overall = &#39;.&#39;) %&gt;% ggplot(aes(x = fct_reorder(Feature, Overall), y = Overall)) + geom_pointrange(aes(ymin = 0, ymax = Overall), color = &quot;cadetblue&quot;, size = .3) + theme_minimal() + coord_flip() + labs(x = &quot;&quot;, y = &quot;&quot;, title = &quot;Variable Importance with Simple Classication&quot;) LoyalCH is by far the most important variable, as expected from its position at the top of the tree, and one level down. You can see how the surrogates appear in the model with the summary() function. summary(oj_model_1b_pruned) ## Call: ## rpart(formula = Purchase ~ ., data = oj_train, method = &quot;class&quot;, ## cp = 0.001) ## n= 857 ## ## CP nsplit rel error xerror xstd ## 1 0.479 0 1.00 1.00 0.043 ## 2 0.033 1 0.52 0.54 0.036 ## 3 0.013 3 0.46 0.47 0.034 ## ## Variable importance ## LoyalCH PriceDiff SalePriceMM StoreID WeekofPurchase ## 67 9 5 4 4 ## DiscMM PriceMM PctDiscMM PriceCH ## 3 3 3 1 ## ## Node number 1: 857 observations, complexity param=0.48 ## predicted class=CH expected loss=0.39 P(node) =1 ## class counts: 523 334 ## probabilities: 0.610 0.390 ## left son=2 (537 obs) right son=3 (320 obs) ## Primary splits: ## LoyalCH &lt; 0.48 to the right, improve=130, (0 missing) ## StoreID &lt; 3.5 to the right, improve= 40, (0 missing) ## PriceDiff &lt; 0.015 to the right, improve= 24, (0 missing) ## ListPriceDiff &lt; 0.26 to the right, improve= 23, (0 missing) ## SalePriceMM &lt; 1.8 to the right, improve= 20, (0 missing) ## Surrogate splits: ## StoreID &lt; 3.5 to the right, agree=0.65, adj=0.053, (0 split) ## PriceMM &lt; 1.9 to the right, agree=0.64, adj=0.031, (0 split) ## WeekofPurchase &lt; 230 to the right, agree=0.63, adj=0.016, (0 split) ## DiscMM &lt; 0.77 to the left, agree=0.63, adj=0.006, (0 split) ## SalePriceMM &lt; 1.4 to the right, agree=0.63, adj=0.006, (0 split) ## ## Node number 2: 537 observations, complexity param=0.033 ## predicted class=CH expected loss=0.18 P(node) =0.63 ## class counts: 443 94 ## probabilities: 0.825 0.175 ## left son=4 (271 obs) right son=5 (266 obs) ## Primary splits: ## LoyalCH &lt; 0.76 to the right, improve=18.0, (0 missing) ## PriceDiff &lt; 0.015 to the right, improve=15.0, (0 missing) ## SalePriceMM &lt; 1.8 to the right, improve=14.0, (0 missing) ## ListPriceDiff &lt; 0.26 to the right, improve=11.0, (0 missing) ## DiscMM &lt; 0.15 to the left, improve= 7.8, (0 missing) ## Surrogate splits: ## WeekofPurchase &lt; 260 to the right, agree=0.59, adj=0.18, (0 split) ## PriceCH &lt; 1.8 to the right, agree=0.59, adj=0.17, (0 split) ## StoreID &lt; 3.5 to the right, agree=0.59, adj=0.16, (0 split) ## PriceMM &lt; 2 to the right, agree=0.59, adj=0.16, (0 split) ## SalePriceMM &lt; 2 to the right, agree=0.59, adj=0.16, (0 split) ## ## Node number 3: 320 observations ## predicted class=MM expected loss=0.25 P(node) =0.37 ## class counts: 80 240 ## probabilities: 0.250 0.750 ## ## Node number 4: 271 observations ## predicted class=CH expected loss=0.048 P(node) =0.32 ## class counts: 258 13 ## probabilities: 0.952 0.048 ## ## Node number 5: 266 observations, complexity param=0.033 ## predicted class=CH expected loss=0.3 P(node) =0.31 ## class counts: 185 81 ## probabilities: 0.695 0.305 ## left son=10 (226 obs) right son=11 (40 obs) ## Primary splits: ## PriceDiff &lt; -0.16 to the right, improve=21, (0 missing) ## ListPriceDiff &lt; 0.24 to the right, improve=21, (0 missing) ## SalePriceMM &lt; 1.8 to the right, improve=17, (0 missing) ## DiscMM &lt; 0.15 to the left, improve=10, (0 missing) ## PctDiscMM &lt; 0.073 to the left, improve=10, (0 missing) ## Surrogate splits: ## SalePriceMM &lt; 1.6 to the right, agree=0.91, adj=0.38, (0 split) ## DiscMM &lt; 0.57 to the left, agree=0.90, adj=0.30, (0 split) ## PctDiscMM &lt; 0.26 to the left, agree=0.90, adj=0.30, (0 split) ## WeekofPurchase &lt; 270 to the left, agree=0.87, adj=0.15, (0 split) ## SalePriceCH &lt; 2.1 to the left, agree=0.86, adj=0.05, (0 split) ## ## Node number 10: 226 observations ## predicted class=CH expected loss=0.22 P(node) =0.26 ## class counts: 176 50 ## probabilities: 0.779 0.221 ## ## Node number 11: 40 observations ## predicted class=MM expected loss=0.22 P(node) =0.047 ## class counts: 9 31 ## probabilities: 0.225 0.775 The last step is to make predictions on the validation data set. For a classification tree, set argument type = \"class\". oj_model_1b_preds &lt;- predict(oj_model_1b_pruned, oj_test, type = &quot;class&quot;) I’ll evaluate the predictions and record the accuracy (correct classification percentage) for comparison to other models. Two ways to evaluate the model are the confusion matrix, and the ROC curve. 10.1.1 Confusion Matrix Print the confusion matrix with caret::confusionMatrix() to see how well does this model performs against the test data set. oj_model_1b_cm &lt;- confusionMatrix(data = oj_model_1b_preds, reference = oj_test$Purchase) oj_model_1b_cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 113 13 ## MM 17 70 ## ## Accuracy : 0.859 ## 95% CI : (0.805, 0.903) ## No Information Rate : 0.61 ## P-Value [Acc &gt; NIR] : 0.00000000000000126 ## ## Kappa : 0.706 ## ## Mcnemar&#39;s Test P-Value : 0.584 ## ## Sensitivity : 0.869 ## Specificity : 0.843 ## Pos Pred Value : 0.897 ## Neg Pred Value : 0.805 ## Prevalence : 0.610 ## Detection Rate : 0.531 ## Detection Prevalence : 0.592 ## Balanced Accuracy : 0.856 ## ## &#39;Positive&#39; Class : CH ## The confusion matrix is at the top. It also includes a lot of statistics. It’s worth getting familiar with the stats. The model accuracy and 95% CI are calculated from the binomial test. binom.test(x = 113 + 70, n = 213) ## ## Exact binomial test ## ## data: 113 + 70 and 213 ## number of successes = 183, number of trials = 213, p-value ## &lt;0.0000000000000002 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.81 0.90 ## sample estimates: ## probability of success ## 0.86 The “No Information Rate” (NIR) statistic is the class rate for the largest class. In this case CH is the largest class, so NIR = 130/213 = 0.6103. “P-Value [Acc &gt; NIR]” is the binomial test that the model accuracy is significantly better than the NIR (i.e., significantly better than just always guessing CH). binom.test(x = 113 + 70, n = 213, p = 130/213, alternative = &quot;greater&quot;) ## ## Exact binomial test ## ## data: 113 + 70 and 213 ## number of successes = 183, number of trials = 213, p-value = ## 0.000000000000001 ## alternative hypothesis: true probability of success is greater than 0.61 ## 95 percent confidence interval: ## 0.81 1.00 ## sample estimates: ## probability of success ## 0.86 The “Accuracy” statistic indicates the model predicts 0.8590 of the observations correctly. That’s good, but less impressive when you consider the prevalence of CH is 0.6103 - you could achieve 61% accuracy just by predicting CH every time. A measure that controls for the prevalence is Cohen’s kappa statistic. The kappa statistic is explained here. It compares the accuracy to the accuracy of a “random system”. It is defined as \\[\\kappa = \\frac{Acc - RA}{1-RA}\\] where \\[RA = \\frac{ActFalse \\times PredFalse + ActTrue \\times PredTrue}{Total \\times Total}\\] is the hypotheical probability of a chance agreement. ActFalse will be the number of “MM” (13 + 70 = 83) and actual true will be the number of “CH” (113 + 17 = 130). The predicted counts are table(oj_model_1b_preds) ## oj_model_1b_preds ## CH MM ## 126 87 So, \\(RA = (83*87 + 130*126) / 213^2 = 0.5202\\) and \\(\\kappa = (0.8592 - 0.5202)/(1 - 0.5202) = 0.7064\\). The kappa statistic varies from 0 to 1 where 0 means accurate predictions occur merely by chance, and 1 means the predictions are in perfect agreement with the observations. In this case, a kappa statistic of 0.7064 is “substantial”. See chart here. The other measures from the confusionMatrix() output are various proportions and you can remind yourself of their definitions in the documentation with ?confusionMatrix. Visuals are almost always helpful. Here is a plot of the confusion matrix. plot(oj_test$Purchase, oj_model_1b_preds, main = &quot;Simple Classification: Predicted vs. Actual&quot;, xlab = &quot;Actual&quot;, ylab = &quot;Predicted&quot;) By the way, how does the validation set accuracy () oj_model_1b_train_preds &lt;- predict(oj_model_1b_pruned, oj_train, type = &quot;class&quot;) oj_model_1b_train_cm &lt;- confusionMatrix(data = oj_model_1b_train_preds, reference = oj_train$Purchase) oj_model_1b_train_cm$overall ## Accuracy ## 0.822637106184364030880828977387864142656326 ## Kappa ## 0.632311348714850951502342013554880395531654 ## AccuracyLower ## 0.795383992748886825552290247287601232528687 ## AccuracyUpper ## 0.847649652472277748138651531917275860905647 ## AccuracyNull ## 0.610268378063010485945483196701388806104660 ## AccuracyPValue ## 0.000000000000000000000000000000000000000019 ## McnemarPValue ## 0.042583955841125085972631580943925655446947 The accuracy on the training data set was a little lower than on the test data set. I though it would be higher, not lower. 10.1.2 ROC Curve Another measure of accuracy is the ROC (receiver operating characteristics) curve (Fawcett 2005). The ROC curve is a plot of the true positive rate (TPR, sensitivity) versus the false positive rate (FPR, 1 - specificity) for a set of thresholds. By default, the threshold for predicting the default classification is 0.50, but it could be any threshold. The ROC curves varies the thresholds. (I’ll use the geom_roc geom from plotROC. data.frame(M = predict(oj_model_1b_pruned, oj_test, &quot;prob&quot;)[, 1], D = if_else(oj_test$Purchase == &quot;CH&quot;, 1, 0)) %&gt;% ggplot() + geom_roc(aes(m = M, d = D), hjust = -0.4, vjust = 1.5, linealpha = 0.6, labelsize = 3, n.cuts = 10) + geom_abline(intercept = 0, slope = 1, linetype = 2) + coord_equal() + theme_minimal() + labs(x = &quot;FPR&quot;, y = &quot;TPR&quot;, title = &quot;Model 1b ROC Curve&quot;, subtitle = &quot;Pruned model using rpart&quot;, caption = &quot;Data: ISLM OJ data set.&quot;) You can also use prediction() and plot.prediction() from the ROCR package. pred &lt;- prediction(predict(oj_model_1b_pruned, newdata = oj_test, type = &quot;prob&quot;)[, 2], oj_test$Purchase) plot(performance(pred, &quot;tpr&quot;, &quot;fpr&quot;)) abline(0, 1, lty = 2) Hmm, not quite the same… A few points on the ROC space are helpful for understanding how to use it. The lower left point (0, 0) is the result of always predicting “negative” or in this case “MM” if “CH” is taken as the default class. Sure, your false positive rate is zero, but since you never predict a positive, your true positive rate is also zero. The upper right point (1, 1) is the results of always predicting “positive” (or “CH” here). You catch all the positives, but you miss all the negatives. The upper left point (0, 1) is the result of perfect accuracy. You catch all the positives and all the negatives. The lower right point (1, 0) is the result of perfect imbecility. You made the exact wrong prediction every time. The 45 degree diagonal is the result of randomly guessing positive (CH) X percent of the time. If you guess positive 90% of the time and the prevalence is 50%, your TPR will be 90% and your FPR will also be 90%, etc. From the last bullet, it is evident that any point below and to the right of the 45 degree diagonal represents an instance where the model would have been better off just predicting entirely one way or the other. The goal is for all nodes to bunch up in the upper left. Points to the left of the diagonal with a low TPR can be thought of as “conservative” predicters - they only make positive (CH) predictions with strong evidence. Points to the left of the diagnonal with a high TPR can be thought of as “liberal” predicters - they make positive (CH) predictions with weak evidence. 10.1.3 Caret Approach I can also fit the model with caret::train(). There are two ways to tune hyperparameters in train(): set the number of tuning parameter values to consider by setting tuneLength, or set particular values to consider for each parameter by defining a tuneGrid. I’ll build the model using 10-fold cross-validation to optimize the hyperparameter CP. If you don’t have any idea what the tuning parameter ought to look like, use tuneLength to get close, then fine-tune with tuneGrid. That’s what I’ll do. I’ll create a training control object that I can re-use in other model builds. oj_trControl = trainControl( method = &quot;cv&quot;, # k-fold cross validation number = 10, # 10 folds savePredictions = &quot;final&quot;, # save predictions for the optimal tuning parameter classProbs = TRUE # return class probabilities in addition to predicted values # summaryFunction = twoClassSummary # computes sensitivity, specificity and the area under the ROC curve. ) Now fit the model. set.seed(1234) oj_model_2 = train( Purchase ~ ., data = oj_train, method = &quot;rpart&quot;, tuneLength = 5, metric = &quot;Accuracy&quot;, trControl = oj_trControl ) caret built a full tree using rpart’s default parameters: gini splitting index, at least 20 observations in a node in order to consider splitting it, and at least 6 observations in each node. Caret then calculated the accuracy for each candidate value of \\(\\alpha\\). Here is the results. print(oj_model_2) ## CART ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... ## Resampling results across tuning parameters: ## ## cp Accuracy Kappa ## 0.006 0.81 0.59 ## 0.009 0.81 0.59 ## 0.013 0.81 0.59 ## 0.033 0.78 0.54 ## 0.479 0.66 0.18 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.009. The second cp (0.008982036) produced the highest accuracy. I can drill into the best value of cp using a tuning grid. I’ll try that now. set.seed(1234) oj_model_3 = train( Purchase ~ ., data = oj_train, method = &quot;rpart&quot;, tuneGrid = expand.grid(cp = seq(from = 0.001, to = 0.010, length = 11)), metric=&#39;Accuracy&#39;, trControl = oj_trControl ) print(oj_model_3) ## CART ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... ## Resampling results across tuning parameters: ## ## cp Accuracy Kappa ## 0.0010 0.80 0.58 ## 0.0019 0.80 0.58 ## 0.0028 0.80 0.58 ## 0.0037 0.81 0.60 ## 0.0046 0.80 0.59 ## 0.0055 0.81 0.59 ## 0.0064 0.81 0.59 ## 0.0073 0.81 0.60 ## 0.0082 0.81 0.60 ## 0.0091 0.81 0.59 ## 0.0100 0.81 0.60 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.0082. The beset model is at cp = 0.009. Here are the rules in the final model. oj_model_3$finalModel ## n= 857 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 857 330 CH (0.610 0.390) ## 2) LoyalCH&gt;=0.48 537 94 CH (0.825 0.175) ## 4) LoyalCH&gt;=0.76 271 13 CH (0.952 0.048) * ## 5) LoyalCH&lt; 0.76 266 81 CH (0.695 0.305) ## 10) PriceDiff&gt;=-0.16 226 50 CH (0.779 0.221) * ## 11) PriceDiff&lt; -0.16 40 9 MM (0.225 0.775) * ## 3) LoyalCH&lt; 0.48 320 80 MM (0.250 0.750) ## 6) LoyalCH&gt;=0.28 146 58 MM (0.397 0.603) ## 12) SalePriceMM&gt;=2 71 31 CH (0.563 0.437) ## 24) LoyalCH&lt; 0.3 7 0 CH (1.000 0.000) * ## 25) LoyalCH&gt;=0.3 64 31 CH (0.516 0.484) ## 50) WeekofPurchase&gt;=2.5e+02 52 22 CH (0.577 0.423) ## 100) PriceCH&lt; 1.9 35 11 CH (0.686 0.314) * ## 101) PriceCH&gt;=1.9 17 6 MM (0.353 0.647) * ## 51) WeekofPurchase&lt; 2.5e+02 12 3 MM (0.250 0.750) * ## 13) SalePriceMM&lt; 2 75 18 MM (0.240 0.760) * ## 7) LoyalCH&lt; 0.28 174 22 MM (0.126 0.874) * Here is the tree. rpart.plot(oj_model_3$finalModel) Here is the ROC curve. library(plotROC) ggplot(oj_model_3$pred) + geom_roc( aes( m = MM, d = factor(obs, levels = c(&quot;CH&quot;, &quot;MM&quot;)) ), hjust = -0.4, vjust = 1.5 ) + coord_equal() ## Warning in verify_d(data$d): D not labeled 0/1, assuming CH = 0 and MM = 1! Here are the cross-validated Accuracy for each candidate cp value. plot(oj_model_3) Evaluate the model by making predictions with the test data set. oj_model_3_preds &lt;- predict(oj_model_3, oj_test, type = &quot;raw&quot;) The confusion matrix shows the true positives and true negatives. oj_model_3_cm &lt;- confusionMatrix( data = oj_model_3_preds, reference = oj_test$Purchase ) oj_model_3_cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 115 18 ## MM 15 65 ## ## Accuracy : 0.845 ## 95% CI : (0.789, 0.891) ## No Information Rate : 0.61 ## P-Value [Acc &gt; NIR] : 0.0000000000000631 ## ## Kappa : 0.672 ## ## Mcnemar&#39;s Test P-Value : 0.728 ## ## Sensitivity : 0.885 ## Specificity : 0.783 ## Pos Pred Value : 0.865 ## Neg Pred Value : 0.812 ## Prevalence : 0.610 ## Detection Rate : 0.540 ## Detection Prevalence : 0.624 ## Balanced Accuracy : 0.834 ## ## &#39;Positive&#39; Class : CH ## The accuracy metric is the slightly worse than in my previous model. Here is a graphical representation of the confusion matrix. plot(oj_test$Purchase, oj_model_3_preds, main = &quot;Simple Classification: Predicted vs. Actual&quot;, xlab = &quot;Actual&quot;, ylab = &quot;Predicted&quot;) Finally, here is the variable importance plot. plot(varImp(oj_model_3), main=&quot;Variable Importance with Simple Classication&quot;) Looks like the manual effort faired best. Here is a summary the accuracy rates of the three models. rbind(data.frame(model = &quot;Manual Class&quot;, Acc = round(oj_model_1b_cm$overall[&quot;Accuracy&quot;], 5)), data.frame(model = &quot;Caret w/tuneGrid&quot;, Acc = round(oj_model_3_cm$overall[&quot;Accuracy&quot;], 5)) ) ## model Acc ## Accuracy Manual Class 0.86 ## Accuracy1 Caret w/tuneGrid 0.85 References "],
["regression-trees.html", "10.2 Regression Trees", " 10.2 Regression Trees A simple regression tree is built in a manner similar to a simple classificatioon tree, and like the simple classification tree, it is rarely invoked on its own; the bagged, random forest, and gradient boosting methods build on this logic. I’ll learn by example again. Using the ISLR::Carseats data set, I will predict Sales using from the 10 feature variables. Load the data. carseats_dat &lt;- Carseats #skim_with(numeric = list(p0 = NULL, p25 = NULL, p50 = NULL, p75 = NULL, # p100 = NULL, hist = NULL)) #skim(carseats_dat) I’ll split careseats_dat (n = 400) into carseats_train (80%, n = 321) and carseats_test (20%, n = 79). I’ll fit a simple decision tree with carseats_train, then later a bagged tree, a random forest, and a gradient boosting tree. I’ll compare their predictive performance with carseats_test. set.seed(12345) partition &lt;- createDataPartition(y = carseats_dat$Sales, p = 0.8, list = FALSE) carseats_train &lt;- carseats_dat[partition, ] carseats_test &lt;- carseats_dat[-partition, ] The first step is to build a full tree, then perform k-fold cross-validation to help select the optimal cost complexity (cp). The only difference here is the rpart() parameter method = \"anova\" to produce a regression tree. set.seed(1234) carseats_model_1 &lt;- rpart( formula = Sales ~ ., data = carseats_train, method = &quot;anova&quot;, xval = 10, model = TRUE # to plot splits with factor variables. ) print(carseats_model_1) ## n= 321 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 321 2600 7.5 ## 2) ShelveLoc=Bad,Medium 251 1500 6.8 ## 4) Price&gt;=1.1e+02 168 720 6.0 ## 8) ShelveLoc=Bad 50 170 4.7 ## 16) Population&lt; 2e+02 20 48 3.6 * ## 17) Population&gt;=2e+02 30 81 5.4 * ## 9) ShelveLoc=Medium 118 430 6.5 ## 18) Advertising&lt; 12 88 290 6.1 ## 36) CompPrice&lt; 1.4e+02 69 190 5.8 ## 72) Price&gt;=1.3e+02 16 51 4.5 * ## 73) Price&lt; 1.3e+02 53 110 6.2 * ## 37) CompPrice&gt;=1.4e+02 19 58 7.4 * ## 19) Advertising&gt;=12 30 83 7.8 * ## 5) Price&lt; 1.1e+02 83 440 8.4 ## 10) Age&gt;=64 32 150 6.9 ## 20) Price&gt;=85 25 67 6.2 ## 40) ShelveLoc=Bad 9 18 4.8 * ## 41) ShelveLoc=Medium 16 21 6.9 * ## 21) Price&lt; 85 7 20 9.6 * ## 11) Age&lt; 64 51 180 9.3 ## 22) Income&lt; 58 12 28 7.7 * ## 23) Income&gt;=58 39 120 9.7 ## 46) Age&gt;=50 14 21 8.5 * ## 47) Age&lt; 50 25 60 10.0 * ## 3) ShelveLoc=Good 70 420 10.0 ## 6) Price&gt;=1.1e+02 49 240 9.4 ## 12) Advertising&lt; 14 41 160 8.9 ## 24) Age&gt;=61 17 53 7.8 * ## 25) Age&lt; 61 24 69 9.8 * ## 13) Advertising&gt;=14 8 13 12.0 * ## 7) Price&lt; 1.1e+02 21 61 12.0 * The output starts with the root node. The predicted Sales at the root is the mean Sales for the training data set, 7.535950 (values are $000s). The deviance at the root is the SSE, 2567.768. The child nodes of node “x” are labeled 2x) and 2x+1), so the child nodes of 1) are 2) and 3), and the child nodes of 2) are 4) and 5). Terminal nodes are labeled with an asterisk (*). The first split is at ShelveLoc = [Bad, Medium] vs Good. Here is what the full (unpruned) tree looks like. rpart.plot(carseats_model_1, yesno = TRUE) The boxes show the node predicted value (mean) and the proportion of observations that are in the node (or child nodes). rpart() not only grew the full tree, it also used cross-validation to test the performance of the possible complexity hyperparameters. printcp() displays the candidate cp values. You can use this table to decide how to prune the tree. printcp(carseats_model_1) ## ## Regression tree: ## rpart(formula = Sales ~ ., data = carseats_train, method = &quot;anova&quot;, ## model = TRUE, xval = 10) ## ## Variables actually used in tree construction: ## [1] Advertising Age CompPrice Income Population Price ## [7] ShelveLoc ## ## Root node error: 2568/321 = 8 ## ## n= 321 ## ## CP nsplit rel error xerror xstd ## 1 0 0 1 1 0 ## 2 0 1 1 1 0 ## 3 0 2 1 1 0 ## 4 0 3 1 1 0 ## 5 0 4 1 1 0 ## 6 0 5 0 1 0 ## 7 0 6 0 1 0 ## 8 0 7 0 1 0 ## 9 0 8 0 1 0 ## 10 0 9 0 1 0 ## 11 0 10 0 1 0 ## 12 0 11 0 1 0 ## 13 0 12 0 1 0 ## 14 0 13 0 1 0 ## 15 0 14 0 1 0 ## 16 0 15 0 1 0 There are 16 possible cp values in this model. The model with the smallest complexity parameter allows the most splits (nsplit). The highest complexity parameter corresponds to a tree with just a root node. rel error is the SSE relative to the root node. The root node SSE is 2567.76800, so its rel error is 2567.76800/2567.76800 = 1.0. That means the absolute error of the full tree (at CP = 0.01) is 0.30963 * 2567.76800 = 795.058. You can verify that by calculating the SSE of the model predicted values: data.frame(pred = predict(carseats_model_1, newdata = carseats_train)) %&gt;% mutate(obs = carseats_train$Sales, sq_err = (obs - pred)^2) %&gt;% summarize(sse = sum(sq_err)) ## sse ## 1 795 Finishing the CP table tour, xerror is the cross-validated SSE and xstd is its standard error. If you want the lowest possible error, then prune to the tree with the smallest relative SSE (xerror). If you want to balance predictive power with simplicity, prune to the smallest tree within 1 SE of the one with the smallest relative SSE. The CP table is not super-helpful for finding that tree. I’ll add a column to find it. carseats_model_1$cptable %&gt;% data.frame() %&gt;% mutate(min_xerror_idx = which.min(carseats_model_1$cptable[, &quot;xerror&quot;]), rownum = row_number(), xerror_cap = carseats_model_1$cptable[min_xerror_idx, &quot;xerror&quot;] + carseats_model_1$cptable[min_xerror_idx, &quot;xstd&quot;], eval = case_when(rownum == min_xerror_idx ~ &quot;min xerror&quot;, xerror &lt; xerror_cap ~ &quot;under cap&quot;, TRUE ~ &quot;&quot;)) %&gt;% select(-rownum, -min_xerror_idx) ## CP nsplit rel.error xerror xstd xerror_cap eval ## 1 0.263 0 1.00 1.01 0.077 0.59 ## 2 0.121 1 0.74 0.75 0.059 0.59 ## 3 0.046 2 0.62 0.65 0.051 0.59 ## 4 0.045 3 0.57 0.67 0.052 0.59 ## 5 0.042 4 0.52 0.66 0.051 0.59 ## 6 0.026 5 0.48 0.62 0.049 0.59 ## 7 0.026 6 0.46 0.62 0.048 0.59 ## 8 0.024 7 0.43 0.62 0.048 0.59 ## 9 0.015 8 0.41 0.58 0.042 0.59 under cap ## 10 0.015 9 0.39 0.56 0.041 0.59 under cap ## 11 0.015 10 0.38 0.56 0.041 0.59 under cap ## 12 0.014 11 0.36 0.56 0.041 0.59 under cap ## 13 0.014 12 0.35 0.56 0.038 0.59 min xerror ## 14 0.014 13 0.33 0.56 0.038 0.59 under cap ## 15 0.011 14 0.32 0.57 0.039 0.59 under cap ## 16 0.010 15 0.31 0.57 0.038 0.59 under cap Okay, so the simplest tree is the one with CP = 0.01544139 (8 splits). Fortunately, plotcp() presents a nice graphical representation of the relationship between xerror and cp. plotcp(carseats_model_1, upper = &quot;splits&quot;) The dashed line is set at the minimum xerror + xstd. The top axis shows the number of splits in the tree. I’m not sure why the CP values are not the same as in the table (they are close, but not the same). The smallest relative error is at 0.01, but the maximum CP below the dashed line (one standard deviation above the mimimum error) is at CP = .019 (8 splits). Use the prune() function to prune the tree by specifying the associated cost-complexity cp. carseats_model_1_pruned &lt;- prune( carseats_model_1, cp = carseats_model_1$cptable[carseats_model_1$cptable[, 2] == 8, &quot;CP&quot;] ) rpart.plot(carseats_model_1_pruned, yesno = TRUE) The most “important” indicator of Sales is ShelveLoc. Here are the importance values from the model. carseats_model_1_pruned$variable.importance %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;Feature&quot;) %&gt;% rename(Overall = &#39;.&#39;) %&gt;% ggplot(aes(x = fct_reorder(Feature, Overall), y = Overall)) + geom_pointrange(aes(ymin = 0, ymax = Overall), color = &quot;cadetblue&quot;, size = .3) + theme_minimal() + coord_flip() + labs(x = &quot;&quot;, y = &quot;&quot;, title = &quot;Variable Importance with Simple Regression&quot;) The most important indicator of Sales is ShelveLoc, then Price, then Age, all of which appear in the final model. CompPrice was also important. The last step is to make predictions on the validation data set. The root mean squared error (\\(RMSE = \\sqrt{(1/2) \\sum{(actual - pred)^2}})\\) and mean absolute error (\\(MAE = (1/n) \\sum{|actual - pred|}\\)) are the two most common measures of predictive accuracy. The key difference is that RMSE punishes large errors more harshly. For a regression tree, set argument type = \"vector\" (or do not specify at all). carseats_model_1_preds &lt;- predict( carseats_model_1_pruned, carseats_test, type = &quot;vector&quot; ) carseats_model_1_pruned_rmse &lt;- RMSE( pred = carseats_model_1_preds, obs = carseats_test$Sales ) carseats_model_1_pruned_rmse ## [1] 2.4 The pruning process leads to an average prediction error of 2.39 in the test data set. Not too bad considering the standard deviation of Sales is 2.8. Here is a predicted vs actual plot. plot(carseats_test$Sales, carseats_model_1_preds, main = &quot;Simple Regression: Predicted vs. Actual&quot;, xlab = &quot;Actual&quot;, ylab = &quot;Predicted&quot;) abline(0, 1) The 6 possible predicted values do a decent job of binning the observations. 10.2.1 Caret Approach I can also fit the model with caret::train(), specifying method = \"rpart\". I’ll build the model using 10-fold cross-validation to optimize the hyperparameter CP. carseats_trControl = trainControl( method = &quot;cv&quot;, # k-fold cross validation number = 10, # 10 folds savePredictions = &quot;final&quot; # save predictions for the optimal tuning parameter ) I’ll let the model look for the best CP tuning parameter with tuneLength to get close, then fine-tune with tuneGrid. set.seed(1234) carseats_model_2 = train( Sales ~ ., data = carseats_train, method = &quot;rpart&quot;, # for classification tree tuneLength = 5, # choose up to 5 combinations of tuning parameters (cp) metric = &quot;RMSE&quot;, # evaluate hyperparamter combinations with RMSE trControl = carseats_trControl ) ## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : ## There were missing values in resampled performance measures. print(carseats_model_2) ## CART ## ## 321 samples ## 10 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... ## Resampling results across tuning parameters: ## ## cp RMSE Rsquared MAE ## 0.042 2.2 0.41 1.8 ## 0.045 2.2 0.38 1.8 ## 0.046 2.3 0.37 1.8 ## 0.121 2.4 0.29 1.9 ## 0.263 2.7 0.19 2.2 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was cp = 0.042. The first cp (0.04167149) produced the smallest RMSE. I can drill into the best value of cp using a tuning grid. I’ll try that now. myGrid &lt;- expand.grid(cp = seq(from = 0, to = 0.1, by = 0.01)) carseats_model_3 = train( Sales ~ ., data = carseats_train, method = &quot;rpart&quot;, # for classification tree tuneGrid = myGrid, # choose up to 5 combinations of tuning parameters (cp) metric = &quot;RMSE&quot;, # evaluate hyperparamter combinations with RMSE trControl = carseats_trControl ) print(carseats_model_3) ## CART ## ## 321 samples ## 10 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 289, 288, 289, ... ## Resampling results across tuning parameters: ## ## cp RMSE Rsquared MAE ## 0.00 2.1 0.46 1.7 ## 0.01 2.2 0.43 1.8 ## 0.02 2.2 0.39 1.8 ## 0.03 2.2 0.41 1.8 ## 0.04 2.3 0.37 1.8 ## 0.05 2.3 0.34 1.8 ## 0.06 2.2 0.37 1.8 ## 0.07 2.3 0.37 1.8 ## 0.08 2.3 0.37 1.8 ## 0.09 2.3 0.37 1.8 ## 0.10 2.3 0.37 1.8 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was cp = 0. It looks like the best performing tree is the unpruned one. plot(carseats_model_3) Lets’s see the final model. rpart.plot(carseats_model_3$finalModel) What were the most important variables? plot(varImp(carseats_model_3), main=&quot;Variable Importance with Simple Regression&quot;) Evaluate the model by making predictions with the test data set. carseats_model_3_preds &lt;- predict(carseats_model_3, carseats_test, type = &quot;raw&quot;) data.frame(Actual = carseats_test$Sales, Predicted = carseats_model_3_preds) %&gt;% ggplot(aes(x = Actual, y = Predicted)) + geom_point() + geom_smooth() + geom_abline(slope = 1, intercept = 0) + scale_y_continuous(limits = c(0, 15)) + labs(title = &quot;Simple Regression: Predicted vs. Actual&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Looks like the model over-estimates at the low end and undestimates at the high end. Calculate the test data set RMSE. carseats_model_3_pruned_rmse &lt;- RMSE( pred = carseats_model_3_preds, obs = carseats_test$Sales ) carseats_model_3_pruned_rmse ## [1] 2.3 Caret faired better in this model. Here is a summary the RMSE values of the two models. rbind(data.frame(model = &quot;Manual ANOVA&quot;, RMSE = round(carseats_model_1_pruned_rmse, 5)), data.frame(model = &quot;Caret&quot;, RMSE = round(carseats_model_3_pruned_rmse, 5)) ) ## model RMSE ## 1 Manual ANOVA 2.4 ## 2 Caret 2.3 "],
["bagging.html", "10.3 Bagging", " 10.3 Bagging Bootstrap aggregation, or bagging, is a general-purpose procedure for reducing the variance of a statistical learning method. The algorithm constructs B regression trees using B bootstrapped training sets, and averages the resulting predictions. These trees are grown deep, and are not pruned. Hence each individual tree has high variance, but low bias. Averaging these B trees reduces the variance. For classification trees, bagging takes the “majority vote” for the prediction. Use a value of B sufficiently large that the error has settled down. To test the model accuracy, the out-of-bag observations are predicted from the models that do not use them. If B/3 of observations are in-bag, there are B/3 predictions per observation. These predictions are averaged for the test prediction. Again, for classification trees, a majority vote is taken. The downside to bagging is that it improves accuracy at the expense of interpretability. There is no longer a single tree to interpret, so it is no longer clear which variables are more important than others. Bagged trees are a special case of random forests, so see the next section for an example. "],
["random-forests.html", "10.4 Random Forests", " 10.4 Random Forests Random forests improve bagged trees by way of a small tweak that de-correlates the trees. As in bagging, the algorithm builds a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of mtry predictors is chosen as split candidates from the full set of p predictors. A fresh sample of mtry predictors is taken at each split. Typically \\(mtry \\sim \\sqrt{b}\\). Bagged trees are thus a special case of random forests where mtry = p. 10.4.0.1 Bagging Classification Example Again using the OJ data set to predict Purchase, this time I’ll use the bagging method by specifying method = \"treebag\". I’ll use tuneLength = 5 and not worry about tuneGrid anymore. Caret has no hyperparameters to tune with this model. oj.bag = train(Purchase ~ ., data = oj_train, method = &quot;treebag&quot;, # for bagging tuneLength = 5, # choose up to 5 combinations of tuning parameters metric = &quot;ROC&quot;, # evaluate hyperparamter combinations with ROC trControl = trainControl( method = &quot;cv&quot;, # k-fold cross validation number = 10, # k=10 folds savePredictions = &quot;final&quot;, # save predictions for the optimal tuning parameters classProbs = TRUE, # return class probabilities in addition to predicted values summaryFunction = twoClassSummary # for binary response variable ) ) oj.bag ## Bagged CART ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 771, 772, 771, 771, 771, 772, ... ## Resampling results: ## ## ROC Sens Spec ## 0.85 0.82 0.72 #plot(oj.bag$) oj.pred &lt;- predict(oj.bag, oj_test, type = &quot;raw&quot;) plot(oj_test$Purchase, oj.pred, main = &quot;Bagging Classification: Predicted vs. Actual&quot;, xlab = &quot;Actual&quot;, ylab = &quot;Predicted&quot;) (oj.conf &lt;- confusionMatrix(data = oj.pred, reference = oj_test$Purchase)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 110 17 ## MM 20 66 ## ## Accuracy : 0.826 ## 95% CI : (0.769, 0.875) ## No Information Rate : 0.61 ## P-Value [Acc &gt; NIR] : 0.00000000000712 ## ## Kappa : 0.637 ## ## Mcnemar&#39;s Test P-Value : 0.742 ## ## Sensitivity : 0.846 ## Specificity : 0.795 ## Pos Pred Value : 0.866 ## Neg Pred Value : 0.767 ## Prevalence : 0.610 ## Detection Rate : 0.516 ## Detection Prevalence : 0.596 ## Balanced Accuracy : 0.821 ## ## &#39;Positive&#39; Class : CH ## oj.bag.acc &lt;- as.numeric(oj.conf$overall[1]) rm(oj.pred) rm(oj.conf) #plot(oj.bag$, oj.bag$finalModel$y) plot(varImp(oj.bag), main=&quot;Variable Importance with Simple Classication&quot;) 10.4.0.2 Random Forest Classification Example Now I’ll try it with the random forest method by specifying method = \"ranger\". I’ll stick with tuneLength = 5. Caret tunes three hyperparameters: mtry: number of randomly selected predictors. Default is sqrt(p). splitrule: splitting rule. For classification, options are “gini” (default) and “extratrees”. min.node.size: minimal node size. Default is 1 for classification. oj.frst = train(Purchase ~ ., data = oj_train, method = &quot;ranger&quot;, # for random forest tuneLength = 5, # choose up to 5 combinations of tuning parameters metric = &quot;ROC&quot;, # evaluate hyperparamter combinations with ROC trControl = trainControl( method = &quot;cv&quot;, # k-fold cross validation number = 10, # 10 folds savePredictions = &quot;final&quot;, # save predictions for the optimal tuning parameter1 classProbs = TRUE, # return class probabilities in addition to predicted values summaryFunction = twoClassSummary # for binary response variable ) ) oj.frst ## Random Forest ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 772, 771, 772, 770, 772, 772, ... ## Resampling results across tuning parameters: ## ## mtry splitrule ROC Sens Spec ## 2 gini 0.86 0.87 0.69 ## 2 extratrees 0.85 0.88 0.63 ## 5 gini 0.87 0.85 0.72 ## 5 extratrees 0.86 0.86 0.69 ## 9 gini 0.87 0.84 0.73 ## 9 extratrees 0.87 0.85 0.69 ## 13 gini 0.86 0.83 0.74 ## 13 extratrees 0.87 0.83 0.71 ## 17 gini 0.86 0.81 0.74 ## 17 extratrees 0.86 0.83 0.71 ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 1 ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were mtry = 9, splitrule = gini ## and min.node.size = 1. plot(oj.frst) oj.pred &lt;- predict(oj.frst, oj_test, type = &quot;raw&quot;) plot(oj_test$Purchase, oj.pred, main = &quot;Random Forest Classification: Predicted vs. Actual&quot;, xlab = &quot;Actual&quot;, ylab = &quot;Predicted&quot;) (oj.conf &lt;- confusionMatrix(data = oj.pred, reference = oj_test$Purchase)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 109 16 ## MM 21 67 ## ## Accuracy : 0.826 ## 95% CI : (0.769, 0.875) ## No Information Rate : 0.61 ## P-Value [Acc &gt; NIR] : 0.00000000000712 ## ## Kappa : 0.639 ## ## Mcnemar&#39;s Test P-Value : 0.511 ## ## Sensitivity : 0.838 ## Specificity : 0.807 ## Pos Pred Value : 0.872 ## Neg Pred Value : 0.761 ## Prevalence : 0.610 ## Detection Rate : 0.512 ## Detection Prevalence : 0.587 ## Balanced Accuracy : 0.823 ## ## &#39;Positive&#39; Class : CH ## oj.frst.acc &lt;- as.numeric(oj.conf$overall[1]) rm(oj.pred) rm(oj.conf) #plot(oj.bag$, oj.bag$finalModel$y) #plot(varImp(oj.frst), main=&quot;Variable Importance with Simple Classication&quot;) The model algorithm explains “ROC was used to select the optimal model using the largest value. The final values used for the model were mtry = 9, splitrule = extratrees and min.node.size = 1.” You can see the results of tuning grid combinations in the associated plot of ROC AUC vs mtry grouped by splitting rule. The bagging (accuracy = 0.80751) and random forest (accuracy = 0.81690) models faired pretty well, but the manual classification tree is still in first place. There’s still gradient boosting to investigate! rbind(data.frame(model = &quot;Manual Class&quot;, Accuracy = round(oj_model_1b_cm$overall[&quot;Accuracy&quot;], 5)), data.frame(model = &quot;Caret w.tuneGrid&quot;, Accuracy = round(oj_model_3_cm$overall[&quot;Accuracy&quot;], 5)), data.frame(model = &quot;Bagging&quot;, Accuracy = round(oj.bag.acc, 5)), data.frame(model = &quot;Random Forest&quot;, Accuracy = round(oj.frst.acc, 5)) ) %&gt;% arrange(desc(Accuracy)) ## model Accuracy ## 1 Manual Class 0.86 ## 2 Caret w.tuneGrid 0.85 ## 3 Bagging 0.83 ## 4 Random Forest 0.83 10.4.0.3 Bagging Regression Example Again using the Carseats data set to predict Sales, this time I’ll use the bagging method by specifying method = \"treebag\". I’ll use tuneLength = 5 and not worry about tuneGrid anymore. Caret has no hyperparameters to tune with this model. carseats.bag = train(Sales ~ ., data = carseats_train, method = &quot;treebag&quot;, # for bagging tuneLength = 5, # choose up to 5 combinations of tuning parameters metric = &quot;RMSE&quot;, # evaluate hyperparamter combinations with RMSE trControl = trainControl( method = &quot;cv&quot;, # k-fold cross validation number = 10, # 10 folds savePredictions = &quot;final&quot; # save predictions for the optimal tuning parameter1 ) ) carseats.bag ## Bagged CART ## ## 321 samples ## 10 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 288, 289, 289, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 1.7 0.65 1.4 #plot(carseats.bag$finalModel) carseats.pred &lt;- predict(carseats.bag, carseats_test, type = &quot;raw&quot;) plot(carseats_test$Sales, carseats.pred, main = &quot;Bagging Regression: Predicted vs. Actual&quot;, xlab = &quot;Actual&quot;, ylab = &quot;Predicted&quot;) abline(0, 1) (carseats.bag.rmse &lt;- RMSE(pred = carseats.pred, obs = carseats_test$Sales)) ## [1] 1.9 rm(carseats.pred) plot(varImp(carseats.bag), main=&quot;Variable Importance with Regression Bagging&quot;) 10.4.0.4 Random Forest Regression Example Now I’ll try it with the random forest method by specifying method = \"ranger\". I’ll stick with tuneLength = 5. Caret tunes three hyperparameters: mtry: number of randomly selected predictors splitrule: splitting rule. For regression, options are “variance” (default), “extratrees”, and “maxstat”. min.node.size: minimal node size carseats.frst = train(Sales ~ ., data = carseats_train, method = &quot;ranger&quot;, # for random forest tuneLength = 5, # choose up to 5 combinations of tuning parameters metric = &quot;RMSE&quot;, # evaluate hyperparamter combinations with RMSE trControl = trainControl( method = &quot;cv&quot;, # k-fold cross validation number = 10, # 10 folds savePredictions = &quot;final&quot; # save predictions for the optimal tuning parameter1 ) ) carseats.frst ## Random Forest ## ## 321 samples ## 10 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 289, 289, 288, ... ## Resampling results across tuning parameters: ## ## mtry splitrule RMSE Rsquared MAE ## 2 variance 1.8 0.70 1.4 ## 2 extratrees 1.9 0.65 1.5 ## 4 variance 1.6 0.73 1.3 ## 4 extratrees 1.7 0.69 1.4 ## 6 variance 1.5 0.73 1.2 ## 6 extratrees 1.6 0.70 1.3 ## 8 variance 1.5 0.72 1.2 ## 8 extratrees 1.6 0.71 1.3 ## 11 variance 1.6 0.72 1.2 ## 11 extratrees 1.6 0.71 1.3 ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were mtry = 6, splitrule = variance ## and min.node.size = 5. plot(carseats.frst) carseats.pred &lt;- predict(carseats.frst, carseats_test, type = &quot;raw&quot;) plot(carseats_test$Sales, carseats.pred, main = &quot;Random Forest Regression: Predicted vs. Actual&quot;, xlab = &quot;Actual&quot;, ylab = &quot;Predicted&quot;) abline(0, 1) (carseats.frst.rmse &lt;- RMSE(pred = carseats.pred, obs = carseats_test$Sales)) ## [1] 1.8 rm(carseats.pred) #plot(varImp(carseats.frst), main=&quot;Variable Importance with Regression Random Forest&quot;) The model algorithm explains “RMSE was used to select the optimal model using the smallest value. The final values used for the model were mtry = 11, splitrule = variance and min.node.size = 5.” You can see the results of tuning grid combinations in the associated plot of ROC AUC vs mtry grouped by splitting rule. The bagging and random forest models faired very well - they took over the first and second place! rbind(data.frame(model = &quot;Manual ANOVA&quot;, RMSE = round(carseats_model_1_pruned_rmse, 5)), data.frame(model = &quot;ANOVA w.tuneGrid&quot;, RMSE = round(carseats_model_3_pruned_rmse, 5)), data.frame(model = &quot;Bagging&quot;, RMSE = round(carseats.bag.rmse, 5)), data.frame(model = &quot;Random Forest&quot;, RMSE = round(carseats.frst.rmse, 5)) ) %&gt;% arrange(RMSE) ## model RMSE ## 1 Random Forest 1.8 ## 2 Bagging 1.9 ## 3 ANOVA w.tuneGrid 2.3 ## 4 Manual ANOVA 2.4 "],
["gradient-boosting.html", "10.5 Gradient Boosting", " 10.5 Gradient Boosting Boosting is a method to improve (boost) the week learners sequentially and increase the model accuracy with a combined model. There are several boosting algorithms. One of the earliest was AdaBoost (adaptive boost). A more recent innovation is gradient boosting. Adaboost creates a single split tree (decision stump) then weights the observations by how well the initial tree performed, putting more weight on the difficult observations. It then creates a second tree using the weights so that it focuses on the difficult observations. Observations that are difficult to classify receive increasing larger weights until the algorithm identifies a model that correctly classifies them. The final model returns predictions that are a majority vode. (I think Adaboost applies only to classification problems, not regressions). Gradient boosting generalizes the AdaBoost method, so that the object is to minimize a loss function. In the case of classification problems, the loss function is the log-loss; for regression problems, the loss function is mean squared error. The regression trees are addative, so that the successive models can be added together to correct the residuals in the earlier models. Gradient boosting constructs its trees in a “greedy” manner, meaning it chooses the best splits based on purity scores like Gini or minimizing the loss. It is common to constrain the weak learners by setting maximum tree size parameters. Gradient boosting continues until it reaches maximum number of trees or an acceptible error level. This can result in overfitting, so it is common to employ regularization methods that penalize aspects of the model. Tree Constraints. In general the more constrained the tree, the more trees need to be grown. Parameters to optimize include number of trees, tree depth, number of nodes, minimmum observations per split, and minimum improvement to loss. Learning Rate. Each successive tree can be weighted to slow down the learning rate. Decreasing the learning rate increases the number of required trees. Common growth rates are 0.1 to 0.3. The gradient boosting algorithm fits a shallow tree \\(T_1\\) to the data, \\(M_1 = T_1\\). Then it fits a tree \\(T_2\\) to the residuals and adds a weighted sum of the tree to the original tree as \\(M_2 = M_1 + \\gamma T_2\\). For regularized boosting, include a learning rate factor \\(\\eta \\in (0..1)\\), \\(M_2 = M_1 + \\eta \\gamma T_2\\). A larger \\(\\eta\\) produces faster learning, but risks overfitting. The process repeats until the residuals are small enough, or until it reaches the maximum iterations. Because overfitting is a risk, use cross-validation to select the appropriate number of trees (the number of trees producing the lowest RMSE). 10.5.0.1 Gradient Boosting Classification Example Again using the OJ data set to predict Purchase, this time I’ll use the gradient boosting method by specifying method = \"gbm\". I’ll use tuneLength = 5 and not worry about tuneGrid anymore. Caret tunes the following hyperparameters (see modelLookup(\"gbm\")). n.trees: number of boosting iterations interaction.depth: maximum tree depth shrinkage: shrinkage n.minobsinnode: mimimum terminal node size oj.gbm &lt;- train(Purchase ~ ., data = oj_train, method = &quot;gbm&quot;, # for bagged tree tuneLength = 5, # choose up to 5 combinations of tuning parameters metric = &quot;ROC&quot;, # evaluate hyperparamter combinations with ROC trControl = trainControl( method = &quot;cv&quot;, # k-fold cross validation number = 10, # 10 folds savePredictions = &quot;final&quot;, # save predictions for the optimal tuning parameter1 classProbs = TRUE, # return class probabilities in addition to predicted values summaryFunction = twoClassSummary # for binary response variable ) ) ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2789 nan 0.1000 0.0273 ## 2 1.2286 nan 0.1000 0.0245 ## 3 1.1929 nan 0.1000 0.0175 ## 4 1.1613 nan 0.1000 0.0148 ## 5 1.1263 nan 0.1000 0.0146 ## 6 1.0991 nan 0.1000 0.0105 ## 7 1.0752 nan 0.1000 0.0102 ## 8 1.0579 nan 0.1000 0.0087 ## 9 1.0433 nan 0.1000 0.0047 ## 10 1.0280 nan 0.1000 0.0082 ## 20 0.9233 nan 0.1000 0.0026 ## 40 0.8226 nan 0.1000 0.0010 ## 60 0.7809 nan 0.1000 -0.0001 ## 80 0.7595 nan 0.1000 -0.0002 ## 100 0.7506 nan 0.1000 -0.0008 ## 120 0.7407 nan 0.1000 -0.0005 ## 140 0.7317 nan 0.1000 -0.0005 ## 160 0.7277 nan 0.1000 -0.0009 ## 180 0.7232 nan 0.1000 -0.0004 ## 200 0.7181 nan 0.1000 -0.0007 ## 220 0.7115 nan 0.1000 -0.0008 ## 240 0.7096 nan 0.1000 -0.0010 ## 250 0.7081 nan 0.1000 -0.0015 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2695 nan 0.1000 0.0319 ## 2 1.2150 nan 0.1000 0.0260 ## 3 1.1702 nan 0.1000 0.0225 ## 4 1.1260 nan 0.1000 0.0186 ## 5 1.0913 nan 0.1000 0.0147 ## 6 1.0586 nan 0.1000 0.0160 ## 7 1.0276 nan 0.1000 0.0146 ## 8 1.0045 nan 0.1000 0.0109 ## 9 0.9836 nan 0.1000 0.0099 ## 10 0.9624 nan 0.1000 0.0068 ## 20 0.8337 nan 0.1000 0.0027 ## 40 0.7525 nan 0.1000 -0.0005 ## 60 0.7240 nan 0.1000 -0.0005 ## 80 0.7063 nan 0.1000 -0.0006 ## 100 0.6879 nan 0.1000 -0.0011 ## 120 0.6751 nan 0.1000 -0.0018 ## 140 0.6605 nan 0.1000 -0.0012 ## 160 0.6477 nan 0.1000 -0.0013 ## 180 0.6359 nan 0.1000 -0.0010 ## 200 0.6274 nan 0.1000 -0.0018 ## 220 0.6166 nan 0.1000 -0.0005 ## 240 0.6078 nan 0.1000 -0.0011 ## 250 0.6014 nan 0.1000 -0.0019 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2548 nan 0.1000 0.0377 ## 2 1.1905 nan 0.1000 0.0294 ## 3 1.1343 nan 0.1000 0.0258 ## 4 1.0935 nan 0.1000 0.0180 ## 5 1.0529 nan 0.1000 0.0168 ## 6 1.0172 nan 0.1000 0.0159 ## 7 0.9824 nan 0.1000 0.0151 ## 8 0.9534 nan 0.1000 0.0127 ## 9 0.9277 nan 0.1000 0.0109 ## 10 0.9066 nan 0.1000 0.0088 ## 20 0.7870 nan 0.1000 0.0023 ## 40 0.7150 nan 0.1000 -0.0008 ## 60 0.6799 nan 0.1000 -0.0023 ## 80 0.6520 nan 0.1000 -0.0012 ## 100 0.6298 nan 0.1000 -0.0005 ## 120 0.6117 nan 0.1000 -0.0024 ## 140 0.5973 nan 0.1000 -0.0016 ## 160 0.5849 nan 0.1000 -0.0023 ## 180 0.5670 nan 0.1000 -0.0015 ## 200 0.5548 nan 0.1000 -0.0006 ## 220 0.5440 nan 0.1000 -0.0024 ## 240 0.5290 nan 0.1000 -0.0020 ## 250 0.5228 nan 0.1000 -0.0016 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2554 nan 0.1000 0.0399 ## 2 1.1847 nan 0.1000 0.0346 ## 3 1.1321 nan 0.1000 0.0199 ## 4 1.0823 nan 0.1000 0.0224 ## 5 1.0392 nan 0.1000 0.0208 ## 6 1.0067 nan 0.1000 0.0145 ## 7 0.9768 nan 0.1000 0.0139 ## 8 0.9462 nan 0.1000 0.0123 ## 9 0.9238 nan 0.1000 0.0095 ## 10 0.8966 nan 0.1000 0.0090 ## 20 0.7681 nan 0.1000 0.0007 ## 40 0.6937 nan 0.1000 -0.0004 ## 60 0.6552 nan 0.1000 -0.0017 ## 80 0.6202 nan 0.1000 -0.0018 ## 100 0.5887 nan 0.1000 -0.0027 ## 120 0.5653 nan 0.1000 -0.0012 ## 140 0.5434 nan 0.1000 -0.0017 ## 160 0.5275 nan 0.1000 -0.0008 ## 180 0.5068 nan 0.1000 -0.0012 ## 200 0.4935 nan 0.1000 -0.0016 ## 220 0.4801 nan 0.1000 -0.0018 ## 240 0.4665 nan 0.1000 -0.0010 ## 250 0.4603 nan 0.1000 -0.0012 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2513 nan 0.1000 0.0375 ## 2 1.1795 nan 0.1000 0.0320 ## 3 1.1264 nan 0.1000 0.0254 ## 4 1.0742 nan 0.1000 0.0225 ## 5 1.0282 nan 0.1000 0.0196 ## 6 0.9888 nan 0.1000 0.0177 ## 7 0.9547 nan 0.1000 0.0136 ## 8 0.9303 nan 0.1000 0.0103 ## 9 0.9008 nan 0.1000 0.0121 ## 10 0.8803 nan 0.1000 0.0073 ## 20 0.7563 nan 0.1000 0.0003 ## 40 0.6715 nan 0.1000 -0.0012 ## 60 0.6253 nan 0.1000 -0.0016 ## 80 0.5868 nan 0.1000 -0.0021 ## 100 0.5538 nan 0.1000 -0.0015 ## 120 0.5285 nan 0.1000 -0.0034 ## 140 0.5070 nan 0.1000 -0.0025 ## 160 0.4872 nan 0.1000 -0.0012 ## 180 0.4736 nan 0.1000 -0.0023 ## 200 0.4566 nan 0.1000 -0.0015 ## 220 0.4407 nan 0.1000 -0.0011 ## 240 0.4262 nan 0.1000 -0.0013 ## 250 0.4186 nan 0.1000 -0.0024 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2743 nan 0.1000 0.0307 ## 2 1.2220 nan 0.1000 0.0240 ## 3 1.1885 nan 0.1000 0.0165 ## 4 1.1522 nan 0.1000 0.0177 ## 5 1.1186 nan 0.1000 0.0136 ## 6 1.0912 nan 0.1000 0.0111 ## 7 1.0693 nan 0.1000 0.0106 ## 8 1.0492 nan 0.1000 0.0089 ## 9 1.0309 nan 0.1000 0.0093 ## 10 1.0172 nan 0.1000 0.0069 ## 20 0.9206 nan 0.1000 0.0030 ## 40 0.8357 nan 0.1000 -0.0002 ## 60 0.7936 nan 0.1000 -0.0000 ## 80 0.7764 nan 0.1000 -0.0009 ## 100 0.7682 nan 0.1000 -0.0004 ## 120 0.7620 nan 0.1000 -0.0008 ## 140 0.7582 nan 0.1000 -0.0011 ## 160 0.7536 nan 0.1000 -0.0005 ## 180 0.7501 nan 0.1000 -0.0006 ## 200 0.7448 nan 0.1000 -0.0008 ## 220 0.7409 nan 0.1000 -0.0006 ## 240 0.7385 nan 0.1000 -0.0011 ## 250 0.7368 nan 0.1000 -0.0007 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2697 nan 0.1000 0.0323 ## 2 1.2121 nan 0.1000 0.0276 ## 3 1.1636 nan 0.1000 0.0251 ## 4 1.1220 nan 0.1000 0.0166 ## 5 1.0826 nan 0.1000 0.0131 ## 6 1.0537 nan 0.1000 0.0134 ## 7 1.0269 nan 0.1000 0.0104 ## 8 1.0061 nan 0.1000 0.0084 ## 9 0.9858 nan 0.1000 0.0082 ## 10 0.9678 nan 0.1000 0.0066 ## 20 0.8429 nan 0.1000 0.0024 ## 40 0.7685 nan 0.1000 -0.0010 ## 60 0.7422 nan 0.1000 -0.0006 ## 80 0.7228 nan 0.1000 -0.0009 ## 100 0.7073 nan 0.1000 -0.0013 ## 120 0.6937 nan 0.1000 -0.0024 ## 140 0.6836 nan 0.1000 -0.0014 ## 160 0.6703 nan 0.1000 -0.0022 ## 180 0.6607 nan 0.1000 -0.0009 ## 200 0.6529 nan 0.1000 -0.0011 ## 220 0.6438 nan 0.1000 -0.0017 ## 240 0.6370 nan 0.1000 -0.0015 ## 250 0.6311 nan 0.1000 -0.0011 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2569 nan 0.1000 0.0361 ## 2 1.1946 nan 0.1000 0.0301 ## 3 1.1386 nan 0.1000 0.0266 ## 4 1.0954 nan 0.1000 0.0205 ## 5 1.0524 nan 0.1000 0.0204 ## 6 1.0186 nan 0.1000 0.0149 ## 7 0.9847 nan 0.1000 0.0126 ## 8 0.9618 nan 0.1000 0.0086 ## 9 0.9344 nan 0.1000 0.0114 ## 10 0.9135 nan 0.1000 0.0095 ## 20 0.8003 nan 0.1000 0.0027 ## 40 0.7353 nan 0.1000 -0.0011 ## 60 0.7042 nan 0.1000 -0.0027 ## 80 0.6800 nan 0.1000 -0.0017 ## 100 0.6602 nan 0.1000 -0.0008 ## 120 0.6393 nan 0.1000 -0.0017 ## 140 0.6231 nan 0.1000 -0.0016 ## 160 0.6077 nan 0.1000 -0.0028 ## 180 0.5977 nan 0.1000 -0.0012 ## 200 0.5863 nan 0.1000 -0.0014 ## 220 0.5749 nan 0.1000 -0.0013 ## 240 0.5618 nan 0.1000 -0.0022 ## 250 0.5577 nan 0.1000 -0.0022 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2495 nan 0.1000 0.0419 ## 2 1.1884 nan 0.1000 0.0265 ## 3 1.1279 nan 0.1000 0.0248 ## 4 1.0820 nan 0.1000 0.0208 ## 5 1.0426 nan 0.1000 0.0166 ## 6 1.0089 nan 0.1000 0.0146 ## 7 0.9799 nan 0.1000 0.0126 ## 8 0.9474 nan 0.1000 0.0140 ## 9 0.9225 nan 0.1000 0.0079 ## 10 0.9066 nan 0.1000 0.0048 ## 20 0.7815 nan 0.1000 0.0014 ## 40 0.7028 nan 0.1000 -0.0019 ## 60 0.6661 nan 0.1000 -0.0011 ## 80 0.6386 nan 0.1000 -0.0006 ## 100 0.6075 nan 0.1000 -0.0005 ## 120 0.5861 nan 0.1000 -0.0019 ## 140 0.5674 nan 0.1000 -0.0020 ## 160 0.5467 nan 0.1000 -0.0016 ## 180 0.5318 nan 0.1000 -0.0020 ## 200 0.5200 nan 0.1000 -0.0025 ## 220 0.5050 nan 0.1000 -0.0009 ## 240 0.4930 nan 0.1000 -0.0020 ## 250 0.4883 nan 0.1000 -0.0016 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2558 nan 0.1000 0.0336 ## 2 1.1856 nan 0.1000 0.0326 ## 3 1.1172 nan 0.1000 0.0305 ## 4 1.0713 nan 0.1000 0.0222 ## 5 1.0313 nan 0.1000 0.0171 ## 6 0.9965 nan 0.1000 0.0164 ## 7 0.9613 nan 0.1000 0.0156 ## 8 0.9354 nan 0.1000 0.0103 ## 9 0.9089 nan 0.1000 0.0111 ## 10 0.8859 nan 0.1000 0.0059 ## 20 0.7690 nan 0.1000 0.0006 ## 40 0.6889 nan 0.1000 -0.0004 ## 60 0.6452 nan 0.1000 -0.0021 ## 80 0.6127 nan 0.1000 -0.0021 ## 100 0.5811 nan 0.1000 -0.0032 ## 120 0.5557 nan 0.1000 -0.0011 ## 140 0.5332 nan 0.1000 -0.0012 ## 160 0.5118 nan 0.1000 -0.0014 ## 180 0.4879 nan 0.1000 -0.0012 ## 200 0.4737 nan 0.1000 -0.0020 ## 220 0.4591 nan 0.1000 -0.0024 ## 240 0.4460 nan 0.1000 -0.0030 ## 250 0.4386 nan 0.1000 -0.0013 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2717 nan 0.1000 0.0320 ## 2 1.2209 nan 0.1000 0.0250 ## 3 1.1751 nan 0.1000 0.0208 ## 4 1.1404 nan 0.1000 0.0149 ## 5 1.1056 nan 0.1000 0.0119 ## 6 1.0782 nan 0.1000 0.0125 ## 7 1.0569 nan 0.1000 0.0081 ## 8 1.0356 nan 0.1000 0.0098 ## 9 1.0176 nan 0.1000 0.0080 ## 10 1.0042 nan 0.1000 0.0066 ## 20 0.9023 nan 0.1000 0.0021 ## 40 0.8156 nan 0.1000 0.0006 ## 60 0.7793 nan 0.1000 0.0004 ## 80 0.7609 nan 0.1000 -0.0014 ## 100 0.7514 nan 0.1000 -0.0006 ## 120 0.7448 nan 0.1000 -0.0005 ## 140 0.7406 nan 0.1000 -0.0008 ## 160 0.7353 nan 0.1000 -0.0007 ## 180 0.7329 nan 0.1000 -0.0008 ## 200 0.7281 nan 0.1000 -0.0014 ## 220 0.7239 nan 0.1000 -0.0008 ## 240 0.7211 nan 0.1000 -0.0011 ## 250 0.7203 nan 0.1000 -0.0010 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2647 nan 0.1000 0.0368 ## 2 1.2060 nan 0.1000 0.0275 ## 3 1.1558 nan 0.1000 0.0232 ## 4 1.1207 nan 0.1000 0.0178 ## 5 1.0787 nan 0.1000 0.0172 ## 6 1.0478 nan 0.1000 0.0141 ## 7 1.0178 nan 0.1000 0.0117 ## 8 0.9963 nan 0.1000 0.0099 ## 9 0.9749 nan 0.1000 0.0107 ## 10 0.9514 nan 0.1000 0.0095 ## 20 0.8341 nan 0.1000 0.0029 ## 40 0.7601 nan 0.1000 -0.0015 ## 60 0.7335 nan 0.1000 -0.0010 ## 80 0.7131 nan 0.1000 -0.0011 ## 100 0.7006 nan 0.1000 -0.0018 ## 120 0.6886 nan 0.1000 -0.0008 ## 140 0.6740 nan 0.1000 -0.0014 ## 160 0.6628 nan 0.1000 -0.0017 ## 180 0.6522 nan 0.1000 -0.0010 ## 200 0.6423 nan 0.1000 -0.0005 ## 220 0.6353 nan 0.1000 -0.0020 ## 240 0.6251 nan 0.1000 -0.0017 ## 250 0.6211 nan 0.1000 -0.0013 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2533 nan 0.1000 0.0369 ## 2 1.1914 nan 0.1000 0.0304 ## 3 1.1304 nan 0.1000 0.0280 ## 4 1.0808 nan 0.1000 0.0214 ## 5 1.0411 nan 0.1000 0.0177 ## 6 1.0095 nan 0.1000 0.0131 ## 7 0.9791 nan 0.1000 0.0118 ## 8 0.9487 nan 0.1000 0.0146 ## 9 0.9234 nan 0.1000 0.0114 ## 10 0.9050 nan 0.1000 0.0081 ## 20 0.7868 nan 0.1000 -0.0018 ## 40 0.7190 nan 0.1000 -0.0002 ## 60 0.6929 nan 0.1000 -0.0013 ## 80 0.6706 nan 0.1000 -0.0012 ## 100 0.6511 nan 0.1000 -0.0008 ## 120 0.6313 nan 0.1000 -0.0028 ## 140 0.6161 nan 0.1000 -0.0010 ## 160 0.6016 nan 0.1000 -0.0019 ## 180 0.5875 nan 0.1000 -0.0013 ## 200 0.5754 nan 0.1000 -0.0021 ## 220 0.5613 nan 0.1000 -0.0011 ## 240 0.5456 nan 0.1000 -0.0014 ## 250 0.5423 nan 0.1000 -0.0016 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2526 nan 0.1000 0.0406 ## 2 1.1845 nan 0.1000 0.0346 ## 3 1.1309 nan 0.1000 0.0246 ## 4 1.0809 nan 0.1000 0.0250 ## 5 1.0380 nan 0.1000 0.0205 ## 6 1.0017 nan 0.1000 0.0164 ## 7 0.9661 nan 0.1000 0.0146 ## 8 0.9372 nan 0.1000 0.0130 ## 9 0.9115 nan 0.1000 0.0105 ## 10 0.8921 nan 0.1000 0.0084 ## 20 0.7698 nan 0.1000 0.0014 ## 40 0.6902 nan 0.1000 -0.0016 ## 60 0.6501 nan 0.1000 -0.0019 ## 80 0.6200 nan 0.1000 -0.0006 ## 100 0.5995 nan 0.1000 -0.0026 ## 120 0.5766 nan 0.1000 -0.0019 ## 140 0.5576 nan 0.1000 -0.0020 ## 160 0.5428 nan 0.1000 -0.0027 ## 180 0.5276 nan 0.1000 -0.0026 ## 200 0.5091 nan 0.1000 -0.0011 ## 220 0.4954 nan 0.1000 -0.0014 ## 240 0.4801 nan 0.1000 -0.0028 ## 250 0.4748 nan 0.1000 -0.0021 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2578 nan 0.1000 0.0379 ## 2 1.1876 nan 0.1000 0.0342 ## 3 1.1214 nan 0.1000 0.0297 ## 4 1.0650 nan 0.1000 0.0254 ## 5 1.0182 nan 0.1000 0.0188 ## 6 0.9812 nan 0.1000 0.0172 ## 7 0.9484 nan 0.1000 0.0116 ## 8 0.9182 nan 0.1000 0.0116 ## 9 0.8929 nan 0.1000 0.0068 ## 10 0.8704 nan 0.1000 0.0085 ## 20 0.7522 nan 0.1000 0.0002 ## 40 0.6778 nan 0.1000 -0.0019 ## 60 0.6318 nan 0.1000 -0.0015 ## 80 0.5982 nan 0.1000 -0.0011 ## 100 0.5669 nan 0.1000 -0.0032 ## 120 0.5451 nan 0.1000 -0.0012 ## 140 0.5224 nan 0.1000 -0.0002 ## 160 0.5005 nan 0.1000 -0.0027 ## 180 0.4834 nan 0.1000 -0.0015 ## 200 0.4693 nan 0.1000 -0.0009 ## 220 0.4530 nan 0.1000 -0.0016 ## 240 0.4419 nan 0.1000 -0.0035 ## 250 0.4324 nan 0.1000 -0.0016 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2761 nan 0.1000 0.0283 ## 2 1.2243 nan 0.1000 0.0231 ## 3 1.1816 nan 0.1000 0.0192 ## 4 1.1477 nan 0.1000 0.0158 ## 5 1.1174 nan 0.1000 0.0122 ## 6 1.0930 nan 0.1000 0.0116 ## 7 1.0704 nan 0.1000 0.0102 ## 8 1.0499 nan 0.1000 0.0083 ## 9 1.0340 nan 0.1000 0.0078 ## 10 1.0167 nan 0.1000 0.0082 ## 20 0.9152 nan 0.1000 0.0021 ## 40 0.8226 nan 0.1000 0.0007 ## 60 0.7895 nan 0.1000 0.0000 ## 80 0.7690 nan 0.1000 -0.0008 ## 100 0.7612 nan 0.1000 -0.0010 ## 120 0.7541 nan 0.1000 -0.0004 ## 140 0.7491 nan 0.1000 -0.0012 ## 160 0.7443 nan 0.1000 -0.0006 ## 180 0.7405 nan 0.1000 -0.0009 ## 200 0.7369 nan 0.1000 -0.0009 ## 220 0.7329 nan 0.1000 -0.0007 ## 240 0.7287 nan 0.1000 -0.0014 ## 250 0.7268 nan 0.1000 -0.0011 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2642 nan 0.1000 0.0359 ## 2 1.2066 nan 0.1000 0.0279 ## 3 1.1624 nan 0.1000 0.0237 ## 4 1.1213 nan 0.1000 0.0202 ## 5 1.0853 nan 0.1000 0.0162 ## 6 1.0560 nan 0.1000 0.0116 ## 7 1.0275 nan 0.1000 0.0125 ## 8 1.0040 nan 0.1000 0.0109 ## 9 0.9823 nan 0.1000 0.0077 ## 10 0.9612 nan 0.1000 0.0105 ## 20 0.8409 nan 0.1000 0.0026 ## 40 0.7578 nan 0.1000 -0.0007 ## 60 0.7294 nan 0.1000 -0.0007 ## 80 0.7095 nan 0.1000 -0.0026 ## 100 0.6965 nan 0.1000 -0.0012 ## 120 0.6857 nan 0.1000 -0.0022 ## 140 0.6751 nan 0.1000 -0.0004 ## 160 0.6650 nan 0.1000 -0.0018 ## 180 0.6581 nan 0.1000 -0.0017 ## 200 0.6520 nan 0.1000 -0.0009 ## 220 0.6436 nan 0.1000 -0.0011 ## 240 0.6351 nan 0.1000 -0.0009 ## 250 0.6312 nan 0.1000 -0.0016 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2577 nan 0.1000 0.0367 ## 2 1.1908 nan 0.1000 0.0288 ## 3 1.1314 nan 0.1000 0.0257 ## 4 1.0832 nan 0.1000 0.0231 ## 5 1.0473 nan 0.1000 0.0147 ## 6 1.0104 nan 0.1000 0.0174 ## 7 0.9743 nan 0.1000 0.0139 ## 8 0.9460 nan 0.1000 0.0113 ## 9 0.9236 nan 0.1000 0.0105 ## 10 0.9026 nan 0.1000 0.0077 ## 20 0.7942 nan 0.1000 0.0009 ## 40 0.7292 nan 0.1000 -0.0018 ## 60 0.6933 nan 0.1000 -0.0013 ## 80 0.6650 nan 0.1000 -0.0008 ## 100 0.6458 nan 0.1000 -0.0020 ## 120 0.6252 nan 0.1000 -0.0018 ## 140 0.6106 nan 0.1000 -0.0010 ## 160 0.5953 nan 0.1000 -0.0009 ## 180 0.5810 nan 0.1000 -0.0014 ## 200 0.5683 nan 0.1000 -0.0016 ## 220 0.5544 nan 0.1000 -0.0009 ## 240 0.5425 nan 0.1000 -0.0010 ## 250 0.5367 nan 0.1000 -0.0012 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2542 nan 0.1000 0.0373 ## 2 1.1874 nan 0.1000 0.0310 ## 3 1.1277 nan 0.1000 0.0279 ## 4 1.0765 nan 0.1000 0.0231 ## 5 1.0393 nan 0.1000 0.0179 ## 6 1.0012 nan 0.1000 0.0141 ## 7 0.9658 nan 0.1000 0.0139 ## 8 0.9421 nan 0.1000 0.0101 ## 9 0.9187 nan 0.1000 0.0088 ## 10 0.8966 nan 0.1000 0.0095 ## 20 0.7715 nan 0.1000 0.0019 ## 40 0.6966 nan 0.1000 -0.0017 ## 60 0.6516 nan 0.1000 -0.0010 ## 80 0.6206 nan 0.1000 -0.0015 ## 100 0.5991 nan 0.1000 -0.0028 ## 120 0.5818 nan 0.1000 -0.0019 ## 140 0.5644 nan 0.1000 -0.0018 ## 160 0.5476 nan 0.1000 -0.0016 ## 180 0.5329 nan 0.1000 -0.0016 ## 200 0.5212 nan 0.1000 -0.0016 ## 220 0.5055 nan 0.1000 -0.0032 ## 240 0.4926 nan 0.1000 -0.0013 ## 250 0.4850 nan 0.1000 -0.0006 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2523 nan 0.1000 0.0397 ## 2 1.1844 nan 0.1000 0.0301 ## 3 1.1269 nan 0.1000 0.0263 ## 4 1.0773 nan 0.1000 0.0216 ## 5 1.0310 nan 0.1000 0.0211 ## 6 0.9902 nan 0.1000 0.0176 ## 7 0.9582 nan 0.1000 0.0119 ## 8 0.9338 nan 0.1000 0.0106 ## 9 0.9054 nan 0.1000 0.0120 ## 10 0.8869 nan 0.1000 0.0083 ## 20 0.7620 nan 0.1000 -0.0001 ## 40 0.6734 nan 0.1000 -0.0010 ## 60 0.6333 nan 0.1000 -0.0004 ## 80 0.5982 nan 0.1000 -0.0023 ## 100 0.5685 nan 0.1000 -0.0013 ## 120 0.5436 nan 0.1000 -0.0024 ## 140 0.5196 nan 0.1000 -0.0012 ## 160 0.5010 nan 0.1000 -0.0031 ## 180 0.4832 nan 0.1000 -0.0032 ## 200 0.4677 nan 0.1000 -0.0014 ## 220 0.4504 nan 0.1000 -0.0016 ## 240 0.4333 nan 0.1000 -0.0013 ## 250 0.4273 nan 0.1000 -0.0022 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2726 nan 0.1000 0.0321 ## 2 1.2186 nan 0.1000 0.0259 ## 3 1.1770 nan 0.1000 0.0190 ## 4 1.1350 nan 0.1000 0.0186 ## 5 1.1017 nan 0.1000 0.0147 ## 6 1.0810 nan 0.1000 0.0096 ## 7 1.0570 nan 0.1000 0.0105 ## 8 1.0375 nan 0.1000 0.0098 ## 9 1.0188 nan 0.1000 0.0081 ## 10 1.0020 nan 0.1000 0.0072 ## 20 0.9034 nan 0.1000 0.0023 ## 40 0.8070 nan 0.1000 -0.0001 ## 60 0.7671 nan 0.1000 -0.0003 ## 80 0.7500 nan 0.1000 -0.0007 ## 100 0.7415 nan 0.1000 -0.0008 ## 120 0.7349 nan 0.1000 -0.0008 ## 140 0.7282 nan 0.1000 -0.0006 ## 160 0.7222 nan 0.1000 -0.0006 ## 180 0.7190 nan 0.1000 -0.0003 ## 200 0.7160 nan 0.1000 -0.0013 ## 220 0.7128 nan 0.1000 -0.0008 ## 240 0.7092 nan 0.1000 -0.0012 ## 250 0.7066 nan 0.1000 -0.0005 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2597 nan 0.1000 0.0362 ## 2 1.2019 nan 0.1000 0.0279 ## 3 1.1504 nan 0.1000 0.0230 ## 4 1.1084 nan 0.1000 0.0168 ## 5 1.0702 nan 0.1000 0.0184 ## 6 1.0378 nan 0.1000 0.0136 ## 7 1.0090 nan 0.1000 0.0131 ## 8 0.9848 nan 0.1000 0.0115 ## 9 0.9632 nan 0.1000 0.0078 ## 10 0.9410 nan 0.1000 0.0094 ## 20 0.8215 nan 0.1000 0.0037 ## 40 0.7478 nan 0.1000 -0.0001 ## 60 0.7184 nan 0.1000 -0.0008 ## 80 0.6985 nan 0.1000 -0.0007 ## 100 0.6799 nan 0.1000 -0.0013 ## 120 0.6660 nan 0.1000 0.0002 ## 140 0.6534 nan 0.1000 -0.0009 ## 160 0.6423 nan 0.1000 -0.0016 ## 180 0.6325 nan 0.1000 -0.0016 ## 200 0.6226 nan 0.1000 -0.0011 ## 220 0.6139 nan 0.1000 -0.0010 ## 240 0.6043 nan 0.1000 -0.0017 ## 250 0.6016 nan 0.1000 -0.0012 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2549 nan 0.1000 0.0413 ## 2 1.1843 nan 0.1000 0.0345 ## 3 1.1274 nan 0.1000 0.0245 ## 4 1.0809 nan 0.1000 0.0213 ## 5 1.0418 nan 0.1000 0.0178 ## 6 1.0056 nan 0.1000 0.0128 ## 7 0.9797 nan 0.1000 0.0122 ## 8 0.9562 nan 0.1000 0.0083 ## 9 0.9288 nan 0.1000 0.0116 ## 10 0.9070 nan 0.1000 0.0088 ## 20 0.7819 nan 0.1000 0.0013 ## 40 0.7119 nan 0.1000 -0.0008 ## 60 0.6706 nan 0.1000 -0.0013 ## 80 0.6482 nan 0.1000 -0.0018 ## 100 0.6271 nan 0.1000 -0.0027 ## 120 0.6065 nan 0.1000 -0.0013 ## 140 0.5901 nan 0.1000 -0.0021 ## 160 0.5709 nan 0.1000 -0.0009 ## 180 0.5563 nan 0.1000 -0.0005 ## 200 0.5427 nan 0.1000 -0.0025 ## 220 0.5333 nan 0.1000 -0.0012 ## 240 0.5239 nan 0.1000 -0.0015 ## 250 0.5186 nan 0.1000 -0.0018 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2503 nan 0.1000 0.0405 ## 2 1.1806 nan 0.1000 0.0343 ## 3 1.1211 nan 0.1000 0.0276 ## 4 1.0676 nan 0.1000 0.0242 ## 5 1.0226 nan 0.1000 0.0193 ## 6 0.9842 nan 0.1000 0.0164 ## 7 0.9549 nan 0.1000 0.0120 ## 8 0.9274 nan 0.1000 0.0125 ## 9 0.8994 nan 0.1000 0.0100 ## 10 0.8810 nan 0.1000 0.0068 ## 20 0.7641 nan 0.1000 0.0015 ## 40 0.6902 nan 0.1000 -0.0013 ## 60 0.6497 nan 0.1000 -0.0026 ## 80 0.6225 nan 0.1000 -0.0020 ## 100 0.5898 nan 0.1000 -0.0009 ## 120 0.5662 nan 0.1000 -0.0014 ## 140 0.5460 nan 0.1000 -0.0015 ## 160 0.5293 nan 0.1000 -0.0029 ## 180 0.5142 nan 0.1000 -0.0018 ## 200 0.5021 nan 0.1000 -0.0021 ## 220 0.4850 nan 0.1000 -0.0025 ## 240 0.4738 nan 0.1000 -0.0010 ## 250 0.4697 nan 0.1000 -0.0025 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2510 nan 0.1000 0.0400 ## 2 1.1788 nan 0.1000 0.0356 ## 3 1.1185 nan 0.1000 0.0284 ## 4 1.0652 nan 0.1000 0.0255 ## 5 1.0228 nan 0.1000 0.0209 ## 6 0.9819 nan 0.1000 0.0175 ## 7 0.9467 nan 0.1000 0.0146 ## 8 0.9149 nan 0.1000 0.0133 ## 9 0.8872 nan 0.1000 0.0105 ## 10 0.8681 nan 0.1000 0.0067 ## 20 0.7435 nan 0.1000 0.0013 ## 40 0.6620 nan 0.1000 -0.0012 ## 60 0.6096 nan 0.1000 -0.0011 ## 80 0.5775 nan 0.1000 -0.0017 ## 100 0.5491 nan 0.1000 -0.0016 ## 120 0.5248 nan 0.1000 -0.0030 ## 140 0.5082 nan 0.1000 -0.0016 ## 160 0.4873 nan 0.1000 -0.0022 ## 180 0.4700 nan 0.1000 -0.0020 ## 200 0.4543 nan 0.1000 -0.0013 ## 220 0.4388 nan 0.1000 -0.0031 ## 240 0.4269 nan 0.1000 -0.0013 ## 250 0.4237 nan 0.1000 -0.0015 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2737 nan 0.1000 0.0306 ## 2 1.2188 nan 0.1000 0.0241 ## 3 1.1752 nan 0.1000 0.0184 ## 4 1.1384 nan 0.1000 0.0168 ## 5 1.1052 nan 0.1000 0.0134 ## 6 1.0800 nan 0.1000 0.0103 ## 7 1.0601 nan 0.1000 0.0106 ## 8 1.0416 nan 0.1000 0.0097 ## 9 1.0234 nan 0.1000 0.0070 ## 10 1.0079 nan 0.1000 0.0055 ## 20 0.9090 nan 0.1000 0.0034 ## 40 0.8145 nan 0.1000 0.0004 ## 60 0.7708 nan 0.1000 0.0003 ## 80 0.7528 nan 0.1000 -0.0009 ## 100 0.7433 nan 0.1000 -0.0010 ## 120 0.7366 nan 0.1000 -0.0002 ## 140 0.7317 nan 0.1000 -0.0007 ## 160 0.7262 nan 0.1000 -0.0009 ## 180 0.7217 nan 0.1000 -0.0005 ## 200 0.7172 nan 0.1000 -0.0013 ## 220 0.7141 nan 0.1000 -0.0012 ## 240 0.7102 nan 0.1000 -0.0002 ## 250 0.7086 nan 0.1000 -0.0012 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2660 nan 0.1000 0.0361 ## 2 1.2076 nan 0.1000 0.0275 ## 3 1.1575 nan 0.1000 0.0247 ## 4 1.1197 nan 0.1000 0.0178 ## 5 1.0845 nan 0.1000 0.0173 ## 6 1.0529 nan 0.1000 0.0131 ## 7 1.0259 nan 0.1000 0.0123 ## 8 0.9969 nan 0.1000 0.0117 ## 9 0.9748 nan 0.1000 0.0082 ## 10 0.9535 nan 0.1000 0.0076 ## 20 0.8305 nan 0.1000 0.0033 ## 40 0.7475 nan 0.1000 -0.0002 ## 60 0.7207 nan 0.1000 -0.0018 ## 80 0.7037 nan 0.1000 -0.0017 ## 100 0.6882 nan 0.1000 -0.0014 ## 120 0.6802 nan 0.1000 -0.0010 ## 140 0.6709 nan 0.1000 -0.0015 ## 160 0.6613 nan 0.1000 -0.0014 ## 180 0.6523 nan 0.1000 -0.0004 ## 200 0.6449 nan 0.1000 -0.0011 ## 220 0.6367 nan 0.1000 -0.0014 ## 240 0.6286 nan 0.1000 -0.0013 ## 250 0.6233 nan 0.1000 -0.0019 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2551 nan 0.1000 0.0346 ## 2 1.1931 nan 0.1000 0.0315 ## 3 1.1316 nan 0.1000 0.0272 ## 4 1.0872 nan 0.1000 0.0204 ## 5 1.0437 nan 0.1000 0.0187 ## 6 1.0098 nan 0.1000 0.0139 ## 7 0.9818 nan 0.1000 0.0107 ## 8 0.9579 nan 0.1000 0.0111 ## 9 0.9321 nan 0.1000 0.0129 ## 10 0.9121 nan 0.1000 0.0066 ## 20 0.7838 nan 0.1000 0.0022 ## 40 0.7153 nan 0.1000 -0.0001 ## 60 0.6832 nan 0.1000 -0.0011 ## 80 0.6612 nan 0.1000 -0.0019 ## 100 0.6415 nan 0.1000 -0.0020 ## 120 0.6252 nan 0.1000 -0.0020 ## 140 0.6084 nan 0.1000 -0.0017 ## 160 0.5936 nan 0.1000 -0.0009 ## 180 0.5834 nan 0.1000 -0.0021 ## 200 0.5669 nan 0.1000 -0.0009 ## 220 0.5568 nan 0.1000 -0.0011 ## 240 0.5467 nan 0.1000 -0.0027 ## 250 0.5422 nan 0.1000 -0.0015 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2574 nan 0.1000 0.0391 ## 2 1.1858 nan 0.1000 0.0330 ## 3 1.1308 nan 0.1000 0.0237 ## 4 1.0768 nan 0.1000 0.0205 ## 5 1.0311 nan 0.1000 0.0203 ## 6 0.9934 nan 0.1000 0.0175 ## 7 0.9643 nan 0.1000 0.0121 ## 8 0.9342 nan 0.1000 0.0118 ## 9 0.9061 nan 0.1000 0.0104 ## 10 0.8869 nan 0.1000 0.0078 ## 20 0.7608 nan 0.1000 0.0000 ## 40 0.6861 nan 0.1000 -0.0012 ## 60 0.6452 nan 0.1000 -0.0007 ## 80 0.6147 nan 0.1000 -0.0020 ## 100 0.5919 nan 0.1000 -0.0015 ## 120 0.5685 nan 0.1000 -0.0015 ## 140 0.5516 nan 0.1000 -0.0007 ## 160 0.5322 nan 0.1000 -0.0024 ## 180 0.5188 nan 0.1000 -0.0013 ## 200 0.5045 nan 0.1000 -0.0011 ## 220 0.4913 nan 0.1000 -0.0012 ## 240 0.4791 nan 0.1000 -0.0021 ## 250 0.4735 nan 0.1000 -0.0011 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2526 nan 0.1000 0.0413 ## 2 1.1759 nan 0.1000 0.0342 ## 3 1.1130 nan 0.1000 0.0286 ## 4 1.0692 nan 0.1000 0.0198 ## 5 1.0244 nan 0.1000 0.0208 ## 6 0.9858 nan 0.1000 0.0160 ## 7 0.9512 nan 0.1000 0.0156 ## 8 0.9230 nan 0.1000 0.0086 ## 9 0.8945 nan 0.1000 0.0128 ## 10 0.8741 nan 0.1000 0.0071 ## 20 0.7489 nan 0.1000 0.0012 ## 40 0.6645 nan 0.1000 -0.0017 ## 60 0.6174 nan 0.1000 -0.0018 ## 80 0.5875 nan 0.1000 -0.0022 ## 100 0.5620 nan 0.1000 -0.0009 ## 120 0.5374 nan 0.1000 -0.0026 ## 140 0.5202 nan 0.1000 -0.0017 ## 160 0.5003 nan 0.1000 -0.0014 ## 180 0.4829 nan 0.1000 -0.0021 ## 200 0.4660 nan 0.1000 -0.0024 ## 220 0.4560 nan 0.1000 -0.0024 ## 240 0.4425 nan 0.1000 -0.0020 ## 250 0.4349 nan 0.1000 -0.0021 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2722 nan 0.1000 0.0322 ## 2 1.2256 nan 0.1000 0.0203 ## 3 1.1806 nan 0.1000 0.0216 ## 4 1.1444 nan 0.1000 0.0179 ## 5 1.1141 nan 0.1000 0.0147 ## 6 1.0890 nan 0.1000 0.0118 ## 7 1.0707 nan 0.1000 0.0085 ## 8 1.0493 nan 0.1000 0.0108 ## 9 1.0332 nan 0.1000 0.0069 ## 10 1.0157 nan 0.1000 0.0086 ## 20 0.9131 nan 0.1000 0.0039 ## 40 0.8200 nan 0.1000 0.0010 ## 60 0.7820 nan 0.1000 0.0004 ## 80 0.7654 nan 0.1000 -0.0014 ## 100 0.7561 nan 0.1000 -0.0009 ## 120 0.7476 nan 0.1000 -0.0011 ## 140 0.7412 nan 0.1000 -0.0003 ## 160 0.7365 nan 0.1000 -0.0004 ## 180 0.7340 nan 0.1000 -0.0005 ## 200 0.7299 nan 0.1000 -0.0007 ## 220 0.7266 nan 0.1000 -0.0008 ## 240 0.7237 nan 0.1000 -0.0008 ## 250 0.7234 nan 0.1000 -0.0005 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2651 nan 0.1000 0.0334 ## 2 1.2070 nan 0.1000 0.0269 ## 3 1.1557 nan 0.1000 0.0231 ## 4 1.1153 nan 0.1000 0.0171 ## 5 1.0815 nan 0.1000 0.0155 ## 6 1.0482 nan 0.1000 0.0152 ## 7 1.0172 nan 0.1000 0.0130 ## 8 0.9922 nan 0.1000 0.0112 ## 9 0.9735 nan 0.1000 0.0078 ## 10 0.9545 nan 0.1000 0.0083 ## 20 0.8345 nan 0.1000 0.0033 ## 40 0.7573 nan 0.1000 -0.0001 ## 60 0.7333 nan 0.1000 -0.0011 ## 80 0.7105 nan 0.1000 -0.0006 ## 100 0.6966 nan 0.1000 -0.0011 ## 120 0.6815 nan 0.1000 -0.0012 ## 140 0.6641 nan 0.1000 -0.0008 ## 160 0.6482 nan 0.1000 -0.0016 ## 180 0.6413 nan 0.1000 -0.0012 ## 200 0.6329 nan 0.1000 -0.0008 ## 220 0.6240 nan 0.1000 -0.0015 ## 240 0.6160 nan 0.1000 -0.0011 ## 250 0.6111 nan 0.1000 -0.0017 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2637 nan 0.1000 0.0352 ## 2 1.1913 nan 0.1000 0.0330 ## 3 1.1333 nan 0.1000 0.0254 ## 4 1.0844 nan 0.1000 0.0231 ## 5 1.0391 nan 0.1000 0.0195 ## 6 1.0021 nan 0.1000 0.0152 ## 7 0.9729 nan 0.1000 0.0118 ## 8 0.9513 nan 0.1000 0.0086 ## 9 0.9272 nan 0.1000 0.0096 ## 10 0.9064 nan 0.1000 0.0085 ## 20 0.7931 nan 0.1000 0.0041 ## 40 0.7237 nan 0.1000 -0.0004 ## 60 0.6981 nan 0.1000 -0.0012 ## 80 0.6718 nan 0.1000 -0.0026 ## 100 0.6529 nan 0.1000 -0.0016 ## 120 0.6396 nan 0.1000 -0.0010 ## 140 0.6233 nan 0.1000 -0.0013 ## 160 0.6101 nan 0.1000 -0.0011 ## 180 0.5932 nan 0.1000 -0.0015 ## 200 0.5794 nan 0.1000 -0.0010 ## 220 0.5645 nan 0.1000 -0.0011 ## 240 0.5498 nan 0.1000 -0.0018 ## 250 0.5448 nan 0.1000 -0.0012 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2583 nan 0.1000 0.0380 ## 2 1.1835 nan 0.1000 0.0345 ## 3 1.1231 nan 0.1000 0.0268 ## 4 1.0730 nan 0.1000 0.0218 ## 5 1.0329 nan 0.1000 0.0173 ## 6 0.9952 nan 0.1000 0.0161 ## 7 0.9630 nan 0.1000 0.0130 ## 8 0.9348 nan 0.1000 0.0126 ## 9 0.9109 nan 0.1000 0.0108 ## 10 0.8940 nan 0.1000 0.0058 ## 20 0.7714 nan 0.1000 0.0009 ## 40 0.6915 nan 0.1000 -0.0022 ## 60 0.6527 nan 0.1000 -0.0006 ## 80 0.6232 nan 0.1000 -0.0011 ## 100 0.6008 nan 0.1000 -0.0018 ## 120 0.5783 nan 0.1000 -0.0017 ## 140 0.5562 nan 0.1000 -0.0023 ## 160 0.5375 nan 0.1000 -0.0014 ## 180 0.5165 nan 0.1000 -0.0019 ## 200 0.5043 nan 0.1000 -0.0021 ## 220 0.4907 nan 0.1000 -0.0022 ## 240 0.4783 nan 0.1000 -0.0019 ## 250 0.4728 nan 0.1000 -0.0022 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2435 nan 0.1000 0.0421 ## 2 1.1748 nan 0.1000 0.0315 ## 3 1.1147 nan 0.1000 0.0294 ## 4 1.0672 nan 0.1000 0.0236 ## 5 1.0265 nan 0.1000 0.0188 ## 6 0.9873 nan 0.1000 0.0165 ## 7 0.9555 nan 0.1000 0.0151 ## 8 0.9289 nan 0.1000 0.0100 ## 9 0.9030 nan 0.1000 0.0085 ## 10 0.8836 nan 0.1000 0.0077 ## 20 0.7615 nan 0.1000 0.0016 ## 40 0.6861 nan 0.1000 -0.0016 ## 60 0.6393 nan 0.1000 -0.0017 ## 80 0.6022 nan 0.1000 -0.0021 ## 100 0.5787 nan 0.1000 -0.0023 ## 120 0.5532 nan 0.1000 -0.0024 ## 140 0.5303 nan 0.1000 -0.0017 ## 160 0.5079 nan 0.1000 -0.0015 ## 180 0.4894 nan 0.1000 -0.0017 ## 200 0.4716 nan 0.1000 -0.0019 ## 220 0.4537 nan 0.1000 -0.0017 ## 240 0.4389 nan 0.1000 -0.0007 ## 250 0.4327 nan 0.1000 -0.0013 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2786 nan 0.1000 0.0297 ## 2 1.2244 nan 0.1000 0.0249 ## 3 1.1873 nan 0.1000 0.0203 ## 4 1.1524 nan 0.1000 0.0140 ## 5 1.1216 nan 0.1000 0.0141 ## 6 1.0961 nan 0.1000 0.0125 ## 7 1.0730 nan 0.1000 0.0103 ## 8 1.0536 nan 0.1000 0.0086 ## 9 1.0376 nan 0.1000 0.0067 ## 10 1.0208 nan 0.1000 0.0083 ## 20 0.9191 nan 0.1000 0.0025 ## 40 0.8304 nan 0.1000 0.0006 ## 60 0.7967 nan 0.1000 -0.0009 ## 80 0.7809 nan 0.1000 -0.0007 ## 100 0.7724 nan 0.1000 -0.0012 ## 120 0.7661 nan 0.1000 -0.0004 ## 140 0.7613 nan 0.1000 -0.0012 ## 160 0.7558 nan 0.1000 -0.0004 ## 180 0.7511 nan 0.1000 -0.0004 ## 200 0.7460 nan 0.1000 -0.0006 ## 220 0.7421 nan 0.1000 -0.0013 ## 240 0.7388 nan 0.1000 -0.0007 ## 250 0.7365 nan 0.1000 -0.0009 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2662 nan 0.1000 0.0324 ## 2 1.2096 nan 0.1000 0.0223 ## 3 1.1565 nan 0.1000 0.0234 ## 4 1.1199 nan 0.1000 0.0184 ## 5 1.0844 nan 0.1000 0.0144 ## 6 1.0520 nan 0.1000 0.0153 ## 7 1.0254 nan 0.1000 0.0130 ## 8 1.0023 nan 0.1000 0.0091 ## 9 0.9789 nan 0.1000 0.0100 ## 10 0.9608 nan 0.1000 0.0074 ## 20 0.8450 nan 0.1000 0.0015 ## 40 0.7728 nan 0.1000 -0.0010 ## 60 0.7478 nan 0.1000 -0.0011 ## 80 0.7312 nan 0.1000 -0.0005 ## 100 0.7193 nan 0.1000 -0.0008 ## 120 0.7053 nan 0.1000 -0.0015 ## 140 0.6952 nan 0.1000 -0.0006 ## 160 0.6855 nan 0.1000 -0.0007 ## 180 0.6758 nan 0.1000 -0.0013 ## 200 0.6663 nan 0.1000 -0.0011 ## 220 0.6599 nan 0.1000 -0.0008 ## 240 0.6511 nan 0.1000 -0.0015 ## 250 0.6471 nan 0.1000 -0.0012 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2605 nan 0.1000 0.0359 ## 2 1.1990 nan 0.1000 0.0295 ## 3 1.1393 nan 0.1000 0.0250 ## 4 1.0933 nan 0.1000 0.0198 ## 5 1.0564 nan 0.1000 0.0142 ## 6 1.0189 nan 0.1000 0.0153 ## 7 0.9922 nan 0.1000 0.0114 ## 8 0.9677 nan 0.1000 0.0095 ## 9 0.9451 nan 0.1000 0.0105 ## 10 0.9247 nan 0.1000 0.0070 ## 20 0.8103 nan 0.1000 0.0028 ## 40 0.7386 nan 0.1000 -0.0008 ## 60 0.7043 nan 0.1000 -0.0029 ## 80 0.6768 nan 0.1000 -0.0006 ## 100 0.6509 nan 0.1000 -0.0012 ## 120 0.6340 nan 0.1000 -0.0015 ## 140 0.6159 nan 0.1000 -0.0020 ## 160 0.6042 nan 0.1000 -0.0015 ## 180 0.5892 nan 0.1000 -0.0009 ## 200 0.5800 nan 0.1000 -0.0015 ## 220 0.5693 nan 0.1000 -0.0015 ## 240 0.5583 nan 0.1000 -0.0014 ## 250 0.5545 nan 0.1000 -0.0009 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2535 nan 0.1000 0.0373 ## 2 1.1854 nan 0.1000 0.0315 ## 3 1.1321 nan 0.1000 0.0243 ## 4 1.0809 nan 0.1000 0.0232 ## 5 1.0425 nan 0.1000 0.0193 ## 6 1.0039 nan 0.1000 0.0155 ## 7 0.9741 nan 0.1000 0.0124 ## 8 0.9470 nan 0.1000 0.0120 ## 9 0.9198 nan 0.1000 0.0119 ## 10 0.9026 nan 0.1000 0.0068 ## 20 0.7913 nan 0.1000 0.0012 ## 40 0.7186 nan 0.1000 -0.0003 ## 60 0.6837 nan 0.1000 -0.0016 ## 80 0.6483 nan 0.1000 -0.0009 ## 100 0.6236 nan 0.1000 -0.0014 ## 120 0.5982 nan 0.1000 -0.0024 ## 140 0.5790 nan 0.1000 -0.0015 ## 160 0.5596 nan 0.1000 -0.0035 ## 180 0.5411 nan 0.1000 -0.0007 ## 200 0.5267 nan 0.1000 -0.0020 ## 220 0.5139 nan 0.1000 -0.0012 ## 240 0.4973 nan 0.1000 -0.0021 ## 250 0.4919 nan 0.1000 -0.0009 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2535 nan 0.1000 0.0388 ## 2 1.1840 nan 0.1000 0.0318 ## 3 1.1232 nan 0.1000 0.0262 ## 4 1.0698 nan 0.1000 0.0244 ## 5 1.0285 nan 0.1000 0.0185 ## 6 0.9955 nan 0.1000 0.0134 ## 7 0.9661 nan 0.1000 0.0146 ## 8 0.9362 nan 0.1000 0.0139 ## 9 0.9138 nan 0.1000 0.0090 ## 10 0.8958 nan 0.1000 0.0069 ## 20 0.7770 nan 0.1000 0.0004 ## 40 0.7015 nan 0.1000 -0.0011 ## 60 0.6610 nan 0.1000 -0.0024 ## 80 0.6300 nan 0.1000 -0.0035 ## 100 0.5995 nan 0.1000 -0.0016 ## 120 0.5673 nan 0.1000 -0.0024 ## 140 0.5459 nan 0.1000 -0.0038 ## 160 0.5245 nan 0.1000 -0.0029 ## 180 0.5053 nan 0.1000 -0.0016 ## 200 0.4902 nan 0.1000 -0.0012 ## 220 0.4748 nan 0.1000 -0.0026 ## 240 0.4577 nan 0.1000 -0.0022 ## 250 0.4513 nan 0.1000 -0.0027 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2725 nan 0.1000 0.0315 ## 2 1.2219 nan 0.1000 0.0246 ## 3 1.1809 nan 0.1000 0.0195 ## 4 1.1501 nan 0.1000 0.0136 ## 5 1.1172 nan 0.1000 0.0151 ## 6 1.0930 nan 0.1000 0.0122 ## 7 1.0669 nan 0.1000 0.0116 ## 8 1.0449 nan 0.1000 0.0093 ## 9 1.0285 nan 0.1000 0.0066 ## 10 1.0180 nan 0.1000 0.0038 ## 20 0.9148 nan 0.1000 0.0024 ## 40 0.8255 nan 0.1000 -0.0002 ## 60 0.7874 nan 0.1000 0.0005 ## 80 0.7699 nan 0.1000 -0.0002 ## 100 0.7627 nan 0.1000 -0.0013 ## 120 0.7552 nan 0.1000 -0.0003 ## 140 0.7490 nan 0.1000 -0.0009 ## 160 0.7430 nan 0.1000 -0.0010 ## 180 0.7382 nan 0.1000 -0.0008 ## 200 0.7349 nan 0.1000 -0.0005 ## 220 0.7314 nan 0.1000 -0.0013 ## 240 0.7268 nan 0.1000 -0.0007 ## 250 0.7253 nan 0.1000 -0.0006 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2623 nan 0.1000 0.0345 ## 2 1.2026 nan 0.1000 0.0294 ## 3 1.1548 nan 0.1000 0.0218 ## 4 1.1129 nan 0.1000 0.0177 ## 5 1.0812 nan 0.1000 0.0157 ## 6 1.0509 nan 0.1000 0.0147 ## 7 1.0280 nan 0.1000 0.0099 ## 8 1.0037 nan 0.1000 0.0104 ## 9 0.9810 nan 0.1000 0.0097 ## 10 0.9609 nan 0.1000 0.0089 ## 20 0.8459 nan 0.1000 0.0019 ## 40 0.7693 nan 0.1000 -0.0002 ## 60 0.7363 nan 0.1000 -0.0008 ## 80 0.7162 nan 0.1000 -0.0023 ## 100 0.7052 nan 0.1000 -0.0015 ## 120 0.6917 nan 0.1000 -0.0028 ## 140 0.6756 nan 0.1000 -0.0020 ## 160 0.6652 nan 0.1000 -0.0003 ## 180 0.6557 nan 0.1000 -0.0014 ## 200 0.6468 nan 0.1000 -0.0006 ## 220 0.6353 nan 0.1000 -0.0006 ## 240 0.6277 nan 0.1000 -0.0007 ## 250 0.6244 nan 0.1000 -0.0011 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2566 nan 0.1000 0.0386 ## 2 1.1937 nan 0.1000 0.0297 ## 3 1.1355 nan 0.1000 0.0258 ## 4 1.0909 nan 0.1000 0.0201 ## 5 1.0509 nan 0.1000 0.0179 ## 6 1.0145 nan 0.1000 0.0162 ## 7 0.9840 nan 0.1000 0.0123 ## 8 0.9551 nan 0.1000 0.0122 ## 9 0.9369 nan 0.1000 0.0088 ## 10 0.9164 nan 0.1000 0.0072 ## 20 0.8017 nan 0.1000 0.0007 ## 40 0.7255 nan 0.1000 -0.0002 ## 60 0.6940 nan 0.1000 -0.0009 ## 80 0.6684 nan 0.1000 -0.0008 ## 100 0.6504 nan 0.1000 -0.0011 ## 120 0.6325 nan 0.1000 -0.0013 ## 140 0.6140 nan 0.1000 -0.0013 ## 160 0.5974 nan 0.1000 -0.0002 ## 180 0.5833 nan 0.1000 -0.0010 ## 200 0.5666 nan 0.1000 -0.0011 ## 220 0.5550 nan 0.1000 -0.0020 ## 240 0.5451 nan 0.1000 -0.0023 ## 250 0.5372 nan 0.1000 -0.0015 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2498 nan 0.1000 0.0417 ## 2 1.1832 nan 0.1000 0.0281 ## 3 1.1271 nan 0.1000 0.0244 ## 4 1.0777 nan 0.1000 0.0218 ## 5 1.0407 nan 0.1000 0.0151 ## 6 1.0058 nan 0.1000 0.0141 ## 7 0.9753 nan 0.1000 0.0148 ## 8 0.9445 nan 0.1000 0.0133 ## 9 0.9192 nan 0.1000 0.0115 ## 10 0.9036 nan 0.1000 0.0060 ## 20 0.7800 nan 0.1000 0.0017 ## 40 0.7027 nan 0.1000 -0.0018 ## 60 0.6571 nan 0.1000 -0.0015 ## 80 0.6285 nan 0.1000 -0.0028 ## 100 0.6088 nan 0.1000 -0.0028 ## 120 0.5816 nan 0.1000 -0.0033 ## 140 0.5649 nan 0.1000 -0.0020 ## 160 0.5430 nan 0.1000 -0.0012 ## 180 0.5294 nan 0.1000 -0.0016 ## 200 0.5109 nan 0.1000 -0.0021 ## 220 0.4943 nan 0.1000 -0.0027 ## 240 0.4793 nan 0.1000 -0.0010 ## 250 0.4732 nan 0.1000 -0.0016 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2514 nan 0.1000 0.0374 ## 2 1.1825 nan 0.1000 0.0323 ## 3 1.1249 nan 0.1000 0.0244 ## 4 1.0758 nan 0.1000 0.0248 ## 5 1.0288 nan 0.1000 0.0205 ## 6 0.9950 nan 0.1000 0.0151 ## 7 0.9607 nan 0.1000 0.0120 ## 8 0.9307 nan 0.1000 0.0132 ## 9 0.9071 nan 0.1000 0.0100 ## 10 0.8828 nan 0.1000 0.0094 ## 20 0.7598 nan 0.1000 0.0013 ## 40 0.6744 nan 0.1000 -0.0017 ## 60 0.6343 nan 0.1000 -0.0018 ## 80 0.5947 nan 0.1000 -0.0013 ## 100 0.5667 nan 0.1000 -0.0019 ## 120 0.5457 nan 0.1000 -0.0029 ## 140 0.5235 nan 0.1000 -0.0024 ## 160 0.5036 nan 0.1000 -0.0019 ## 180 0.4856 nan 0.1000 -0.0030 ## 200 0.4678 nan 0.1000 -0.0020 ## 220 0.4534 nan 0.1000 -0.0023 ## 240 0.4412 nan 0.1000 -0.0018 ## 250 0.4336 nan 0.1000 -0.0019 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2803 nan 0.1000 0.0300 ## 2 1.2282 nan 0.1000 0.0252 ## 3 1.1859 nan 0.1000 0.0212 ## 4 1.1515 nan 0.1000 0.0150 ## 5 1.1195 nan 0.1000 0.0146 ## 6 1.0928 nan 0.1000 0.0127 ## 7 1.0679 nan 0.1000 0.0107 ## 8 1.0480 nan 0.1000 0.0092 ## 9 1.0320 nan 0.1000 0.0078 ## 10 1.0185 nan 0.1000 0.0060 ## 20 0.9144 nan 0.1000 0.0035 ## 40 0.8227 nan 0.1000 0.0009 ## 60 0.7776 nan 0.1000 -0.0003 ## 80 0.7595 nan 0.1000 -0.0005 ## 100 0.7498 nan 0.1000 -0.0002 ## 120 0.7441 nan 0.1000 -0.0002 ## 140 0.7392 nan 0.1000 -0.0006 ## 160 0.7347 nan 0.1000 -0.0004 ## 180 0.7323 nan 0.1000 -0.0013 ## 200 0.7259 nan 0.1000 -0.0003 ## 220 0.7226 nan 0.1000 -0.0003 ## 240 0.7204 nan 0.1000 -0.0004 ## 250 0.7180 nan 0.1000 -0.0010 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2639 nan 0.1000 0.0365 ## 2 1.2041 nan 0.1000 0.0277 ## 3 1.1555 nan 0.1000 0.0224 ## 4 1.1135 nan 0.1000 0.0190 ## 5 1.0741 nan 0.1000 0.0174 ## 6 1.0444 nan 0.1000 0.0128 ## 7 1.0172 nan 0.1000 0.0120 ## 8 0.9928 nan 0.1000 0.0114 ## 9 0.9698 nan 0.1000 0.0105 ## 10 0.9523 nan 0.1000 0.0071 ## 20 0.8290 nan 0.1000 0.0023 ## 40 0.7563 nan 0.1000 -0.0012 ## 60 0.7274 nan 0.1000 -0.0011 ## 80 0.7078 nan 0.1000 -0.0014 ## 100 0.6940 nan 0.1000 -0.0008 ## 120 0.6795 nan 0.1000 -0.0015 ## 140 0.6698 nan 0.1000 -0.0008 ## 160 0.6585 nan 0.1000 -0.0011 ## 180 0.6453 nan 0.1000 -0.0002 ## 200 0.6358 nan 0.1000 -0.0007 ## 220 0.6313 nan 0.1000 -0.0019 ## 240 0.6263 nan 0.1000 -0.0020 ## 250 0.6222 nan 0.1000 -0.0013 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2566 nan 0.1000 0.0352 ## 2 1.1939 nan 0.1000 0.0309 ## 3 1.1369 nan 0.1000 0.0252 ## 4 1.0871 nan 0.1000 0.0217 ## 5 1.0427 nan 0.1000 0.0188 ## 6 1.0083 nan 0.1000 0.0132 ## 7 0.9772 nan 0.1000 0.0138 ## 8 0.9496 nan 0.1000 0.0146 ## 9 0.9264 nan 0.1000 0.0103 ## 10 0.9027 nan 0.1000 0.0097 ## 20 0.7804 nan 0.1000 0.0026 ## 40 0.7150 nan 0.1000 -0.0014 ## 60 0.6814 nan 0.1000 -0.0023 ## 80 0.6575 nan 0.1000 -0.0021 ## 100 0.6395 nan 0.1000 -0.0021 ## 120 0.6198 nan 0.1000 -0.0014 ## 140 0.6030 nan 0.1000 -0.0016 ## 160 0.5852 nan 0.1000 -0.0013 ## 180 0.5747 nan 0.1000 -0.0017 ## 200 0.5616 nan 0.1000 -0.0014 ## 220 0.5461 nan 0.1000 -0.0019 ## 240 0.5350 nan 0.1000 -0.0025 ## 250 0.5289 nan 0.1000 -0.0008 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2508 nan 0.1000 0.0399 ## 2 1.1799 nan 0.1000 0.0327 ## 3 1.1202 nan 0.1000 0.0267 ## 4 1.0681 nan 0.1000 0.0230 ## 5 1.0267 nan 0.1000 0.0188 ## 6 0.9903 nan 0.1000 0.0144 ## 7 0.9579 nan 0.1000 0.0146 ## 8 0.9311 nan 0.1000 0.0122 ## 9 0.9093 nan 0.1000 0.0100 ## 10 0.8920 nan 0.1000 0.0071 ## 20 0.7669 nan 0.1000 0.0027 ## 40 0.6912 nan 0.1000 -0.0021 ## 60 0.6458 nan 0.1000 -0.0026 ## 80 0.6124 nan 0.1000 -0.0004 ## 100 0.5870 nan 0.1000 -0.0014 ## 120 0.5656 nan 0.1000 -0.0013 ## 140 0.5518 nan 0.1000 -0.0023 ## 160 0.5341 nan 0.1000 -0.0011 ## 180 0.5194 nan 0.1000 -0.0010 ## 200 0.5076 nan 0.1000 -0.0026 ## 220 0.4978 nan 0.1000 -0.0018 ## 240 0.4803 nan 0.1000 -0.0012 ## 250 0.4720 nan 0.1000 -0.0022 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2479 nan 0.1000 0.0415 ## 2 1.1762 nan 0.1000 0.0296 ## 3 1.1123 nan 0.1000 0.0268 ## 4 1.0657 nan 0.1000 0.0197 ## 5 1.0210 nan 0.1000 0.0199 ## 6 0.9815 nan 0.1000 0.0159 ## 7 0.9479 nan 0.1000 0.0105 ## 8 0.9197 nan 0.1000 0.0126 ## 9 0.8955 nan 0.1000 0.0088 ## 10 0.8742 nan 0.1000 0.0090 ## 20 0.7586 nan 0.1000 0.0012 ## 40 0.6843 nan 0.1000 -0.0018 ## 60 0.6355 nan 0.1000 -0.0027 ## 80 0.5939 nan 0.1000 -0.0014 ## 100 0.5648 nan 0.1000 -0.0014 ## 120 0.5434 nan 0.1000 -0.0016 ## 140 0.5223 nan 0.1000 -0.0021 ## 160 0.5026 nan 0.1000 -0.0021 ## 180 0.4889 nan 0.1000 -0.0021 ## 200 0.4758 nan 0.1000 -0.0012 ## 220 0.4520 nan 0.1000 -0.0008 ## 240 0.4417 nan 0.1000 -0.0027 ## 250 0.4358 nan 0.1000 -0.0023 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2643 nan 0.1000 0.0332 ## 2 1.2100 nan 0.1000 0.0272 ## 3 1.1593 nan 0.1000 0.0240 ## 4 1.1180 nan 0.1000 0.0196 ## 5 1.0825 nan 0.1000 0.0170 ## 6 1.0508 nan 0.1000 0.0135 ## 7 1.0219 nan 0.1000 0.0112 ## 8 0.9984 nan 0.1000 0.0118 ## 9 0.9778 nan 0.1000 0.0072 ## 10 0.9565 nan 0.1000 0.0096 ## 20 0.8389 nan 0.1000 0.0025 ## 40 0.7624 nan 0.1000 -0.0009 ## 50 0.7453 nan 0.1000 -0.0006 oj.gbm ## Stochastic Gradient Boosting ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 772, 770, 771, 771, 772, 771, ... ## Resampling results across tuning parameters: ## ## interaction.depth n.trees ROC Sens Spec ## 1 50 0.89 0.87 0.72 ## 1 100 0.89 0.87 0.73 ## 1 150 0.89 0.87 0.73 ## 1 200 0.88 0.87 0.72 ## 1 250 0.88 0.86 0.73 ## 2 50 0.89 0.87 0.75 ## 2 100 0.88 0.86 0.73 ## 2 150 0.88 0.87 0.74 ## 2 200 0.88 0.86 0.72 ## 2 250 0.88 0.85 0.73 ## 3 50 0.88 0.87 0.72 ## 3 100 0.88 0.86 0.75 ## 3 150 0.87 0.85 0.72 ## 3 200 0.87 0.84 0.72 ## 3 250 0.87 0.83 0.72 ## 4 50 0.89 0.86 0.76 ## 4 100 0.88 0.84 0.73 ## 4 150 0.87 0.84 0.73 ## 4 200 0.87 0.85 0.72 ## 4 250 0.87 0.85 0.72 ## 5 50 0.89 0.86 0.76 ## 5 100 0.88 0.85 0.75 ## 5 150 0.87 0.84 0.75 ## 5 200 0.86 0.85 0.75 ## 5 250 0.86 0.84 0.73 ## ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## ## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10 ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were n.trees = 50, interaction.depth = ## 2, shrinkage = 0.1 and n.minobsinnode = 10. plot(oj.gbm) oj.pred &lt;- predict(oj.gbm, oj_test, type = &quot;raw&quot;) plot(oj_test$Purchase, oj.pred, main = &quot;Gradient Boosing Classification: Predicted vs. Actual&quot;, xlab = &quot;Actual&quot;, ylab = &quot;Predicted&quot;) (oj.conf &lt;- confusionMatrix(data = oj.pred, reference = oj_test$Purchase)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 117 18 ## MM 13 65 ## ## Accuracy : 0.854 ## 95% CI : (0.8, 0.899) ## No Information Rate : 0.61 ## P-Value [Acc &gt; NIR] : 0.00000000000000483 ## ## Kappa : 0.691 ## ## Mcnemar&#39;s Test P-Value : 0.472 ## ## Sensitivity : 0.900 ## Specificity : 0.783 ## Pos Pred Value : 0.867 ## Neg Pred Value : 0.833 ## Prevalence : 0.610 ## Detection Rate : 0.549 ## Detection Prevalence : 0.634 ## Balanced Accuracy : 0.842 ## ## &#39;Positive&#39; Class : CH ## oj.gbm.acc &lt;- as.numeric(oj.conf$overall[1]) rm(oj.pred) rm(oj.conf) #plot(oj.bag$, oj.bag$finalModel$y) #plot(varImp(oj.gbm), main=&quot;Variable Importance with Gradient Boosting&quot;) 10.5.0.2 Gradient Boosting Regression Example Again using the Carseats data set to predict Sales, this time I’ll use the gradient boosting method by specifying method = \"gbm\". I’ll use tuneLength = 5 and not worry about tuneGrid anymore. Caret tunes the following hyperparameters. n.trees: number of boosting iterations (increasing n.trees reduces the error on training set, but may lead to over-fitting) interaction.depth: maximum tree depth (the default six - node tree appears to do an excellent job) shrinkage: learning rate (reduces the impact of each additional fitted base-learner (tree) by reducing the size of incremental steps and thus penalizes the importance of each consecutive iteration. The intuition is that it is better to improve a model by taking many small steps than by taking fewer large steps. If one of the boosting iterations turns out to be erroneous, its negative impact can be easily corrected in subsequent steps.) n.minobsinnode: mimimum terminal node size carseats.gbm &lt;- train(Sales ~ ., data = carseats_train, method = &quot;gbm&quot;, # for bagged tree tuneLength = 5, # choose up to 5 combinations of tuning parameters metric = &quot;RMSE&quot;, # evaluate hyperparamter combinations with ROC trControl = trainControl( method = &quot;cv&quot;, # k-fold cross validation number = 10, # 10 folds savePredictions = &quot;final&quot;, # save predictions for the optimal tuning parameter1 verboseIter = FALSE, returnData = FALSE ) ) ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.6155 nan 0.1000 0.3126 ## 2 7.3677 nan 0.1000 0.2471 ## 3 7.1383 nan 0.1000 0.1559 ## 4 6.8796 nan 0.1000 0.3000 ## 5 6.6084 nan 0.1000 0.2696 ## 6 6.3846 nan 0.1000 0.1575 ## 7 6.1551 nan 0.1000 0.2016 ## 8 5.9837 nan 0.1000 0.1171 ## 9 5.7969 nan 0.1000 0.1558 ## 10 5.6503 nan 0.1000 0.1243 ## 20 4.5758 nan 0.1000 0.0472 ## 40 3.3276 nan 0.1000 0.0043 ## 60 2.6161 nan 0.1000 0.0154 ## 80 2.1215 nan 0.1000 -0.0029 ## 100 1.7822 nan 0.1000 -0.0166 ## 120 1.5354 nan 0.1000 -0.0016 ## 140 1.3313 nan 0.1000 0.0067 ## 160 1.2074 nan 0.1000 -0.0030 ## 180 1.0966 nan 0.1000 0.0005 ## 200 1.0083 nan 0.1000 -0.0019 ## 220 0.9572 nan 0.1000 -0.0008 ## 240 0.9048 nan 0.1000 -0.0052 ## 250 0.8922 nan 0.1000 -0.0051 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.4939 nan 0.1000 0.5303 ## 2 6.9609 nan 0.1000 0.4503 ## 3 6.5646 nan 0.1000 0.3312 ## 4 6.2135 nan 0.1000 0.2836 ## 5 5.9347 nan 0.1000 0.2472 ## 6 5.6654 nan 0.1000 0.2045 ## 7 5.3757 nan 0.1000 0.2134 ## 8 5.1883 nan 0.1000 0.1810 ## 9 5.0431 nan 0.1000 0.1205 ## 10 4.8440 nan 0.1000 0.0749 ## 20 3.4421 nan 0.1000 0.1291 ## 40 2.0285 nan 0.1000 0.0246 ## 60 1.4136 nan 0.1000 0.0003 ## 80 1.0839 nan 0.1000 0.0003 ## 100 0.9011 nan 0.1000 0.0052 ## 120 0.8195 nan 0.1000 -0.0075 ## 140 0.7664 nan 0.1000 -0.0036 ## 160 0.7243 nan 0.1000 -0.0010 ## 180 0.6839 nan 0.1000 -0.0028 ## 200 0.6448 nan 0.1000 -0.0048 ## 220 0.6123 nan 0.1000 -0.0035 ## 240 0.5897 nan 0.1000 -0.0038 ## 250 0.5767 nan 0.1000 -0.0050 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.3122 nan 0.1000 0.5956 ## 2 6.7737 nan 0.1000 0.4295 ## 3 6.3033 nan 0.1000 0.4215 ## 4 5.9081 nan 0.1000 0.2966 ## 5 5.5342 nan 0.1000 0.3320 ## 6 5.1666 nan 0.1000 0.2748 ## 7 4.9365 nan 0.1000 0.1688 ## 8 4.6557 nan 0.1000 0.2144 ## 9 4.4412 nan 0.1000 0.1072 ## 10 4.2075 nan 0.1000 0.1861 ## 20 2.7722 nan 0.1000 0.0107 ## 40 1.4659 nan 0.1000 0.0203 ## 60 1.0163 nan 0.1000 -0.0055 ## 80 0.8430 nan 0.1000 -0.0074 ## 100 0.7427 nan 0.1000 -0.0031 ## 120 0.6731 nan 0.1000 -0.0078 ## 140 0.6151 nan 0.1000 -0.0080 ## 160 0.5814 nan 0.1000 -0.0074 ## 180 0.5452 nan 0.1000 -0.0054 ## 200 0.5023 nan 0.1000 -0.0071 ## 220 0.4697 nan 0.1000 -0.0058 ## 240 0.4340 nan 0.1000 -0.0022 ## 250 0.4207 nan 0.1000 -0.0088 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.3508 nan 0.1000 0.7012 ## 2 6.7011 nan 0.1000 0.5533 ## 3 6.1585 nan 0.1000 0.5138 ## 4 5.7274 nan 0.1000 0.3542 ## 5 5.2438 nan 0.1000 0.3597 ## 6 4.9357 nan 0.1000 0.2545 ## 7 4.6359 nan 0.1000 0.2195 ## 8 4.3960 nan 0.1000 0.2294 ## 9 4.1786 nan 0.1000 0.1539 ## 10 3.9850 nan 0.1000 0.1414 ## 20 2.4927 nan 0.1000 0.0532 ## 40 1.2882 nan 0.1000 0.0112 ## 60 0.8551 nan 0.1000 -0.0074 ## 80 0.6752 nan 0.1000 -0.0044 ## 100 0.5795 nan 0.1000 -0.0097 ## 120 0.4983 nan 0.1000 -0.0038 ## 140 0.4440 nan 0.1000 -0.0032 ## 160 0.4044 nan 0.1000 -0.0056 ## 180 0.3666 nan 0.1000 -0.0076 ## 200 0.3337 nan 0.1000 -0.0065 ## 220 0.3066 nan 0.1000 -0.0022 ## 240 0.2736 nan 0.1000 -0.0039 ## 250 0.2608 nan 0.1000 -0.0038 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.2719 nan 0.1000 0.7029 ## 2 6.5623 nan 0.1000 0.5180 ## 3 6.0426 nan 0.1000 0.4144 ## 4 5.5613 nan 0.1000 0.4419 ## 5 5.1070 nan 0.1000 0.2878 ## 6 4.7641 nan 0.1000 0.2383 ## 7 4.4677 nan 0.1000 0.2141 ## 8 4.2399 nan 0.1000 0.0706 ## 9 3.9751 nan 0.1000 0.1927 ## 10 3.7100 nan 0.1000 0.1701 ## 20 2.2565 nan 0.1000 0.0859 ## 40 1.1000 nan 0.1000 -0.0080 ## 60 0.7464 nan 0.1000 -0.0020 ## 80 0.5790 nan 0.1000 -0.0024 ## 100 0.4941 nan 0.1000 -0.0054 ## 120 0.4290 nan 0.1000 -0.0030 ## 140 0.3723 nan 0.1000 -0.0052 ## 160 0.3330 nan 0.1000 -0.0042 ## 180 0.2951 nan 0.1000 -0.0056 ## 200 0.2627 nan 0.1000 -0.0038 ## 220 0.2336 nan 0.1000 -0.0018 ## 240 0.2089 nan 0.1000 -0.0017 ## 250 0.1964 nan 0.1000 -0.0029 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.8758 nan 0.1000 0.1611 ## 2 7.4638 nan 0.1000 0.4117 ## 3 7.1509 nan 0.1000 0.2868 ## 4 6.8550 nan 0.1000 0.2921 ## 5 6.6144 nan 0.1000 0.2482 ## 6 6.4519 nan 0.1000 0.1654 ## 7 6.3108 nan 0.1000 0.0792 ## 8 6.1631 nan 0.1000 0.1260 ## 9 6.0160 nan 0.1000 0.1119 ## 10 5.8043 nan 0.1000 0.1567 ## 20 4.6275 nan 0.1000 0.0715 ## 40 3.4435 nan 0.1000 -0.0080 ## 60 2.6905 nan 0.1000 0.0048 ## 80 2.1544 nan 0.1000 0.0127 ## 100 1.7772 nan 0.1000 -0.0062 ## 120 1.4927 nan 0.1000 -0.0058 ## 140 1.3013 nan 0.1000 0.0010 ## 160 1.1609 nan 0.1000 0.0023 ## 180 1.0670 nan 0.1000 -0.0058 ## 200 0.9890 nan 0.1000 -0.0088 ## 220 0.9407 nan 0.1000 0.0000 ## 240 0.9016 nan 0.1000 -0.0042 ## 250 0.8853 nan 0.1000 -0.0037 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.6412 nan 0.1000 0.5298 ## 2 7.0819 nan 0.1000 0.4917 ## 3 6.5521 nan 0.1000 0.3297 ## 4 6.1791 nan 0.1000 0.2744 ## 5 5.9412 nan 0.1000 0.1842 ## 6 5.6434 nan 0.1000 0.2504 ## 7 5.3703 nan 0.1000 0.2167 ## 8 5.1224 nan 0.1000 0.1739 ## 9 4.9715 nan 0.1000 0.1184 ## 10 4.7654 nan 0.1000 0.1615 ## 20 3.3795 nan 0.1000 0.0636 ## 40 2.0395 nan 0.1000 0.0080 ## 60 1.4605 nan 0.1000 -0.0029 ## 80 1.1344 nan 0.1000 -0.0025 ## 100 0.9495 nan 0.1000 -0.0087 ## 120 0.8562 nan 0.1000 -0.0037 ## 140 0.7855 nan 0.1000 -0.0043 ## 160 0.7298 nan 0.1000 -0.0057 ## 180 0.6813 nan 0.1000 -0.0010 ## 200 0.6441 nan 0.1000 -0.0027 ## 220 0.6118 nan 0.1000 -0.0040 ## 240 0.5838 nan 0.1000 -0.0071 ## 250 0.5734 nan 0.1000 -0.0007 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.4922 nan 0.1000 0.6088 ## 2 6.9029 nan 0.1000 0.5644 ## 3 6.4669 nan 0.1000 0.4526 ## 4 5.9980 nan 0.1000 0.4160 ## 5 5.5505 nan 0.1000 0.2603 ## 6 5.2643 nan 0.1000 0.2640 ## 7 5.0169 nan 0.1000 0.1944 ## 8 4.8024 nan 0.1000 0.1732 ## 9 4.5720 nan 0.1000 0.1124 ## 10 4.3508 nan 0.1000 0.1700 ## 20 2.8015 nan 0.1000 0.0839 ## 40 1.5058 nan 0.1000 0.0041 ## 60 1.0433 nan 0.1000 -0.0028 ## 80 0.8409 nan 0.1000 -0.0038 ## 100 0.7262 nan 0.1000 -0.0091 ## 120 0.6504 nan 0.1000 -0.0011 ## 140 0.5944 nan 0.1000 -0.0082 ## 160 0.5390 nan 0.1000 -0.0053 ## 180 0.5004 nan 0.1000 -0.0058 ## 200 0.4655 nan 0.1000 -0.0034 ## 220 0.4306 nan 0.1000 -0.0051 ## 240 0.4014 nan 0.1000 -0.0052 ## 250 0.3922 nan 0.1000 -0.0045 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.5384 nan 0.1000 0.6216 ## 2 6.8492 nan 0.1000 0.5668 ## 3 6.2425 nan 0.1000 0.4236 ## 4 5.7761 nan 0.1000 0.4459 ## 5 5.3374 nan 0.1000 0.3815 ## 6 5.0229 nan 0.1000 0.2852 ## 7 4.7392 nan 0.1000 0.2818 ## 8 4.4323 nan 0.1000 0.1682 ## 9 4.2010 nan 0.1000 0.2173 ## 10 3.9190 nan 0.1000 0.1539 ## 20 2.3973 nan 0.1000 0.0126 ## 40 1.2163 nan 0.1000 0.0078 ## 60 0.8634 nan 0.1000 -0.0088 ## 80 0.7010 nan 0.1000 -0.0174 ## 100 0.6125 nan 0.1000 -0.0047 ## 120 0.5409 nan 0.1000 -0.0043 ## 140 0.4880 nan 0.1000 -0.0087 ## 160 0.4416 nan 0.1000 -0.0077 ## 180 0.4022 nan 0.1000 -0.0087 ## 200 0.3700 nan 0.1000 -0.0066 ## 220 0.3326 nan 0.1000 -0.0048 ## 240 0.3010 nan 0.1000 -0.0020 ## 250 0.2897 nan 0.1000 -0.0057 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.4641 nan 0.1000 0.6228 ## 2 6.7842 nan 0.1000 0.6612 ## 3 6.2574 nan 0.1000 0.5100 ## 4 5.6693 nan 0.1000 0.5417 ## 5 5.2506 nan 0.1000 0.3664 ## 6 4.8195 nan 0.1000 0.3499 ## 7 4.4803 nan 0.1000 0.2959 ## 8 4.1807 nan 0.1000 0.1964 ## 9 3.9058 nan 0.1000 0.1460 ## 10 3.6831 nan 0.1000 0.1246 ## 20 2.1373 nan 0.1000 0.0659 ## 40 1.0923 nan 0.1000 0.0073 ## 60 0.7575 nan 0.1000 -0.0129 ## 80 0.6127 nan 0.1000 -0.0132 ## 100 0.5130 nan 0.1000 -0.0123 ## 120 0.4313 nan 0.1000 -0.0060 ## 140 0.3732 nan 0.1000 -0.0102 ## 160 0.3229 nan 0.1000 -0.0040 ## 180 0.2862 nan 0.1000 -0.0015 ## 200 0.2556 nan 0.1000 -0.0035 ## 220 0.2289 nan 0.1000 -0.0049 ## 240 0.2066 nan 0.1000 -0.0024 ## 250 0.1957 nan 0.1000 -0.0030 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 8.0133 nan 0.1000 0.2136 ## 2 7.6170 nan 0.1000 0.3813 ## 3 7.2757 nan 0.1000 0.3566 ## 4 6.9741 nan 0.1000 0.2780 ## 5 6.7490 nan 0.1000 0.2272 ## 6 6.6030 nan 0.1000 0.0955 ## 7 6.3914 nan 0.1000 0.1949 ## 8 6.2217 nan 0.1000 0.1345 ## 9 6.0156 nan 0.1000 0.1629 ## 10 5.8483 nan 0.1000 0.1217 ## 20 4.7672 nan 0.1000 0.0084 ## 40 3.5325 nan 0.1000 0.0072 ## 60 2.7373 nan 0.1000 0.0204 ## 80 2.2393 nan 0.1000 0.0177 ## 100 1.8533 nan 0.1000 -0.0056 ## 120 1.5588 nan 0.1000 -0.0006 ## 140 1.3684 nan 0.1000 -0.0059 ## 160 1.2137 nan 0.1000 -0.0029 ## 180 1.0929 nan 0.1000 0.0053 ## 200 1.0225 nan 0.1000 -0.0018 ## 220 0.9612 nan 0.1000 -0.0014 ## 240 0.9268 nan 0.1000 -0.0089 ## 250 0.9073 nan 0.1000 -0.0005 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.6769 nan 0.1000 0.4910 ## 2 7.1336 nan 0.1000 0.4624 ## 3 6.7130 nan 0.1000 0.3977 ## 4 6.3914 nan 0.1000 0.2512 ## 5 6.0813 nan 0.1000 0.3267 ## 6 5.8205 nan 0.1000 0.1912 ## 7 5.5575 nan 0.1000 0.1760 ## 8 5.3300 nan 0.1000 0.1566 ## 9 5.1406 nan 0.1000 0.0895 ## 10 4.9999 nan 0.1000 0.0993 ## 20 3.4990 nan 0.1000 0.1416 ## 40 2.1301 nan 0.1000 0.0205 ## 60 1.4553 nan 0.1000 -0.0010 ## 80 1.1386 nan 0.1000 -0.0129 ## 100 0.9532 nan 0.1000 -0.0062 ## 120 0.8461 nan 0.1000 -0.0020 ## 140 0.7941 nan 0.1000 -0.0103 ## 160 0.7479 nan 0.1000 -0.0069 ## 180 0.7076 nan 0.1000 -0.0046 ## 200 0.6764 nan 0.1000 -0.0034 ## 220 0.6375 nan 0.1000 -0.0039 ## 240 0.6118 nan 0.1000 -0.0084 ## 250 0.5967 nan 0.1000 -0.0038 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.5573 nan 0.1000 0.6889 ## 2 6.9522 nan 0.1000 0.5641 ## 3 6.4789 nan 0.1000 0.3647 ## 4 6.1178 nan 0.1000 0.1990 ## 5 5.7289 nan 0.1000 0.3137 ## 6 5.4295 nan 0.1000 0.1935 ## 7 5.0791 nan 0.1000 0.2922 ## 8 4.8212 nan 0.1000 0.2147 ## 9 4.5435 nan 0.1000 0.1970 ## 10 4.3324 nan 0.1000 0.1800 ## 20 2.8598 nan 0.1000 0.0779 ## 40 1.5947 nan 0.1000 0.0099 ## 60 1.0802 nan 0.1000 -0.0061 ## 80 0.8896 nan 0.1000 -0.0082 ## 100 0.7621 nan 0.1000 -0.0069 ## 120 0.6690 nan 0.1000 -0.0080 ## 140 0.6158 nan 0.1000 -0.0050 ## 160 0.5707 nan 0.1000 -0.0019 ## 180 0.5336 nan 0.1000 -0.0046 ## 200 0.4984 nan 0.1000 -0.0062 ## 220 0.4607 nan 0.1000 -0.0045 ## 240 0.4323 nan 0.1000 -0.0052 ## 250 0.4196 nan 0.1000 -0.0041 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.4906 nan 0.1000 0.6789 ## 2 6.9506 nan 0.1000 0.3624 ## 3 6.3953 nan 0.1000 0.4020 ## 4 5.9823 nan 0.1000 0.3891 ## 5 5.5966 nan 0.1000 0.2061 ## 6 5.1879 nan 0.1000 0.2865 ## 7 4.8319 nan 0.1000 0.2808 ## 8 4.5432 nan 0.1000 0.2153 ## 9 4.2635 nan 0.1000 0.2355 ## 10 3.9960 nan 0.1000 0.1091 ## 20 2.4108 nan 0.1000 0.1056 ## 40 1.2650 nan 0.1000 0.0002 ## 60 0.8721 nan 0.1000 0.0013 ## 80 0.6979 nan 0.1000 -0.0035 ## 100 0.5984 nan 0.1000 -0.0062 ## 120 0.5296 nan 0.1000 -0.0059 ## 140 0.4772 nan 0.1000 -0.0068 ## 160 0.4300 nan 0.1000 -0.0125 ## 180 0.3866 nan 0.1000 -0.0057 ## 200 0.3463 nan 0.1000 -0.0064 ## 220 0.3136 nan 0.1000 -0.0027 ## 240 0.2853 nan 0.1000 -0.0041 ## 250 0.2719 nan 0.1000 -0.0052 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.4823 nan 0.1000 0.6726 ## 2 6.8225 nan 0.1000 0.6054 ## 3 6.2303 nan 0.1000 0.4431 ## 4 5.7262 nan 0.1000 0.3136 ## 5 5.2917 nan 0.1000 0.3020 ## 6 4.9481 nan 0.1000 0.2905 ## 7 4.5620 nan 0.1000 0.2910 ## 8 4.3225 nan 0.1000 0.1791 ## 9 4.0699 nan 0.1000 0.2349 ## 10 3.8179 nan 0.1000 0.1849 ## 20 2.3059 nan 0.1000 0.0418 ## 40 1.1688 nan 0.1000 -0.0035 ## 60 0.7851 nan 0.1000 0.0072 ## 80 0.6170 nan 0.1000 -0.0038 ## 100 0.5177 nan 0.1000 -0.0026 ## 120 0.4381 nan 0.1000 -0.0056 ## 140 0.3801 nan 0.1000 -0.0035 ## 160 0.3340 nan 0.1000 -0.0031 ## 180 0.2933 nan 0.1000 -0.0080 ## 200 0.2558 nan 0.1000 -0.0058 ## 220 0.2289 nan 0.1000 -0.0028 ## 240 0.2021 nan 0.1000 -0.0050 ## 250 0.1882 nan 0.1000 -0.0018 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.4102 nan 0.1000 0.3839 ## 2 7.0494 nan 0.1000 0.3040 ## 3 6.8614 nan 0.1000 0.1707 ## 4 6.6267 nan 0.1000 0.2365 ## 5 6.4159 nan 0.1000 0.2192 ## 6 6.2072 nan 0.1000 0.1818 ## 7 5.9995 nan 0.1000 0.1441 ## 8 5.8238 nan 0.1000 0.1470 ## 9 5.6784 nan 0.1000 0.0881 ## 10 5.5111 nan 0.1000 0.1191 ## 20 4.4453 nan 0.1000 0.0629 ## 40 3.2797 nan 0.1000 0.0303 ## 60 2.5893 nan 0.1000 0.0080 ## 80 2.1025 nan 0.1000 -0.0054 ## 100 1.7328 nan 0.1000 0.0016 ## 120 1.5056 nan 0.1000 0.0039 ## 140 1.3304 nan 0.1000 -0.0021 ## 160 1.2081 nan 0.1000 -0.0085 ## 180 1.1043 nan 0.1000 0.0002 ## 200 1.0182 nan 0.1000 0.0011 ## 220 0.9519 nan 0.1000 -0.0138 ## 240 0.9161 nan 0.1000 -0.0036 ## 250 0.9019 nan 0.1000 -0.0080 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.3296 nan 0.1000 0.5238 ## 2 6.8236 nan 0.1000 0.4935 ## 3 6.4024 nan 0.1000 0.3814 ## 4 6.0460 nan 0.1000 0.3352 ## 5 5.7799 nan 0.1000 0.2079 ## 6 5.5073 nan 0.1000 0.2249 ## 7 5.2509 nan 0.1000 0.1626 ## 8 5.0841 nan 0.1000 0.0927 ## 9 4.8799 nan 0.1000 0.1436 ## 10 4.7477 nan 0.1000 0.0329 ## 20 3.3287 nan 0.1000 0.0705 ## 40 2.0283 nan 0.1000 0.0048 ## 60 1.3816 nan 0.1000 0.0072 ## 80 1.0728 nan 0.1000 -0.0016 ## 100 0.9094 nan 0.1000 0.0076 ## 120 0.8114 nan 0.1000 -0.0058 ## 140 0.7446 nan 0.1000 -0.0069 ## 160 0.7042 nan 0.1000 -0.0008 ## 180 0.6679 nan 0.1000 -0.0038 ## 200 0.6315 nan 0.1000 -0.0026 ## 220 0.6063 nan 0.1000 -0.0076 ## 240 0.5797 nan 0.1000 -0.0034 ## 250 0.5662 nan 0.1000 -0.0046 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.2648 nan 0.1000 0.6118 ## 2 6.7163 nan 0.1000 0.5338 ## 3 6.2474 nan 0.1000 0.4288 ## 4 5.8703 nan 0.1000 0.2640 ## 5 5.5532 nan 0.1000 0.3193 ## 6 5.2463 nan 0.1000 0.3198 ## 7 4.9416 nan 0.1000 0.1943 ## 8 4.6990 nan 0.1000 0.1722 ## 9 4.4949 nan 0.1000 0.1469 ## 10 4.2995 nan 0.1000 0.1804 ## 20 2.7721 nan 0.1000 0.0861 ## 40 1.4803 nan 0.1000 0.0123 ## 60 1.0423 nan 0.1000 -0.0095 ## 80 0.8424 nan 0.1000 0.0072 ## 100 0.7137 nan 0.1000 -0.0079 ## 120 0.6440 nan 0.1000 -0.0074 ## 140 0.5827 nan 0.1000 -0.0044 ## 160 0.5432 nan 0.1000 -0.0097 ## 180 0.5079 nan 0.1000 -0.0044 ## 200 0.4749 nan 0.1000 -0.0006 ## 220 0.4448 nan 0.1000 -0.0068 ## 240 0.4197 nan 0.1000 -0.0041 ## 250 0.4073 nan 0.1000 -0.0040 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.1301 nan 0.1000 0.6518 ## 2 6.4611 nan 0.1000 0.5540 ## 3 5.8979 nan 0.1000 0.4264 ## 4 5.4531 nan 0.1000 0.3997 ## 5 5.0649 nan 0.1000 0.2551 ## 6 4.7495 nan 0.1000 0.2099 ## 7 4.4799 nan 0.1000 0.2317 ## 8 4.1899 nan 0.1000 0.2269 ## 9 4.0084 nan 0.1000 0.0679 ## 10 3.8183 nan 0.1000 0.0965 ## 20 2.3118 nan 0.1000 0.0409 ## 40 1.2467 nan 0.1000 0.0054 ## 60 0.8880 nan 0.1000 -0.0007 ## 80 0.7226 nan 0.1000 -0.0048 ## 100 0.6209 nan 0.1000 -0.0051 ## 120 0.5444 nan 0.1000 -0.0087 ## 140 0.4935 nan 0.1000 -0.0049 ## 160 0.4445 nan 0.1000 -0.0053 ## 180 0.3989 nan 0.1000 -0.0063 ## 200 0.3638 nan 0.1000 -0.0079 ## 220 0.3337 nan 0.1000 -0.0052 ## 240 0.3055 nan 0.1000 -0.0048 ## 250 0.2926 nan 0.1000 -0.0020 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.1126 nan 0.1000 0.7800 ## 2 6.4349 nan 0.1000 0.5736 ## 3 5.8711 nan 0.1000 0.5557 ## 4 5.4355 nan 0.1000 0.3727 ## 5 5.0058 nan 0.1000 0.2490 ## 6 4.5813 nan 0.1000 0.2892 ## 7 4.2200 nan 0.1000 0.3013 ## 8 3.8990 nan 0.1000 0.2893 ## 9 3.6677 nan 0.1000 0.1964 ## 10 3.4290 nan 0.1000 0.1762 ## 20 2.1135 nan 0.1000 0.0418 ## 40 1.0761 nan 0.1000 -0.0015 ## 60 0.7465 nan 0.1000 -0.0119 ## 80 0.6002 nan 0.1000 -0.0082 ## 100 0.4915 nan 0.1000 -0.0042 ## 120 0.4217 nan 0.1000 -0.0055 ## 140 0.3558 nan 0.1000 -0.0027 ## 160 0.3103 nan 0.1000 -0.0039 ## 180 0.2649 nan 0.1000 -0.0052 ## 200 0.2323 nan 0.1000 -0.0017 ## 220 0.2076 nan 0.1000 -0.0018 ## 240 0.1858 nan 0.1000 -0.0025 ## 250 0.1740 nan 0.1000 -0.0033 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.3456 nan 0.1000 0.2909 ## 2 7.1114 nan 0.1000 0.1263 ## 3 6.7570 nan 0.1000 0.2751 ## 4 6.5482 nan 0.1000 0.1337 ## 5 6.3257 nan 0.1000 0.2345 ## 6 6.1206 nan 0.1000 0.1383 ## 7 5.9301 nan 0.1000 0.1914 ## 8 5.7797 nan 0.1000 0.1295 ## 9 5.6019 nan 0.1000 0.1135 ## 10 5.4685 nan 0.1000 0.0847 ## 20 4.4722 nan 0.1000 0.0460 ## 40 3.3338 nan 0.1000 0.0101 ## 60 2.6477 nan 0.1000 0.0048 ## 80 2.1648 nan 0.1000 0.0153 ## 100 1.7916 nan 0.1000 -0.0011 ## 120 1.5267 nan 0.1000 0.0018 ## 140 1.3281 nan 0.1000 -0.0029 ## 160 1.1995 nan 0.1000 -0.0011 ## 180 1.1018 nan 0.1000 0.0001 ## 200 1.0288 nan 0.1000 -0.0059 ## 220 0.9667 nan 0.1000 -0.0033 ## 240 0.9174 nan 0.1000 -0.0045 ## 250 0.8974 nan 0.1000 -0.0025 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.1892 nan 0.1000 0.5473 ## 2 6.7512 nan 0.1000 0.4586 ## 3 6.4426 nan 0.1000 0.2760 ## 4 6.1358 nan 0.1000 0.2140 ## 5 5.8079 nan 0.1000 0.2639 ## 6 5.5512 nan 0.1000 0.2127 ## 7 5.3613 nan 0.1000 0.1317 ## 8 5.0590 nan 0.1000 0.2354 ## 9 4.9117 nan 0.1000 0.1361 ## 10 4.7130 nan 0.1000 0.1626 ## 20 3.4020 nan 0.1000 0.0257 ## 40 2.0751 nan 0.1000 0.0224 ## 60 1.4101 nan 0.1000 -0.0005 ## 80 1.1014 nan 0.1000 0.0065 ## 100 0.9405 nan 0.1000 -0.0067 ## 120 0.8391 nan 0.1000 -0.0066 ## 140 0.7718 nan 0.1000 -0.0054 ## 160 0.7291 nan 0.1000 -0.0095 ## 180 0.6810 nan 0.1000 -0.0036 ## 200 0.6457 nan 0.1000 -0.0071 ## 220 0.6189 nan 0.1000 -0.0034 ## 240 0.5895 nan 0.1000 -0.0023 ## 250 0.5794 nan 0.1000 -0.0084 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.1359 nan 0.1000 0.6309 ## 2 6.5967 nan 0.1000 0.4591 ## 3 6.1698 nan 0.1000 0.4132 ## 4 5.7599 nan 0.1000 0.3928 ## 5 5.4519 nan 0.1000 0.3237 ## 6 5.1401 nan 0.1000 0.2872 ## 7 4.9050 nan 0.1000 0.1392 ## 8 4.6196 nan 0.1000 0.2846 ## 9 4.3738 nan 0.1000 0.1828 ## 10 4.1835 nan 0.1000 0.1700 ## 20 2.8099 nan 0.1000 0.0580 ## 40 1.6151 nan 0.1000 -0.0065 ## 60 1.1301 nan 0.1000 0.0002 ## 80 0.8944 nan 0.1000 -0.0077 ## 100 0.7517 nan 0.1000 -0.0139 ## 120 0.6730 nan 0.1000 -0.0085 ## 140 0.6057 nan 0.1000 -0.0038 ## 160 0.5547 nan 0.1000 -0.0097 ## 180 0.5094 nan 0.1000 -0.0103 ## 200 0.4766 nan 0.1000 -0.0066 ## 220 0.4450 nan 0.1000 -0.0040 ## 240 0.4151 nan 0.1000 -0.0027 ## 250 0.4013 nan 0.1000 -0.0047 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.0294 nan 0.1000 0.7373 ## 2 6.4316 nan 0.1000 0.6149 ## 3 5.9113 nan 0.1000 0.3359 ## 4 5.5546 nan 0.1000 0.3535 ## 5 5.1917 nan 0.1000 0.2802 ## 6 4.8371 nan 0.1000 0.3563 ## 7 4.5468 nan 0.1000 0.2089 ## 8 4.3248 nan 0.1000 0.1893 ## 9 4.0651 nan 0.1000 0.1052 ## 10 3.8138 nan 0.1000 0.1666 ## 20 2.3320 nan 0.1000 0.0422 ## 40 1.2332 nan 0.1000 0.0086 ## 60 0.8464 nan 0.1000 0.0058 ## 80 0.6737 nan 0.1000 -0.0128 ## 100 0.5934 nan 0.1000 -0.0097 ## 120 0.5242 nan 0.1000 -0.0050 ## 140 0.4705 nan 0.1000 -0.0050 ## 160 0.4233 nan 0.1000 -0.0078 ## 180 0.3867 nan 0.1000 -0.0076 ## 200 0.3531 nan 0.1000 -0.0040 ## 220 0.3242 nan 0.1000 -0.0039 ## 240 0.3013 nan 0.1000 -0.0036 ## 250 0.2843 nan 0.1000 -0.0029 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.0098 nan 0.1000 0.5845 ## 2 6.3343 nan 0.1000 0.4158 ## 3 5.8514 nan 0.1000 0.3832 ## 4 5.3437 nan 0.1000 0.4617 ## 5 4.9793 nan 0.1000 0.3545 ## 6 4.6111 nan 0.1000 0.3660 ## 7 4.2957 nan 0.1000 0.2518 ## 8 3.9570 nan 0.1000 0.2273 ## 9 3.7320 nan 0.1000 0.1854 ## 10 3.5140 nan 0.1000 0.1503 ## 20 2.1057 nan 0.1000 0.0764 ## 40 1.0592 nan 0.1000 0.0046 ## 60 0.7323 nan 0.1000 -0.0010 ## 80 0.5881 nan 0.1000 -0.0058 ## 100 0.4956 nan 0.1000 -0.0051 ## 120 0.4248 nan 0.1000 -0.0025 ## 140 0.3718 nan 0.1000 -0.0073 ## 160 0.3294 nan 0.1000 -0.0062 ## 180 0.2819 nan 0.1000 -0.0026 ## 200 0.2497 nan 0.1000 -0.0034 ## 220 0.2233 nan 0.1000 -0.0058 ## 240 0.1990 nan 0.1000 -0.0018 ## 250 0.1871 nan 0.1000 -0.0021 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.5868 nan 0.1000 0.4115 ## 2 7.2833 nan 0.1000 0.2331 ## 3 7.0345 nan 0.1000 0.2570 ## 4 6.8101 nan 0.1000 0.1071 ## 5 6.5893 nan 0.1000 0.1563 ## 6 6.3092 nan 0.1000 0.1664 ## 7 6.1025 nan 0.1000 0.1476 ## 8 5.9663 nan 0.1000 0.0749 ## 9 5.7474 nan 0.1000 0.1445 ## 10 5.5960 nan 0.1000 0.0876 ## 20 4.4901 nan 0.1000 0.0532 ## 40 3.2925 nan 0.1000 0.0364 ## 60 2.6190 nan 0.1000 0.0090 ## 80 2.1208 nan 0.1000 0.0132 ## 100 1.7732 nan 0.1000 -0.0007 ## 120 1.5132 nan 0.1000 0.0046 ## 140 1.3283 nan 0.1000 0.0031 ## 160 1.1925 nan 0.1000 -0.0002 ## 180 1.0847 nan 0.1000 -0.0013 ## 200 0.9981 nan 0.1000 -0.0003 ## 220 0.9475 nan 0.1000 -0.0035 ## 240 0.9021 nan 0.1000 -0.0047 ## 250 0.8843 nan 0.1000 -0.0029 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.3282 nan 0.1000 0.5172 ## 2 6.9512 nan 0.1000 0.3872 ## 3 6.5318 nan 0.1000 0.3539 ## 4 6.1978 nan 0.1000 0.2757 ## 5 5.9066 nan 0.1000 0.2460 ## 6 5.6366 nan 0.1000 0.2517 ## 7 5.3731 nan 0.1000 0.1878 ## 8 5.1982 nan 0.1000 0.1405 ## 9 5.0115 nan 0.1000 0.1659 ## 10 4.8495 nan 0.1000 0.1095 ## 20 3.3891 nan 0.1000 0.1033 ## 40 2.0749 nan 0.1000 0.0310 ## 60 1.4400 nan 0.1000 0.0118 ## 80 1.1141 nan 0.1000 0.0030 ## 100 0.9360 nan 0.1000 0.0034 ## 120 0.8234 nan 0.1000 -0.0051 ## 140 0.7648 nan 0.1000 -0.0025 ## 160 0.7108 nan 0.1000 -0.0075 ## 180 0.6698 nan 0.1000 -0.0052 ## 200 0.6366 nan 0.1000 -0.0069 ## 220 0.6020 nan 0.1000 -0.0060 ## 240 0.5770 nan 0.1000 -0.0049 ## 250 0.5640 nan 0.1000 -0.0035 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.2616 nan 0.1000 0.6773 ## 2 6.8094 nan 0.1000 0.2167 ## 3 6.3219 nan 0.1000 0.4713 ## 4 5.9148 nan 0.1000 0.3566 ## 5 5.5959 nan 0.1000 0.2337 ## 6 5.2724 nan 0.1000 0.3200 ## 7 4.9638 nan 0.1000 0.1900 ## 8 4.7283 nan 0.1000 0.1678 ## 9 4.5198 nan 0.1000 0.1260 ## 10 4.3024 nan 0.1000 0.1451 ## 20 2.7404 nan 0.1000 0.0379 ## 40 1.5392 nan 0.1000 -0.0055 ## 60 1.0399 nan 0.1000 0.0123 ## 80 0.8192 nan 0.1000 0.0004 ## 100 0.7016 nan 0.1000 -0.0083 ## 120 0.6325 nan 0.1000 -0.0080 ## 140 0.5744 nan 0.1000 -0.0063 ## 160 0.5259 nan 0.1000 -0.0054 ## 180 0.4819 nan 0.1000 -0.0111 ## 200 0.4502 nan 0.1000 -0.0055 ## 220 0.4164 nan 0.1000 -0.0031 ## 240 0.3874 nan 0.1000 -0.0056 ## 250 0.3749 nan 0.1000 -0.0012 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.4180 nan 0.1000 0.3827 ## 2 6.7805 nan 0.1000 0.4994 ## 3 6.2576 nan 0.1000 0.4550 ## 4 5.8389 nan 0.1000 0.2288 ## 5 5.4627 nan 0.1000 0.3687 ## 6 5.1119 nan 0.1000 0.3303 ## 7 4.7963 nan 0.1000 0.3248 ## 8 4.5143 nan 0.1000 0.2401 ## 9 4.2403 nan 0.1000 0.1986 ## 10 4.0630 nan 0.1000 0.1325 ## 20 2.4827 nan 0.1000 0.0652 ## 40 1.2766 nan 0.1000 0.0153 ## 60 0.8585 nan 0.1000 -0.0103 ## 80 0.6931 nan 0.1000 -0.0009 ## 100 0.5966 nan 0.1000 -0.0085 ## 120 0.5283 nan 0.1000 -0.0111 ## 140 0.4718 nan 0.1000 -0.0106 ## 160 0.4263 nan 0.1000 -0.0058 ## 180 0.3870 nan 0.1000 -0.0051 ## 200 0.3533 nan 0.1000 -0.0027 ## 220 0.3194 nan 0.1000 -0.0032 ## 240 0.2922 nan 0.1000 -0.0019 ## 250 0.2785 nan 0.1000 -0.0040 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.1462 nan 0.1000 0.6854 ## 2 6.5338 nan 0.1000 0.3961 ## 3 6.0020 nan 0.1000 0.4159 ## 4 5.5196 nan 0.1000 0.3879 ## 5 5.0940 nan 0.1000 0.3601 ## 6 4.7927 nan 0.1000 0.2963 ## 7 4.4249 nan 0.1000 0.2827 ## 8 4.1084 nan 0.1000 0.2133 ## 9 3.8253 nan 0.1000 0.1881 ## 10 3.6343 nan 0.1000 0.1233 ## 20 2.1522 nan 0.1000 0.0354 ## 40 1.0580 nan 0.1000 0.0161 ## 60 0.7191 nan 0.1000 0.0018 ## 80 0.5670 nan 0.1000 -0.0057 ## 100 0.4832 nan 0.1000 -0.0050 ## 120 0.4124 nan 0.1000 -0.0054 ## 140 0.3602 nan 0.1000 -0.0078 ## 160 0.3179 nan 0.1000 -0.0036 ## 180 0.2778 nan 0.1000 -0.0047 ## 200 0.2450 nan 0.1000 -0.0059 ## 220 0.2183 nan 0.1000 -0.0040 ## 240 0.1929 nan 0.1000 -0.0048 ## 250 0.1839 nan 0.1000 -0.0040 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.4540 nan 0.1000 0.3641 ## 2 7.1373 nan 0.1000 0.3291 ## 3 6.8416 nan 0.1000 0.2730 ## 4 6.5864 nan 0.1000 0.1830 ## 5 6.3867 nan 0.1000 0.1603 ## 6 6.1413 nan 0.1000 0.2019 ## 7 5.9610 nan 0.1000 0.1395 ## 8 5.8194 nan 0.1000 0.1027 ## 9 5.6546 nan 0.1000 0.0827 ## 10 5.4631 nan 0.1000 0.0971 ## 20 4.3922 nan 0.1000 0.0188 ## 40 3.2426 nan 0.1000 0.0345 ## 60 2.5668 nan 0.1000 0.0223 ## 80 2.0713 nan 0.1000 0.0088 ## 100 1.7438 nan 0.1000 -0.0065 ## 120 1.4921 nan 0.1000 0.0043 ## 140 1.3219 nan 0.1000 0.0085 ## 160 1.1951 nan 0.1000 0.0011 ## 180 1.1011 nan 0.1000 -0.0045 ## 200 1.0331 nan 0.1000 -0.0013 ## 220 0.9765 nan 0.1000 -0.0020 ## 240 0.9362 nan 0.1000 -0.0053 ## 250 0.9130 nan 0.1000 0.0009 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.2752 nan 0.1000 0.5651 ## 2 6.7895 nan 0.1000 0.4457 ## 3 6.4533 nan 0.1000 0.2484 ## 4 6.1292 nan 0.1000 0.3299 ## 5 5.8643 nan 0.1000 0.2757 ## 6 5.6456 nan 0.1000 0.1455 ## 7 5.3780 nan 0.1000 0.2465 ## 8 5.1363 nan 0.1000 0.1833 ## 9 4.9400 nan 0.1000 0.1011 ## 10 4.7344 nan 0.1000 0.1801 ## 20 3.2834 nan 0.1000 0.0459 ## 40 2.0454 nan 0.1000 0.0186 ## 60 1.4428 nan 0.1000 0.0008 ## 80 1.0931 nan 0.1000 0.0050 ## 100 0.9245 nan 0.1000 -0.0066 ## 120 0.8152 nan 0.1000 -0.0040 ## 140 0.7456 nan 0.1000 -0.0028 ## 160 0.6979 nan 0.1000 -0.0052 ## 180 0.6573 nan 0.1000 -0.0035 ## 200 0.6239 nan 0.1000 -0.0037 ## 220 0.5949 nan 0.1000 -0.0042 ## 240 0.5695 nan 0.1000 -0.0039 ## 250 0.5578 nan 0.1000 -0.0039 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.2004 nan 0.1000 0.5575 ## 2 6.6875 nan 0.1000 0.5080 ## 3 6.1988 nan 0.1000 0.4364 ## 4 5.7727 nan 0.1000 0.3191 ## 5 5.4082 nan 0.1000 0.3497 ## 6 5.1722 nan 0.1000 0.1589 ## 7 4.8592 nan 0.1000 0.1847 ## 8 4.6133 nan 0.1000 0.2159 ## 9 4.3824 nan 0.1000 0.2002 ## 10 4.1895 nan 0.1000 0.1182 ## 20 2.7390 nan 0.1000 0.0733 ## 40 1.5162 nan 0.1000 -0.0008 ## 60 1.0207 nan 0.1000 0.0026 ## 80 0.8292 nan 0.1000 -0.0016 ## 100 0.7057 nan 0.1000 -0.0089 ## 120 0.6346 nan 0.1000 -0.0085 ## 140 0.5675 nan 0.1000 -0.0056 ## 160 0.5176 nan 0.1000 -0.0074 ## 180 0.4752 nan 0.1000 -0.0050 ## 200 0.4333 nan 0.1000 -0.0054 ## 220 0.4032 nan 0.1000 -0.0070 ## 240 0.3743 nan 0.1000 -0.0022 ## 250 0.3642 nan 0.1000 -0.0062 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.1706 nan 0.1000 0.6334 ## 2 6.5472 nan 0.1000 0.5452 ## 3 5.9610 nan 0.1000 0.4332 ## 4 5.5223 nan 0.1000 0.3950 ## 5 5.1475 nan 0.1000 0.2716 ## 6 4.8185 nan 0.1000 0.2069 ## 7 4.5333 nan 0.1000 0.2125 ## 8 4.2886 nan 0.1000 0.2125 ## 9 4.0286 nan 0.1000 0.1507 ## 10 3.8317 nan 0.1000 0.1457 ## 20 2.3895 nan 0.1000 0.0665 ## 40 1.2552 nan 0.1000 0.0223 ## 60 0.8795 nan 0.1000 0.0039 ## 80 0.7069 nan 0.1000 -0.0107 ## 100 0.6120 nan 0.1000 -0.0067 ## 120 0.5331 nan 0.1000 -0.0065 ## 140 0.4731 nan 0.1000 -0.0091 ## 160 0.4256 nan 0.1000 -0.0038 ## 180 0.3789 nan 0.1000 -0.0053 ## 200 0.3447 nan 0.1000 -0.0045 ## 220 0.3123 nan 0.1000 -0.0067 ## 240 0.2807 nan 0.1000 -0.0018 ## 250 0.2696 nan 0.1000 -0.0060 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.1181 nan 0.1000 0.6333 ## 2 6.4637 nan 0.1000 0.5209 ## 3 5.8982 nan 0.1000 0.5202 ## 4 5.4003 nan 0.1000 0.3026 ## 5 4.9882 nan 0.1000 0.2926 ## 6 4.6393 nan 0.1000 0.2601 ## 7 4.3294 nan 0.1000 0.2207 ## 8 4.0488 nan 0.1000 0.2276 ## 9 3.7542 nan 0.1000 0.2489 ## 10 3.5442 nan 0.1000 0.1313 ## 20 2.0640 nan 0.1000 0.0561 ## 40 1.0503 nan 0.1000 0.0116 ## 60 0.7338 nan 0.1000 -0.0011 ## 80 0.5913 nan 0.1000 -0.0046 ## 100 0.4891 nan 0.1000 -0.0047 ## 120 0.4050 nan 0.1000 -0.0044 ## 140 0.3428 nan 0.1000 -0.0083 ## 160 0.2935 nan 0.1000 -0.0027 ## 180 0.2561 nan 0.1000 0.0005 ## 200 0.2280 nan 0.1000 -0.0051 ## 220 0.1969 nan 0.1000 -0.0034 ## 240 0.1764 nan 0.1000 -0.0047 ## 250 0.1670 nan 0.1000 -0.0035 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.9913 nan 0.1000 0.3761 ## 2 7.7852 nan 0.1000 0.1808 ## 3 7.4353 nan 0.1000 0.3342 ## 4 7.2002 nan 0.1000 0.1743 ## 5 6.8764 nan 0.1000 0.3014 ## 6 6.7249 nan 0.1000 0.1377 ## 7 6.5608 nan 0.1000 0.0969 ## 8 6.3857 nan 0.1000 0.1119 ## 9 6.2081 nan 0.1000 0.2099 ## 10 6.0676 nan 0.1000 0.0350 ## 20 4.8481 nan 0.1000 0.0326 ## 40 3.5311 nan 0.1000 0.0243 ## 60 2.7637 nan 0.1000 -0.0042 ## 80 2.2298 nan 0.1000 0.0035 ## 100 1.8541 nan 0.1000 0.0040 ## 120 1.5720 nan 0.1000 -0.0142 ## 140 1.3723 nan 0.1000 0.0089 ## 160 1.2296 nan 0.1000 -0.0073 ## 180 1.1206 nan 0.1000 -0.0095 ## 200 1.0353 nan 0.1000 -0.0009 ## 220 0.9629 nan 0.1000 -0.0014 ## 240 0.9290 nan 0.1000 -0.0011 ## 250 0.9156 nan 0.1000 -0.0036 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.7709 nan 0.1000 0.4607 ## 2 7.2930 nan 0.1000 0.4519 ## 3 6.7154 nan 0.1000 0.2983 ## 4 6.4052 nan 0.1000 0.2385 ## 5 6.0937 nan 0.1000 0.2545 ## 6 5.7996 nan 0.1000 0.2344 ## 7 5.6159 nan 0.1000 0.1053 ## 8 5.4356 nan 0.1000 0.1415 ## 9 5.2146 nan 0.1000 0.1220 ## 10 5.0671 nan 0.1000 0.1222 ## 20 3.5111 nan 0.1000 0.0306 ## 40 2.0860 nan 0.1000 0.0249 ## 60 1.4474 nan 0.1000 0.0088 ## 80 1.1308 nan 0.1000 -0.0062 ## 100 0.9648 nan 0.1000 0.0020 ## 120 0.8736 nan 0.1000 -0.0089 ## 140 0.8043 nan 0.1000 -0.0064 ## 160 0.7498 nan 0.1000 -0.0083 ## 180 0.7018 nan 0.1000 0.0001 ## 200 0.6598 nan 0.1000 -0.0037 ## 220 0.6329 nan 0.1000 -0.0035 ## 240 0.6049 nan 0.1000 -0.0068 ## 250 0.5905 nan 0.1000 -0.0046 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.5947 nan 0.1000 0.5713 ## 2 6.9919 nan 0.1000 0.5903 ## 3 6.5417 nan 0.1000 0.4225 ## 4 6.0968 nan 0.1000 0.3213 ## 5 5.7124 nan 0.1000 0.3371 ## 6 5.3694 nan 0.1000 0.2772 ## 7 5.0464 nan 0.1000 0.2062 ## 8 4.7692 nan 0.1000 0.2608 ## 9 4.6153 nan 0.1000 0.0811 ## 10 4.4195 nan 0.1000 0.0977 ## 20 2.9502 nan 0.1000 0.1132 ## 40 1.5511 nan 0.1000 0.0204 ## 60 1.0570 nan 0.1000 0.0142 ## 80 0.8503 nan 0.1000 -0.0047 ## 100 0.7300 nan 0.1000 -0.0177 ## 120 0.6452 nan 0.1000 -0.0077 ## 140 0.5915 nan 0.1000 -0.0082 ## 160 0.5400 nan 0.1000 -0.0058 ## 180 0.5009 nan 0.1000 -0.0040 ## 200 0.4669 nan 0.1000 -0.0059 ## 220 0.4400 nan 0.1000 -0.0040 ## 240 0.4131 nan 0.1000 -0.0079 ## 250 0.4039 nan 0.1000 -0.0059 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.6580 nan 0.1000 0.6541 ## 2 7.0177 nan 0.1000 0.5653 ## 3 6.4657 nan 0.1000 0.5278 ## 4 5.9869 nan 0.1000 0.4523 ## 5 5.5715 nan 0.1000 0.3559 ## 6 5.1874 nan 0.1000 0.2871 ## 7 4.8558 nan 0.1000 0.2317 ## 8 4.5402 nan 0.1000 0.2236 ## 9 4.3011 nan 0.1000 0.1610 ## 10 4.0022 nan 0.1000 0.2235 ## 20 2.4352 nan 0.1000 0.0364 ## 40 1.2432 nan 0.1000 0.0032 ## 60 0.8629 nan 0.1000 -0.0036 ## 80 0.7137 nan 0.1000 -0.0020 ## 100 0.6171 nan 0.1000 -0.0102 ## 120 0.5389 nan 0.1000 -0.0066 ## 140 0.4772 nan 0.1000 -0.0067 ## 160 0.4290 nan 0.1000 -0.0091 ## 180 0.3873 nan 0.1000 -0.0116 ## 200 0.3411 nan 0.1000 -0.0052 ## 220 0.3122 nan 0.1000 -0.0030 ## 240 0.2868 nan 0.1000 -0.0020 ## 250 0.2726 nan 0.1000 -0.0027 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.4799 nan 0.1000 0.8543 ## 2 6.8255 nan 0.1000 0.5990 ## 3 6.2454 nan 0.1000 0.4987 ## 4 5.7558 nan 0.1000 0.3832 ## 5 5.3790 nan 0.1000 0.3350 ## 6 5.0578 nan 0.1000 0.2406 ## 7 4.7324 nan 0.1000 0.1941 ## 8 4.3901 nan 0.1000 0.2642 ## 9 4.0528 nan 0.1000 0.2389 ## 10 3.7874 nan 0.1000 0.2426 ## 20 2.1952 nan 0.1000 0.0768 ## 40 1.1025 nan 0.1000 -0.0081 ## 60 0.7689 nan 0.1000 -0.0002 ## 80 0.6122 nan 0.1000 -0.0061 ## 100 0.5200 nan 0.1000 -0.0109 ## 120 0.4400 nan 0.1000 -0.0069 ## 140 0.3837 nan 0.1000 -0.0055 ## 160 0.3328 nan 0.1000 -0.0034 ## 180 0.2920 nan 0.1000 -0.0077 ## 200 0.2544 nan 0.1000 -0.0035 ## 220 0.2241 nan 0.1000 -0.0036 ## 240 0.1988 nan 0.1000 -0.0047 ## 250 0.1865 nan 0.1000 -0.0028 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.2748 nan 0.1000 0.3849 ## 2 6.9549 nan 0.1000 0.3086 ## 3 6.6188 nan 0.1000 0.2131 ## 4 6.3662 nan 0.1000 0.2414 ## 5 6.1655 nan 0.1000 0.1744 ## 6 5.9621 nan 0.1000 0.1800 ## 7 5.7756 nan 0.1000 0.1729 ## 8 5.6347 nan 0.1000 0.1348 ## 9 5.5429 nan 0.1000 0.0634 ## 10 5.3761 nan 0.1000 0.0928 ## 20 4.3688 nan 0.1000 0.0591 ## 40 3.2068 nan 0.1000 0.0402 ## 60 2.5475 nan 0.1000 0.0114 ## 80 2.0354 nan 0.1000 0.0023 ## 100 1.7011 nan 0.1000 -0.0072 ## 120 1.4362 nan 0.1000 -0.0015 ## 140 1.2570 nan 0.1000 -0.0034 ## 160 1.1369 nan 0.1000 0.0021 ## 180 1.0223 nan 0.1000 -0.0062 ## 200 0.9563 nan 0.1000 -0.0038 ## 220 0.8991 nan 0.1000 -0.0058 ## 240 0.8598 nan 0.1000 -0.0059 ## 250 0.8460 nan 0.1000 -0.0086 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.1432 nan 0.1000 0.5851 ## 2 6.7241 nan 0.1000 0.4090 ## 3 6.2802 nan 0.1000 0.3640 ## 4 5.9965 nan 0.1000 0.1559 ## 5 5.7218 nan 0.1000 0.2864 ## 6 5.4669 nan 0.1000 0.2326 ## 7 5.1666 nan 0.1000 0.2190 ## 8 4.9344 nan 0.1000 0.2265 ## 9 4.7538 nan 0.1000 0.1466 ## 10 4.5570 nan 0.1000 0.1311 ## 20 3.2524 nan 0.1000 0.1132 ## 40 1.9609 nan 0.1000 0.0491 ## 60 1.3733 nan 0.1000 -0.0028 ## 80 1.0764 nan 0.1000 -0.0083 ## 100 0.8912 nan 0.1000 -0.0043 ## 120 0.7917 nan 0.1000 -0.0084 ## 140 0.7298 nan 0.1000 -0.0027 ## 160 0.6894 nan 0.1000 -0.0061 ## 180 0.6543 nan 0.1000 -0.0077 ## 200 0.6175 nan 0.1000 -0.0009 ## 220 0.5885 nan 0.1000 -0.0089 ## 240 0.5540 nan 0.1000 -0.0036 ## 250 0.5386 nan 0.1000 -0.0019 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.0286 nan 0.1000 0.5874 ## 2 6.5422 nan 0.1000 0.4614 ## 3 6.0807 nan 0.1000 0.4788 ## 4 5.6220 nan 0.1000 0.3139 ## 5 5.3339 nan 0.1000 0.1849 ## 6 4.9990 nan 0.1000 0.2584 ## 7 4.7023 nan 0.1000 0.2324 ## 8 4.4521 nan 0.1000 0.2002 ## 9 4.2254 nan 0.1000 0.1721 ## 10 4.0158 nan 0.1000 0.1061 ## 20 2.5603 nan 0.1000 0.0649 ## 40 1.4633 nan 0.1000 0.0221 ## 60 1.0126 nan 0.1000 0.0040 ## 80 0.8089 nan 0.1000 -0.0011 ## 100 0.6913 nan 0.1000 -0.0016 ## 120 0.6212 nan 0.1000 -0.0039 ## 140 0.5555 nan 0.1000 0.0013 ## 160 0.5178 nan 0.1000 -0.0070 ## 180 0.4779 nan 0.1000 -0.0062 ## 200 0.4357 nan 0.1000 -0.0056 ## 220 0.4076 nan 0.1000 -0.0034 ## 240 0.3838 nan 0.1000 -0.0051 ## 250 0.3724 nan 0.1000 -0.0045 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.0052 nan 0.1000 0.7357 ## 2 6.3136 nan 0.1000 0.6391 ## 3 5.8579 nan 0.1000 0.3395 ## 4 5.4261 nan 0.1000 0.4118 ## 5 5.0380 nan 0.1000 0.2541 ## 6 4.7502 nan 0.1000 0.2364 ## 7 4.5107 nan 0.1000 0.1393 ## 8 4.2559 nan 0.1000 0.2325 ## 9 4.0139 nan 0.1000 0.1431 ## 10 3.7788 nan 0.1000 0.1690 ## 20 2.2976 nan 0.1000 0.0670 ## 40 1.1641 nan 0.1000 0.0258 ## 60 0.7945 nan 0.1000 -0.0042 ## 80 0.6385 nan 0.1000 -0.0010 ## 100 0.5468 nan 0.1000 -0.0065 ## 120 0.4711 nan 0.1000 -0.0070 ## 140 0.4186 nan 0.1000 -0.0051 ## 160 0.3792 nan 0.1000 -0.0062 ## 180 0.3426 nan 0.1000 -0.0072 ## 200 0.3149 nan 0.1000 -0.0066 ## 220 0.2897 nan 0.1000 -0.0081 ## 240 0.2642 nan 0.1000 -0.0020 ## 250 0.2526 nan 0.1000 -0.0020 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.0074 nan 0.1000 0.6779 ## 2 6.3049 nan 0.1000 0.6277 ## 3 5.7428 nan 0.1000 0.5037 ## 4 5.3843 nan 0.1000 0.2738 ## 5 4.9412 nan 0.1000 0.4013 ## 6 4.5370 nan 0.1000 0.2175 ## 7 4.2296 nan 0.1000 0.2194 ## 8 3.9306 nan 0.1000 0.2566 ## 9 3.6976 nan 0.1000 0.1642 ## 10 3.4742 nan 0.1000 0.1155 ## 20 2.0717 nan 0.1000 0.0554 ## 40 1.0961 nan 0.1000 0.0158 ## 60 0.7387 nan 0.1000 -0.0071 ## 80 0.5947 nan 0.1000 -0.0086 ## 100 0.4878 nan 0.1000 -0.0035 ## 120 0.4191 nan 0.1000 -0.0069 ## 140 0.3558 nan 0.1000 -0.0047 ## 160 0.3193 nan 0.1000 -0.0029 ## 180 0.2757 nan 0.1000 -0.0060 ## 200 0.2412 nan 0.1000 -0.0011 ## 220 0.2154 nan 0.1000 -0.0022 ## 240 0.1911 nan 0.1000 -0.0019 ## 250 0.1798 nan 0.1000 -0.0025 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.5373 nan 0.1000 0.3255 ## 2 7.2302 nan 0.1000 0.3081 ## 3 7.0287 nan 0.1000 0.0792 ## 4 6.8235 nan 0.1000 0.1648 ## 5 6.5846 nan 0.1000 0.2528 ## 6 6.3572 nan 0.1000 0.2142 ## 7 6.2170 nan 0.1000 0.0731 ## 8 6.0241 nan 0.1000 0.1693 ## 9 5.8911 nan 0.1000 0.1379 ## 10 5.7890 nan 0.1000 0.0396 ## 20 4.6672 nan 0.1000 0.0610 ## 40 3.3823 nan 0.1000 0.0241 ## 60 2.6081 nan 0.1000 0.0076 ## 80 2.1083 nan 0.1000 0.0027 ## 100 1.7823 nan 0.1000 -0.0038 ## 120 1.5329 nan 0.1000 0.0083 ## 140 1.3371 nan 0.1000 -0.0004 ## 160 1.1999 nan 0.1000 -0.0035 ## 180 1.0969 nan 0.1000 -0.0001 ## 200 1.0178 nan 0.1000 -0.0020 ## 220 0.9601 nan 0.1000 -0.0065 ## 240 0.9259 nan 0.1000 -0.0059 ## 250 0.9093 nan 0.1000 -0.0037 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.3367 nan 0.1000 0.5068 ## 2 6.8403 nan 0.1000 0.4133 ## 3 6.4575 nan 0.1000 0.3430 ## 4 6.1040 nan 0.1000 0.2802 ## 5 5.9071 nan 0.1000 0.1333 ## 6 5.6534 nan 0.1000 0.2078 ## 7 5.4050 nan 0.1000 0.2252 ## 8 5.2113 nan 0.1000 0.1294 ## 9 4.9828 nan 0.1000 0.1934 ## 10 4.8171 nan 0.1000 0.1146 ## 20 3.3732 nan 0.1000 0.0232 ## 40 2.0529 nan 0.1000 0.0254 ## 60 1.4630 nan 0.1000 0.0107 ## 80 1.1301 nan 0.1000 -0.0184 ## 100 0.9347 nan 0.1000 -0.0020 ## 120 0.8346 nan 0.1000 -0.0053 ## 140 0.7561 nan 0.1000 -0.0116 ## 160 0.7038 nan 0.1000 -0.0083 ## 180 0.6608 nan 0.1000 -0.0050 ## 200 0.6231 nan 0.1000 -0.0062 ## 220 0.5885 nan 0.1000 -0.0053 ## 240 0.5666 nan 0.1000 -0.0073 ## 250 0.5537 nan 0.1000 -0.0016 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.3914 nan 0.1000 0.5561 ## 2 6.7379 nan 0.1000 0.5924 ## 3 6.1931 nan 0.1000 0.3638 ## 4 5.7353 nan 0.1000 0.2706 ## 5 5.4272 nan 0.1000 0.2604 ## 6 5.1269 nan 0.1000 0.1462 ## 7 4.9406 nan 0.1000 0.0649 ## 8 4.7122 nan 0.1000 0.1369 ## 9 4.5181 nan 0.1000 0.1104 ## 10 4.3030 nan 0.1000 0.1313 ## 20 2.8485 nan 0.1000 0.0934 ## 40 1.6130 nan 0.1000 -0.0081 ## 60 1.1086 nan 0.1000 -0.0165 ## 80 0.8744 nan 0.1000 -0.0020 ## 100 0.7519 nan 0.1000 -0.0066 ## 120 0.6638 nan 0.1000 -0.0039 ## 140 0.5997 nan 0.1000 -0.0044 ## 160 0.5553 nan 0.1000 -0.0082 ## 180 0.5230 nan 0.1000 -0.0054 ## 200 0.4907 nan 0.1000 -0.0056 ## 220 0.4566 nan 0.1000 -0.0051 ## 240 0.4234 nan 0.1000 -0.0047 ## 250 0.4103 nan 0.1000 -0.0062 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.3959 nan 0.1000 0.5344 ## 2 6.7840 nan 0.1000 0.6032 ## 3 6.2042 nan 0.1000 0.5214 ## 4 5.7561 nan 0.1000 0.3567 ## 5 5.3542 nan 0.1000 0.3154 ## 6 5.0748 nan 0.1000 0.2697 ## 7 4.6856 nan 0.1000 0.3381 ## 8 4.4209 nan 0.1000 0.2125 ## 9 4.1834 nan 0.1000 0.1959 ## 10 3.9558 nan 0.1000 0.1757 ## 20 2.5107 nan 0.1000 0.0757 ## 40 1.3385 nan 0.1000 0.0197 ## 60 0.9193 nan 0.1000 0.0067 ## 80 0.7247 nan 0.1000 -0.0105 ## 100 0.6080 nan 0.1000 -0.0060 ## 120 0.5370 nan 0.1000 -0.0086 ## 140 0.4808 nan 0.1000 -0.0094 ## 160 0.4331 nan 0.1000 -0.0018 ## 180 0.3896 nan 0.1000 -0.0076 ## 200 0.3487 nan 0.1000 -0.0060 ## 220 0.3156 nan 0.1000 -0.0044 ## 240 0.2857 nan 0.1000 -0.0023 ## 250 0.2735 nan 0.1000 -0.0045 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.2002 nan 0.1000 0.7128 ## 2 6.5508 nan 0.1000 0.5996 ## 3 5.9803 nan 0.1000 0.5293 ## 4 5.5269 nan 0.1000 0.4154 ## 5 5.1301 nan 0.1000 0.3923 ## 6 4.7897 nan 0.1000 0.2007 ## 7 4.4224 nan 0.1000 0.2482 ## 8 4.1528 nan 0.1000 0.1840 ## 9 3.8740 nan 0.1000 0.1938 ## 10 3.6367 nan 0.1000 0.1894 ## 20 2.1725 nan 0.1000 0.0933 ## 40 1.0934 nan 0.1000 0.0063 ## 60 0.7734 nan 0.1000 -0.0003 ## 80 0.6267 nan 0.1000 -0.0113 ## 100 0.5248 nan 0.1000 -0.0115 ## 120 0.4509 nan 0.1000 -0.0086 ## 140 0.3909 nan 0.1000 -0.0042 ## 160 0.3417 nan 0.1000 -0.0095 ## 180 0.2979 nan 0.1000 -0.0010 ## 200 0.2641 nan 0.1000 -0.0051 ## 220 0.2324 nan 0.1000 -0.0048 ## 240 0.2065 nan 0.1000 -0.0029 ## 250 0.1944 nan 0.1000 -0.0033 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.5853 nan 0.1000 0.4622 ## 2 7.2268 nan 0.1000 0.2276 ## 3 6.9725 nan 0.1000 0.2317 ## 4 6.7255 nan 0.1000 0.1945 ## 5 6.5310 nan 0.1000 0.1967 ## 6 6.3548 nan 0.1000 0.1452 ## 7 6.1331 nan 0.1000 0.1877 ## 8 6.0205 nan 0.1000 0.0816 ## 9 5.8651 nan 0.1000 0.1151 ## 10 5.7272 nan 0.1000 0.0983 ## 20 4.5797 nan 0.1000 0.0457 ## 40 3.3805 nan 0.1000 0.0281 ## 60 2.6821 nan 0.1000 -0.0024 ## 80 2.1887 nan 0.1000 0.0083 ## 100 1.8328 nan 0.1000 0.0108 ## 120 1.5572 nan 0.1000 -0.0039 ## 140 1.3454 nan 0.1000 0.0011 ## 160 1.2090 nan 0.1000 -0.0017 ## 180 1.0998 nan 0.1000 0.0009 ## 200 1.0155 nan 0.1000 -0.0061 ## 220 0.9510 nan 0.1000 -0.0040 ## 240 0.9159 nan 0.1000 -0.0058 ## 250 0.8952 nan 0.1000 -0.0036 carseats.gbm ## Stochastic Gradient Boosting ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... ## Resampling results across tuning parameters: ## ## interaction.depth n.trees RMSE Rsquared MAE ## 1 50 1.9 0.66 1.50 ## 1 100 1.5 0.75 1.26 ## 1 150 1.4 0.79 1.11 ## 1 200 1.3 0.82 1.03 ## 1 250 1.2 0.82 0.98 ## 2 50 1.6 0.75 1.27 ## 2 100 1.3 0.81 1.06 ## 2 150 1.2 0.83 0.99 ## 2 200 1.2 0.83 0.98 ## 2 250 1.2 0.83 0.99 ## 3 50 1.4 0.78 1.16 ## 3 100 1.3 0.82 1.02 ## 3 150 1.2 0.82 0.99 ## 3 200 1.3 0.81 1.01 ## 3 250 1.3 0.81 1.02 ## 4 50 1.4 0.79 1.12 ## 4 100 1.3 0.81 1.03 ## 4 150 1.3 0.80 1.04 ## 4 200 1.3 0.80 1.06 ## 4 250 1.3 0.80 1.06 ## 5 50 1.3 0.80 1.10 ## 5 100 1.3 0.81 1.04 ## 5 150 1.3 0.81 1.04 ## 5 200 1.3 0.80 1.04 ## 5 250 1.3 0.80 1.06 ## ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## ## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were n.trees = 250, interaction.depth = ## 1, shrinkage = 0.1 and n.minobsinnode = 10. plot(carseats.gbm) carseats.pred &lt;- predict(carseats.gbm, carseats_test, type = &quot;raw&quot;) plot(carseats_test$Sales, carseats.pred, main = &quot;Gradient Boosing Regression: Predicted vs. Actual&quot;, xlab = &quot;Actual&quot;, ylab = &quot;Predicted&quot;) abline(0,1) (carseats.gbm.rmse &lt;- RMSE(pred = carseats.pred, obs = carseats_test$Sales)) ## [1] 1.4 rm(carseats.pred) #plot(varImp(carseats.gbm), main=&quot;Variable Importance with Gradient Boosting&quot;) "],
["summary.html", "10.6 Summary", " 10.6 Summary Okay, I’m going to tally up the results! For the classification division, the winner is the manual classification tree! Gradient boosting made a valiant run at it, but came up just a little short. rbind(data.frame(model = &quot;Manual Class&quot;, Acc = round(oj_model_1b_cm$overall[&quot;Accuracy&quot;], 5)), data.frame(model = &quot;Class w.tuneGrid&quot;, Acc = round(oj_model_3_cm$overall[&quot;Accuracy&quot;], 5)), data.frame(model = &quot;Bagging&quot;, Acc = round(oj.bag.acc, 5)), data.frame(model = &quot;Random Forest&quot;, Acc = round(oj.frst.acc, 5)), data.frame(model = &quot;Gradient Boosting&quot;, Acc = round(oj.gbm.acc, 5)) ) %&gt;% arrange(desc(Acc)) ## model Acc ## 1 Manual Class 0.86 ## 2 Gradient Boosting 0.85 ## 3 Class w.tuneGrid 0.85 ## 4 Bagging 0.83 ## 5 Random Forest 0.83 And now for the regression division, the winnner is… gradient boosting! rbind(data.frame(model = &quot;Manual ANOVA&quot;, RMSE = round(carseats_model_1_pruned_rmse, 5)), data.frame(model = &quot;ANOVA w.tuneGrid&quot;, RMSE = round(carseats_model_3_pruned_rmse, 5)), data.frame(model = &quot;Bagging&quot;, RMSE = round(carseats.bag.rmse, 5)), data.frame(model = &quot;Random Forest&quot;, RMSE = round(carseats.frst.rmse, 5)), data.frame(model = &quot;Gradient Boosting&quot;, RMSE = round(carseats.gbm.rmse, 5)) ) %&gt;% arrange(RMSE) ## model RMSE ## 1 Gradient Boosting 1.4 ## 2 Random Forest 1.8 ## 3 Bagging 1.9 ## 4 ANOVA w.tuneGrid 2.3 ## 5 Manual ANOVA 2.4 Here are plots of the ROC curves for all the models (one from each chapter) on the same graph. The ROCR package provides the prediction() and performance() functions which generate the data required for plotting the ROC curve, given a set of predictions and actual (true) values. The more “up and to the left” the ROC curve of a model is, the better the model. The AUC performance metric is literally the “Area Under the ROC Curve”, so the greater the area under this curve, the higher the AUC, and the better-performing the model is. library(ROCR) # List of predictions oj.class.pred &lt;- predict(oj_model_3, oj_test, type = &quot;prob&quot;)[,2] oj.bag.pred &lt;- predict(oj.bag, oj_test, type = &quot;prob&quot;)[,2] oj.frst.pred &lt;- predict(oj.frst, oj_test, type = &quot;prob&quot;)[,2] oj.gbm.pred &lt;- predict(oj.gbm, oj_test, type = &quot;prob&quot;)[,2] preds_list &lt;- list(oj.class.pred, oj.bag.pred, oj.frst.pred, oj.gbm.pred) #preds_list &lt;- list(oj.class.pred) # List of actual values (same for all) m &lt;- length(preds_list) actuals_list &lt;- rep(list(oj_test$Purchase), m) # Plot the ROC curves pred &lt;- prediction(preds_list, actuals_list) #pred &lt;- prediction(oj.class.pred[,2], oj_test$Purchase) rocs &lt;- performance(pred, &quot;tpr&quot;, &quot;fpr&quot;) plot(rocs, col = as.list(1:m), main = &quot;Test Set ROC Curves&quot;) legend(x = &quot;bottomright&quot;, legend = c(&quot;Decision Tree&quot;, &quot;Bagged Trees&quot;, &quot;Random Forest&quot;, &quot;GBM&quot;), fill = 1:m) "],
["reference.html", "10.7 Reference", " 10.7 Reference Penn State University, STAT 508: Applied Data Mining and Statistical Learning, “Lesson 11: Tree-based Methods”. https://newonlinecourses.science.psu.edu/stat508/lesson/11. Brownlee, Jason. “Classification And Regression Trees for Machine Learning”, Machine Learning Mastery. https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/. Brownlee, Jason. “A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning”, Machine Learning Mastery. https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/. DataCamp: Machine Learning with Tree-Based Models in R An Introduction to Statistical Learning by Gareth James, et al. SAS Documentation StatMethods: Tree-Based Models Machine Learning Plus GBM (Boosted Models) Tuning Parameters from Listen Data Harry Southworth on GitHub Gradient Boosting Classification with GBM in R in DataTechNotes Molnar, Christoph. “Interpretable machine learning. A Guide for Making Black Box Models Explainable”, 2019. https://christophm.github.io/interpretable-ml-book/. "],
["regularization.html", "Chapter 11 Regularization", " Chapter 11 Regularization "],
["non-linear-models.html", "Chapter 12 Non-linear Models", " Chapter 12 Non-linear Models Linear methods can model nonlinear relationships by including polynomial terms, interaction effects, and variable transformations. However, it is often difficult to identify how to formulate the model. Nonlinear models may be preferable because you do not need to know the the exact form of the nonlinearity prior to model training. "],
["splines.html", "12.1 Splines", " 12.1 Splines A regression spline fits a piecewise polynomial to the range of X partitioned by knots (K knots produce K + 1 piecewise polynomials) James et al (James et al. 2013). The polynomials can be of any degree d, but are usually in the range [0, 3], most commonly 3 (a cubic spline). To avoid discontinuities in the fit, a degree-d spline is constrained to have continuity in derivatives up to degree d−1 at each knot. A cubic spline fit to a data set with K knots, performs least squares regression with an intercept and 3 + K predictors, of the form \\[y_i = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\beta_4h(X, \\xi_1) + \\beta_5h(X, \\xi_2) + \\dots + \\beta_{K+3}h(X, \\xi_K)\\] where \\(\\xi_1, \\dots, \\xi_K\\) are the knots are truncated power basis functions \\(h(X, \\xi) = (X - \\xi)^3\\) if \\(X &gt; \\xi\\), else 0. Splines can have high variance at the outer range of the predictors. A natural spline is a regression spline additionally constrained to be linear at the boundaries. How many knots should there be, and Where should the knots be placed? It is common to place knots in a uniform fashion, with equal numbers of points between each knot. The number of knots is typically chosen by trial and error using cross-validation to minimize the RSS. The number of knots is usually expressed in terms of degrees of freedom. A cubic spline will have K + 3 + 1 degrees of freedom. A natural spline has K + 3 + 1 - 5 degrees of freedom due to the constraints at the endpoints. A further constraint can be added to reduce overfitting by enforcing smoothness in the spline. Instead of minimizing the loss function \\(\\sum{(y - g(x))^2}\\) where \\(g(x)\\) is a natural spline, minimize a loss function with an additional penalty for variability: \\[L = \\sum{(y_i - g(x_i))^2 + \\lambda \\int g&#39;&#39;(t)^2dt}.\\] The function \\(g(x)\\) that minimizes the loss function is a natural cubic spline with knots at each \\(x_1, \\dots, x_n\\). This is called a smoothing spline. The larger g is, the greater the penalty on variation in the spline. In a smoothing spline, you do not optimize the number or location of the knots – there is a knot at each training observation. Instead, you optimize \\(\\lambda\\). One way to optimze \\(\\lambda\\) is cross-validation to minimize RSS. Leave-one-out cross-validation (LOOCV) can be computed efficiently for smoothing splines. References "],
["mars.html", "12.2 MARS", " 12.2 MARS Multivariate adaptive regression splines (MARS) is a non-parametric algorithm that creates a piecewise linear model to capture nonlinearities and interactions effects. The resulting model is a weighted sum of basis functions \\(B_i(X)\\): \\[\\hat{y} = \\sum_{i=1}^{k}{w_iB_i(x)}\\] The basis functions are either a constant (for the intercept), a hinge function of the form \\(\\max(0, x - x_0)\\) or \\(\\max(0, x_0 - x)\\) (a more concise representation is \\([\\pm(x - x_0)]_+\\)), or products of two or more hinge functions (for interactions). MARS automatically selects which predictors to use and what predictor values to serve as the knots of the hinge functions. MARS builds a model in two phases: the forward pass and the backward pass, similar to growing and pruning of tree models. MARS starts with a model consisting of just the intercept term equaling the mean of the response values. It then asseses every predictor to find a basis function pair consisting of opposing sides of a mirrored hinge function which produces the maximum improvement in the model error. MARS repeats the process until either it reaches a predefined limit of terms or the error improvement reaches a predefined limit. MARS generalizes the model by removing terms according to the generalized cross validation (GCV) criterion. GCV is a form of regularization: it trades off goodness-of-fit against model complexity. The earth::earth() function (documentation) performs the MARS algorithm (the term “MARS” is trademarked, so open-source implementations use “Earth” instead). The caret implementation tunes two parameters: nprune and degree. nprune is the maximum number of terms in the pruned model. degree is the maximum degree of interaction (default is 1 (no interactions)). However, there are other hyperparameters in the model that may improve performance, including minspan which regulates the number of knots in the predictors. Here is an example using the Ames housing data set (following this tutorial. library(tidyverse) library(earth) library(caret) # set up ames &lt;- AmesHousing::make_ames() set.seed(12345) idx &lt;- createDataPartition(ames$Sale_Price, p = 0.80, list = FALSE) ames_train &lt;- ames[idx, ] %&gt;% as.data.frame() ames_test &lt;- ames[-idx, ] m &lt;- train( x = subset(ames_train, select = -Sale_Price), y = ames_train$Sale_Price, method = &quot;earth&quot;, metric = &quot;RMSE&quot;, minspan = -15, trControl = trainControl(method = &quot;cv&quot;, number = 10), tuneGrid = expand.grid( degree = 1:3, nprune = seq(2, 100, length.out = 10) %&gt;% floor() ) ) The model plot shows the best tuning parameter combination. plot(m, main = &quot;MARS Parameter Tuning&quot;) m$bestTune ## nprune degree ## 25 45 3 How does this model perform against the holdout data? caret::postResample( pred = log(predict(m, newdata = ames_test)), obs = log(ames_test$Sale_Price) ) ## RMSE Rsquared MAE ## 0.165 0.855 0.093 "],
["gam.html", "12.3 GAM", " 12.3 GAM Generalized additive models (GAM) allow for non-linear relationships between each feature and the response by replacing each linear component \\(\\beta_j x_{ij}\\) with a nonlinear function \\(f_j(x_{ij})\\). The GAM model is of the form \\[y_i = \\beta_0 + \\sum{f_j(x_{ij})} + \\epsilon_i.\\] It is called an additive model because we calculate a separate \\(f_j\\) for each \\(X_j\\), and then add together all of their contributions. The advantage of GAMs is that they automatically model non-linear relationships so you do not need to manually try out many diﬀerent transformations on each variable individually. And because the model is additive, you can still examine the eﬀect of each \\(X_j\\) on \\(Y\\) individually while holding all of the other variables ﬁxed. The main limitation of GAMs is that the model is restricted to be additive, so important interactions can be missed unless you explicitly add them. "],
["support-vector-machines.html", "Chapter 13 Support Vector Machines", " Chapter 13 Support Vector Machines These notes rely on (James et al. 2013), (Hastie, Tibshirani, and Friedman 2017), and (Kuhn and Johnson 2016). I also reviewed the material in PSU’s Applied Data Mining and Statistical Learning (STAT 508), and the e1071 Support Vector Machines vignette. The Support Vector Machines (SVM) algorithm finds the optimal separating hyperplane between members of two classes using an appropriate nonlinear mapping to a sufficiently high dimension. The hyperplane is defined by the observations that lie within a margin optimized by a cost hyperparameter. These observations are called the support vectors. SVM is an extension of the support vector classifier which in turn is a generalization of the simple and intuitive maximal margin classifier. References "],
["maximal-margin-classifier.html", "13.1 Maximal Margin Classifier", " 13.1 Maximal Margin Classifier The maximal margin classifier is the optimal hyperplane defined in the (rare) case where two classes are linearly separable. Given an \\(n \\times p\\) data matrix \\(X\\) with binary response variable defined as \\(y \\in [-1, 1]\\) it may be possible to define a p-dimensional hyperplane \\(h(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 \\dots + \\beta_pX_p = x_i^T \\beta + \\beta_0 = 0\\) such that all observations of each class fall on opposite sides of the hyperplane. This “separating hyperplane” has the property that if \\(\\beta\\) is constrained to be a unit vector, \\(||\\beta|| = \\sum\\beta^2 = 1\\), then the product of the hyperplane and response variables are positive perpendicular distances from the hyperplane, the smallest of which may be termed the hyperplane margin, \\(M\\), \\[y_i (x_i^{&#39;} \\beta + \\beta_0) \\ge M.\\] The maximal margin classifier is the hyperplane with the maximum margin. That is, \\(\\max \\{M\\}\\) subject to \\(||\\beta|| = 1\\). A separating hyperplane rarely exists. In fact, even if a separating hyperplane does exist, its resulting margin is probably undesirably narrow. "],
["support-vector-classifier.html", "13.2 Support Vector Classifier", " 13.2 Support Vector Classifier The maximal margin classifier can be generalized to non-separable cases using a so-called “soft margin”. The generalization is called the support vector classifier. The soft margin allows some misclassification in the interest of greater robustness to individual observations. The support vector classifier optimizes \\[y_i (x_i^{&#39;} \\beta + \\beta_0) \\ge M(1 - \\xi_i)\\] where the \\(\\xi_i\\) are positive slack variables whose sum is bounded by some constant tuning parameter \\(\\sum{\\xi_i} \\le constant\\). The slack variable values indicate where the observation lies: \\(\\xi_i = 0\\) observations lie on the correct side of the margin; \\(\\xi_i &gt; 0\\) observation lie on the wrong side of the margin; \\(\\xi_i &gt; 1\\) observations lie on the wrong side of the hyperplane. The constant sets the tolerance for margin violation. If \\(constant = 0\\), then all observations must reside on the correct side of the margin, as in the maximal margin classifier. The \\(constant\\) controls the bias-variance trade-off. As the \\(constant\\) increases, the margin widens and allows more violations. The classifier bias increases but its variance decreases. The support vector classifier is usually defined by dropping the \\(||\\beta|| = 1\\) constraint, and defining \\(M = 1 / ||\\beta||\\). The optimization problem then becomes \\[ \\min ||\\beta|| \\hspace{2mm} s.t. \\hspace{2mm} \\begin{cases} y_i(x_i^T\\beta + \\beta_0) \\ge 1 - \\xi_i, \\hspace{2mm} \\forall i &amp; \\\\ \\xi_i \\ge 0, \\hspace{2mm} \\sum \\xi_i \\le constant. \\end{cases} \\] This is a quadratic equation with linear inequality constraints, so it is a convex optimization problem which can be solved using Lagrange multipliers. Re-express the optimization problem as \\[ \\min_{\\beta_0, \\beta} \\frac{1}{2}||\\beta||^2 = C\\sum_{i = 1}^N \\xi_i \\\\ s.t. \\xi_i \\ge 0, \\hspace{2mm} y_i(x_i^T\\beta + \\beta_0) \\ge 1 - \\xi_i, \\hspace{2mm} \\forall i \\] where the “cost” parameter \\(C\\) replaces the constant and penalizes large residuals. This optimization problem is equivalent to another optimization problem, the familiar loss + penalty formulation: \\[\\min_{\\beta_0, \\beta} \\sum_{i=1}^N{[1 - y_if(x_i)]_+} + \\frac{\\lambda}{2} ||\\beta||^2 \\] where \\(\\lambda = 1 / C\\) and \\([1 - y_if(x_i)]_+\\) is a “hinge” loss function with \\(f(x_i) = sign[Pr(Y = +1|x) - 1 / 2]\\). The parameter estimates can be written as functions of a set of unknown parameters \\((\\alpha_i)\\) and data points. The solution to the optimization problem requires only the inner products of the observations, represented as \\(\\langle x_i, x_j \\rangle\\), \\[f(x) = \\beta_0 + \\sum_{i = 1}^n {\\alpha_i \\langle x, x_i \\rangle}\\] The solution has the interesting property that only observations on or within the margin affect the hyperplane. These observations are known as support vectors. As the constant increases, the number of violating observations increase, and thus the number of support vectors increases. This property makes the algorithm robust to the extreme observations far away from the hyperplane. The parameter estimators for \\(\\alpha_i\\) are nonzero only for the support vectors in the solution—that is, if a training observation is not a support vector, then its \\(\\alpha_i\\) equals zero. The only shortcoming with the algorithm is that it presumes a linear decision boundary. "],
["support-vector-machines-1.html", "13.3 Support Vector Machines", " 13.3 Support Vector Machines Enlarging the feature space of the support vector classifier accommodates nonlinar relationships. Support vector machines do this in a specific way, using kernals. The kernal is a generalization of the inner product with form \\(K(x_i, x_i^{&#39;})\\). So the linear kernal is simply \\[K(x_i, x_i^{&#39;}) = \\langle x, x_i \\rangle\\] and the solution is \\[f(x) = \\beta_0 + \\sum_{i = 1}^n {\\alpha_i K(x_i, x_i^{&#39;})}\\] \\(K\\) can take onother form instead, such as polynomial \\[K(x, x&#39;) = (\\gamma \\langle x, x&#39; \\rangle + c_0)^d\\] or radial \\[K(x, x&#39;) = \\exp\\{-\\gamma ||x - x&#39;||^2\\}.\\] "],
["example-9.html", "13.4 Example", " 13.4 Example Here is a data set of two classes \\(y \\in [-1, 1]\\) described by two features \\(X1\\) and \\(X2\\). library(tidyverse) set.seed(1) x &lt;- matrix(rnorm (20*2), ncol=2) y &lt;- c(rep(-1, 10), rep(1, 10)) x[y==1, ] &lt;- x[y==1, ] + 1 train_data &lt;- data.frame(x, y) train_data$y &lt;- as.factor(y) A scatter plot reveals whether the classes are linearly separable. ggplot(train_data, aes(x = X1, y = X2, color = y)) + geom_point(size = 2) + labs(title = &quot;Binary response with two features&quot;) + theme(legend.position = &quot;top&quot;) No, they are not linearly separable. Now fit a support vector machine. The e1071 library implements the SVM algorithm. svm(..., kernel=\"linear\") fits a support vector classifier. Change the kernal to c(\"polynomial\", \"radial\") for SVM. Try a cost of 10. library(e1071) ## Warning: package &#39;e1071&#39; was built under R version 3.6.2 m &lt;- svm( y ~ ., data = train_data, kernel = &quot;linear&quot;, type = &quot;C-classification&quot;, # (default) for classification cost = 10, # default is 1 scale = FALSE # do not standardize features ) plot(m, train_data) The support vectors are plotted as “x’s”. There are seven of them. m$index ## [1] 1 2 5 7 14 16 17 The summary shows adds additional information, including the distribution of the support vector classes. summary(m) ## ## Call: ## svm(formula = y ~ ., data = train_data, kernel = &quot;linear&quot;, type = &quot;C-classification&quot;, ## cost = 10, scale = FALSE) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: linear ## cost: 10 ## ## Number of Support Vectors: 7 ## ## ( 4 3 ) ## ## ## Number of Classes: 2 ## ## Levels: ## -1 1 The seven support vectors are comprised of four in one class, three in the other. What if we lower the cost of margin violations? This will increase bias and lower variance. m &lt;- svm( y ~ ., data = train_data, kernel = &quot;linear&quot;, type = &quot;C-classification&quot;, cost = 0.1, scale = FALSE ) plot(m, train_data) There are many more support vectors now. (In case you hoped to see the linear decision boundary formulation, or at least a graphical representation of the margins, keep hoping. The model is generalized beyond two features, so it evidently does not worry too much about supporting sanitized two-feature demos.) Which cost level yields the best predictive performance on holdout data? Use cross validation to find out. SVM defaults to 10-fold CV. I’ll try seven candidate values for cost. set.seed(1) m_tune &lt;- tune( svm, y ~ ., data = train_data, kernel =&quot;linear&quot;, ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)) ) summary(m_tune) ## ## Parameter tuning of &#39;svm&#39;: ## ## - sampling method: 10-fold cross validation ## ## - best parameters: ## cost ## 0.1 ## ## - best performance: 0.05 ## ## - Detailed performance results: ## cost error dispersion ## 1 0.001 0.55 0.44 ## 2 0.010 0.55 0.44 ## 3 0.100 0.05 0.16 ## 4 1.000 0.15 0.24 ## 5 5.000 0.15 0.24 ## 6 10.000 0.15 0.24 ## 7 100.000 0.15 0.24 The lowest cross-validation error rate is 0.10 with cost = 0.1. tune() saves the best tuning parameter value. m_best &lt;- m_tune$best.model summary(m_best) ## ## Call: ## best.tune(method = svm, train.x = y ~ ., data = train_data, ranges = list(cost = c(0.001, ## 0.01, 0.1, 1, 5, 10, 100)), kernel = &quot;linear&quot;) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: linear ## cost: 0.1 ## ## Number of Support Vectors: 16 ## ## ( 8 8 ) ## ## ## Number of Classes: 2 ## ## Levels: ## -1 1 There are 16 support vectors, 8 in each class. This is a pretty wide margin. plot(m_best, train_data) What if the classes had been linearly separable? Then we could create a maximal margin classifier. train_data_2 &lt;- train_data %&gt;% mutate( X1 = X1 + ifelse(y==1, 1.0, 0), X2 = X2 + ifelse(y==1, 1.0, 0) ) ggplot(train_data_2, aes(x = X1, y = X2, color = y)) + geom_point(size = 2) + labs(title = &quot;Binary response with two features, linearly separable&quot;) Specify a huge cost = 1e5 so that no support vectors violate the margin. m2 &lt;- svm( y ~ ., data = train_data_2, kernel = &quot;linear&quot;, cost = 1e5, scale = FALSE # do not standardize features ) plot(m2, train_data_2) summary(m2) ## ## Call: ## svm(formula = y ~ ., data = train_data_2, kernel = &quot;linear&quot;, cost = 100000, ## scale = FALSE) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: linear ## cost: 100000 ## ## Number of Support Vectors: 3 ## ## ( 1 2 ) ## ## ## Number of Classes: 2 ## ## Levels: ## -1 1 This model will have very low bias, but very high variance. To fit an SVM, use a different kernel. You can use kernal = c(\"polynomial\", \"radial\", \"sigmoid\"). For a polynomial model, also specify the polynomial degree. For a radial model, include the gamma value. set.seed(1) m3_tune &lt;- tune( svm, y ~ ., data = train_data, kernel =&quot;polynomial&quot;, ranges = list( cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100), degree = c(1, 2, 3) ) ) summary(m3_tune) ## ## Parameter tuning of &#39;svm&#39;: ## ## - sampling method: 10-fold cross validation ## ## - best parameters: ## cost degree ## 1 1 ## ## - best performance: 0.1 ## ## - Detailed performance results: ## cost degree error dispersion ## 1 0.001 1 0.55 0.44 ## 2 0.010 1 0.55 0.44 ## 3 0.100 1 0.30 0.26 ## 4 1.000 1 0.10 0.21 ## 5 5.000 1 0.10 0.21 ## 6 10.000 1 0.15 0.24 ## 7 100.000 1 0.15 0.24 ## 8 0.001 2 0.70 0.42 ## 9 0.010 2 0.70 0.42 ## 10 0.100 2 0.70 0.42 ## 11 1.000 2 0.65 0.24 ## 12 5.000 2 0.50 0.33 ## 13 10.000 2 0.50 0.33 ## 14 100.000 2 0.50 0.33 ## 15 0.001 3 0.65 0.34 ## 16 0.010 3 0.65 0.34 ## 17 0.100 3 0.50 0.33 ## 18 1.000 3 0.40 0.32 ## 19 5.000 3 0.35 0.34 ## 20 10.000 3 0.35 0.34 ## 21 100.000 3 0.35 0.34 The lowest cross-validation error rate is 0.10 with cost = 1, polynomial degree 1. m3_best &lt;- m3_tune$best.model summary(m3_best) ## ## Call: ## best.tune(method = svm, train.x = y ~ ., data = train_data, ranges = list(cost = c(0.001, ## 0.01, 0.1, 1, 5, 10, 100), degree = c(1, 2, 3)), kernel = &quot;polynomial&quot;) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: polynomial ## cost: 1 ## degree: 1 ## coef.0: 0 ## ## Number of Support Vectors: 12 ## ## ( 6 6 ) ## ## ## Number of Classes: 2 ## ## Levels: ## -1 1 There are 12 support vectors, 6 in each class. This is a pretty wide margin. plot(m3_best, train_data) "],
["using-caret.html", "13.5 Using Caret", " 13.5 Using Caret The model can also be fit using caret. I’ll used LOOCV since the data set is so small. Normalize the variables to make their scale comparable. library(caret) library(kernlab) train_data_3 &lt;- train_data %&gt;% mutate(y = factor(y, labels = c(&quot;A&quot;, &quot;B&quot;))) m4 &lt;- train( y ~ ., data = train_data_3, method = &quot;svmPoly&quot;, preProcess = c(&quot;center&quot;, &quot;scale&quot;), trControl = trainControl( method = &quot;cv&quot;, number = 5, summaryFunction = twoClassSummary, # Use AUC to pick the best model classProbs=TRUE ) ) m4$bestTune ## degree scale C ## 8 1 0.1 0.5 #plot(m4) "],
["principal-components-analysis.html", "Chapter 14 Principal Components Analysis", " Chapter 14 Principal Components Analysis "],
["clustering.html", "Chapter 15 Clustering", " Chapter 15 Clustering "],
["text-mining.html", "Chapter 16 Text Mining", " Chapter 16 Text Mining "],
["appendix.html", "Appendix", " Appendix Here are miscellaneous skills, knowledge, and technologies I should know. "],
["publishing-to-bookdown.html", "Publishing to BookDown", " Publishing to BookDown The bookdown package, written by Yihui Xie, is built on top of R Markdown and the knitr package. Use it to publish a book or long manuscript where each chapter is a separate file. There are instructions for how to author a book in his bookdown book (Xie 2019). The main advantage of bookdown over R Markdown is that you can produce multi-page HTML output with numbered headers, equations, figures, etc., just like in a book. I’m using bookdown to create a compendium of all my data science notes. The first step to using bookdown is installing the **bookdown* package with install.packages(\"bookdown\"). Next, create an account at bookdown.org, and connect the account to RStudio. Follow the instructions at https://bookdown.org/home/about/. Finally, create a project in R Studio by creating a new project of type Book Project using Bookdown. After creating all of your Markdown pages, knit the book or click the Build Book button in the Build panel. References "],
["shiny-apps.html", "Shiny Apps", " Shiny Apps "],
["packages.html", "Packages", " Packages R Packages (Wickham 2015) by Hadley Wickham is a good manual on packages, but it does not include a full tutorial. The Developing R Packages Data Camp course is also helpful. I will set up my own exercise and present it here. I will create a package for my pretend organization, “MF”. The package will include the following: R Markdown template. My template will integrate code, output, and commentary in a single R Markdown. The template will produce a familiar work product containing standard content (summary, data management, exploratory analysis, methods, results, conclusions), and a standard style (colors, typeface, size, logo). Functions. Common I/O functions for database retrieval, writing to Excel. Common graphing functions for ggplot styling. I am mostly copying the logic and code from the ggthemes economist.R script. Create a package In the RStudio IDE, click File &gt; New Project. Select “New Directory”. Select “R Package”. You can also use devtools::create(\"mfstylr\"). This will create the minimum items for an R package. + R directory: R scripts with function definitions. + man directory: documentation + NAMESPACE file: information about imported functions and functions made available (managed by **roxygen2**) + DESCRIPTION file: metadata about the package Write functions in R scripts in R directory. Document with tags readable by roxygen2 package. Select XYZ &gt; Install and Restart. 16.0.1 Document Functions with roxygen Add roxygen documentation with #' characters. The first three lines are always the title, Description, and Details. They don’t need any tags, but you need to separate them with blank lines. Create Data Add an RData file to your package with use_data() Create Vignette Add a directory and template vignette with use_vignette(name, title). use_vignette(&quot;Creating-Plots-with-mfstylr&quot;, &quot;Creating Plots with mfstylr&quot;) Step 2: Create an R Markdown template I relied on this blog at free range statistics for a lot what follows. There is also good information about R Markdown and templates in Yihui Xie’s R Markdown: The Definitive Guide (Xie, Allaire, and Grolemund 2019). Use usethis::use_rmarkdown_template() to create an Rmd template. I will create a “Kaggle Report” template. In the Console (or a script), enter usethis::use_rmarkdown_template( template_name = &quot;Kaggle Report&quot;, template_dir = &quot;kaggle_report&quot;, template_description = &quot;Template for creating Kaggle reports in RMarkdown.&quot;, template_create_dir = FALSE ) Since my project directory is C:\\Users\\mpfol\\OneDrive\\Documents\\GitHub\\mfstylr, use_rmarkdown_template() creates subdirectories .\\inst\\rmarkdown\\templates\\kaggle_report\\skeleton with three files .\\inst\\rmarkdown\\templates\\kaggle_report\\template.yaml .\\inst\\rmarkdown\\templates\\kaggle_report\\skeleton\\skeleton.Rmd My kaggle report template will include a logo. Looks like there are two ways to embed an image in your document. One is a direct image loading reference !(), but I don’t think you can control the attributes this way. A second way is adding html tags. ![](logo.png) # or for more control &lt;img src=&quot;logo.png&quot; style=&quot;position:absolute;top:0px;right:0px;&quot; /&gt; References "],
["references.html", "References", " References "]
]
