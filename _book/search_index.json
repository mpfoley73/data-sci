[
["cluster-analysis.html", "Chapter 11 Cluster Analysis", " Chapter 11 Cluster Analysis These notes are primarily taken from studying DataCamp courses Cluster Analysis in R and Unsupervised Learning in R, AIHR, UC Business Analytics R Programming Guide, and PSU STAT-505. Cluster analysis is a data exploration (mining) tool for dividing features into clusters, distinct populations with no a priori defining characteristics. The goal is to describe those populations with the observed data. Popular uses of clustering are audience segmentation, creating personas, detecting anomalies, and pattern recognition in images. There are two common approaches to cluster analysis. Agglomerative hierarchical algorithms start by defining each data point as a cluster, then repeatedly combine the two closest clusters into a new cluster until all data points are merged into a single cluster. Non-hierarchical methods such as K-means initially randomly partitions the data into a set of K clusters, then iteratively moves observations into different clusters until there is no sensible reassignment possible. 11.0.0.1 Setup I will learn by example, using the IBM HR Analytics Employee Attrition &amp; Performance data set from Kaggle to discover which factors are associated with employee turnover and whether distinct clusters of employees are more susceptible to turnover. The clusters can help personalize employee experience (AIHR). This data set includes 1,470 employee records consisting of the EmployeeNumber, a flag for Attrition during some time frame, and 32 other descriptive variables. library(tidyverse) library(plotly) # interactive graphing library(cluster) # daisy and pam library(Rtsne) # dimensionality reduction and visualization library(dendextend) # color_branches set.seed(1234) # reproducibility dat &lt;- read_csv(&quot;./input/WA_Fn-UseC_-HR-Employee-Attrition.csv&quot;) dat &lt;- dat %&gt;% mutate_if(is.character, as_factor) %&gt;% select(EmployeeNumber, Attrition, everything()) my_skim &lt;- skimr::skim_with(numeric = skimr::sfl(p25 = NULL, p50 = NULL, p75 = NULL, hist = NULL)) my_skim(dat) Table 11.1: Data summary Name dat Number of rows 1470 Number of columns 35 _______________________ Column type frequency: factor 9 numeric 26 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts Attrition 0 1 FALSE 2 No: 1233, Yes: 237 BusinessTravel 0 1 FALSE 3 Tra: 1043, Tra: 277, Non: 150 Department 0 1 FALSE 3 Res: 961, Sal: 446, Hum: 63 EducationField 0 1 FALSE 6 Lif: 606, Med: 464, Mar: 159, Tec: 132 Gender 0 1 FALSE 2 Mal: 882, Fem: 588 JobRole 0 1 FALSE 9 Sal: 326, Res: 292, Lab: 259, Man: 145 MaritalStatus 0 1 FALSE 3 Mar: 673, Sin: 470, Div: 327 Over18 0 1 FALSE 1 Y: 1470 OverTime 0 1 FALSE 2 No: 1054, Yes: 416 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p100 EmployeeNumber 0 1 1024.87 602.02 1 2068 Age 0 1 36.92 9.14 18 60 DailyRate 0 1 802.49 403.51 102 1499 DistanceFromHome 0 1 9.19 8.11 1 29 Education 0 1 2.91 1.02 1 5 EmployeeCount 0 1 1.00 0.00 1 1 EnvironmentSatisfaction 0 1 2.72 1.09 1 4 HourlyRate 0 1 65.89 20.33 30 100 JobInvolvement 0 1 2.73 0.71 1 4 JobLevel 0 1 2.06 1.11 1 5 JobSatisfaction 0 1 2.73 1.10 1 4 MonthlyIncome 0 1 6502.93 4707.96 1009 19999 MonthlyRate 0 1 14313.10 7117.79 2094 26999 NumCompaniesWorked 0 1 2.69 2.50 0 9 PercentSalaryHike 0 1 15.21 3.66 11 25 PerformanceRating 0 1 3.15 0.36 3 4 RelationshipSatisfaction 0 1 2.71 1.08 1 4 StandardHours 0 1 80.00 0.00 80 80 StockOptionLevel 0 1 0.79 0.85 0 3 TotalWorkingYears 0 1 11.28 7.78 0 40 TrainingTimesLastYear 0 1 2.80 1.29 0 6 WorkLifeBalance 0 1 2.76 0.71 1 4 YearsAtCompany 0 1 7.01 6.13 0 40 YearsInCurrentRole 0 1 4.23 3.62 0 18 YearsSinceLastPromotion 0 1 2.19 3.22 0 15 YearsWithCurrManager 0 1 4.12 3.57 0 17 You would normally start a cluster analysis with an exploration of the data to determine which variables are interesting and relevant to your goal. I’ll bypass that rigor and just use a binary correlation analysis. Binary correlation analysis converts features into binary format by binning the continuous features and one-hot encoding the binary features. correlate() calculates the correlation coefficient for each binary feature to the response variable. A Correlation Funnel is an tornado plot that lists the highest correlation features (based on absolute magnitude) at the top of the and the lowest correlation features at the bottom. Read more on the correlationfunel GitHub README. Using binary correlation, I’ll include just the variables with a correlation coefficient of at least 0.10. For our employee attrition data set, OverTime (Y|N) has the largest correlation, JobLevel = 1, MonthlyIncome &lt;= 2,695.80, etc. dat %&gt;% select(-EmployeeNumber) %&gt;% correlationfunnel::binarize(n_bins = 5, thresh_infreq = 0.01) %&gt;% correlationfunnel::correlate(Attrition__Yes) %&gt;% correlationfunnel::plot_correlation_funnel(interactive = FALSE) %&gt;% ggplotly() # Makes prettier, but drops the labels Using the cutoff of 0.1 leaves 14 features for the analysis. vars &lt;- c( &quot;EmployeeNumber&quot;, &quot;Attrition&quot;, &quot;OverTime&quot;, &quot;JobLevel&quot;, &quot;MonthlyIncome&quot;, &quot;YearsAtCompany&quot;, &quot;StockOptionLevel&quot;, &quot;YearsWithCurrManager&quot;, &quot;TotalWorkingYears&quot;, &quot;MaritalStatus&quot;, &quot;Age&quot;, &quot;YearsInCurrentRole&quot;, &quot;JobRole&quot;, &quot;EnvironmentSatisfaction&quot;, &quot;JobInvolvement&quot;, &quot;BusinessTravel&quot; ) dat_2 &lt;- dat %&gt;% select(one_of(vars)) Data Preparation The concept of distance is central to clustering. Two observations are similar if the distance between their features is relatively small. To compare feature distances, they should be on a similar scale. There are many ways to define distance (see options in ?dist), but the two most common are Euclidean, \\(d = \\sqrt{\\sum{(x_i - y_i)^2}}\\), and binary, 1 minus the proportion of shared features (Wikipedia). If you have a mix a feature types, use the Gower distance (Analytics Vidhya) range-normalizes the quantitative variables, one-hot encodes the nominal variables, and ranks the ordinal variables. Then it calculates distances using the Manhattan distance for quantitative and ordinal variables, and the Dice coefficient for nominal variables. Gower’s distance is computationally expensive, so you could one-hot encode the data and standardize the variables as \\((x - \\bar{x}) / sd(x)\\) so that each feature has a mean of 0 and standard deviation of 1, like this: dat_2_mtrx &lt;- mltools::one_hot(data.table::as.data.table(dat_2[, 2:16])) %&gt;% as.matrix() row.names(dat_2_mtrx) &lt;- dat_2$EmployeeNumber dat_2_mtrx &lt;- na.omit(dat_2_mtrx) dat_2_mtrx &lt;- scale(dat_2_mtrx) dat_2_dist &lt;- dist(dat_2_mtrx) But normally you would go ahead and calculate Gower’s distance using daisy(). dat_2_gwr &lt;- cluster::daisy(dat_2[, 2:16], metric = &quot;gower&quot;) As a sanity check, let’s see the most similar and dissimilar pairs of employees according to their Gower distance. Here are the most similar employees. x &lt;- as.matrix(dat_2_gwr) dat_2[which(x == min(x[x != 0]), arr.ind = TRUE)[1, ], ] %&gt;% t() %&gt;% as.data.frame() %&gt;% rownames_to_column() %&gt;% flextable::flextable() %&gt;% flextable::autofit() rownameV1V2EmployeeNumber1624 614AttritionYesYesOverTimeYesYesJobLevel11MonthlyIncome15691878YearsAtCompany00StockOptionLevel00YearsWithCurrManager00TotalWorkingYears00MaritalStatusSingleSingleAge1818YearsInCurrentRole00JobRoleSales RepresentativeSales RepresentativeEnvironmentSatisfaction22JobInvolvement33BusinessTravelTravel_FrequentlyTravel_Frequently They are identical except for MonthlyIncome. Here are the most dissimilar employees. dat_2[which(x == max(x), arr.ind = TRUE)[1, ], ] %&gt;% t() %&gt;% as.data.frame() %&gt;% rownames_to_column() %&gt;% flextable::flextable() %&gt;% flextable::autofit() rownameV1V2EmployeeNumber1094 825AttritionNoYesOverTimeNoYesJobLevel15MonthlyIncome 462119246YearsAtCompany 331StockOptionLevel30YearsWithCurrManager28TotalWorkingYears 340MaritalStatusMarriedSingleAge2758YearsInCurrentRole 215JobRoleLaboratory TechnicianResearch DirectorEnvironmentSatisfaction14JobInvolvement13BusinessTravelNon-TravelTravel_Rarely These two employees have nothing in common. With the data preparation complete, we can finally perform our cluster analysis. I’ll try K-means and HCA. "],
["k-means.html", "11.1 K-Means", " 11.1 K-Means The k-means clustering algorithm randomly assigns all observations to one of \\(k\\) clusters. K-means then iteratively calculates the cluster centroids and reassigns the observations to their nearest centroid. The centroid is the mean of the points in the cluster (Hence the name “k-means”). The iterations continue until either the centroids stabilize or the iterations reach a set maximum, iter.max (typically 50). The result is k clusters with the minimum total intra-cluster variation. The centroid of cluster \\(c_i \\in C\\) is the mean of the cluster observations \\(S_i\\): \\(c_i = \\frac{1}{|S_i|} \\sum_{x_i \\in S_i}{x_i}\\). The nearest centroid is the minimum squared Euclidean distance, \\(\\underset{c_i \\in C}{\\operatorname{arg min}} dist(c_i, x)^2\\).1 The algorithm will converge to a result, but the result may only be a local optimum. Other random starting centroids may yield a different local optimum. Common practice is to run the k-means algorithm nstart times and select the lowest within-cluster sum of squared distances among the cluster members. A typical number of runs is nstart = 20. Choosing K What is the best number of clusters? You may have a preference in advance, but more likely you will use a scree plot or use the silhouette method. The scree plot is a plot of the total within-cluster sum of squared distances as a function of k. The sum of squares always decreases as k increases, but at a declining rate. The optimal k is at the “elbow” in the curve - the point at which the curve flattens. kmeans() returns an object of class kmeans, a list in which one of the components is the model sum of squares tot.withinss. In the scree plot below, the elbow may be k = 5. wss &lt;- map_dbl(2:10, ~ kmeans(dat_2_gwr, centers = .x)$tot.withinss) wss_tbl &lt;- tibble(k = 2:10, wss) ggplot(wss_tbl, aes(x = k, y = wss)) + geom_point(size = 2) + geom_line() + scale_x_continuous(breaks = 2:10) + labs(title = &quot;Scree Plot&quot;) The silhouette method calculates the within-cluster distance \\(C(i)\\) for each observation, and its distance to the nearest cluster \\(N(i)\\). The silhouette width is \\(S = 1 - C(i) / N(i)\\) for \\(C(i) &lt; N(i)\\) and \\(S = N(i) / C(i) - 1\\) for \\(C(i) &gt; N(i)\\). A value close to 1 means the observation is well-matched to its current cluster; A value near 0 means the observation is on the border between the two clusters; and a value near -1 means the observation is better-matched to the other cluster. The optimal number of clusters is the number that maximizes the total silhouette width. cluster::pam() returns a list in which one of the components is the average width silinfo$avg.width. In the silhouette plot below, the maximum width is at k = 6. sil &lt;- map_dbl(2:10, ~ pam(dat_2_gwr, k = .x)$silinfo$avg.width) sil_tbl &lt;- tibble(k = 2:10, sil) ggplot(sil_tbl, aes(x = k, y = sil)) + geom_point(size = 2) + geom_line() + scale_x_continuous(breaks = 2:10) + labs(title = &quot;Silhouette Plot&quot;) Run pam() again and attach the results to the original table for visualization and summary statistics. mdl &lt;- pam(dat_2_gwr, k = 6) dat_3 &lt;- dat_2 %&gt;% mutate(cluster = mdl$clustering) Here are the six medoids from our data set. dat_2[mdl$medoids, ] ## # A tibble: 6 x 16 ## EmployeeNumber Attrition OverTime JobLevel MonthlyIncome YearsAtCompany ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1171 No No 2 5155 6 ## 2 35 No No 2 6825 9 ## 3 65 Yes Yes 1 3441 2 ## 4 221 No No 1 2713 5 ## 5 747 No No 2 5304 8 ## 6 1408 No No 4 16799 20 ## # ... with 10 more variables: StockOptionLevel &lt;dbl&gt;, ## # YearsWithCurrManager &lt;dbl&gt;, TotalWorkingYears &lt;dbl&gt;, MaritalStatus &lt;fct&gt;, ## # Age &lt;dbl&gt;, YearsInCurrentRole &lt;dbl&gt;, JobRole &lt;fct&gt;, ## # EnvironmentSatisfaction &lt;dbl&gt;, JobInvolvement &lt;dbl&gt;, BusinessTravel &lt;fct&gt; We’re most concerned about attrition. Do high-attrition employees fall into a particular cluster? Yes! 79.7% of cluster 3 left the company - that’s 59.5% of all turnover in the company. dat_3_smry &lt;- dat_3 %&gt;% count(cluster, Attrition) %&gt;% group_by(cluster) %&gt;% mutate(cluster_n = sum(n), turnover_rate = scales::percent(n / sum(n), accuracy = 0.1)) %&gt;% ungroup() %&gt;% filter(Attrition == &quot;Yes&quot;) %&gt;% mutate(pct_of_turnover = scales::percent(n / sum(n), accuracy = 0.1)) %&gt;% select(cluster, turnover_rate, turnover_n = n, cluster_n, pct_of_turnover) dat_3_smry ## # A tibble: 6 x 5 ## cluster turnover_rate turnover_n cluster_n pct_of_turnover ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 8.6% 23 268 9.7% ## 2 2 8.6% 24 280 10.1% ## 3 3 79.7% 141 177 59.5% ## 4 4 8.0% 29 364 12.2% ## 5 5 4.0% 8 202 3.4% ## 6 6 6.7% 12 179 5.1% You can get some sense of the quality of clustering by constructing the Barnes-Hut t-Distributed Stochastic Neighbor Embedding (t-SNE). dat_4 &lt;- dat_3 %&gt;% left_join(dat_3_smry, by = &quot;cluster&quot;) %&gt;% rename(Cluster = cluster) %&gt;% mutate( MonthlyIncome = MonthlyIncome %&gt;% scales::dollar(), description = str_glue(&quot;Turnover = {Attrition} MaritalDesc = {MaritalStatus} Age = {Age} Job Role = {JobRole} Job Level {JobLevel} Overtime = {OverTime} Current Role Tenure = {YearsInCurrentRole} Professional Tenure = {TotalWorkingYears} Monthly Income = {MonthlyIncome} Cluster: {Cluster} Cluster Size: {cluster_n} Cluster Turnover Rate: {turnover_rate} Cluster Turnover Count: {turnover_n} &quot;)) tsne_obj &lt;- Rtsne(dat_2_gwr, is_distance = TRUE) tsne_tbl &lt;- tsne_obj$Y %&gt;% as_tibble() %&gt;% setNames(c(&quot;X&quot;, &quot;Y&quot;)) %&gt;% bind_cols(dat_4) %&gt;% mutate(Cluster = as_factor(Cluster)) g &lt;- tsne_tbl %&gt;% ggplot(aes(x = X, y = Y, colour = Cluster, text = description)) + geom_point() ggplotly(g) Another common approach is to take summary statistics and draw your own conclusions. dat_3 %&gt;% group_by(cluster, Attrition) %&gt;% summarise_if(is.numeric, &quot;mean&quot;) ## # A tibble: 12 x 13 ## # Groups: cluster [6] ## cluster Attrition EmployeeNumber JobLevel MonthlyIncome YearsAtCompany ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Yes 1003. 2.26 6530. 7.17 ## 2 1 No 985. 2.12 6324. 7.29 ## 3 2 Yes 1136. 2.42 7171. 7.92 ## 4 2 No 1049. 2.25 6621. 7.33 ## 5 3 Yes 954. 1.28 3547. 3.62 ## 6 3 No 1139. 1.25 3477. 3.67 ## 7 4 Yes 1119. 1.10 2900. 2.83 ## 8 4 No 1032. 1.14 3106. 3.98 ## 9 5 Yes 1372. 2.38 7296. 3.12 ## 10 5 No 1013. 2.16 6492. 7.74 ## 11 6 Yes 930. 3.83 14134 20.3 ## 12 6 No 1041. 4.23 16498. 14.7 ## # ... with 7 more variables: StockOptionLevel &lt;dbl&gt;, ## # YearsWithCurrManager &lt;dbl&gt;, TotalWorkingYears &lt;dbl&gt;, Age &lt;dbl&gt;, ## # YearsInCurrentRole &lt;dbl&gt;, EnvironmentSatisfaction &lt;dbl&gt;, ## # JobInvolvement &lt;dbl&gt; Euclidean distances are appropriate for quantitative variables. What about categorical variables? This discussion at StackExchange explains that k-modes is suitable for categorical data. It may be okay to convert categorical variables into binary values and treating them as numeric.↩︎ "],
["hca.html", "11.2 HCA", " 11.2 HCA Hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which builds a hierarchy of clusters. One usually presents the HCA results in a dendrogram. The HCA process is: Calculate the distance between each observation with dist() or daisy(). We did that above when we created dat_2_gwr. Cluster the distances with hclust(dist, method = c(\"complete\", \"single\", \"average\", \"centroid\"). hclust groups the two closest observations into a cluster, then calculates the cluster’s distance to the remaining observations. If the shortest distance is between two observations, hclust defines a second cluster, otherwise hclust adds the observation as a new level to the cluster. The process repeats until all observations belong to a single cluster. The “distance” to a cluster can be defined as: complete: distance to the furthest member of the cluster, single: distance to the closest member of the cluster, average: average distance to all members of the cluster, or centroid: distance between the centroids of each cluster. Complete and average distances tend to produce more balanced trees and are most common. Pruning an unbalanced tree can result in most observations assigned to one cluster and only a few observations assigned to other clusters. This is useful for identifying outliers. hclust() returns a value of class hclust. mdl_hc &lt;- hclust(dat_2_gwr, method = &quot;complete&quot;) Evaluate the hclust tree with a dendogram, principal component analysis (PCA), and/or summary statistics. The vertical lines in a dendogram indicate the distance between nodes and their associated cluster. Dendograms are difficult to visualize When the number of features is greater than two. One work-around is to plot just two dimensions at a time. # Inspect the tree to choose a size. plot(color_branches(as.dendrogram(mdl_hc), k = 7)) abline(h = .65, col = &quot;red&quot;) “Cut” the hierarchical tree into the desired number of clusters (k) or height h with cutree(hclust, k = NULL, h = NULL). cutree() returns a vector of cluster memberships. Attach this vector back to the original dataframe for visualization and summary statistics. The dendogram suggests the optimal number of clusters is six. Build a cluster with k = 6 means. Attach the cluster assignment vector back to the original dataframe for visualization and/or summary statistics. dat_2_clstr_hca &lt;- dat_2 %&gt;% mutate(cluster = cutree(mdl_hc, k = 6)) Calculate summary statistics and draw conclusions. Useful summary statistics are typically membership count, and feature averages (or proportions). dat_2_clstr_hca %&gt;% group_by(cluster) %&gt;% summarise_if(is.numeric, funs(mean(.))) ## # A tibble: 6 x 12 ## cluster EmployeeNumber JobLevel MonthlyIncome YearsAtCompany StockOptionLevel ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1051. 2.59 8056. 8.21 0.515 ## 2 2 1029. 1.76 5146. 5.98 0.964 ## 3 3 974. 1.59 4450. 5.29 0 ## 4 4 1088. 1.46 3961. 4.60 1.14 ## 5 5 999. 3.99 15272. 14.4 0.662 ## 6 6 993. 3.67 13733. 12.8 0.955 ## # ... with 6 more variables: YearsWithCurrManager &lt;dbl&gt;, ## # TotalWorkingYears &lt;dbl&gt;, Age &lt;dbl&gt;, YearsInCurrentRole &lt;dbl&gt;, ## # EnvironmentSatisfaction &lt;dbl&gt;, JobInvolvement &lt;dbl&gt; 11.2.0.1 K-Means vs HCA Hierarchical clustering has some advantages over k-means. It can use any distance method - not just euclidean. The results are stable - k-means can produce different results each time. While they can both be evaluated with the silhouette and elbow plots, hierachical clustering can also be evaluated with a dendogram. But hierarchical clusters has one significant drawback: it is computationally complex compared to k-means. For this last reason, k-means is more common. "]
]
