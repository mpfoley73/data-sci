[
["decision-trees.html", "Chapter 9 Decision Trees", " Chapter 9 Decision Trees Decision trees, also known as classification and regression tree (CART) models, are tree-based methods for supervised machine learning. Simple classification trees and regression trees are easy to use and interpret, but are not competitive with the best machine learning methods. However, they form the foundation for ensemble models such as bagged trees, random forests, and boosted trees, which although less interpretable, are very accurate. CART models segment the predictor space into \\(K\\) non-overlapping terminal nodes (leaves). Each node is described by a set of rules which can be used to predict new responses. The predicted value \\(\\hat{y}\\) for each node is the mode (classification) or mean (regression). CART models define the nodes through a top-down greedy process called recursive binary splitting. The process is top-down because it begins at the top of the tree with all observations in a single region and successively splits the predictor space. It is greedy because at each splitting step, the best split is made at that particular step without consideration to subsequent splits. The best split is the predictor variable and cutpoint that minimizes a cost function. The most common cost function for regression trees is the sum of squared residuals, \\[RSS = \\sum_{k=1}^K\\sum_{i \\in A_k}{\\left(y_i - \\hat{y}_{A_k} \\right)^2}.\\] For classification trees, it is the Gini index, \\[G = \\sum_{c=1}^C{\\hat{p}_{kc}(1 - \\hat{p}_{kc})},\\] and the entropy (aka information statistic) \\[D = - \\sum_{c=1}^C{\\hat{p}_{kc} \\log \\hat{p}_{kc}}\\] where \\(\\hat{p}_{kc}\\) is the proportion of training observations in node \\(k\\) that are class \\(c\\). A completely pure node in a binary tree would have \\(\\hat{p} \\in \\{ 0, 1 \\}\\) and \\(G = D = 0\\). A completely impure node in a binary tree would have \\(\\hat{p} = 0.5\\) and \\(G = 0.5^2 \\cdot 2 = 0.25\\) and \\(D = -(0.5 \\log(0.5)) \\cdot 2 = 0.69\\). CART repeats the splitting process for each child node until a stopping criterion is satisfied, usually when no node size surpasses a predefined maximum, or continued splitting does not improve the model significantly. CART may also impose a minimum number of observations in each node. The resulting tree likely over-fits the training data and therefore does not generalize well to test data, so CART prunes the tree, minimizing the cross-validated prediction error. Rather than cross-validating every possible subtree to find the one with minimum error, CART uses cost-complexity pruning. Cost-complexity is the tradeoff between error (cost) and tree size (complexity) where the tradeoff is quantified with cost-complexity parameter \\(c_p\\). The cost complexity of the tree, \\(R_{c_p}(T)\\), is the sum of its risk (error) plus a “cost complexity” factor \\(c_p\\) multiple of the tree size \\(|T|\\). \\[R_{c_p}(T) = R(T) + c_p|T|\\] \\(c_p\\) can take on any value from \\([0..\\infty]\\), but it turns out there is an optimal tree for ranges of \\(c_p\\) values, so there are only a finite set of interesting values for \\(c_p\\) (James et al. 2013) (Therneau and Atkinson 2019) (Kuhn and Johnson 2016). A parametric algorithm identifies the interesting \\(c_p\\) values and their associated pruned trees, \\(T_{c_p}\\). CART uses cross-validation to determine which \\(c_p\\) is optimal. References "],
["classification-tree.html", "9.1 Classification Tree", " 9.1 Classification Tree You don’t usually build a simple classification tree on its own, but it is a good way to build understanding, and the ensemble models build on the logic. I’ll learn by example, using the ISLR::OJ data set to predict which brand of orange juice, Citrus Hill (CH) or Minute Maid = (MM), customers Purchase from its 17 predictor variables. library(tidyverse) library(caret) library(rpart) # classification and regression trees library(rpart.plot) # better formatted plots than the ones in rpart oj_dat &lt;- ISLR::OJ skimr::skim(oj_dat) Table 9.1: Data summary Name oj_dat Number of rows 1070 Number of columns 18 _______________________ Column type frequency: factor 2 numeric 16 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts Purchase 0 1 FALSE 2 CH: 653, MM: 417 Store7 0 1 FALSE 2 No: 714, Yes: 356 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist WeekofPurchase 0 1 254.38 15.56 227.00 240.00 257.00 268.00 278.00 ▆▅▅▇▇ StoreID 0 1 3.96 2.31 1.00 2.00 3.00 7.00 7.00 ▇▅▃▁▇ PriceCH 0 1 1.87 0.10 1.69 1.79 1.86 1.99 2.09 ▅▂▇▆▁ PriceMM 0 1 2.09 0.13 1.69 1.99 2.09 2.18 2.29 ▂▁▃▇▆ DiscCH 0 1 0.05 0.12 0.00 0.00 0.00 0.00 0.50 ▇▁▁▁▁ DiscMM 0 1 0.12 0.21 0.00 0.00 0.00 0.23 0.80 ▇▁▂▁▁ SpecialCH 0 1 0.15 0.35 0.00 0.00 0.00 0.00 1.00 ▇▁▁▁▂ SpecialMM 0 1 0.16 0.37 0.00 0.00 0.00 0.00 1.00 ▇▁▁▁▂ LoyalCH 0 1 0.57 0.31 0.00 0.33 0.60 0.85 1.00 ▅▃▆▆▇ SalePriceMM 0 1 1.96 0.25 1.19 1.69 2.09 2.13 2.29 ▁▂▂▂▇ SalePriceCH 0 1 1.82 0.14 1.39 1.75 1.86 1.89 2.09 ▂▁▇▇▅ PriceDiff 0 1 0.15 0.27 -0.67 0.00 0.23 0.32 0.64 ▁▂▃▇▂ PctDiscMM 0 1 0.06 0.10 0.00 0.00 0.00 0.11 0.40 ▇▁▂▁▁ PctDiscCH 0 1 0.03 0.06 0.00 0.00 0.00 0.00 0.25 ▇▁▁▁▁ ListPriceDiff 0 1 0.22 0.11 0.00 0.14 0.24 0.30 0.44 ▂▃▆▇▁ STORE 0 1 1.63 1.43 0.00 0.00 2.00 3.00 4.00 ▇▃▅▅▃ I’ll split oj_dat (n = 1,070) into oj_train (80%, n = 857) to fit various models, and oj_test (20%, n = 213) to compare their performance on new data. set.seed(12345) partition &lt;- createDataPartition(y = oj_dat$Purchase, p = 0.8, list = FALSE) oj_train &lt;- oj_dat[partition, ] oj_test &lt;- oj_dat[-partition, ] Function rpart::rpart() builds a full tree, minimizing the Gini index \\(G\\) by default (parms = list(split = \"gini\")), until the stopping criterion is satisfied. The default stopping criterion is only attempt a split if the current node has at least minsplit = 20 observations, and only accept a split if the resulting nodes have at least minbucket = round(minsplit/3) observations, and the resulting overall fit improves by cp = 0.01 (i.e., \\(\\Delta G &lt;= 0.01\\)). # Use method = &quot;class&quot; for classification, method = &quot;anova&quot; for regression set.seed(123) oj_mdl_cart_full &lt;- rpart(formula = Purchase ~ ., data = oj_train, method = &quot;class&quot;) print(oj_mdl_cart_full) ## n= 857 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 857 334 CH (0.61026838 0.38973162) ## 2) LoyalCH&gt;=0.48285 537 94 CH (0.82495345 0.17504655) ## 4) LoyalCH&gt;=0.7648795 271 13 CH (0.95202952 0.04797048) * ## 5) LoyalCH&lt; 0.7648795 266 81 CH (0.69548872 0.30451128) ## 10) PriceDiff&gt;=-0.165 226 50 CH (0.77876106 0.22123894) * ## 11) PriceDiff&lt; -0.165 40 9 MM (0.22500000 0.77500000) * ## 3) LoyalCH&lt; 0.48285 320 80 MM (0.25000000 0.75000000) ## 6) LoyalCH&gt;=0.2761415 146 58 MM (0.39726027 0.60273973) ## 12) SalePriceMM&gt;=2.04 71 31 CH (0.56338028 0.43661972) * ## 13) SalePriceMM&lt; 2.04 75 18 MM (0.24000000 0.76000000) * ## 7) LoyalCH&lt; 0.2761415 174 22 MM (0.12643678 0.87356322) * The output starts with the root node. The predicted class at the root is CH and this prediction produces 334 errors on the 857 observations for a success rate (accuracy) of 61% (0.61026838) and an error rate of 39% (0.38973162). The child nodes of node “x” are labeled 2x) and 2x+1), so the child nodes of 1) are 2) and 3), and the child nodes of 2) are 4) and 5). Terminal nodes are labeled with an asterisk (*). Surprisingly, only 3 of the 17 features were used the in full tree: LoyalCH (Customer brand loyalty for CH), PriceDiff (relative price of MM over CH), and SalePriceMM (absolute price of MM). The first split is at LoyalCH = 0.48285. Here is a diagram of the full (unpruned) tree. rpart.plot(oj_mdl_cart_full, yesno = TRUE) The boxes show the node classification (based on mode), the proportion of observations that are not CH, and the proportion of observations included in the node. rpart() not only grew the full tree, it identified the set of cost complexity parameters, and measured the model performance of each corresponding tree using cross-validation. printcp() displays the candidate \\(c_p\\) values. You can use this table to decide how to prune the tree. printcp(oj_mdl_cart_full) ## ## Classification tree: ## rpart(formula = Purchase ~ ., data = oj_train, method = &quot;class&quot;) ## ## Variables actually used in tree construction: ## [1] LoyalCH PriceDiff SalePriceMM ## ## Root node error: 334/857 = 0.38973 ## ## n= 857 ## ## CP nsplit rel error xerror xstd ## 1 0.479042 0 1.00000 1.00000 0.042745 ## 2 0.032934 1 0.52096 0.54192 0.035775 ## 3 0.013473 3 0.45509 0.47006 0.033905 ## 4 0.010000 5 0.42814 0.46407 0.033736 There are 4 \\(c_p\\) values in this model. The model with the smallest complexity parameter allows the most splits (nsplit). The highest complexity parameter corresponds to a tree with just a root node. rel error is the error rate relative to the root node. The root node absolute error is 0.38973162 (the proportion of MM), so its rel error is 0.38973162/0.38973162 = 1.0. That means the absolute error of the full tree (at CP = 0.01) is 0.42814 * 0.38973162 = 0.1669. You can verify that by calculating the error rate of the predicted values: data.frame(pred = predict(oj_mdl_cart_full, newdata = oj_train, type = &quot;class&quot;)) %&gt;% mutate(obs = oj_train$Purchase, err = if_else(pred != obs, 1, 0)) %&gt;% summarize(mean_err = mean(err)) ## mean_err ## 1 0.1668611 Finishing the CP table tour, xerror is the relative cross-validated error rate and xstd is its standard error. If you want the lowest possible error, then prune to the tree with the smallest relative CV error, \\(c_p\\) = 0.01. If you want to balance predictive power with simplicity, prune to the smallest tree within 1 SE of the one with the smallest relative error. The CP table is not super-helpful for finding that tree, so add a column to find it. oj_mdl_cart_full$cptable %&gt;% data.frame() %&gt;% mutate( min_idx = which.min(oj_mdl_cart_full$cptable[, &quot;xerror&quot;]), rownum = row_number(), xerror_cap = oj_mdl_cart_full$cptable[min_idx, &quot;xerror&quot;] + oj_mdl_cart_full$cptable[min_idx, &quot;xstd&quot;], eval = case_when(rownum == min_idx ~ &quot;min xerror&quot;, xerror &lt; xerror_cap ~ &quot;under cap&quot;, TRUE ~ &quot;&quot;) ) %&gt;% select(-rownum, -min_idx) ## CP nsplit rel.error xerror xstd xerror_cap eval ## 1 0.47904192 0 1.0000000 1.0000000 0.04274518 0.4978082 ## 2 0.03293413 1 0.5209581 0.5419162 0.03577468 0.4978082 ## 3 0.01347305 3 0.4550898 0.4700599 0.03390486 0.4978082 under cap ## 4 0.01000000 5 0.4281437 0.4640719 0.03373631 0.4978082 min xerror The simplest tree using the 1-SE rule is $c_p = 0.01347305, CV error = 0.1832). Fortunately, plotcp() presents a nice graphical representation of the relationship between xerror and cp. plotcp(oj_mdl_cart_full, upper = &quot;splits&quot;) The dashed line is set at the minimum xerror + xstd. The top axis shows the number of splits in the tree. I’m not sure why the CP values are not the same as in the table (they are close, but not the same). The figure suggests I should prune to 5 or 3 splits. I see this curve never really hits a minimum - it is still decreasing at 5 splits. The default tuning parameter value cp = 0.01 may be too large, so I’ll set it to cp = 0.001 and start over. set.seed(123) oj_mdl_cart_full &lt;- rpart( formula = Purchase ~ ., data = oj_train, method = &quot;class&quot;, cp = 0.001 ) print(oj_mdl_cart_full) ## n= 857 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 857 334 CH (0.61026838 0.38973162) ## 2) LoyalCH&gt;=0.48285 537 94 CH (0.82495345 0.17504655) ## 4) LoyalCH&gt;=0.7648795 271 13 CH (0.95202952 0.04797048) * ## 5) LoyalCH&lt; 0.7648795 266 81 CH (0.69548872 0.30451128) ## 10) PriceDiff&gt;=-0.165 226 50 CH (0.77876106 0.22123894) ## 20) ListPriceDiff&gt;=0.255 115 11 CH (0.90434783 0.09565217) * ## 21) ListPriceDiff&lt; 0.255 111 39 CH (0.64864865 0.35135135) ## 42) PriceMM&gt;=2.155 19 2 CH (0.89473684 0.10526316) * ## 43) PriceMM&lt; 2.155 92 37 CH (0.59782609 0.40217391) ## 86) DiscCH&gt;=0.115 7 0 CH (1.00000000 0.00000000) * ## 87) DiscCH&lt; 0.115 85 37 CH (0.56470588 0.43529412) ## 174) ListPriceDiff&gt;=0.215 45 15 CH (0.66666667 0.33333333) * ## 175) ListPriceDiff&lt; 0.215 40 18 MM (0.45000000 0.55000000) ## 350) LoyalCH&gt;=0.527571 28 13 CH (0.53571429 0.46428571) ## 700) WeekofPurchase&lt; 266.5 21 8 CH (0.61904762 0.38095238) * ## 701) WeekofPurchase&gt;=266.5 7 2 MM (0.28571429 0.71428571) * ## 351) LoyalCH&lt; 0.527571 12 3 MM (0.25000000 0.75000000) * ## 11) PriceDiff&lt; -0.165 40 9 MM (0.22500000 0.77500000) * ## 3) LoyalCH&lt; 0.48285 320 80 MM (0.25000000 0.75000000) ## 6) LoyalCH&gt;=0.2761415 146 58 MM (0.39726027 0.60273973) ## 12) SalePriceMM&gt;=2.04 71 31 CH (0.56338028 0.43661972) ## 24) LoyalCH&lt; 0.303104 7 0 CH (1.00000000 0.00000000) * ## 25) LoyalCH&gt;=0.303104 64 31 CH (0.51562500 0.48437500) ## 50) WeekofPurchase&gt;=246.5 52 22 CH (0.57692308 0.42307692) ## 100) PriceCH&lt; 1.94 35 11 CH (0.68571429 0.31428571) ## 200) StoreID&lt; 1.5 9 1 CH (0.88888889 0.11111111) * ## 201) StoreID&gt;=1.5 26 10 CH (0.61538462 0.38461538) ## 402) LoyalCH&lt; 0.410969 17 4 CH (0.76470588 0.23529412) * ## 403) LoyalCH&gt;=0.410969 9 3 MM (0.33333333 0.66666667) * ## 101) PriceCH&gt;=1.94 17 6 MM (0.35294118 0.64705882) * ## 51) WeekofPurchase&lt; 246.5 12 3 MM (0.25000000 0.75000000) * ## 13) SalePriceMM&lt; 2.04 75 18 MM (0.24000000 0.76000000) ## 26) SpecialCH&gt;=0.5 14 6 CH (0.57142857 0.42857143) * ## 27) SpecialCH&lt; 0.5 61 10 MM (0.16393443 0.83606557) * ## 7) LoyalCH&lt; 0.2761415 174 22 MM (0.12643678 0.87356322) ## 14) LoyalCH&gt;=0.035047 117 21 MM (0.17948718 0.82051282) ## 28) WeekofPurchase&lt; 273.5 104 21 MM (0.20192308 0.79807692) ## 56) PriceCH&gt;=1.875 20 9 MM (0.45000000 0.55000000) ## 112) WeekofPurchase&gt;=252.5 12 5 CH (0.58333333 0.41666667) * ## 113) WeekofPurchase&lt; 252.5 8 2 MM (0.25000000 0.75000000) * ## 57) PriceCH&lt; 1.875 84 12 MM (0.14285714 0.85714286) * ## 29) WeekofPurchase&gt;=273.5 13 0 MM (0.00000000 1.00000000) * ## 15) LoyalCH&lt; 0.035047 57 1 MM (0.01754386 0.98245614) * This is a much larger tree. Did I find a cp value that produces a local min? plotcp(oj_mdl_cart_full, upper = &quot;splits&quot;) Yes, the min is at CP = 0.011 with 5 splits. The min + 1 SE is at CP = 0.021 with 3 splits. I’ll prune the tree to 3 splits. oj_mdl_cart &lt;- prune( oj_mdl_cart_full, cp = oj_mdl_cart_full$cptable[oj_mdl_cart_full$cptable[, 2] == 3, &quot;CP&quot;] ) rpart.plot(oj_mdl_cart, yesno = TRUE) The most “important” indicator of Purchase appears to be LoyalCH. From the rpart vignette (page 12), “An overall measure of variable importance is the sum of the goodness of split measures for each split for which it was the primary variable, plus goodness (adjusted agreement) for all splits in which it was a surrogate.” Surrogates refer to alternative features for a node to handle missing data. For each split, CART evaluates a variety of alternative “surrogate” splits to use when the feature value for the primary split is NA. Surrogate splits are splits that produce results similar to the original split. A variable’s importance is the sum of the improvement in the overall Gini (or RMSE) measure produced by the nodes in which it appears. Here is the variable importance for this model. oj_mdl_cart$variable.importance %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;Feature&quot;) %&gt;% rename(Overall = &#39;.&#39;) %&gt;% ggplot(aes(x = fct_reorder(Feature, Overall), y = Overall)) + geom_pointrange(aes(ymin = 0, ymax = Overall), color = &quot;cadetblue&quot;, size = .3) + theme_minimal() + coord_flip() + labs(x = &quot;&quot;, y = &quot;&quot;, title = &quot;Variable Importance with Simple Classication&quot;) LoyalCH is by far the most important variable, as expected from its position at the top of the tree, and one level down. You can see how the surrogates appear in the model with the summary() function. summary(oj_mdl_cart) ## Call: ## rpart(formula = Purchase ~ ., data = oj_train, method = &quot;class&quot;, ## cp = 0.001) ## n= 857 ## ## CP nsplit rel error xerror xstd ## 1 0.47904192 0 1.0000000 1.0000000 0.04274518 ## 2 0.03293413 1 0.5209581 0.5419162 0.03577468 ## 3 0.01347305 3 0.4550898 0.4700599 0.03390486 ## ## Variable importance ## LoyalCH PriceDiff SalePriceMM StoreID WeekofPurchase ## 67 9 5 4 4 ## DiscMM PriceMM PctDiscMM PriceCH ## 3 3 3 1 ## ## Node number 1: 857 observations, complexity param=0.4790419 ## predicted class=CH expected loss=0.3897316 P(node) =1 ## class counts: 523 334 ## probabilities: 0.610 0.390 ## left son=2 (537 obs) right son=3 (320 obs) ## Primary splits: ## LoyalCH &lt; 0.48285 to the right, improve=132.56800, (0 missing) ## StoreID &lt; 3.5 to the right, improve= 40.12097, (0 missing) ## PriceDiff &lt; 0.015 to the right, improve= 24.26552, (0 missing) ## ListPriceDiff &lt; 0.255 to the right, improve= 22.79117, (0 missing) ## SalePriceMM &lt; 1.84 to the right, improve= 20.16447, (0 missing) ## Surrogate splits: ## StoreID &lt; 3.5 to the right, agree=0.646, adj=0.053, (0 split) ## PriceMM &lt; 1.89 to the right, agree=0.638, adj=0.031, (0 split) ## WeekofPurchase &lt; 229.5 to the right, agree=0.632, adj=0.016, (0 split) ## DiscMM &lt; 0.77 to the left, agree=0.629, adj=0.006, (0 split) ## SalePriceMM &lt; 1.385 to the right, agree=0.629, adj=0.006, (0 split) ## ## Node number 2: 537 observations, complexity param=0.03293413 ## predicted class=CH expected loss=0.1750466 P(node) =0.6266044 ## class counts: 443 94 ## probabilities: 0.825 0.175 ## left son=4 (271 obs) right son=5 (266 obs) ## Primary splits: ## LoyalCH &lt; 0.7648795 to the right, improve=17.669310, (0 missing) ## PriceDiff &lt; 0.015 to the right, improve=15.475200, (0 missing) ## SalePriceMM &lt; 1.84 to the right, improve=13.951730, (0 missing) ## ListPriceDiff &lt; 0.255 to the right, improve=11.407560, (0 missing) ## DiscMM &lt; 0.15 to the left, improve= 7.795122, (0 missing) ## Surrogate splits: ## WeekofPurchase &lt; 257.5 to the right, agree=0.594, adj=0.180, (0 split) ## PriceCH &lt; 1.775 to the right, agree=0.590, adj=0.173, (0 split) ## StoreID &lt; 3.5 to the right, agree=0.587, adj=0.165, (0 split) ## PriceMM &lt; 2.04 to the right, agree=0.587, adj=0.165, (0 split) ## SalePriceMM &lt; 2.04 to the right, agree=0.587, adj=0.165, (0 split) ## ## Node number 3: 320 observations ## predicted class=MM expected loss=0.25 P(node) =0.3733956 ## class counts: 80 240 ## probabilities: 0.250 0.750 ## ## Node number 4: 271 observations ## predicted class=CH expected loss=0.04797048 P(node) =0.3162194 ## class counts: 258 13 ## probabilities: 0.952 0.048 ## ## Node number 5: 266 observations, complexity param=0.03293413 ## predicted class=CH expected loss=0.3045113 P(node) =0.3103851 ## class counts: 185 81 ## probabilities: 0.695 0.305 ## left son=10 (226 obs) right son=11 (40 obs) ## Primary splits: ## PriceDiff &lt; -0.165 to the right, improve=20.84307, (0 missing) ## ListPriceDiff &lt; 0.235 to the right, improve=20.82404, (0 missing) ## SalePriceMM &lt; 1.84 to the right, improve=16.80587, (0 missing) ## DiscMM &lt; 0.15 to the left, improve=10.05120, (0 missing) ## PctDiscMM &lt; 0.0729725 to the left, improve=10.05120, (0 missing) ## Surrogate splits: ## SalePriceMM &lt; 1.585 to the right, agree=0.906, adj=0.375, (0 split) ## DiscMM &lt; 0.57 to the left, agree=0.895, adj=0.300, (0 split) ## PctDiscMM &lt; 0.264375 to the left, agree=0.895, adj=0.300, (0 split) ## WeekofPurchase &lt; 274.5 to the left, agree=0.872, adj=0.150, (0 split) ## SalePriceCH &lt; 2.075 to the left, agree=0.857, adj=0.050, (0 split) ## ## Node number 10: 226 observations ## predicted class=CH expected loss=0.2212389 P(node) =0.2637106 ## class counts: 176 50 ## probabilities: 0.779 0.221 ## ## Node number 11: 40 observations ## predicted class=MM expected loss=0.225 P(node) =0.04667445 ## class counts: 9 31 ## probabilities: 0.225 0.775 I’ll evaluate the predictions and record the accuracy (correct classification percentage) for comparison to other models. Two ways to evaluate the model are the confusion matrix, and the ROC curve. 9.1.1 Holdout Performance 9.1.1.1 Confusion Matrix Print the confusion matrix with caret::confusionMatrix() to see how well does this model performs against the holdout set. oj_preds_cart &lt;- bind_cols( predict(oj_mdl_cart, newdata = oj_test, type = &quot;prob&quot;), predicted = predict(oj_mdl_cart, newdata = oj_test, type = &quot;class&quot;), actual = oj_test$Purchase ) oj_cm_cart &lt;- confusionMatrix(oj_preds_cart$predicted, reference = oj_preds_cart$actual) oj_cm_cart ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 113 13 ## MM 17 70 ## ## Accuracy : 0.8592 ## 95% CI : (0.8051, 0.9029) ## No Information Rate : 0.6103 ## P-Value [Acc &gt; NIR] : 1.265e-15 ## ## Kappa : 0.7064 ## ## Mcnemar&#39;s Test P-Value : 0.5839 ## ## Sensitivity : 0.8692 ## Specificity : 0.8434 ## Pos Pred Value : 0.8968 ## Neg Pred Value : 0.8046 ## Prevalence : 0.6103 ## Detection Rate : 0.5305 ## Detection Prevalence : 0.5915 ## Balanced Accuracy : 0.8563 ## ## &#39;Positive&#39; Class : CH ## The confusion matrix is at the top. It also includes a lot of statistics. It’s worth getting familiar with the stats. The model accuracy and 95% CI are calculated from the binomial test. binom.test(x = 113 + 70, n = 213) ## ## Exact binomial test ## ## data: 113 + 70 and 213 ## number of successes = 183, number of trials = 213, p-value &lt; 2.2e-16 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.8050785 0.9029123 ## sample estimates: ## probability of success ## 0.8591549 The “No Information Rate” (NIR) statistic is the class rate for the largest class. In this case CH is the largest class, so NIR = 130/213 = 0.6103. “P-Value [Acc &gt; NIR]” is the binomial test that the model accuracy is significantly better than the NIR (i.e., significantly better than just always guessing CH). binom.test(x = 113 + 70, n = 213, p = 130/213, alternative = &quot;greater&quot;) ## ## Exact binomial test ## ## data: 113 + 70 and 213 ## number of successes = 183, number of trials = 213, p-value = 1.265e-15 ## alternative hypothesis: true probability of success is greater than 0.6103286 ## 95 percent confidence interval: ## 0.8138446 1.0000000 ## sample estimates: ## probability of success ## 0.8591549 The “Accuracy” statistic indicates the model predicts 0.8590 of the observations correctly. That’s good, but less impressive when you consider the prevalence of CH is 0.6103 - you could achieve 61% accuracy just by predicting CH every time. A measure that controls for the prevalence is Cohen’s kappa statistic. The kappa statistic is explained here. It compares the accuracy to the accuracy of a “random system”. It is defined as \\[\\kappa = \\frac{Acc - RA}{1-RA}\\] where \\[RA = \\frac{ActFalse \\times PredFalse + ActTrue \\times PredTrue}{Total \\times Total}\\] is the hypothetical probability of a chance agreement. ActFalse will be the number of “MM” (13 + 70 = 83) and actual true will be the number of “CH” (113 + 17 = 130). The predicted counts are table(oj_preds_cart$predicted) ## ## CH MM ## 126 87 So, \\(RA = (83*87 + 130*126) / 213^2 = 0.5202\\) and \\(\\kappa = (0.8592 - 0.5202)/(1 - 0.5202) = 0.7064\\). The kappa statistic varies from 0 to 1 where 0 means accurate predictions occur merely by chance, and 1 means the predictions are in perfect agreement with the observations. In this case, a kappa statistic of 0.7064 is “substantial”. See chart here. The other measures from the confusionMatrix() output are various proportions and you can remind yourself of their definitions in the documentation with ?confusionMatrix. Visuals are almost always helpful. Here is a plot of the confusion matrix. plot(oj_preds_cart$actual, oj_preds_cart$predicted, main = &quot;Simple Classification: Predicted vs. Actual&quot;, xlab = &quot;Actual&quot;, ylab = &quot;Predicted&quot;) 9.1.1.2 ROC Curve The ROC (receiver operating characteristics) curve (Fawcett 2005) is another measure of accuracy. The ROC curve is a plot of the true positive rate (TPR, sensitivity) versus the false positive rate (FPR, 1 - specificity) for a set of thresholds. By default, the threshold for predicting the default classification is 0.50, but it could be any threshold. precrec::evalmod() calculates the confusion matrix values from the model using the holdout data set. The AUC on the holdout set is 0.8848. pRoc::plot.roc(), plotROC::geom_roc(), and yardstick::roc_curve() are all options for plotting a ROC curve. mdl_auc &lt;- Metrics::auc(actual = oj_preds_cart$actual == &quot;CH&quot;, oj_preds_cart$CH) yardstick::roc_curve(oj_preds_cart, actual, CH) %&gt;% autoplot() + labs( title = &quot;OJ CART ROC Curve&quot;, subtitle = paste0(&quot;AUC = &quot;, round(mdl_auc, 4)) ) A few points on the ROC space are helpful for understanding how to use it. The lower left point (0, 0) is the result of always predicting “negative” or in this case “MM” if “CH” is taken as the default class. No false positives, but no true positives either. The upper right point (1, 1) is the result of always predicting “positive” (“CH” here). You catch all true positives, but miss all the true negatives. The upper left point (0, 1) is the result of perfect accuracy. The lower right point (1, 0) is the result of perfect imbecility. You made the exact wrong prediction every time. The 45 degree diagonal is the result of randomly guessing positive (CH) X percent of the time. If you guess positive 90% of the time and the prevalence is 50%, your TPR will be 90% and your FPR will also be 90%, etc. The goal is for all nodes to bunch up in the upper left. Points to the left of the diagonal with a low TPR can be thought of as “conservative” predictors - they only make positive (CH) predictions with strong evidence. Points to the left of the diagonal with a high TPR can be thought of as “liberal” predictors - they make positive (CH) predictions with weak evidence. 9.1.1.3 Gain Curve The gain curve plots the cumulative summed true outcome versus the fraction of items seen when sorted by the predicted value. The “wizard” curve is the gain curve when the data is sorted by the true outcome. If the model’s gain curve is close to the wizard curve, then the model predicted the response variable well. The gray area is the “gain” over a random prediction. 130 of the 213 consumers in the holdout set purchased CH. The gain curve encountered 77 CH purchasers (59%) within the first 79 observations (37%). It encountered all 130 CH purchasers on the 213th observation (100%). The bottom of the gray area is the outcome of a random model. Only half the CH purchasers would be observed within 50% of the observations. The top of the gray area is the outcome of the perfect model, the “wizard curve”. Half the CH purchasers would be observed in 65/213=31% of the observations. yardstick::gain_curve(oj_preds_cart, actual, CH) %&gt;% autoplot() + labs( title = &quot;OJ CART Gain Curve&quot; ) 9.1.2 Caret Approach I can also fit the model with caret::train(). There are two ways to tune hyperparameters in train(): set the number of tuning parameter values to consider by setting tuneLength, or set particular values to consider for each parameter by defining a tuneGrid. I’ll build the model using 10-fold cross-validation to optimize the hyperparameter CP. If you have no idea what is the optimal tuning parameter, start with tuneLength to get close, then fine-tune with tuneGrid. That’s what I’ll do. I’ll create a training control object that I can re-use in other model builds. oj_trControl = trainControl( method = &quot;cv&quot;, number = 10, savePredictions = &quot;final&quot;, # save predictions for the optimal tuning parameter classProbs = TRUE # return class probabilities in addition to predicted values ) Now fit the model. set.seed(1234) oj_mdl_cart2 &lt;- train( Purchase ~ ., data = oj_train, method = &quot;rpart&quot;, tuneLength = 5, metric = &quot;Accuracy&quot;, trControl = oj_trControl ) caret built a full tree using rpart’s default parameters: gini splitting index, at least 20 observations in a node in order to consider splitting it, and at least 6 observations in each node. Caret then calculated the accuracy for each candidate value of \\(\\alpha\\). Here is the results. print(oj_mdl_cart2) ## CART ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... ## Resampling results across tuning parameters: ## ## cp Accuracy Kappa ## 0.005988024 0.8085999 0.5931149 ## 0.008982036 0.8086267 0.5943277 ## 0.013473054 0.8051657 0.5885521 ## 0.032934132 0.7841798 0.5371171 ## 0.479041916 0.6603904 0.1774773 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.008982036. The second cp (0.008982036) produced the highest accuracy. I can drill into the best value of cp using a tuning grid. set.seed(1234) oj_mdl_cart2 &lt;- train( Purchase ~ ., data = oj_train, method = &quot;rpart&quot;, tuneGrid = expand.grid(cp = seq(from = 0.001, to = 0.010, length = 11)), metric=&#39;Accuracy&#39;, trControl = oj_trControl ) print(oj_mdl_cart2) ## CART ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... ## Resampling results across tuning parameters: ## ## cp Accuracy Kappa ## 0.0010 0.8004874 0.5753480 ## 0.0019 0.8016502 0.5785232 ## 0.0028 0.8039758 0.5845653 ## 0.0037 0.8085999 0.5955198 ## 0.0046 0.8039351 0.5851273 ## 0.0055 0.8085863 0.5937949 ## 0.0064 0.8085999 0.5931149 ## 0.0073 0.8120883 0.6011446 ## 0.0082 0.8120883 0.6011446 ## 0.0091 0.8086267 0.5943277 ## 0.0100 0.8086540 0.5953150 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.0082. The best model is at cp = 0.0082. Here are the cross-validated accuracies for the candidate cp values. plot(oj_mdl_cart2) Here are the rules in the final model. oj_mdl_cart2$finalModel ## n= 857 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 857 334 CH (0.61026838 0.38973162) ## 2) LoyalCH&gt;=0.48285 537 94 CH (0.82495345 0.17504655) ## 4) LoyalCH&gt;=0.7648795 271 13 CH (0.95202952 0.04797048) * ## 5) LoyalCH&lt; 0.7648795 266 81 CH (0.69548872 0.30451128) ## 10) PriceDiff&gt;=-0.165 226 50 CH (0.77876106 0.22123894) * ## 11) PriceDiff&lt; -0.165 40 9 MM (0.22500000 0.77500000) * ## 3) LoyalCH&lt; 0.48285 320 80 MM (0.25000000 0.75000000) ## 6) LoyalCH&gt;=0.2761415 146 58 MM (0.39726027 0.60273973) ## 12) SalePriceMM&gt;=2.04 71 31 CH (0.56338028 0.43661972) ## 24) LoyalCH&lt; 0.303104 7 0 CH (1.00000000 0.00000000) * ## 25) LoyalCH&gt;=0.303104 64 31 CH (0.51562500 0.48437500) ## 50) WeekofPurchase&gt;=246.5 52 22 CH (0.57692308 0.42307692) ## 100) PriceCH&lt; 1.94 35 11 CH (0.68571429 0.31428571) * ## 101) PriceCH&gt;=1.94 17 6 MM (0.35294118 0.64705882) * ## 51) WeekofPurchase&lt; 246.5 12 3 MM (0.25000000 0.75000000) * ## 13) SalePriceMM&lt; 2.04 75 18 MM (0.24000000 0.76000000) * ## 7) LoyalCH&lt; 0.2761415 174 22 MM (0.12643678 0.87356322) * rpart.plot(oj_mdl_cart2$finalModel) Let’s look at the performance on the holdout data set. oj_preds_cart2 &lt;- bind_cols( predict(oj_mdl_cart2, newdata = oj_test, type = &quot;prob&quot;), Predicted = predict(oj_mdl_cart2, newdata = oj_test, type = &quot;raw&quot;), Actual = oj_test$Purchase ) oj_cm_cart2 &lt;- confusionMatrix(oj_preds_cart2$Predicted, oj_preds_cart2$Actual) oj_cm_cart2 ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 115 18 ## MM 15 65 ## ## Accuracy : 0.8451 ## 95% CI : (0.7894, 0.8909) ## No Information Rate : 0.6103 ## P-Value [Acc &gt; NIR] : 6.311e-14 ## ## Kappa : 0.6721 ## ## Mcnemar&#39;s Test P-Value : 0.7277 ## ## Sensitivity : 0.8846 ## Specificity : 0.7831 ## Pos Pred Value : 0.8647 ## Neg Pred Value : 0.8125 ## Prevalence : 0.6103 ## Detection Rate : 0.5399 ## Detection Prevalence : 0.6244 ## Balanced Accuracy : 0.8339 ## ## &#39;Positive&#39; Class : CH ## The accuracy is 0.8451 - a little worse than the 0.8592 from the direct method. The AUC is 0.9102. mdl_auc &lt;- Metrics::auc(actual = oj_preds_cart2$Actual == &quot;CH&quot;, oj_preds_cart2$CH) yardstick::roc_curve(oj_preds_cart2, Actual, CH) %&gt;% autoplot() + labs( title = &quot;OJ CART ROC Curve (caret)&quot;, subtitle = paste0(&quot;AUC = &quot;, round(mdl_auc, 4)) ) yardstick::gain_curve(oj_preds_cart2, Actual, CH) %&gt;% autoplot() + labs(title = &quot;OJ CART Gain Curve (caret)&quot;) Finally, here is the variable importance plot. Brand loyalty is most important, followed by price difference. plot(varImp(oj_mdl_cart2), main=&quot;Variable Importance with CART (caret)&quot;) Looks like the manual effort fared best. Here is a summary the accuracy rates of the two models. scoreboard &lt;- rbind( data.frame(model = &quot;Manual Class&quot;, Acc = round(oj_cm_cart$overall[&quot;Accuracy&quot;], 5)), data.frame(model = &quot;Caret w/tuneGrid&quot;, Acc = round(oj_cm_cart2$overall[&quot;Accuracy&quot;], 5)) ) scoreboard ## model Acc ## Accuracy Manual Class 0.85915 ## Accuracy1 Caret w/tuneGrid 0.84507 References "],
["regression-tree.html", "9.2 Regression Tree", " 9.2 Regression Tree A simple regression tree is built in a manner similar to a simple classification tree, and like the simple classification tree, it is rarely invoked on its own; the bagged, random forest, and gradient boosting methods build on this logic. I’ll learn by example again. Using the ISLR::Carseats data set, and predict Sales using from the 10 feature variables. cs_dat &lt;- ISLR::Carseats skimr::skim(cs_dat) Table 9.2: Data summary Name cs_dat Number of rows 400 Number of columns 11 _______________________ Column type frequency: factor 3 numeric 8 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts ShelveLoc 0 1 FALSE 3 Med: 219, Bad: 96, Goo: 85 Urban 0 1 FALSE 2 Yes: 282, No: 118 US 0 1 FALSE 2 Yes: 258, No: 142 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist Sales 0 1 7.50 2.82 0 5.39 7.49 9.32 16.27 ▁▆▇▃▁ CompPrice 0 1 124.97 15.33 77 115.00 125.00 135.00 175.00 ▁▅▇▃▁ Income 0 1 68.66 27.99 21 42.75 69.00 91.00 120.00 ▇▆▇▆▅ Advertising 0 1 6.64 6.65 0 0.00 5.00 12.00 29.00 ▇▃▃▁▁ Population 0 1 264.84 147.38 10 139.00 272.00 398.50 509.00 ▇▇▇▇▇ Price 0 1 115.80 23.68 24 100.00 117.00 131.00 191.00 ▁▂▇▆▁ Age 0 1 53.32 16.20 25 39.75 54.50 66.00 80.00 ▇▆▇▇▇ Education 0 1 13.90 2.62 10 12.00 14.00 16.00 18.00 ▇▇▃▇▇ Split careseats_dat (n = 400) into cs_train (80%, n = 321) and cs_test (20%, n = 79). set.seed(12345) partition &lt;- createDataPartition(y = cs_dat$Sales, p = 0.8, list = FALSE) cs_train &lt;- cs_dat[partition, ] cs_test &lt;- cs_dat[-partition, ] The first step is to build a full tree, then perform k-fold cross-validation to help select the optimal cost complexity (cp). The only difference here is the rpart() parameter method = \"anova\" to produce a regression tree. set.seed(1234) cs_mdl_cart_full &lt;- rpart(Sales ~ ., cs_train, method = &quot;anova&quot;) print(cs_mdl_cart_full) ## n= 321 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 321 2567.76800 7.535950 ## 2) ShelveLoc=Bad,Medium 251 1474.14100 6.770359 ## 4) Price&gt;=105.5 168 719.70630 5.987024 ## 8) ShelveLoc=Bad 50 165.70160 4.693600 ## 16) Population&lt; 201.5 20 48.35505 3.646500 * ## 17) Population&gt;=201.5 30 80.79922 5.391667 * ## 9) ShelveLoc=Medium 118 434.91370 6.535085 ## 18) Advertising&lt; 11.5 88 290.05490 6.113068 ## 36) CompPrice&lt; 142 69 193.86340 5.769420 ## 72) Price&gt;=132.5 16 50.75440 4.455000 * ## 73) Price&lt; 132.5 53 107.12060 6.166226 * ## 37) CompPrice&gt;=142 19 58.45118 7.361053 * ## 19) Advertising&gt;=11.5 30 83.21323 7.773000 * ## 5) Price&lt; 105.5 83 442.68920 8.355904 ## 10) Age&gt;=63.5 32 153.42300 6.922500 ## 20) Price&gt;=85 25 66.89398 6.160800 ## 40) ShelveLoc=Bad 9 18.39396 4.772222 * ## 41) ShelveLoc=Medium 16 21.38544 6.941875 * ## 21) Price&lt; 85 7 20.22194 9.642857 * ## 11) Age&lt; 63.5 51 182.26350 9.255294 ## 22) Income&lt; 57.5 12 28.03042 7.707500 * ## 23) Income&gt;=57.5 39 116.63950 9.731538 ## 46) Age&gt;=50.5 14 21.32597 8.451429 * ## 47) Age&lt; 50.5 25 59.52474 10.448400 * ## 3) ShelveLoc=Good 70 418.98290 10.281140 ## 6) Price&gt;=107.5 49 242.58730 9.441633 ## 12) Advertising&lt; 13.5 41 162.47820 8.926098 ## 24) Age&gt;=61 17 53.37051 7.757647 * ## 25) Age&lt; 61 24 69.45776 9.753750 * ## 13) Advertising&gt;=13.5 8 13.36599 12.083750 * ## 7) Price&lt; 107.5 21 61.28200 12.240000 * The predicted Sales at the root is the mean Sales for the training data set, 7.535950 (values are $000s). The deviance at the root is the SSE, 2567.768. The first split is at ShelveLoc = [Bad, Medium] vs Good. Here is the unpruned tree diagram. rpart.plot(cs_mdl_cart_full, yesno = TRUE) The boxes show the node predicted value (mean) and the proportion of observations that are in the node (or child nodes). rpart() grew the full tree, and used cross-validation to test the performance of the possible complexity hyperparameters. printcp() displays the candidate cp values. You can use this table to decide how to prune the tree. printcp(cs_mdl_cart_full) ## ## Regression tree: ## rpart(formula = Sales ~ ., data = cs_train, method = &quot;anova&quot;) ## ## Variables actually used in tree construction: ## [1] Advertising Age CompPrice Income Population Price ## [7] ShelveLoc ## ## Root node error: 2567.8/321 = 7.9993 ## ## n= 321 ## ## CP nsplit rel error xerror xstd ## 1 0.262736 0 1.00000 1.00635 0.076664 ## 2 0.121407 1 0.73726 0.74888 0.058981 ## 3 0.046379 2 0.61586 0.65278 0.050839 ## 4 0.044830 3 0.56948 0.67245 0.051638 ## 5 0.041671 4 0.52465 0.66230 0.051065 ## 6 0.025993 5 0.48298 0.62345 0.049368 ## 7 0.025823 6 0.45698 0.61980 0.048026 ## 8 0.024007 7 0.43116 0.62058 0.048213 ## 9 0.015441 8 0.40715 0.58061 0.041738 ## 10 0.014698 9 0.39171 0.56413 0.041368 ## 11 0.014641 10 0.37701 0.56277 0.041271 ## 12 0.014233 11 0.36237 0.56081 0.041097 ## 13 0.014015 12 0.34814 0.55647 0.038308 ## 14 0.013938 13 0.33413 0.55647 0.038308 ## 15 0.010560 14 0.32019 0.57110 0.038872 ## 16 0.010000 15 0.30963 0.56676 0.038090 There were 16 possible cp values in this model. The model with the smallest complexity parameter allows the most splits (nsplit). The highest complexity parameter corresponds to a tree with just a root node. rel error is the SSE relative to the root node. The root node SSE is 2567.76800, so its rel error is 2567.76800/2567.76800 = 1.0. That means the absolute error of the full tree (at CP = 0.01) is 0.30963 * 2567.76800 = 795.058. You can verify that by calculating the SSE of the model predicted values: data.frame(pred = predict(cs_mdl_cart_full, newdata = cs_train)) %&gt;% mutate(obs = cs_train$Sales, sq_err = (obs - pred)^2) %&gt;% summarize(sse = sum(sq_err)) ## sse ## 1 795.0525 Finishing the CP table tour, xerror is the cross-validated SSE and xstd is its standard error. If you want the lowest possible error, then prune to the tree with the smallest relative SSE (xerror). If you want to balance predictive power with simplicity, prune to the smallest tree within 1 SE of the one with the smallest relative SSE. The CP table is not super-helpful for finding that tree. I’ll add a column to find it. cs_mdl_cart_full$cptable %&gt;% data.frame() %&gt;% mutate(min_xerror_idx = which.min(cs_mdl_cart_full$cptable[, &quot;xerror&quot;]), rownum = row_number(), xerror_cap = cs_mdl_cart_full$cptable[min_xerror_idx, &quot;xerror&quot;] + cs_mdl_cart_full$cptable[min_xerror_idx, &quot;xstd&quot;], eval = case_when(rownum == min_xerror_idx ~ &quot;min xerror&quot;, xerror &lt; xerror_cap ~ &quot;under cap&quot;, TRUE ~ &quot;&quot;)) %&gt;% select(-rownum, -min_xerror_idx) ## CP nsplit rel.error xerror xstd xerror_cap eval ## 1 0.26273578 0 1.0000000 1.0063530 0.07666355 0.5947744 ## 2 0.12140705 1 0.7372642 0.7488767 0.05898146 0.5947744 ## 3 0.04637919 2 0.6158572 0.6527823 0.05083938 0.5947744 ## 4 0.04483023 3 0.5694780 0.6724529 0.05163819 0.5947744 ## 5 0.04167149 4 0.5246478 0.6623028 0.05106530 0.5947744 ## 6 0.02599265 5 0.4829763 0.6234457 0.04936799 0.5947744 ## 7 0.02582284 6 0.4569836 0.6198034 0.04802643 0.5947744 ## 8 0.02400748 7 0.4311608 0.6205756 0.04821332 0.5947744 ## 9 0.01544139 8 0.4071533 0.5806072 0.04173785 0.5947744 under cap ## 10 0.01469771 9 0.3917119 0.5641331 0.04136793 0.5947744 under cap ## 11 0.01464055 10 0.3770142 0.5627713 0.04127139 0.5947744 under cap ## 12 0.01423309 11 0.3623736 0.5608073 0.04109662 0.5947744 under cap ## 13 0.01401541 12 0.3481405 0.5564663 0.03830810 0.5947744 min xerror ## 14 0.01393771 13 0.3341251 0.5564663 0.03830810 0.5947744 under cap ## 15 0.01055959 14 0.3201874 0.5710951 0.03887227 0.5947744 under cap ## 16 0.01000000 15 0.3096278 0.5667561 0.03808991 0.5947744 under cap Okay, so the simplest tree is the one with CP = 0.02599265 (5 splits). Fortunately, plotcp() presents a nice graphical representation of the relationship between xerror and cp. plotcp(cs_mdl_cart_full, upper = &quot;splits&quot;) The dashed line is set at the minimum xerror + xstd. The top axis shows the number of splits in the tree. I’m not sure why the CP values are not the same as in the table (they are close, but not the same). The smallest relative error is at CP = 0.01000000 (15 splits), but the maximum CP below the dashed line (one standard deviation above the minimum error) is at CP = 0.02599265 (5 splits). Use the prune() function to prune the tree by specifying the associated cost-complexity cp. cs_mdl_cart &lt;- prune( cs_mdl_cart_full, cp = cs_mdl_cart_full$cptable[cs_mdl_cart_full$cptable[, 2] == 5, &quot;CP&quot;] ) rpart.plot(cs_mdl_cart, yesno = TRUE) The most “important” indicator of Sales is ShelveLoc. Here are the importance values from the model. cs_mdl_cart$variable.importance %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;Feature&quot;) %&gt;% rename(Overall = &#39;.&#39;) %&gt;% ggplot(aes(x = fct_reorder(Feature, Overall), y = Overall)) + geom_pointrange(aes(ymin = 0, ymax = Overall), color = &quot;cadetblue&quot;, size = .3) + theme_minimal() + coord_flip() + labs(x = &quot;&quot;, y = &quot;&quot;, title = &quot;Variable Importance with Simple Regression&quot;) The most important indicator of Sales is ShelveLoc, then Price, then Age, all of which appear in the final model. CompPrice was also important. The last step is to make predictions on the validation data set. The root mean squared error (\\(RMSE = \\sqrt{(1/2) \\sum{(actual - pred)^2}})\\) and mean absolute error (\\(MAE = (1/n) \\sum{|actual - pred|}\\)) are the two most common measures of predictive accuracy. The key difference is that RMSE punishes large errors more harshly. For a regression tree, set argument type = \"vector\" (or do not specify at all). cs_preds_cart &lt;- predict(cs_mdl_cart, cs_test, type = &quot;vector&quot;) cs_rmse_cart &lt;- RMSE( pred = cs_preds_cart, obs = cs_test$Sales ) cs_rmse_cart ## [1] 2.363202 The pruning process leads to an average prediction error of 2.363 in the test data set. Not too bad considering the standard deviation of Sales is 2.801. Here is a predicted vs actual plot. data.frame(Predicted = cs_preds_cart, Actual = cs_test$Sales) %&gt;% ggplot(aes(x = Actual, y = Predicted)) + geom_point(alpha = 0.6, color = &quot;cadetblue&quot;) + geom_smooth() + geom_abline(intercept = 0, slope = 1, linetype = 2) + labs(title = &quot;Carseats CART, Predicted vs Actual&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; The 6 possible predicted values do a decent job of binning the observations. 9.2.1 Caret Approach I can also fit the model with caret::train(), specifying method = \"rpart\". I’ll build the model using 10-fold cross-validation to optimize the hyperparameter CP. cs_trControl = trainControl( method = &quot;cv&quot;, number = 10, savePredictions = &quot;final&quot; # save predictions for the optimal tuning parameter ) I’ll let the model look for the best CP tuning parameter with tuneLength to get close, then fine-tune with tuneGrid. set.seed(1234) cs_mdl_cart2 = train( Sales ~ ., data = cs_train, method = &quot;rpart&quot;, tuneLength = 5, metric = &quot;RMSE&quot;, trControl = cs_trControl ) ## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : ## There were missing values in resampled performance measures. print(cs_mdl_cart2) ## CART ## ## 321 samples ## 10 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... ## Resampling results across tuning parameters: ## ## cp RMSE Rsquared MAE ## 0.04167149 2.209383 0.4065251 1.778797 ## 0.04483023 2.243618 0.3849728 1.805027 ## 0.04637919 2.275563 0.3684309 1.808814 ## 0.12140705 2.400455 0.2942663 1.936927 ## 0.26273578 2.692867 0.1898998 2.192774 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was cp = 0.04167149. The first cp (0.04167149) produced the smallest RMSE. I can drill into the best value of cp using a tuning grid. I’ll try that now. set.seed(1234) cs_mdl_cart2 = train( Sales ~ ., data = cs_train, method = &quot;rpart&quot;, tuneGrid = expand.grid(cp = seq(from = 0, to = 0.1, by = 0.01)), metric = &quot;RMSE&quot;, trControl = cs_trControl ) print(cs_mdl_cart2) ## CART ## ## 321 samples ## 10 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... ## Resampling results across tuning parameters: ## ## cp RMSE Rsquared MAE ## 0.00 2.055676 0.5027431 1.695453 ## 0.01 2.135096 0.4642577 1.745937 ## 0.02 2.095767 0.4733269 1.699235 ## 0.03 2.131246 0.4534544 1.690453 ## 0.04 2.146886 0.4411380 1.712705 ## 0.05 2.284937 0.3614130 1.837782 ## 0.06 2.265498 0.3709523 1.808319 ## 0.07 2.282630 0.3597216 1.836227 ## 0.08 2.282630 0.3597216 1.836227 ## 0.09 2.282630 0.3597216 1.836227 ## 0.10 2.282630 0.3597216 1.836227 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was cp = 0. It looks like the best performing tree is the unpruned one. plot(cs_mdl_cart2) Let’s see the final model. rpart.plot(cs_mdl_cart2$finalModel) What were the most important variables? plot(varImp(cs_mdl_cart2), main=&quot;Variable Importance with Simple Regression&quot;) Evaluate the model by making predictions with the test data set. cs_preds_cart2 &lt;- predict(cs_mdl_cart2, cs_test, type = &quot;raw&quot;) data.frame(Actual = cs_test$Sales, Predicted = cs_preds_cart2) %&gt;% ggplot(aes(x = Actual, y = Predicted)) + geom_point(alpha = 0.6, color = &quot;cadetblue&quot;) + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;) + geom_abline(intercept = 0, slope = 1, linetype = 2) + labs(title = &quot;Carseats CART, Predicted vs Actual (caret)&quot;) The model over-estimates at the low end and underestimates at the high end. Calculate the test data set RMSE. cs_rmse_cart2 &lt;- RMSE( pred = cs_preds_cart2, obs = cs_test$Sales ) cs_rmse_cart2 ## [1] 2.298331 Caret performed better in this model. Here is a summary the RMSE values of the two models. scoreboard_r &lt;- rbind( data.frame(model = &quot;Manual ANOVA&quot;, RMSE = round(cs_rmse_cart, 5)), data.frame(model = &quot;Caret w/tuneGrid&quot;, RMSE = round(cs_rmse_cart2, 5)) ) scoreboard_r ## model RMSE ## 1 Manual ANOVA 2.36320 ## 2 Caret w/tuneGrid 2.29833 "],
["bagging.html", "9.3 Bagging", " 9.3 Bagging Bootstrap aggregation, or bagging, is a general-purpose procedure for reducing the variance of a statistical learning method. The algorithm constructs B regression trees using B bootstrapped training sets, and averages the resulting predictions. These trees are grown deep, and are not pruned. Hence each individual tree has high variance, but low bias. Averaging these B trees reduces the variance. For classification trees, bagging takes the “majority vote” for the prediction. Use a value of B sufficiently large that the error has settled down. To test the model accuracy, the out-of-bag observations are predicted from the models. E.g., If B/3 of observations are in-bag, there are B/3 predictions per observation. These predictions are averaged for the test prediction. Again, for classification trees, a majority vote is taken. The downside to bagging is that it improves accuracy at the expense of interpretability. There is no longer a single tree to interpret, so it is unclear which variables are more important than others. Bagged trees are a special case of random forests, so see the next section for an example. "],
["random-forests.html", "9.4 Random Forests", " 9.4 Random Forests Random forests improve bagged trees by way of a small tweak that de-correlates the trees. As in bagging, the algorithm builds a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of mtry predictors is chosen as split candidates from the full set of p predictors. A fresh sample of mtry predictors is taken at each split. Typically \\(mtry \\sim \\sqrt{p}\\). Bagged trees are thus a special case of random forests where mtry = p. 9.4.0.1 Bagging Classification Example I’ll predict Purchase from the OJ data set again, this time using the bagging method by specifying method = \"treebag\". Caret has no hyperparameters to tune with this model, so I won’t set tuneLegth or tuneGrid. set.seed(1234) oj_mdl_bag &lt;- train( Purchase ~ ., data = oj_train, method = &quot;treebag&quot;, trControl = oj_trControl ) oj_mdl_bag ## Bagged CART ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... ## Resampling results: ## ## Accuracy Kappa ## 0.7945774 0.5653649 oj_preds_bag &lt;- bind_cols( predict(oj_mdl_bag, newdata = oj_test, type = &quot;prob&quot;), Predicted = predict(oj_mdl_bag, newdata = oj_test, type = &quot;raw&quot;), Actual = oj_test$Purchase ) oj_cm_bag &lt;- confusionMatrix(oj_preds_bag$Predicted, reference = oj_preds_bag$Actual) oj_cm_bag ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 113 16 ## MM 17 67 ## ## Accuracy : 0.8451 ## 95% CI : (0.7894, 0.8909) ## No Information Rate : 0.6103 ## P-Value [Acc &gt; NIR] : 6.311e-14 ## ## Kappa : 0.675 ## ## Mcnemar&#39;s Test P-Value : 1 ## ## Sensitivity : 0.8692 ## Specificity : 0.8072 ## Pos Pred Value : 0.8760 ## Neg Pred Value : 0.7976 ## Prevalence : 0.6103 ## Detection Rate : 0.5305 ## Detection Prevalence : 0.6056 ## Balanced Accuracy : 0.8382 ## ## &#39;Positive&#39; Class : CH ## mdl_auc &lt;- Metrics::auc(actual = oj_preds_bag$Actual == &quot;CH&quot;, oj_preds_bag$CH) yardstick::roc_curve(oj_preds_bag, Actual, CH) %&gt;% autoplot() + labs( title = &quot;OJ Bagging ROC Curve&quot;, subtitle = paste0(&quot;AUC = &quot;, round(mdl_auc, 4)) ) yardstick::gain_curve(oj_preds_bag, Actual, CH) %&gt;% autoplot() + labs(title = &quot;OJ Bagging Gain Curve&quot;) plot(varImp(oj_mdl_bag), main=&quot;Variable Importance with Bagging&quot;) scoreboard &lt;- rbind( scoreboard, data.frame(model = &quot;Bagging&quot;, Acc = round(oj_cm_bag$overall[&quot;Accuracy&quot;], 5)) ) scoreboard %&gt;% arrange(desc(Acc)) ## model Acc ## Accuracy Manual Class 0.85915 ## Accuracy1 Caret w/tuneGrid 0.84507 ## Accuracy2 Bagging 0.84507 9.4.0.2 Random Forest Classification Example Now I’ll try it with the random forest method by specifying method = \"rf\". Hyperparameter mtry can take any value from 1 to 17 (the number of predictors) and I expect the best value to be near \\(\\sqrt{17} \\sim 4\\). set.seed(1234) oj_mdl_rf &lt;- train( Purchase ~ ., data = oj_train, method = &quot;rf&quot;, tuneGrid = expand.grid(mtry = 1:7), # searching around mtry=4 trControl = oj_trControl ) oj_mdl_rf ## Random Forest ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... ## Resampling results across tuning parameters: ## ## mtry Accuracy Kappa ## 1 0.7608011 0.4678965 ## 2 0.8050845 0.5797842 ## 3 0.8086002 0.5909228 ## 4 0.8003653 0.5741523 ## 5 0.7945780 0.5629979 ## 6 0.7981482 0.5707518 ## 7 0.7969444 0.5698499 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 3. # The peak accuracy is at mtry = 3 plot(oj_mdl_rf) oj_preds_rf &lt;- bind_cols( predict(oj_mdl_rf, newdata = oj_test, type = &quot;prob&quot;), Predicted = predict(oj_mdl_rf, newdata = oj_test, type = &quot;raw&quot;), Actual = oj_test$Purchase ) # Accuracy on holdout is 0.8263 oj_cm_rf &lt;- confusionMatrix(oj_preds_rf$Predicted, reference = oj_preds_rf$Actual) oj_cm_rf ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 114 21 ## MM 16 62 ## ## Accuracy : 0.8263 ## 95% CI : (0.7686, 0.8746) ## No Information Rate : 0.6103 ## P-Value [Acc &gt; NIR] : 7.121e-12 ## ## Kappa : 0.6308 ## ## Mcnemar&#39;s Test P-Value : 0.5108 ## ## Sensitivity : 0.8769 ## Specificity : 0.7470 ## Pos Pred Value : 0.8444 ## Neg Pred Value : 0.7949 ## Prevalence : 0.6103 ## Detection Rate : 0.5352 ## Detection Prevalence : 0.6338 ## Balanced Accuracy : 0.8120 ## ## &#39;Positive&#39; Class : CH ## # AUC is 0.9190 mdl_auc &lt;- Metrics::auc(actual = oj_preds_rf$Actual == &quot;CH&quot;, oj_preds_rf$CH) yardstick::roc_curve(oj_preds_rf, Actual, CH) %&gt;% autoplot() + labs( title = &quot;OJ Random Forest ROC Curve&quot;, subtitle = paste0(&quot;AUC = &quot;, round(mdl_auc, 4)) ) yardstick::gain_curve(oj_preds_rf, Actual, CH) %&gt;% autoplot() + labs(title = &quot;OJ Random Forest Gain Curve&quot;) plot(varImp(oj_mdl_rf), main=&quot;Variable Importance with Random Forest&quot;) scoreboard &lt;- rbind( scoreboard, data.frame(model = &quot;Random Forest&quot;, Acc = round(oj_cm_rf$overall[&quot;Accuracy&quot;], 5)) ) scoreboard %&gt;% arrange(desc(Acc)) ## model Acc ## Accuracy Manual Class 0.85915 ## Accuracy1 Caret w/tuneGrid 0.84507 ## Accuracy2 Bagging 0.84507 ## Accuracy3 Random Forest 0.82629 The bagging and random forest models did pretty well, but the manual classification tree is still in first place. There’s still gradient boosting to investigate! 9.4.0.3 Bagging Regression Example I’ll predict Sales from the Carseats data set again, this time using the bagging method by specifying method = \"treebag\". set.seed(1234) cs_mdl_bag &lt;- train( Sales ~ ., data = cs_train, method = &quot;treebag&quot;, trControl = cs_trControl ) cs_mdl_bag ## Bagged CART ## ## 321 samples ## 10 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 1.681889 0.675239 1.343427 cs_preds_bag &lt;- bind_cols( Predicted = predict(cs_mdl_bag, newdata = cs_test), Actual = cs_test$Sales ) # Model over-predicts at low end of Sales and under-predicts at high end cs_preds_bag %&gt;% ggplot(aes(x = Actual, y = Predicted)) + geom_point(alpha = 0.6, color = &quot;cadetblue&quot;) + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;) + geom_abline(intercept = 0, slope = 1, linetype = 2) + labs(title = &quot;Carseats Bagging, Predicted vs Actual (caret)&quot;) plot(varImp(cs_mdl_bag), main=&quot;Variable Importance with Bagging&quot;) # RMSE of 1.9185 is much improved cs_rmse_bag &lt;- RMSE(pred = cs_preds_bag$Predicted, obs = cs_preds_bag$Actual) scoreboard_r &lt;- rbind( scoreboard_r, data.frame(model = &quot;Bagging&quot;, RMSE = round(cs_rmse_bag, 5)) ) scoreboard_r %&gt;% arrange(RMSE) ## model RMSE ## 1 Bagging 1.91847 ## 2 Caret w/tuneGrid 2.29833 ## 3 Manual ANOVA 2.36320 9.4.0.4 Random Forest Regression Example Now I’ll try it with the random forest method by specifying method = \"rf\". Hyperparameter mtry can take any value from 1 to 10 (the number of predictors) and I expect the best value to be near \\(\\sqrt{10} \\sim 3\\). set.seed(1234) cs_mdl_rf &lt;- train( Sales ~ ., data = cs_train, method = &quot;rf&quot;, tuneGrid = expand.grid(mtry = 1:10), # searching around mtry=3 trControl = cs_trControl ) cs_mdl_rf ## Random Forest ## ## 321 samples ## 10 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... ## Resampling results across tuning parameters: ## ## mtry RMSE Rsquared MAE ## 1 2.170362 0.6401338 1.739791 ## 2 1.806516 0.7281537 1.444411 ## 3 1.661626 0.7539811 1.320989 ## 4 1.588878 0.7531926 1.259214 ## 5 1.539960 0.7580374 1.222062 ## 6 1.526479 0.7536928 1.211417 ## 7 1.515426 0.7541277 1.205956 ## 8 1.523217 0.7456623 1.215768 ## 9 1.521271 0.7447813 1.217091 ## 10 1.527277 0.7380014 1.218469 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was mtry = 7. # The minimum RMSE is at mtry = 7 plot(cs_mdl_rf) cs_preds_rf &lt;- bind_cols( Predicted = predict(cs_mdl_rf, newdata = cs_test), Actual = cs_test$Sales ) # Model over-predicts at low end of Sales and under-predicts at high end cs_preds_rf %&gt;% ggplot(aes(x = Actual, y = Predicted)) + geom_point(alpha = 0.6, color = &quot;cadetblue&quot;) + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;) + geom_abline(intercept = 0, slope = 1, linetype = 2) + labs(title = &quot;Carseats Random Forest, Predicted vs Actual (caret)&quot;) plot(varImp(cs_mdl_rf), main=&quot;Variable Importance with Random Forest&quot;) # RMSE of 1.7184 is better than bagging&#39;s 1.9185. cs_rmse_rf &lt;- RMSE(pred = cs_preds_rf$Predicted, obs = cs_preds_rf$Actual) scoreboard_r &lt;- rbind( scoreboard_r, data.frame(model = &quot;Random Forest&quot;, RMSE = round(cs_rmse_rf, 5)) ) scoreboard_r %&gt;% arrange(RMSE) ## model RMSE ## 1 Random Forest 1.71836 ## 2 Bagging 1.91847 ## 3 Caret w/tuneGrid 2.29833 ## 4 Manual ANOVA 2.36320 The bagging and random forest models did very well - they took over the top positions! "],
["gradient-boosting.html", "9.5 Gradient Boosting", " 9.5 Gradient Boosting Note: I learned gradient boosting from explained.ai. Gradient boosting machine (GBM) is an additive modeling algorithm that gradually builds a composite model by iteratively adding M weak sub-models based on the performance of the prior iteration’s composite, \\[F_M(x) = \\sum_m^M f_m(x).\\] The idea is to fit a weak model, then replace the response values with the residuals from that model, and fit another model. Adding the residual prediction model to the original response prediction model produces a more accurate model. GBM repeats this process over and over, running new models to predict the residuals of the previous composite models, and adding the results to produce new composites. With each iteration, the model becomes stronger and stronger. The successive trees are usually weighted to slow down the learning rate. “Shrinkage” reduces the influence of each individual tree and leaves space for future trees to improve the model. \\[F_M(x) = f_0 + \\eta\\sum_{m = 1}^M f_m(x).\\] The smaller the learning rate, \\(\\eta\\), the larger the number of trees, \\(M\\). \\(\\eta\\) and \\(M\\) are hyperparameters. Other constraints to the trees are usually applied as additional hyperparameters, including, tree depth, number of nodes, minimum observations per split, and minimum improvement to loss. The name “gradient boosting” refers to the boosting of a model with a gradient. Each round of training builds a weak learner and uses the residuals to calculate a gradient, the partial derivative of the loss function. Gradient boosting “descends the gradient” to adjust the model parameters to reduce the error in the next round of training. In the case of classification problems, the loss function is the log-loss; for regression problems, the loss function is mean squared error. GBM continues until it reaches maximum number of trees or an acceptable error level. 9.5.0.1 Gradient Boosting Classification Example I’ll predict Purchase from the OJ data set again, this time using the GBM method by specifying method = \"gbm\". gbm has the following tuneable hyperparameters (see modelLookup(\"gbm\")). n.trees: number of boosting iterations, \\(M\\) interaction.depth: maximum tree depth shrinkage: shrinkage, \\(\\eta\\) n.minobsinnode: mimimum terminal node size I’ll use tuneLength = 5. set.seed(1234) oj_mdl_gbm &lt;- train( Purchase ~ ., data = oj_train, method = &quot;gbm&quot;, tuneLength = 5, trControl = oj_trControl ) ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2729 nan 0.1000 0.0296 ## 2 1.2250 nan 0.1000 0.0250 ## 3 1.1846 nan 0.1000 0.0221 ## 4 1.1479 nan 0.1000 0.0178 ## 5 1.1188 nan 0.1000 0.0128 ## 6 1.0930 nan 0.1000 0.0130 ## 7 1.0696 nan 0.1000 0.0114 ## 8 1.0507 nan 0.1000 0.0089 ## 9 1.0328 nan 0.1000 0.0072 ## 10 1.0187 nan 0.1000 0.0066 ## 20 0.9193 nan 0.1000 0.0025 ## 40 0.8234 nan 0.1000 0.0005 ## 60 0.7858 nan 0.1000 -0.0010 ## 80 0.7666 nan 0.1000 -0.0010 ## 100 0.7579 nan 0.1000 -0.0001 ## 120 0.7509 nan 0.1000 -0.0007 ## 140 0.7447 nan 0.1000 -0.0005 ## 160 0.7394 nan 0.1000 -0.0002 ## 180 0.7338 nan 0.1000 -0.0001 ## 200 0.7292 nan 0.1000 -0.0007 ## 220 0.7259 nan 0.1000 -0.0005 ## 240 0.7222 nan 0.1000 -0.0009 ## 250 0.7204 nan 0.1000 -0.0012 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2662 nan 0.1000 0.0349 ## 2 1.2068 nan 0.1000 0.0266 ## 3 1.1624 nan 0.1000 0.0230 ## 4 1.1230 nan 0.1000 0.0186 ## 5 1.0876 nan 0.1000 0.0180 ## 6 1.0556 nan 0.1000 0.0141 ## 7 1.0268 nan 0.1000 0.0124 ## 8 1.0002 nan 0.1000 0.0105 ## 9 0.9802 nan 0.1000 0.0075 ## 10 0.9601 nan 0.1000 0.0093 ## 20 0.8410 nan 0.1000 0.0039 ## 40 0.7669 nan 0.1000 -0.0003 ## 60 0.7387 nan 0.1000 -0.0013 ## 80 0.7202 nan 0.1000 -0.0020 ## 100 0.7076 nan 0.1000 -0.0011 ## 120 0.6892 nan 0.1000 -0.0009 ## 140 0.6762 nan 0.1000 -0.0013 ## 160 0.6680 nan 0.1000 -0.0008 ## 180 0.6605 nan 0.1000 -0.0015 ## 200 0.6519 nan 0.1000 -0.0008 ## 220 0.6418 nan 0.1000 -0.0015 ## 240 0.6328 nan 0.1000 -0.0007 ## 250 0.6306 nan 0.1000 -0.0018 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2535 nan 0.1000 0.0377 ## 2 1.1930 nan 0.1000 0.0311 ## 3 1.1360 nan 0.1000 0.0252 ## 4 1.0891 nan 0.1000 0.0196 ## 5 1.0520 nan 0.1000 0.0159 ## 6 1.0180 nan 0.1000 0.0147 ## 7 0.9846 nan 0.1000 0.0133 ## 8 0.9553 nan 0.1000 0.0139 ## 9 0.9336 nan 0.1000 0.0105 ## 10 0.9150 nan 0.1000 0.0058 ## 20 0.7989 nan 0.1000 0.0020 ## 40 0.7300 nan 0.1000 -0.0018 ## 60 0.6912 nan 0.1000 -0.0007 ## 80 0.6669 nan 0.1000 -0.0006 ## 100 0.6481 nan 0.1000 -0.0014 ## 120 0.6275 nan 0.1000 -0.0015 ## 140 0.6114 nan 0.1000 -0.0009 ## 160 0.5970 nan 0.1000 -0.0016 ## 180 0.5868 nan 0.1000 -0.0011 ## 200 0.5727 nan 0.1000 -0.0013 ## 220 0.5586 nan 0.1000 -0.0018 ## 240 0.5432 nan 0.1000 -0.0008 ## 250 0.5374 nan 0.1000 -0.0010 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2593 nan 0.1000 0.0373 ## 2 1.1862 nan 0.1000 0.0307 ## 3 1.1262 nan 0.1000 0.0268 ## 4 1.0792 nan 0.1000 0.0196 ## 5 1.0320 nan 0.1000 0.0197 ## 6 0.9935 nan 0.1000 0.0172 ## 7 0.9620 nan 0.1000 0.0143 ## 8 0.9358 nan 0.1000 0.0093 ## 9 0.9112 nan 0.1000 0.0097 ## 10 0.8911 nan 0.1000 0.0094 ## 20 0.7721 nan 0.1000 0.0022 ## 40 0.6974 nan 0.1000 -0.0018 ## 60 0.6568 nan 0.1000 -0.0015 ## 80 0.6273 nan 0.1000 -0.0020 ## 100 0.6055 nan 0.1000 -0.0009 ## 120 0.5835 nan 0.1000 -0.0011 ## 140 0.5657 nan 0.1000 -0.0018 ## 160 0.5468 nan 0.1000 -0.0020 ## 180 0.5339 nan 0.1000 -0.0021 ## 200 0.5208 nan 0.1000 -0.0004 ## 220 0.5108 nan 0.1000 -0.0026 ## 240 0.5002 nan 0.1000 -0.0008 ## 250 0.4910 nan 0.1000 -0.0019 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2498 nan 0.1000 0.0406 ## 2 1.1806 nan 0.1000 0.0325 ## 3 1.1216 nan 0.1000 0.0288 ## 4 1.0715 nan 0.1000 0.0212 ## 5 1.0283 nan 0.1000 0.0182 ## 6 0.9935 nan 0.1000 0.0146 ## 7 0.9629 nan 0.1000 0.0111 ## 8 0.9352 nan 0.1000 0.0105 ## 9 0.9068 nan 0.1000 0.0093 ## 10 0.8816 nan 0.1000 0.0086 ## 20 0.7662 nan 0.1000 0.0025 ## 40 0.6844 nan 0.1000 -0.0017 ## 60 0.6381 nan 0.1000 -0.0028 ## 80 0.6018 nan 0.1000 -0.0020 ## 100 0.5710 nan 0.1000 -0.0018 ## 120 0.5402 nan 0.1000 -0.0021 ## 140 0.5137 nan 0.1000 -0.0022 ## 160 0.4950 nan 0.1000 -0.0015 ## 180 0.4810 nan 0.1000 -0.0012 ## 200 0.4687 nan 0.1000 -0.0029 ## 220 0.4559 nan 0.1000 -0.0025 ## 240 0.4427 nan 0.1000 -0.0019 ## 250 0.4369 nan 0.1000 -0.0015 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2817 nan 0.1000 0.0283 ## 2 1.2326 nan 0.1000 0.0242 ## 3 1.1940 nan 0.1000 0.0154 ## 4 1.1582 nan 0.1000 0.0172 ## 5 1.1267 nan 0.1000 0.0137 ## 6 1.1022 nan 0.1000 0.0106 ## 7 1.0821 nan 0.1000 0.0101 ## 8 1.0624 nan 0.1000 0.0077 ## 9 1.0468 nan 0.1000 0.0071 ## 10 1.0325 nan 0.1000 0.0053 ## 20 0.9318 nan 0.1000 0.0031 ## 40 0.8366 nan 0.1000 -0.0005 ## 60 0.7998 nan 0.1000 -0.0009 ## 80 0.7815 nan 0.1000 -0.0003 ## 100 0.7719 nan 0.1000 -0.0008 ## 120 0.7656 nan 0.1000 0.0000 ## 140 0.7608 nan 0.1000 -0.0021 ## 160 0.7543 nan 0.1000 -0.0006 ## 180 0.7508 nan 0.1000 -0.0007 ## 200 0.7451 nan 0.1000 -0.0004 ## 220 0.7407 nan 0.1000 -0.0003 ## 240 0.7366 nan 0.1000 -0.0008 ## 250 0.7351 nan 0.1000 -0.0005 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2694 nan 0.1000 0.0342 ## 2 1.2132 nan 0.1000 0.0270 ## 3 1.1673 nan 0.1000 0.0193 ## 4 1.1268 nan 0.1000 0.0172 ## 5 1.0943 nan 0.1000 0.0149 ## 6 1.0633 nan 0.1000 0.0143 ## 7 1.0323 nan 0.1000 0.0114 ## 8 1.0081 nan 0.1000 0.0104 ## 9 0.9849 nan 0.1000 0.0097 ## 10 0.9682 nan 0.1000 0.0076 ## 20 0.8555 nan 0.1000 0.0019 ## 40 0.7751 nan 0.1000 0.0004 ## 60 0.7477 nan 0.1000 -0.0007 ## 80 0.7317 nan 0.1000 -0.0003 ## 100 0.7192 nan 0.1000 -0.0011 ## 120 0.7046 nan 0.1000 -0.0024 ## 140 0.6923 nan 0.1000 -0.0013 ## 160 0.6820 nan 0.1000 -0.0010 ## 180 0.6706 nan 0.1000 -0.0011 ## 200 0.6575 nan 0.1000 -0.0004 ## 220 0.6481 nan 0.1000 -0.0018 ## 240 0.6418 nan 0.1000 -0.0019 ## 250 0.6370 nan 0.1000 -0.0009 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2589 nan 0.1000 0.0367 ## 2 1.1957 nan 0.1000 0.0314 ## 3 1.1433 nan 0.1000 0.0247 ## 4 1.1035 nan 0.1000 0.0175 ## 5 1.0668 nan 0.1000 0.0178 ## 6 1.0274 nan 0.1000 0.0182 ## 7 0.9992 nan 0.1000 0.0109 ## 8 0.9724 nan 0.1000 0.0112 ## 9 0.9481 nan 0.1000 0.0101 ## 10 0.9292 nan 0.1000 0.0067 ## 20 0.8146 nan 0.1000 0.0007 ## 40 0.7401 nan 0.1000 -0.0012 ## 60 0.7061 nan 0.1000 -0.0009 ## 80 0.6831 nan 0.1000 -0.0017 ## 100 0.6580 nan 0.1000 -0.0013 ## 120 0.6439 nan 0.1000 -0.0028 ## 140 0.6297 nan 0.1000 -0.0019 ## 160 0.6157 nan 0.1000 -0.0010 ## 180 0.6035 nan 0.1000 -0.0012 ## 200 0.5892 nan 0.1000 -0.0014 ## 220 0.5762 nan 0.1000 -0.0008 ## 240 0.5697 nan 0.1000 -0.0016 ## 250 0.5648 nan 0.1000 -0.0013 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2583 nan 0.1000 0.0336 ## 2 1.1935 nan 0.1000 0.0295 ## 3 1.1363 nan 0.1000 0.0271 ## 4 1.0833 nan 0.1000 0.0234 ## 5 1.0470 nan 0.1000 0.0183 ## 6 1.0131 nan 0.1000 0.0151 ## 7 0.9860 nan 0.1000 0.0109 ## 8 0.9597 nan 0.1000 0.0098 ## 9 0.9378 nan 0.1000 0.0101 ## 10 0.9137 nan 0.1000 0.0096 ## 20 0.7863 nan 0.1000 0.0010 ## 40 0.7127 nan 0.1000 -0.0010 ## 60 0.6731 nan 0.1000 -0.0031 ## 80 0.6418 nan 0.1000 -0.0012 ## 100 0.6154 nan 0.1000 -0.0020 ## 120 0.5964 nan 0.1000 -0.0031 ## 140 0.5777 nan 0.1000 -0.0007 ## 160 0.5601 nan 0.1000 -0.0012 ## 180 0.5441 nan 0.1000 -0.0020 ## 200 0.5316 nan 0.1000 -0.0018 ## 220 0.5188 nan 0.1000 -0.0021 ## 240 0.5068 nan 0.1000 -0.0022 ## 250 0.5020 nan 0.1000 -0.0013 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2438 nan 0.1000 0.0389 ## 2 1.1756 nan 0.1000 0.0314 ## 3 1.1166 nan 0.1000 0.0262 ## 4 1.0687 nan 0.1000 0.0208 ## 5 1.0259 nan 0.1000 0.0158 ## 6 0.9881 nan 0.1000 0.0152 ## 7 0.9600 nan 0.1000 0.0114 ## 8 0.9341 nan 0.1000 0.0100 ## 9 0.9111 nan 0.1000 0.0103 ## 10 0.8920 nan 0.1000 0.0055 ## 20 0.7743 nan 0.1000 -0.0000 ## 40 0.6857 nan 0.1000 -0.0025 ## 60 0.6370 nan 0.1000 -0.0013 ## 80 0.6043 nan 0.1000 -0.0013 ## 100 0.5774 nan 0.1000 -0.0023 ## 120 0.5563 nan 0.1000 -0.0022 ## 140 0.5316 nan 0.1000 -0.0016 ## 160 0.5145 nan 0.1000 -0.0035 ## 180 0.4996 nan 0.1000 -0.0023 ## 200 0.4842 nan 0.1000 -0.0027 ## 220 0.4674 nan 0.1000 -0.0013 ## 240 0.4538 nan 0.1000 -0.0021 ## 250 0.4457 nan 0.1000 -0.0020 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2744 nan 0.1000 0.0326 ## 2 1.2253 nan 0.1000 0.0260 ## 3 1.1785 nan 0.1000 0.0210 ## 4 1.1409 nan 0.1000 0.0170 ## 5 1.1052 nan 0.1000 0.0158 ## 6 1.0771 nan 0.1000 0.0128 ## 7 1.0544 nan 0.1000 0.0107 ## 8 1.0328 nan 0.1000 0.0096 ## 9 1.0138 nan 0.1000 0.0057 ## 10 0.9978 nan 0.1000 0.0068 ## 20 0.8883 nan 0.1000 0.0025 ## 40 0.7959 nan 0.1000 -0.0004 ## 60 0.7558 nan 0.1000 0.0003 ## 80 0.7390 nan 0.1000 -0.0003 ## 100 0.7288 nan 0.1000 -0.0008 ## 120 0.7215 nan 0.1000 -0.0009 ## 140 0.7147 nan 0.1000 -0.0012 ## 160 0.7091 nan 0.1000 -0.0016 ## 180 0.7057 nan 0.1000 -0.0012 ## 200 0.7021 nan 0.1000 -0.0008 ## 220 0.6978 nan 0.1000 -0.0012 ## 240 0.6948 nan 0.1000 -0.0009 ## 250 0.6928 nan 0.1000 -0.0018 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2654 nan 0.1000 0.0358 ## 2 1.2003 nan 0.1000 0.0312 ## 3 1.1508 nan 0.1000 0.0239 ## 4 1.1085 nan 0.1000 0.0196 ## 5 1.0690 nan 0.1000 0.0172 ## 6 1.0403 nan 0.1000 0.0149 ## 7 1.0097 nan 0.1000 0.0129 ## 8 0.9834 nan 0.1000 0.0118 ## 9 0.9615 nan 0.1000 0.0101 ## 10 0.9417 nan 0.1000 0.0068 ## 20 0.8167 nan 0.1000 0.0036 ## 40 0.7389 nan 0.1000 -0.0002 ## 60 0.7116 nan 0.1000 -0.0009 ## 80 0.6908 nan 0.1000 -0.0004 ## 100 0.6771 nan 0.1000 -0.0004 ## 120 0.6624 nan 0.1000 -0.0006 ## 140 0.6511 nan 0.1000 -0.0013 ## 160 0.6426 nan 0.1000 -0.0007 ## 180 0.6335 nan 0.1000 -0.0013 ## 200 0.6221 nan 0.1000 -0.0012 ## 220 0.6123 nan 0.1000 -0.0021 ## 240 0.6031 nan 0.1000 -0.0017 ## 250 0.5995 nan 0.1000 -0.0011 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2554 nan 0.1000 0.0395 ## 2 1.1886 nan 0.1000 0.0295 ## 3 1.1308 nan 0.1000 0.0266 ## 4 1.0790 nan 0.1000 0.0228 ## 5 1.0361 nan 0.1000 0.0193 ## 6 0.9993 nan 0.1000 0.0156 ## 7 0.9707 nan 0.1000 0.0113 ## 8 0.9435 nan 0.1000 0.0116 ## 9 0.9230 nan 0.1000 0.0083 ## 10 0.8951 nan 0.1000 0.0114 ## 20 0.7665 nan 0.1000 0.0008 ## 40 0.7004 nan 0.1000 -0.0025 ## 60 0.6702 nan 0.1000 -0.0011 ## 80 0.6485 nan 0.1000 -0.0010 ## 100 0.6296 nan 0.1000 -0.0016 ## 120 0.6095 nan 0.1000 -0.0029 ## 140 0.5934 nan 0.1000 -0.0012 ## 160 0.5812 nan 0.1000 -0.0012 ## 180 0.5656 nan 0.1000 -0.0016 ## 200 0.5523 nan 0.1000 -0.0017 ## 220 0.5426 nan 0.1000 -0.0016 ## 240 0.5316 nan 0.1000 -0.0009 ## 250 0.5270 nan 0.1000 -0.0010 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2505 nan 0.1000 0.0437 ## 2 1.1741 nan 0.1000 0.0374 ## 3 1.1163 nan 0.1000 0.0252 ## 4 1.0636 nan 0.1000 0.0243 ## 5 1.0153 nan 0.1000 0.0190 ## 6 0.9820 nan 0.1000 0.0148 ## 7 0.9452 nan 0.1000 0.0141 ## 8 0.9207 nan 0.1000 0.0094 ## 9 0.8964 nan 0.1000 0.0106 ## 10 0.8735 nan 0.1000 0.0083 ## 20 0.7543 nan 0.1000 0.0016 ## 40 0.6815 nan 0.1000 -0.0017 ## 60 0.6381 nan 0.1000 -0.0024 ## 80 0.6108 nan 0.1000 -0.0015 ## 100 0.5869 nan 0.1000 -0.0020 ## 120 0.5637 nan 0.1000 -0.0021 ## 140 0.5481 nan 0.1000 -0.0020 ## 160 0.5262 nan 0.1000 -0.0023 ## 180 0.5072 nan 0.1000 -0.0016 ## 200 0.4947 nan 0.1000 -0.0025 ## 220 0.4799 nan 0.1000 -0.0011 ## 240 0.4691 nan 0.1000 -0.0018 ## 250 0.4627 nan 0.1000 -0.0017 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2448 nan 0.1000 0.0450 ## 2 1.1726 nan 0.1000 0.0307 ## 3 1.1134 nan 0.1000 0.0289 ## 4 1.0543 nan 0.1000 0.0220 ## 5 1.0142 nan 0.1000 0.0184 ## 6 0.9726 nan 0.1000 0.0195 ## 7 0.9416 nan 0.1000 0.0134 ## 8 0.9152 nan 0.1000 0.0121 ## 9 0.8898 nan 0.1000 0.0110 ## 10 0.8655 nan 0.1000 0.0096 ## 20 0.7372 nan 0.1000 0.0006 ## 40 0.6572 nan 0.1000 -0.0009 ## 60 0.6154 nan 0.1000 -0.0005 ## 80 0.5848 nan 0.1000 -0.0022 ## 100 0.5515 nan 0.1000 -0.0018 ## 120 0.5248 nan 0.1000 -0.0024 ## 140 0.5051 nan 0.1000 -0.0029 ## 160 0.4820 nan 0.1000 -0.0012 ## 180 0.4647 nan 0.1000 -0.0016 ## 200 0.4459 nan 0.1000 -0.0019 ## 220 0.4280 nan 0.1000 -0.0027 ## 240 0.4156 nan 0.1000 -0.0023 ## 250 0.4068 nan 0.1000 -0.0005 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2762 nan 0.1000 0.0308 ## 2 1.2245 nan 0.1000 0.0249 ## 3 1.1864 nan 0.1000 0.0196 ## 4 1.1519 nan 0.1000 0.0155 ## 5 1.1237 nan 0.1000 0.0136 ## 6 1.0939 nan 0.1000 0.0111 ## 7 1.0738 nan 0.1000 0.0090 ## 8 1.0531 nan 0.1000 0.0100 ## 9 1.0340 nan 0.1000 0.0067 ## 10 1.0186 nan 0.1000 0.0081 ## 20 0.9204 nan 0.1000 0.0022 ## 40 0.8343 nan 0.1000 0.0003 ## 60 0.7959 nan 0.1000 -0.0000 ## 80 0.7758 nan 0.1000 0.0001 ## 100 0.7682 nan 0.1000 -0.0001 ## 120 0.7605 nan 0.1000 -0.0005 ## 140 0.7558 nan 0.1000 -0.0009 ## 160 0.7511 nan 0.1000 -0.0010 ## 180 0.7480 nan 0.1000 -0.0013 ## 200 0.7441 nan 0.1000 -0.0010 ## 220 0.7406 nan 0.1000 -0.0003 ## 240 0.7378 nan 0.1000 -0.0005 ## 250 0.7360 nan 0.1000 -0.0012 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2682 nan 0.1000 0.0308 ## 2 1.2098 nan 0.1000 0.0304 ## 3 1.1625 nan 0.1000 0.0205 ## 4 1.1229 nan 0.1000 0.0208 ## 5 1.0891 nan 0.1000 0.0173 ## 6 1.0589 nan 0.1000 0.0140 ## 7 1.0296 nan 0.1000 0.0138 ## 8 1.0048 nan 0.1000 0.0106 ## 9 0.9807 nan 0.1000 0.0101 ## 10 0.9610 nan 0.1000 0.0081 ## 20 0.8458 nan 0.1000 0.0033 ## 40 0.7726 nan 0.1000 -0.0009 ## 60 0.7425 nan 0.1000 -0.0015 ## 80 0.7215 nan 0.1000 -0.0009 ## 100 0.7074 nan 0.1000 -0.0001 ## 120 0.6962 nan 0.1000 -0.0011 ## 140 0.6862 nan 0.1000 -0.0006 ## 160 0.6760 nan 0.1000 -0.0018 ## 180 0.6656 nan 0.1000 -0.0017 ## 200 0.6589 nan 0.1000 -0.0020 ## 220 0.6525 nan 0.1000 -0.0008 ## 240 0.6443 nan 0.1000 -0.0018 ## 250 0.6380 nan 0.1000 -0.0027 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2605 nan 0.1000 0.0383 ## 2 1.1943 nan 0.1000 0.0307 ## 3 1.1408 nan 0.1000 0.0262 ## 4 1.1000 nan 0.1000 0.0199 ## 5 1.0583 nan 0.1000 0.0188 ## 6 1.0235 nan 0.1000 0.0148 ## 7 0.9881 nan 0.1000 0.0154 ## 8 0.9603 nan 0.1000 0.0093 ## 9 0.9385 nan 0.1000 0.0077 ## 10 0.9217 nan 0.1000 0.0064 ## 20 0.8050 nan 0.1000 0.0008 ## 40 0.7371 nan 0.1000 -0.0016 ## 60 0.7033 nan 0.1000 -0.0011 ## 80 0.6772 nan 0.1000 -0.0016 ## 100 0.6570 nan 0.1000 -0.0008 ## 120 0.6413 nan 0.1000 -0.0016 ## 140 0.6280 nan 0.1000 -0.0010 ## 160 0.6108 nan 0.1000 -0.0011 ## 180 0.5963 nan 0.1000 -0.0017 ## 200 0.5846 nan 0.1000 -0.0019 ## 220 0.5687 nan 0.1000 -0.0014 ## 240 0.5584 nan 0.1000 -0.0016 ## 250 0.5534 nan 0.1000 -0.0008 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2540 nan 0.1000 0.0417 ## 2 1.1819 nan 0.1000 0.0335 ## 3 1.1257 nan 0.1000 0.0243 ## 4 1.0775 nan 0.1000 0.0199 ## 5 1.0352 nan 0.1000 0.0193 ## 6 0.9973 nan 0.1000 0.0156 ## 7 0.9709 nan 0.1000 0.0113 ## 8 0.9455 nan 0.1000 0.0102 ## 9 0.9206 nan 0.1000 0.0102 ## 10 0.8977 nan 0.1000 0.0099 ## 20 0.7806 nan 0.1000 0.0023 ## 40 0.7057 nan 0.1000 -0.0022 ## 60 0.6621 nan 0.1000 -0.0014 ## 80 0.6314 nan 0.1000 -0.0012 ## 100 0.6089 nan 0.1000 -0.0032 ## 120 0.5900 nan 0.1000 -0.0012 ## 140 0.5680 nan 0.1000 -0.0025 ## 160 0.5463 nan 0.1000 -0.0009 ## 180 0.5336 nan 0.1000 -0.0025 ## 200 0.5166 nan 0.1000 -0.0020 ## 220 0.5015 nan 0.1000 -0.0015 ## 240 0.4869 nan 0.1000 -0.0015 ## 250 0.4803 nan 0.1000 -0.0026 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2480 nan 0.1000 0.0440 ## 2 1.1726 nan 0.1000 0.0316 ## 3 1.1131 nan 0.1000 0.0243 ## 4 1.0572 nan 0.1000 0.0226 ## 5 1.0135 nan 0.1000 0.0177 ## 6 0.9761 nan 0.1000 0.0146 ## 7 0.9457 nan 0.1000 0.0141 ## 8 0.9156 nan 0.1000 0.0098 ## 9 0.8948 nan 0.1000 0.0078 ## 10 0.8754 nan 0.1000 0.0077 ## 20 0.7648 nan 0.1000 -0.0002 ## 40 0.6878 nan 0.1000 -0.0013 ## 60 0.6439 nan 0.1000 -0.0013 ## 80 0.6061 nan 0.1000 -0.0035 ## 100 0.5768 nan 0.1000 -0.0001 ## 120 0.5497 nan 0.1000 -0.0021 ## 140 0.5269 nan 0.1000 -0.0021 ## 160 0.5087 nan 0.1000 -0.0030 ## 180 0.4924 nan 0.1000 -0.0023 ## 200 0.4748 nan 0.1000 -0.0014 ## 220 0.4589 nan 0.1000 -0.0026 ## 240 0.4443 nan 0.1000 -0.0009 ## 250 0.4391 nan 0.1000 -0.0016 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2817 nan 0.1000 0.0286 ## 2 1.2318 nan 0.1000 0.0196 ## 3 1.1833 nan 0.1000 0.0207 ## 4 1.1473 nan 0.1000 0.0169 ## 5 1.1172 nan 0.1000 0.0141 ## 6 1.0908 nan 0.1000 0.0125 ## 7 1.0682 nan 0.1000 0.0104 ## 8 1.0447 nan 0.1000 0.0083 ## 9 1.0267 nan 0.1000 0.0073 ## 10 1.0134 nan 0.1000 0.0065 ## 20 0.9106 nan 0.1000 0.0022 ## 40 0.8243 nan 0.1000 -0.0005 ## 60 0.7839 nan 0.1000 -0.0004 ## 80 0.7633 nan 0.1000 -0.0011 ## 100 0.7528 nan 0.1000 -0.0016 ## 120 0.7453 nan 0.1000 -0.0006 ## 140 0.7409 nan 0.1000 -0.0007 ## 160 0.7368 nan 0.1000 -0.0009 ## 180 0.7326 nan 0.1000 -0.0010 ## 200 0.7280 nan 0.1000 -0.0006 ## 220 0.7245 nan 0.1000 -0.0007 ## 240 0.7211 nan 0.1000 -0.0010 ## 250 0.7192 nan 0.1000 -0.0007 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2650 nan 0.1000 0.0342 ## 2 1.1992 nan 0.1000 0.0264 ## 3 1.1480 nan 0.1000 0.0224 ## 4 1.1119 nan 0.1000 0.0158 ## 5 1.0761 nan 0.1000 0.0157 ## 6 1.0450 nan 0.1000 0.0129 ## 7 1.0179 nan 0.1000 0.0101 ## 8 0.9971 nan 0.1000 0.0076 ## 9 0.9718 nan 0.1000 0.0112 ## 10 0.9508 nan 0.1000 0.0090 ## 20 0.8388 nan 0.1000 0.0019 ## 40 0.7541 nan 0.1000 -0.0003 ## 60 0.7289 nan 0.1000 -0.0023 ## 80 0.7074 nan 0.1000 -0.0013 ## 100 0.6927 nan 0.1000 -0.0009 ## 120 0.6829 nan 0.1000 -0.0020 ## 140 0.6734 nan 0.1000 -0.0022 ## 160 0.6632 nan 0.1000 -0.0015 ## 180 0.6523 nan 0.1000 -0.0007 ## 200 0.6429 nan 0.1000 -0.0007 ## 220 0.6336 nan 0.1000 -0.0015 ## 240 0.6272 nan 0.1000 -0.0014 ## 250 0.6229 nan 0.1000 -0.0009 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2552 nan 0.1000 0.0378 ## 2 1.1847 nan 0.1000 0.0326 ## 3 1.1324 nan 0.1000 0.0271 ## 4 1.0835 nan 0.1000 0.0228 ## 5 1.0472 nan 0.1000 0.0176 ## 6 1.0118 nan 0.1000 0.0148 ## 7 0.9811 nan 0.1000 0.0165 ## 8 0.9578 nan 0.1000 0.0087 ## 9 0.9327 nan 0.1000 0.0110 ## 10 0.9076 nan 0.1000 0.0085 ## 20 0.7927 nan 0.1000 0.0013 ## 40 0.7215 nan 0.1000 0.0001 ## 60 0.6949 nan 0.1000 -0.0024 ## 80 0.6675 nan 0.1000 -0.0010 ## 100 0.6476 nan 0.1000 -0.0018 ## 120 0.6315 nan 0.1000 -0.0020 ## 140 0.6128 nan 0.1000 -0.0014 ## 160 0.5951 nan 0.1000 -0.0011 ## 180 0.5790 nan 0.1000 -0.0014 ## 200 0.5655 nan 0.1000 -0.0015 ## 220 0.5529 nan 0.1000 -0.0013 ## 240 0.5408 nan 0.1000 -0.0015 ## 250 0.5335 nan 0.1000 -0.0009 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2493 nan 0.1000 0.0414 ## 2 1.1784 nan 0.1000 0.0338 ## 3 1.1202 nan 0.1000 0.0274 ## 4 1.0722 nan 0.1000 0.0237 ## 5 1.0327 nan 0.1000 0.0167 ## 6 0.9953 nan 0.1000 0.0173 ## 7 0.9607 nan 0.1000 0.0155 ## 8 0.9356 nan 0.1000 0.0098 ## 9 0.9078 nan 0.1000 0.0104 ## 10 0.8864 nan 0.1000 0.0082 ## 20 0.7682 nan 0.1000 0.0017 ## 40 0.6961 nan 0.1000 -0.0020 ## 60 0.6554 nan 0.1000 -0.0022 ## 80 0.6206 nan 0.1000 -0.0008 ## 100 0.5969 nan 0.1000 -0.0018 ## 120 0.5720 nan 0.1000 -0.0021 ## 140 0.5518 nan 0.1000 -0.0018 ## 160 0.5346 nan 0.1000 -0.0028 ## 180 0.5219 nan 0.1000 -0.0011 ## 200 0.5061 nan 0.1000 -0.0017 ## 220 0.4920 nan 0.1000 -0.0015 ## 240 0.4776 nan 0.1000 -0.0021 ## 250 0.4705 nan 0.1000 -0.0026 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2467 nan 0.1000 0.0423 ## 2 1.1749 nan 0.1000 0.0334 ## 3 1.1120 nan 0.1000 0.0250 ## 4 1.0588 nan 0.1000 0.0251 ## 5 1.0142 nan 0.1000 0.0200 ## 6 0.9777 nan 0.1000 0.0139 ## 7 0.9459 nan 0.1000 0.0115 ## 8 0.9176 nan 0.1000 0.0111 ## 9 0.8922 nan 0.1000 0.0111 ## 10 0.8699 nan 0.1000 0.0083 ## 20 0.7529 nan 0.1000 0.0010 ## 40 0.6714 nan 0.1000 -0.0007 ## 60 0.6188 nan 0.1000 -0.0022 ## 80 0.5867 nan 0.1000 -0.0028 ## 100 0.5628 nan 0.1000 -0.0023 ## 120 0.5400 nan 0.1000 -0.0029 ## 140 0.5213 nan 0.1000 -0.0032 ## 160 0.5009 nan 0.1000 -0.0021 ## 180 0.4833 nan 0.1000 -0.0013 ## 200 0.4646 nan 0.1000 -0.0024 ## 220 0.4499 nan 0.1000 -0.0020 ## 240 0.4308 nan 0.1000 -0.0016 ## 250 0.4217 nan 0.1000 -0.0019 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2762 nan 0.1000 0.0307 ## 2 1.2237 nan 0.1000 0.0252 ## 3 1.1895 nan 0.1000 0.0156 ## 4 1.1525 nan 0.1000 0.0173 ## 5 1.1218 nan 0.1000 0.0137 ## 6 1.0958 nan 0.1000 0.0129 ## 7 1.0752 nan 0.1000 0.0098 ## 8 1.0572 nan 0.1000 0.0081 ## 9 1.0382 nan 0.1000 0.0091 ## 10 1.0194 nan 0.1000 0.0062 ## 20 0.9202 nan 0.1000 0.0026 ## 40 0.8290 nan 0.1000 0.0005 ## 60 0.7926 nan 0.1000 -0.0007 ## 80 0.7742 nan 0.1000 -0.0010 ## 100 0.7654 nan 0.1000 -0.0003 ## 120 0.7614 nan 0.1000 -0.0009 ## 140 0.7548 nan 0.1000 -0.0006 ## 160 0.7494 nan 0.1000 -0.0010 ## 180 0.7440 nan 0.1000 -0.0007 ## 200 0.7406 nan 0.1000 -0.0005 ## 220 0.7374 nan 0.1000 -0.0010 ## 240 0.7342 nan 0.1000 -0.0011 ## 250 0.7331 nan 0.1000 -0.0012 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2627 nan 0.1000 0.0328 ## 2 1.2039 nan 0.1000 0.0274 ## 3 1.1596 nan 0.1000 0.0202 ## 4 1.1186 nan 0.1000 0.0184 ## 5 1.0835 nan 0.1000 0.0165 ## 6 1.0516 nan 0.1000 0.0159 ## 7 1.0272 nan 0.1000 0.0104 ## 8 1.0067 nan 0.1000 0.0110 ## 9 0.9845 nan 0.1000 0.0084 ## 10 0.9642 nan 0.1000 0.0094 ## 20 0.8460 nan 0.1000 0.0021 ## 40 0.7620 nan 0.1000 -0.0010 ## 60 0.7353 nan 0.1000 -0.0011 ## 80 0.7187 nan 0.1000 -0.0004 ## 100 0.7038 nan 0.1000 -0.0020 ## 120 0.6912 nan 0.1000 -0.0018 ## 140 0.6767 nan 0.1000 -0.0012 ## 160 0.6672 nan 0.1000 -0.0008 ## 180 0.6574 nan 0.1000 -0.0014 ## 200 0.6477 nan 0.1000 -0.0006 ## 220 0.6382 nan 0.1000 -0.0019 ## 240 0.6308 nan 0.1000 0.0000 ## 250 0.6260 nan 0.1000 -0.0004 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2518 nan 0.1000 0.0382 ## 2 1.1870 nan 0.1000 0.0317 ## 3 1.1397 nan 0.1000 0.0236 ## 4 1.0970 nan 0.1000 0.0193 ## 5 1.0549 nan 0.1000 0.0177 ## 6 1.0165 nan 0.1000 0.0175 ## 7 0.9884 nan 0.1000 0.0134 ## 8 0.9611 nan 0.1000 0.0102 ## 9 0.9392 nan 0.1000 0.0079 ## 10 0.9213 nan 0.1000 0.0071 ## 20 0.8017 nan 0.1000 0.0002 ## 40 0.7286 nan 0.1000 -0.0015 ## 60 0.6916 nan 0.1000 -0.0018 ## 80 0.6702 nan 0.1000 -0.0025 ## 100 0.6508 nan 0.1000 -0.0010 ## 120 0.6274 nan 0.1000 -0.0014 ## 140 0.6171 nan 0.1000 -0.0015 ## 160 0.6021 nan 0.1000 -0.0031 ## 180 0.5842 nan 0.1000 -0.0015 ## 200 0.5677 nan 0.1000 -0.0017 ## 220 0.5545 nan 0.1000 -0.0018 ## 240 0.5422 nan 0.1000 -0.0006 ## 250 0.5393 nan 0.1000 -0.0023 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2508 nan 0.1000 0.0416 ## 2 1.1876 nan 0.1000 0.0319 ## 3 1.1345 nan 0.1000 0.0268 ## 4 1.0859 nan 0.1000 0.0231 ## 5 1.0477 nan 0.1000 0.0145 ## 6 1.0147 nan 0.1000 0.0142 ## 7 0.9757 nan 0.1000 0.0150 ## 8 0.9465 nan 0.1000 0.0117 ## 9 0.9224 nan 0.1000 0.0086 ## 10 0.9007 nan 0.1000 0.0076 ## 20 0.7826 nan 0.1000 0.0011 ## 40 0.7046 nan 0.1000 0.0001 ## 60 0.6675 nan 0.1000 -0.0010 ## 80 0.6397 nan 0.1000 -0.0010 ## 100 0.6105 nan 0.1000 -0.0010 ## 120 0.5894 nan 0.1000 -0.0011 ## 140 0.5678 nan 0.1000 -0.0015 ## 160 0.5573 nan 0.1000 -0.0019 ## 180 0.5361 nan 0.1000 -0.0006 ## 200 0.5163 nan 0.1000 -0.0016 ## 220 0.5013 nan 0.1000 -0.0012 ## 240 0.4892 nan 0.1000 -0.0018 ## 250 0.4826 nan 0.1000 -0.0006 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2508 nan 0.1000 0.0385 ## 2 1.1781 nan 0.1000 0.0328 ## 3 1.1201 nan 0.1000 0.0278 ## 4 1.0647 nan 0.1000 0.0234 ## 5 1.0222 nan 0.1000 0.0166 ## 6 0.9885 nan 0.1000 0.0131 ## 7 0.9619 nan 0.1000 0.0105 ## 8 0.9382 nan 0.1000 0.0108 ## 9 0.9133 nan 0.1000 0.0090 ## 10 0.8955 nan 0.1000 0.0069 ## 20 0.7667 nan 0.1000 0.0001 ## 40 0.6844 nan 0.1000 -0.0025 ## 60 0.6404 nan 0.1000 -0.0005 ## 80 0.6008 nan 0.1000 -0.0030 ## 100 0.5677 nan 0.1000 -0.0019 ## 120 0.5438 nan 0.1000 -0.0020 ## 140 0.5229 nan 0.1000 -0.0021 ## 160 0.5044 nan 0.1000 -0.0032 ## 180 0.4825 nan 0.1000 -0.0019 ## 200 0.4665 nan 0.1000 -0.0016 ## 220 0.4503 nan 0.1000 -0.0010 ## 240 0.4358 nan 0.1000 -0.0013 ## 250 0.4267 nan 0.1000 -0.0017 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2724 nan 0.1000 0.0297 ## 2 1.2244 nan 0.1000 0.0229 ## 3 1.1857 nan 0.1000 0.0198 ## 4 1.1477 nan 0.1000 0.0177 ## 5 1.1169 nan 0.1000 0.0136 ## 6 1.0859 nan 0.1000 0.0124 ## 7 1.0609 nan 0.1000 0.0110 ## 8 1.0412 nan 0.1000 0.0097 ## 9 1.0249 nan 0.1000 0.0078 ## 10 1.0118 nan 0.1000 0.0062 ## 20 0.9127 nan 0.1000 0.0028 ## 40 0.8179 nan 0.1000 0.0001 ## 60 0.7791 nan 0.1000 -0.0004 ## 80 0.7582 nan 0.1000 -0.0005 ## 100 0.7503 nan 0.1000 -0.0011 ## 120 0.7446 nan 0.1000 -0.0012 ## 140 0.7367 nan 0.1000 -0.0005 ## 160 0.7331 nan 0.1000 -0.0004 ## 180 0.7284 nan 0.1000 -0.0006 ## 200 0.7230 nan 0.1000 -0.0011 ## 220 0.7177 nan 0.1000 -0.0010 ## 240 0.7139 nan 0.1000 -0.0007 ## 250 0.7117 nan 0.1000 -0.0009 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2735 nan 0.1000 0.0321 ## 2 1.2098 nan 0.1000 0.0280 ## 3 1.1616 nan 0.1000 0.0247 ## 4 1.1171 nan 0.1000 0.0207 ## 5 1.0819 nan 0.1000 0.0149 ## 6 1.0509 nan 0.1000 0.0129 ## 7 1.0189 nan 0.1000 0.0142 ## 8 0.9928 nan 0.1000 0.0116 ## 9 0.9769 nan 0.1000 0.0067 ## 10 0.9561 nan 0.1000 0.0069 ## 20 0.8347 nan 0.1000 0.0032 ## 40 0.7568 nan 0.1000 -0.0007 ## 60 0.7246 nan 0.1000 -0.0028 ## 80 0.7083 nan 0.1000 -0.0028 ## 100 0.6912 nan 0.1000 -0.0008 ## 120 0.6766 nan 0.1000 -0.0004 ## 140 0.6637 nan 0.1000 -0.0010 ## 160 0.6531 nan 0.1000 -0.0003 ## 180 0.6431 nan 0.1000 -0.0007 ## 200 0.6329 nan 0.1000 -0.0017 ## 220 0.6258 nan 0.1000 -0.0016 ## 240 0.6164 nan 0.1000 -0.0014 ## 250 0.6141 nan 0.1000 -0.0013 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2531 nan 0.1000 0.0330 ## 2 1.1935 nan 0.1000 0.0295 ## 3 1.1325 nan 0.1000 0.0290 ## 4 1.0852 nan 0.1000 0.0231 ## 5 1.0476 nan 0.1000 0.0185 ## 6 1.0160 nan 0.1000 0.0132 ## 7 0.9835 nan 0.1000 0.0129 ## 8 0.9597 nan 0.1000 0.0096 ## 9 0.9335 nan 0.1000 0.0106 ## 10 0.9156 nan 0.1000 0.0068 ## 20 0.7979 nan 0.1000 0.0008 ## 40 0.7258 nan 0.1000 -0.0013 ## 60 0.6903 nan 0.1000 -0.0007 ## 80 0.6607 nan 0.1000 -0.0002 ## 100 0.6382 nan 0.1000 -0.0014 ## 120 0.6213 nan 0.1000 -0.0015 ## 140 0.6018 nan 0.1000 -0.0014 ## 160 0.5865 nan 0.1000 -0.0011 ## 180 0.5724 nan 0.1000 -0.0006 ## 200 0.5610 nan 0.1000 -0.0015 ## 220 0.5495 nan 0.1000 -0.0017 ## 240 0.5368 nan 0.1000 -0.0019 ## 250 0.5320 nan 0.1000 -0.0023 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2513 nan 0.1000 0.0428 ## 2 1.1807 nan 0.1000 0.0294 ## 3 1.1249 nan 0.1000 0.0272 ## 4 1.0785 nan 0.1000 0.0195 ## 5 1.0329 nan 0.1000 0.0183 ## 6 1.0004 nan 0.1000 0.0136 ## 7 0.9728 nan 0.1000 0.0113 ## 8 0.9416 nan 0.1000 0.0149 ## 9 0.9110 nan 0.1000 0.0093 ## 10 0.8887 nan 0.1000 0.0091 ## 20 0.7692 nan 0.1000 -0.0001 ## 40 0.6928 nan 0.1000 -0.0013 ## 60 0.6541 nan 0.1000 -0.0014 ## 80 0.6260 nan 0.1000 -0.0010 ## 100 0.5998 nan 0.1000 -0.0022 ## 120 0.5754 nan 0.1000 -0.0017 ## 140 0.5532 nan 0.1000 -0.0014 ## 160 0.5348 nan 0.1000 -0.0029 ## 180 0.5153 nan 0.1000 -0.0014 ## 200 0.4980 nan 0.1000 -0.0013 ## 220 0.4855 nan 0.1000 -0.0008 ## 240 0.4754 nan 0.1000 -0.0021 ## 250 0.4694 nan 0.1000 -0.0016 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2475 nan 0.1000 0.0425 ## 2 1.1759 nan 0.1000 0.0344 ## 3 1.1233 nan 0.1000 0.0223 ## 4 1.0717 nan 0.1000 0.0187 ## 5 1.0297 nan 0.1000 0.0189 ## 6 0.9918 nan 0.1000 0.0166 ## 7 0.9597 nan 0.1000 0.0156 ## 8 0.9278 nan 0.1000 0.0129 ## 9 0.9033 nan 0.1000 0.0097 ## 10 0.8799 nan 0.1000 0.0095 ## 20 0.7574 nan 0.1000 -0.0001 ## 40 0.6716 nan 0.1000 -0.0023 ## 60 0.6282 nan 0.1000 -0.0013 ## 80 0.5979 nan 0.1000 -0.0028 ## 100 0.5737 nan 0.1000 -0.0008 ## 120 0.5512 nan 0.1000 -0.0021 ## 140 0.5289 nan 0.1000 -0.0020 ## 160 0.5000 nan 0.1000 -0.0017 ## 180 0.4823 nan 0.1000 -0.0028 ## 200 0.4611 nan 0.1000 -0.0043 ## 220 0.4439 nan 0.1000 -0.0019 ## 240 0.4286 nan 0.1000 -0.0013 ## 250 0.4205 nan 0.1000 -0.0019 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2742 nan 0.1000 0.0296 ## 2 1.2281 nan 0.1000 0.0207 ## 3 1.1808 nan 0.1000 0.0210 ## 4 1.1405 nan 0.1000 0.0146 ## 5 1.1119 nan 0.1000 0.0136 ## 6 1.0888 nan 0.1000 0.0120 ## 7 1.0655 nan 0.1000 0.0108 ## 8 1.0464 nan 0.1000 0.0060 ## 9 1.0310 nan 0.1000 0.0052 ## 10 1.0123 nan 0.1000 0.0085 ## 20 0.9188 nan 0.1000 0.0021 ## 40 0.8232 nan 0.1000 0.0010 ## 60 0.7849 nan 0.1000 0.0000 ## 80 0.7707 nan 0.1000 -0.0004 ## 100 0.7590 nan 0.1000 -0.0005 ## 120 0.7527 nan 0.1000 -0.0010 ## 140 0.7481 nan 0.1000 -0.0008 ## 160 0.7426 nan 0.1000 -0.0017 ## 180 0.7386 nan 0.1000 -0.0008 ## 200 0.7339 nan 0.1000 -0.0005 ## 220 0.7290 nan 0.1000 -0.0004 ## 240 0.7250 nan 0.1000 -0.0003 ## 250 0.7238 nan 0.1000 -0.0009 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2693 nan 0.1000 0.0317 ## 2 1.2097 nan 0.1000 0.0254 ## 3 1.1595 nan 0.1000 0.0221 ## 4 1.1196 nan 0.1000 0.0177 ## 5 1.0830 nan 0.1000 0.0162 ## 6 1.0516 nan 0.1000 0.0137 ## 7 1.0214 nan 0.1000 0.0118 ## 8 0.9989 nan 0.1000 0.0114 ## 9 0.9786 nan 0.1000 0.0086 ## 10 0.9619 nan 0.1000 0.0070 ## 20 0.8441 nan 0.1000 0.0022 ## 40 0.7603 nan 0.1000 0.0005 ## 60 0.7335 nan 0.1000 -0.0010 ## 80 0.7190 nan 0.1000 -0.0008 ## 100 0.6995 nan 0.1000 -0.0007 ## 120 0.6880 nan 0.1000 -0.0011 ## 140 0.6790 nan 0.1000 -0.0011 ## 160 0.6647 nan 0.1000 -0.0011 ## 180 0.6549 nan 0.1000 -0.0012 ## 200 0.6461 nan 0.1000 0.0002 ## 220 0.6386 nan 0.1000 -0.0008 ## 240 0.6317 nan 0.1000 -0.0012 ## 250 0.6281 nan 0.1000 -0.0010 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2640 nan 0.1000 0.0358 ## 2 1.1969 nan 0.1000 0.0308 ## 3 1.1371 nan 0.1000 0.0270 ## 4 1.0927 nan 0.1000 0.0229 ## 5 1.0577 nan 0.1000 0.0180 ## 6 1.0214 nan 0.1000 0.0158 ## 7 0.9897 nan 0.1000 0.0146 ## 8 0.9667 nan 0.1000 0.0103 ## 9 0.9406 nan 0.1000 0.0113 ## 10 0.9180 nan 0.1000 0.0068 ## 20 0.7990 nan 0.1000 0.0016 ## 40 0.7272 nan 0.1000 -0.0017 ## 60 0.6907 nan 0.1000 -0.0022 ## 80 0.6700 nan 0.1000 -0.0014 ## 100 0.6504 nan 0.1000 -0.0011 ## 120 0.6350 nan 0.1000 -0.0018 ## 140 0.6234 nan 0.1000 -0.0019 ## 160 0.6082 nan 0.1000 -0.0014 ## 180 0.5987 nan 0.1000 -0.0034 ## 200 0.5848 nan 0.1000 -0.0022 ## 220 0.5746 nan 0.1000 -0.0025 ## 240 0.5639 nan 0.1000 -0.0018 ## 250 0.5565 nan 0.1000 -0.0003 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2558 nan 0.1000 0.0387 ## 2 1.1883 nan 0.1000 0.0309 ## 3 1.1313 nan 0.1000 0.0259 ## 4 1.0867 nan 0.1000 0.0220 ## 5 1.0448 nan 0.1000 0.0210 ## 6 1.0088 nan 0.1000 0.0154 ## 7 0.9775 nan 0.1000 0.0152 ## 8 0.9428 nan 0.1000 0.0118 ## 9 0.9179 nan 0.1000 0.0092 ## 10 0.8962 nan 0.1000 0.0090 ## 20 0.7825 nan 0.1000 0.0013 ## 40 0.7073 nan 0.1000 -0.0023 ## 60 0.6686 nan 0.1000 -0.0027 ## 80 0.6423 nan 0.1000 -0.0014 ## 100 0.6154 nan 0.1000 -0.0029 ## 120 0.5965 nan 0.1000 -0.0012 ## 140 0.5753 nan 0.1000 -0.0015 ## 160 0.5601 nan 0.1000 -0.0019 ## 180 0.5447 nan 0.1000 -0.0017 ## 200 0.5315 nan 0.1000 -0.0012 ## 220 0.5192 nan 0.1000 -0.0019 ## 240 0.5022 nan 0.1000 -0.0015 ## 250 0.4979 nan 0.1000 -0.0020 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2433 nan 0.1000 0.0391 ## 2 1.1705 nan 0.1000 0.0324 ## 3 1.1103 nan 0.1000 0.0279 ## 4 1.0628 nan 0.1000 0.0183 ## 5 1.0198 nan 0.1000 0.0182 ## 6 0.9854 nan 0.1000 0.0126 ## 7 0.9561 nan 0.1000 0.0118 ## 8 0.9272 nan 0.1000 0.0122 ## 9 0.9025 nan 0.1000 0.0084 ## 10 0.8811 nan 0.1000 0.0073 ## 20 0.7568 nan 0.1000 0.0016 ## 40 0.6802 nan 0.1000 -0.0015 ## 60 0.6427 nan 0.1000 -0.0018 ## 80 0.6116 nan 0.1000 -0.0025 ## 100 0.5804 nan 0.1000 -0.0025 ## 120 0.5581 nan 0.1000 -0.0016 ## 140 0.5340 nan 0.1000 -0.0013 ## 160 0.5115 nan 0.1000 -0.0024 ## 180 0.4936 nan 0.1000 -0.0025 ## 200 0.4790 nan 0.1000 -0.0016 ## 220 0.4646 nan 0.1000 -0.0014 ## 240 0.4497 nan 0.1000 -0.0019 ## 250 0.4417 nan 0.1000 -0.0023 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2763 nan 0.1000 0.0311 ## 2 1.2274 nan 0.1000 0.0247 ## 3 1.1846 nan 0.1000 0.0193 ## 4 1.1482 nan 0.1000 0.0174 ## 5 1.1193 nan 0.1000 0.0126 ## 6 1.0906 nan 0.1000 0.0107 ## 7 1.0706 nan 0.1000 0.0090 ## 8 1.0499 nan 0.1000 0.0084 ## 9 1.0323 nan 0.1000 0.0073 ## 10 1.0162 nan 0.1000 0.0062 ## 20 0.9200 nan 0.1000 0.0018 ## 40 0.8283 nan 0.1000 0.0011 ## 60 0.7906 nan 0.1000 0.0001 ## 80 0.7758 nan 0.1000 -0.0013 ## 100 0.7668 nan 0.1000 -0.0001 ## 120 0.7594 nan 0.1000 -0.0006 ## 140 0.7563 nan 0.1000 -0.0011 ## 160 0.7528 nan 0.1000 -0.0003 ## 180 0.7490 nan 0.1000 -0.0008 ## 200 0.7429 nan 0.1000 -0.0003 ## 220 0.7378 nan 0.1000 -0.0009 ## 240 0.7346 nan 0.1000 -0.0012 ## 250 0.7317 nan 0.1000 -0.0009 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2643 nan 0.1000 0.0325 ## 2 1.2094 nan 0.1000 0.0251 ## 3 1.1601 nan 0.1000 0.0231 ## 4 1.1150 nan 0.1000 0.0206 ## 5 1.0822 nan 0.1000 0.0159 ## 6 1.0496 nan 0.1000 0.0142 ## 7 1.0238 nan 0.1000 0.0113 ## 8 0.9958 nan 0.1000 0.0115 ## 9 0.9767 nan 0.1000 0.0070 ## 10 0.9586 nan 0.1000 0.0066 ## 20 0.8444 nan 0.1000 0.0030 ## 40 0.7691 nan 0.1000 0.0003 ## 60 0.7425 nan 0.1000 -0.0016 ## 80 0.7204 nan 0.1000 0.0000 ## 100 0.7076 nan 0.1000 -0.0016 ## 120 0.6923 nan 0.1000 -0.0015 ## 140 0.6812 nan 0.1000 -0.0010 ## 160 0.6730 nan 0.1000 -0.0010 ## 180 0.6633 nan 0.1000 -0.0011 ## 200 0.6530 nan 0.1000 -0.0016 ## 220 0.6440 nan 0.1000 -0.0007 ## 240 0.6342 nan 0.1000 -0.0008 ## 250 0.6291 nan 0.1000 -0.0005 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2609 nan 0.1000 0.0393 ## 2 1.1974 nan 0.1000 0.0318 ## 3 1.1398 nan 0.1000 0.0239 ## 4 1.0909 nan 0.1000 0.0216 ## 5 1.0540 nan 0.1000 0.0178 ## 6 1.0179 nan 0.1000 0.0156 ## 7 0.9858 nan 0.1000 0.0134 ## 8 0.9578 nan 0.1000 0.0105 ## 9 0.9392 nan 0.1000 0.0080 ## 10 0.9187 nan 0.1000 0.0094 ## 20 0.8029 nan 0.1000 0.0003 ## 40 0.7280 nan 0.1000 -0.0006 ## 60 0.6966 nan 0.1000 -0.0014 ## 80 0.6696 nan 0.1000 -0.0015 ## 100 0.6510 nan 0.1000 -0.0014 ## 120 0.6336 nan 0.1000 -0.0007 ## 140 0.6195 nan 0.1000 -0.0010 ## 160 0.6058 nan 0.1000 -0.0017 ## 180 0.5919 nan 0.1000 -0.0004 ## 200 0.5793 nan 0.1000 -0.0006 ## 220 0.5709 nan 0.1000 -0.0012 ## 240 0.5592 nan 0.1000 -0.0013 ## 250 0.5541 nan 0.1000 -0.0019 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2519 nan 0.1000 0.0412 ## 2 1.1837 nan 0.1000 0.0334 ## 3 1.1256 nan 0.1000 0.0275 ## 4 1.0792 nan 0.1000 0.0198 ## 5 1.0409 nan 0.1000 0.0173 ## 6 1.0113 nan 0.1000 0.0118 ## 7 0.9773 nan 0.1000 0.0149 ## 8 0.9492 nan 0.1000 0.0113 ## 9 0.9259 nan 0.1000 0.0085 ## 10 0.9026 nan 0.1000 0.0082 ## 20 0.7905 nan 0.1000 0.0018 ## 40 0.7084 nan 0.1000 -0.0012 ## 60 0.6664 nan 0.1000 -0.0013 ## 80 0.6278 nan 0.1000 -0.0010 ## 100 0.6054 nan 0.1000 -0.0017 ## 120 0.5855 nan 0.1000 -0.0015 ## 140 0.5667 nan 0.1000 -0.0011 ## 160 0.5493 nan 0.1000 -0.0014 ## 180 0.5302 nan 0.1000 -0.0025 ## 200 0.5133 nan 0.1000 -0.0013 ## 220 0.4994 nan 0.1000 -0.0017 ## 240 0.4872 nan 0.1000 -0.0015 ## 250 0.4796 nan 0.1000 -0.0015 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2560 nan 0.1000 0.0382 ## 2 1.1882 nan 0.1000 0.0328 ## 3 1.1247 nan 0.1000 0.0294 ## 4 1.0700 nan 0.1000 0.0239 ## 5 1.0237 nan 0.1000 0.0193 ## 6 0.9914 nan 0.1000 0.0133 ## 7 0.9628 nan 0.1000 0.0120 ## 8 0.9349 nan 0.1000 0.0090 ## 9 0.9092 nan 0.1000 0.0067 ## 10 0.8910 nan 0.1000 0.0062 ## 20 0.7725 nan 0.1000 0.0020 ## 40 0.6814 nan 0.1000 -0.0020 ## 60 0.6337 nan 0.1000 -0.0020 ## 80 0.5988 nan 0.1000 -0.0020 ## 100 0.5675 nan 0.1000 -0.0012 ## 120 0.5419 nan 0.1000 -0.0020 ## 140 0.5210 nan 0.1000 -0.0012 ## 160 0.5011 nan 0.1000 -0.0023 ## 180 0.4834 nan 0.1000 -0.0023 ## 200 0.4674 nan 0.1000 -0.0014 ## 220 0.4517 nan 0.1000 -0.0019 ## 240 0.4409 nan 0.1000 -0.0019 ## 250 0.4344 nan 0.1000 -0.0018 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2715 nan 0.1000 0.0331 ## 2 1.2172 nan 0.1000 0.0265 ## 3 1.1717 nan 0.1000 0.0218 ## 4 1.1340 nan 0.1000 0.0169 ## 5 1.1041 nan 0.1000 0.0128 ## 6 1.0732 nan 0.1000 0.0129 ## 7 1.0504 nan 0.1000 0.0107 ## 8 1.0272 nan 0.1000 0.0096 ## 9 1.0078 nan 0.1000 0.0079 ## 10 0.9909 nan 0.1000 0.0058 ## 20 0.8935 nan 0.1000 0.0022 ## 40 0.8028 nan 0.1000 -0.0001 ## 60 0.7589 nan 0.1000 -0.0011 ## 80 0.7387 nan 0.1000 -0.0003 ## 100 0.7263 nan 0.1000 -0.0002 ## 120 0.7215 nan 0.1000 -0.0007 ## 140 0.7171 nan 0.1000 -0.0006 ## 160 0.7135 nan 0.1000 -0.0014 ## 180 0.7071 nan 0.1000 -0.0012 ## 200 0.7025 nan 0.1000 -0.0007 ## 220 0.6974 nan 0.1000 -0.0006 ## 240 0.6949 nan 0.1000 -0.0010 ## 250 0.6936 nan 0.1000 -0.0008 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2589 nan 0.1000 0.0361 ## 2 1.1977 nan 0.1000 0.0276 ## 3 1.1441 nan 0.1000 0.0265 ## 4 1.1042 nan 0.1000 0.0196 ## 5 1.0621 nan 0.1000 0.0167 ## 6 1.0261 nan 0.1000 0.0141 ## 7 1.0012 nan 0.1000 0.0116 ## 8 0.9734 nan 0.1000 0.0109 ## 9 0.9478 nan 0.1000 0.0095 ## 10 0.9294 nan 0.1000 0.0086 ## 20 0.8129 nan 0.1000 0.0034 ## 40 0.7275 nan 0.1000 -0.0007 ## 60 0.7015 nan 0.1000 0.0002 ## 80 0.6844 nan 0.1000 -0.0010 ## 100 0.6700 nan 0.1000 -0.0016 ## 120 0.6543 nan 0.1000 -0.0010 ## 140 0.6461 nan 0.1000 -0.0006 ## 160 0.6361 nan 0.1000 -0.0009 ## 180 0.6258 nan 0.1000 -0.0006 ## 200 0.6156 nan 0.1000 -0.0023 ## 220 0.6062 nan 0.1000 -0.0017 ## 240 0.5951 nan 0.1000 -0.0011 ## 250 0.5933 nan 0.1000 -0.0019 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2560 nan 0.1000 0.0380 ## 2 1.1901 nan 0.1000 0.0320 ## 3 1.1322 nan 0.1000 0.0268 ## 4 1.0833 nan 0.1000 0.0224 ## 5 1.0430 nan 0.1000 0.0179 ## 6 1.0024 nan 0.1000 0.0188 ## 7 0.9656 nan 0.1000 0.0154 ## 8 0.9387 nan 0.1000 0.0111 ## 9 0.9127 nan 0.1000 0.0103 ## 10 0.8870 nan 0.1000 0.0101 ## 20 0.7604 nan 0.1000 0.0014 ## 40 0.6862 nan 0.1000 -0.0010 ## 60 0.6558 nan 0.1000 -0.0011 ## 80 0.6373 nan 0.1000 -0.0020 ## 100 0.6167 nan 0.1000 -0.0028 ## 120 0.5995 nan 0.1000 -0.0007 ## 140 0.5854 nan 0.1000 -0.0014 ## 160 0.5702 nan 0.1000 -0.0013 ## 180 0.5597 nan 0.1000 -0.0024 ## 200 0.5458 nan 0.1000 -0.0017 ## 220 0.5299 nan 0.1000 -0.0007 ## 240 0.5210 nan 0.1000 -0.0017 ## 250 0.5158 nan 0.1000 -0.0021 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2540 nan 0.1000 0.0396 ## 2 1.1751 nan 0.1000 0.0348 ## 3 1.1172 nan 0.1000 0.0243 ## 4 1.0668 nan 0.1000 0.0216 ## 5 1.0185 nan 0.1000 0.0230 ## 6 0.9814 nan 0.1000 0.0176 ## 7 0.9460 nan 0.1000 0.0161 ## 8 0.9120 nan 0.1000 0.0127 ## 9 0.8872 nan 0.1000 0.0112 ## 10 0.8636 nan 0.1000 0.0100 ## 20 0.7438 nan 0.1000 0.0006 ## 40 0.6630 nan 0.1000 -0.0007 ## 60 0.6264 nan 0.1000 -0.0014 ## 80 0.5998 nan 0.1000 -0.0014 ## 100 0.5756 nan 0.1000 -0.0012 ## 120 0.5593 nan 0.1000 -0.0016 ## 140 0.5415 nan 0.1000 -0.0006 ## 160 0.5199 nan 0.1000 -0.0020 ## 180 0.5069 nan 0.1000 -0.0014 ## 200 0.4916 nan 0.1000 -0.0020 ## 220 0.4795 nan 0.1000 -0.0025 ## 240 0.4676 nan 0.1000 -0.0019 ## 250 0.4592 nan 0.1000 -0.0012 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2474 nan 0.1000 0.0402 ## 2 1.1733 nan 0.1000 0.0343 ## 3 1.1026 nan 0.1000 0.0290 ## 4 1.0494 nan 0.1000 0.0228 ## 5 1.0054 nan 0.1000 0.0192 ## 6 0.9667 nan 0.1000 0.0177 ## 7 0.9356 nan 0.1000 0.0141 ## 8 0.9057 nan 0.1000 0.0125 ## 9 0.8742 nan 0.1000 0.0129 ## 10 0.8514 nan 0.1000 0.0093 ## 20 0.7278 nan 0.1000 0.0014 ## 40 0.6492 nan 0.1000 -0.0019 ## 60 0.6081 nan 0.1000 -0.0021 ## 80 0.5711 nan 0.1000 -0.0018 ## 100 0.5428 nan 0.1000 -0.0033 ## 120 0.5211 nan 0.1000 -0.0030 ## 140 0.4982 nan 0.1000 -0.0013 ## 160 0.4759 nan 0.1000 -0.0007 ## 180 0.4587 nan 0.1000 -0.0015 ## 200 0.4430 nan 0.1000 -0.0010 ## 220 0.4284 nan 0.1000 -0.0008 ## 240 0.4158 nan 0.1000 -0.0012 ## 250 0.4072 nan 0.1000 -0.0029 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2614 nan 0.1000 0.0341 ## 2 1.1926 nan 0.1000 0.0318 ## 3 1.1367 nan 0.1000 0.0262 ## 4 1.0951 nan 0.1000 0.0204 ## 5 1.0568 nan 0.1000 0.0181 ## 6 1.0211 nan 0.1000 0.0187 ## 7 0.9907 nan 0.1000 0.0123 ## 8 0.9627 nan 0.1000 0.0122 ## 9 0.9379 nan 0.1000 0.0116 ## 10 0.9161 nan 0.1000 0.0089 ## 20 0.7971 nan 0.1000 0.0007 ## 40 0.7338 nan 0.1000 -0.0030 ## 60 0.7006 nan 0.1000 -0.0009 ## 80 0.6799 nan 0.1000 -0.0011 ## 100 0.6613 nan 0.1000 -0.0015 ## 120 0.6434 nan 0.1000 -0.0003 ## 140 0.6228 nan 0.1000 -0.0006 ## 160 0.6101 nan 0.1000 -0.0006 ## 180 0.5951 nan 0.1000 -0.0015 ## 200 0.5828 nan 0.1000 -0.0009 oj_mdl_gbm ## Stochastic Gradient Boosting ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... ## Resampling results across tuning parameters: ## ## interaction.depth n.trees Accuracy Kappa ## 1 50 0.8097901 0.5929519 ## 1 100 0.8215001 0.6195920 ## 1 150 0.8273414 0.6327182 ## 1 200 0.8249884 0.6276335 ## 1 250 0.8191879 0.6145884 ## 2 50 0.8261376 0.6300827 ## 2 100 0.8238530 0.6252885 ## 2 150 0.8179977 0.6127957 ## 2 200 0.8203236 0.6180661 ## 2 250 0.8133195 0.6047832 ## 3 50 0.8262060 0.6313198 ## 3 100 0.8261923 0.6328516 ## 3 150 0.8203242 0.6196244 ## 3 200 0.8285045 0.6365233 ## 3 250 0.8214730 0.6220406 ## 4 50 0.8109532 0.6005857 ## 4 100 0.8214733 0.6219633 ## 4 150 0.8202695 0.6209094 ## 4 200 0.8179576 0.6148779 ## 4 250 0.8086139 0.5957869 ## 5 50 0.8226632 0.6245795 ## 5 100 0.8109805 0.5990268 ## 5 150 0.8133609 0.6058617 ## 5 200 0.8063157 0.5905055 ## 5 250 0.8040172 0.5867695 ## ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## ## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were n.trees = 200, interaction.depth = ## 3, shrinkage = 0.1 and n.minobsinnode = 10. train() tuned n.trees ($M) and interaction.depth, holding shrinkage = 0.1 (), and n.minobsinnode = 10. The optimal hyperparameter values were n.trees = 200, and interaction.depth = 3. You can see from the tuning plot that accuracy is maximized at \\(M=200\\) for tree depth of 3, but \\(M=150\\) with tree depth of 1 worked nearly as well. plot(oj_mdl_gbm) Let’s see how the model performed on the holdout set. oj_preds_gbm &lt;- bind_cols( predict(oj_mdl_gbm, newdata = oj_test, type = &quot;prob&quot;), Predicted = predict(oj_mdl_gbm, newdata = oj_test, type = &quot;raw&quot;), Actual = oj_test$Purchase ) oj_cm_gbm &lt;- confusionMatrix(oj_preds_gbm$Predicted, reference = oj_preds_gbm$Actual) oj_cm_gbm ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 114 16 ## MM 16 67 ## ## Accuracy : 0.8498 ## 95% CI : (0.7946, 0.8949) ## No Information Rate : 0.6103 ## P-Value [Acc &gt; NIR] : 1.778e-14 ## ## Kappa : 0.6842 ## ## Mcnemar&#39;s Test P-Value : 1 ## ## Sensitivity : 0.8769 ## Specificity : 0.8072 ## Pos Pred Value : 0.8769 ## Neg Pred Value : 0.8072 ## Prevalence : 0.6103 ## Detection Rate : 0.5352 ## Detection Prevalence : 0.6103 ## Balanced Accuracy : 0.8421 ## ## &#39;Positive&#39; Class : CH ## GBM improved upon boosting and random forest with accuracy of 0.8498. mdl_auc &lt;- Metrics::auc(actual = oj_preds_gbm$Actual == &quot;CH&quot;, oj_preds_bag$CH) yardstick::roc_curve(oj_preds_gbm, Actual, CH) %&gt;% autoplot() + labs( title = &quot;OJ GBM ROC Curve&quot;, subtitle = paste0(&quot;AUC = &quot;, round(mdl_auc, 4)) ) yardstick::gain_curve(oj_preds_gbm, Actual, CH) %&gt;% autoplot() + labs(title = &quot;OJ GBM Gain Curve&quot;) plot(varImp(oj_mdl_bag), main=&quot;Variable Importance with Bagging&quot;) scoreboard &lt;- rbind( scoreboard, data.frame(model = &quot;GBM&quot;, Acc = round(oj_cm_gbm$overall[&quot;Accuracy&quot;], 5)) ) scoreboard %&gt;% arrange(desc(Acc)) ## model Acc ## Accuracy Manual Class 0.85915 ## Accuracy4 GBM 0.84977 ## Accuracy1 Caret w/tuneGrid 0.84507 ## Accuracy2 Bagging 0.84507 ## Accuracy3 Random Forest 0.82629 9.5.0.2 Gradient Boosting Regression Example I’ll predict Sales from the Carseats data set again, this time using the bagging method by specifying method = \"gbm\" set.seed(1234) garbage &lt;- capture.output( cs_mdl_gbm &lt;- train( Sales ~ ., data = cs_train, method = &quot;gbm&quot;, tuneLength = 5, trControl = cs_trControl )) cs_mdl_gbm ## Stochastic Gradient Boosting ## ## 321 samples ## 10 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... ## Resampling results across tuning parameters: ## ## interaction.depth n.trees RMSE Rsquared MAE ## 1 50 1.842468 0.6718370 1.4969754 ## 1 100 1.516967 0.7823612 1.2407807 ## 1 150 1.309295 0.8277888 1.0639501 ## 1 200 1.216079 0.8429002 0.9866820 ## 1 250 1.161540 0.8488463 0.9384418 ## 2 50 1.527454 0.7801995 1.2207991 ## 2 100 1.240990 0.8381156 1.0063802 ## 2 150 1.187603 0.8415216 0.9616681 ## 2 200 1.174303 0.8425011 0.9527720 ## 2 250 1.172116 0.8403490 0.9500902 ## 3 50 1.390969 0.8071393 1.1316570 ## 3 100 1.227525 0.8321632 0.9888203 ## 3 150 1.201264 0.8345775 0.9694065 ## 3 200 1.214462 0.8282833 0.9761625 ## 3 250 1.232145 0.8221405 0.9882254 ## 4 50 1.341893 0.8128778 1.0949502 ## 4 100 1.252282 0.8230712 0.9907410 ## 4 150 1.243045 0.8229433 0.9860813 ## 4 200 1.258093 0.8162033 0.9947218 ## 4 250 1.271058 0.8114156 1.0144873 ## 5 50 1.318251 0.8128033 1.0552929 ## 5 100 1.250053 0.8226441 0.9958713 ## 5 150 1.248402 0.8214824 0.9888330 ## 5 200 1.263445 0.8158033 1.0106345 ## 5 250 1.273024 0.8124672 1.0213099 ## ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## ## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were n.trees = 250, interaction.depth = ## 1, shrinkage = 0.1 and n.minobsinnode = 10. The optimal tuning parameters were at \\(M = 250\\) and interation.depth = 1. plot(cs_mdl_gbm) Here is the holdout set peformance. cs_preds_gbm &lt;- bind_cols( Predicted = predict(cs_mdl_gbm, newdata = cs_test), Actual = cs_test$Sales ) # Model over-predicts at low end of Sales and under-predicts at high end cs_preds_gbm %&gt;% ggplot(aes(x = Actual, y = Predicted)) + geom_point(alpha = 0.6, color = &quot;cadetblue&quot;) + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;) + geom_abline(intercept = 0, slope = 1, linetype = 2) + labs(title = &quot;Carseats GBM, Predicted vs Actual&quot;) The RMSE is 1.438 - the best of the bunch. # RMSE of 1.7184 is better than bagging&#39;s 1.9185. cs_rmse_gbm &lt;- RMSE(pred = cs_preds_gbm$Predicted, obs = cs_preds_gbm$Actual) scoreboard_r &lt;- rbind( scoreboard_r, data.frame(model = &quot;GBM&quot;, RMSE = round(cs_rmse_gbm, 5)) ) scoreboard_r %&gt;% arrange(RMSE) ## model RMSE ## 1 GBM 1.43806 ## 2 Random Forest 1.71836 ## 3 Bagging 1.91847 ## 4 Caret w/tuneGrid 2.29833 ## 5 Manual ANOVA 2.36320 "],
["summary.html", "9.6 Summary", " 9.6 Summary resamps &lt;- resamples(list(&#39;CART&#39; = oj_mdl_cart2, &#39;Bagging&#39; = oj_mdl_bag, &#39;Random Forest&#39; = oj_mdl_bag, &#39;GBM&#39; = oj_mdl_gbm)) summary(resamps) ## ## Call: ## summary.resamples(object = resamps) ## ## Models: CART, Bagging, Random Forest, GBM ## Number of resamples: 10 ## ## Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## CART 0.7058824 0.7936047 0.8314402 0.8120883 0.8441176 0.8837209 0 ## Bagging 0.7294118 0.7888509 0.8011628 0.7945774 0.8116145 0.8255814 0 ## Random Forest 0.7294118 0.7888509 0.8011628 0.7945774 0.8116145 0.8255814 0 ## GBM 0.7058824 0.8139535 0.8439588 0.8285045 0.8651505 0.8941176 0 ## ## Kappa ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## CART 0.3773806 0.5635201 0.6386667 0.6011446 0.6743840 0.7517321 0 ## Bagging 0.4334975 0.5540262 0.5716526 0.5653649 0.6090299 0.6345658 0 ## Random Forest 0.4334975 0.5540262 0.5716526 0.5653649 0.6090299 0.6345658 0 ## GBM 0.3703704 0.6006641 0.6738871 0.6365233 0.7203674 0.7783251 0 bwplot(resamps) dotplot(resamps, metric = &quot;Accuracy&quot;) Compute the differences among models and use a t-test to evaluate the null hypothesis that there is no difference between models. (difValues &lt;- diff(resamps)) ## ## Call: ## diff.resamples(x = resamps) ## ## Models: CART, Bagging, Random Forest, GBM ## Metrics: Accuracy, Kappa ## Number of differences: 6 ## p-value adjustment: bonferroni summary(difValues) ## ## Call: ## summary.diff.resamples(object = difValues) ## ## p-value adjustment: bonferroni ## Upper diagonal: estimates of the difference ## Lower diagonal: p-value for H0: difference = 0 ## ## Accuracy ## CART Bagging Random Forest GBM ## CART 0.01751 0.01751 -0.01642 ## Bagging 0.5526 0.00000 -0.03393 ## Random Forest 0.5526 NA -0.03393 ## GBM 0.7832 0.1218 0.1218 ## ## Kappa ## CART Bagging Random Forest GBM ## CART 0.03578 0.03578 -0.03538 ## Bagging 0.5617 0.00000 -0.07116 ## Random Forest 0.5617 NA -0.07116 ## GBM 0.7735 0.1466 0.1466 bwplot(difValues, layout = c(3, 1)) resamps &lt;- resamples(list(&#39;CART&#39; = cs_mdl_cart2, &#39;Bagging&#39; = cs_mdl_bag, &#39;Random Forest&#39; = cs_mdl_bag, &#39;GBM&#39; = cs_mdl_gbm)) summary(resamps) ## ## Call: ## summary.resamples(object = resamps) ## ## Models: CART, Bagging, Random Forest, GBM ## Number of resamples: 10 ## ## MAE ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## CART 1.3654588 1.5880059 1.7313313 1.6954527 1.799792 2.064407 0 ## Bagging 0.9303614 1.1254345 1.3464588 1.3434268 1.571260 1.737744 0 ## Random Forest 0.9303614 1.1254345 1.3464588 1.3434268 1.571260 1.737744 0 ## GBM 0.7158107 0.8573375 0.9156746 0.9384418 1.043579 1.113679 0 ## ## RMSE ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## CART 1.694854 1.902683 2.038878 2.055676 2.178660 2.496287 0 ## Bagging 1.209929 1.405502 1.693264 1.681889 1.979521 2.069958 0 ## Random Forest 1.209929 1.405502 1.693264 1.681889 1.979521 2.069958 0 ## GBM 0.930552 1.082300 1.159712 1.161540 1.271790 1.390241 0 ## ## Rsquared ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## CART 0.3858765 0.4977272 0.5185928 0.5027431 0.5272691 0.5852449 0 ## Bagging 0.5280595 0.6322136 0.6805770 0.6752390 0.7281293 0.7848460 0 ## Random Forest 0.5280595 0.6322136 0.6805770 0.6752390 0.7281293 0.7848460 0 ## GBM 0.8176434 0.8319850 0.8536650 0.8488463 0.8628293 0.8721381 0 bwplot(resamps) dotplot(resamps, metric = &quot;RMSE&quot;) Compute the differences among models and use a t-test to evaluate the null hypothesis that there is no difference between models. (difValues &lt;- diff(resamps)) ## ## Call: ## diff.resamples(x = resamps) ## ## Models: CART, Bagging, Random Forest, GBM ## Metrics: MAE, RMSE, Rsquared ## Number of differences: 6 ## p-value adjustment: bonferroni summary(difValues) ## ## Call: ## summary.diff.resamples(object = difValues) ## ## p-value adjustment: bonferroni ## Upper diagonal: estimates of the difference ## Lower diagonal: p-value for H0: difference = 0 ## ## MAE ## CART Bagging Random Forest GBM ## CART 0.352 0.352 0.757 ## Bagging 0.0004408 0.000 0.405 ## Random Forest 0.0004408 NA 0.405 ## GBM 1.134e-06 0.0036058 0.0036058 ## ## RMSE ## CART Bagging Random Forest GBM ## CART 0.3738 0.3738 0.8941 ## Bagging 0.0001734 0.0000 0.5203 ## Random Forest 0.0001734 NA 0.5203 ## GBM 3.338e-07 0.0003272 0.0003272 ## ## Rsquared ## CART Bagging Random Forest GBM ## CART -0.1725 -0.1725 -0.3461 ## Bagging 0.0001668 0.0000 -0.1736 ## Random Forest 0.0001668 NA -0.1736 ## GBM 1.468e-07 0.0006759 0.0006759 bwplot(difValues, layout = c(3, 1)) "],
["reference-1.html", "9.7 Reference", " 9.7 Reference Penn State University, STAT 508: Applied Data Mining and Statistical Learning, “Lesson 11: Tree-based Methods”. https://newonlinecourses.science.psu.edu/stat508/lesson/11. Brownlee, Jason. “Classification And Regression Trees for Machine Learning”, Machine Learning Mastery. https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/. Brownlee, Jason. “A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning”, Machine Learning Mastery. https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/. DataCamp: Machine Learning with Tree-Based Models in R An Introduction to Statistical Learning by Gareth James, et al. SAS Documentation StatMethods: Tree-Based Models Machine Learning Plus GBM (Boosted Models) Tuning Parameters from Listen Data Harry Southworth on GitHub Gradient Boosting Classification with GBM in R in DataTechNotes Molnar, Christoph. “Interpretable machine learning. A Guide for Making Black Box Models Explainable”, 2019. https://christophm.github.io/interpretable-ml-book/. "]
]
