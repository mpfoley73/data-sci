[
["cluster-analysis.html", "Chapter 11 Cluster Analysis", " Chapter 11 Cluster Analysis These note are primarily taken from the DataCamp courses Cluster Analysis in R and Unsupervised Learning in R, AIHR, and the UC Business Analytics R Programming Guide. Unsupervised machine learning searches for structure in unlabeled data (data without a response variable). The goal of unsupervised learning is clustering into homogenous subgroups, and dimensionality reduction. Examples of cluster analysis are k-means clustering and hierarchical cluster analysis (HCA) (others here). Clustering is used for audience segmentation, creating personas, detecting anomalies, and pattern recognition in images. I will learn by example, using the IBM HR Analytics Employee Attrition &amp; Performance data set from Kaggle to discover what factors are associated with employee turnover and whether distinct clusters of employees are more susceptible to turnover. The clusters can help personalize employee experience (AIHR). This data set includes 1,470 employee records consisting of the EmployeeNumber, a flag for Attrition during some timeframe, and 32 other descriptive variables. library(tidyverse) library(correlationfunnel) # rapid exploratory data analysis ## Warning: package &#39;correlationfunnel&#39; was built under R version 4.0.2 library(cluster) # daisy and pam library(Rtsne) # dimensionality reduction and visualization ## Warning: package &#39;Rtsne&#39; was built under R version 4.0.2 library(plotly) # interactive graphing ## Warning: package &#39;plotly&#39; was built under R version 4.0.2 library(dendextend) # color_branches ## Warning: package &#39;dendextend&#39; was built under R version 4.0.2 set.seed(1234) # reproducibility dat &lt;- read_csv(&quot;./input/WA_Fn-UseC_-HR-Employee-Attrition.csv&quot;) my_skim &lt;- skimr::skim_with(numeric = skimr::sfl(p25 = NULL, p50 = NULL, p75 = NULL, hist = NULL)) my_skim(dat) Table 11.1: Data summary Name dat Number of rows 1470 Number of columns 35 _______________________ Column type frequency: character 9 numeric 26 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace Attrition 0 1 2 3 0 2 0 BusinessTravel 0 1 10 17 0 3 0 Department 0 1 5 22 0 3 0 EducationField 0 1 5 16 0 6 0 Gender 0 1 4 6 0 2 0 JobRole 0 1 7 25 0 9 0 MaritalStatus 0 1 6 8 0 3 0 Over18 0 1 1 1 0 1 0 OverTime 0 1 2 3 0 2 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p100 Age 0 1 36.92 9.14 18 60 DailyRate 0 1 802.49 403.51 102 1499 DistanceFromHome 0 1 9.19 8.11 1 29 Education 0 1 2.91 1.02 1 5 EmployeeCount 0 1 1.00 0.00 1 1 EmployeeNumber 0 1 1024.87 602.02 1 2068 EnvironmentSatisfaction 0 1 2.72 1.09 1 4 HourlyRate 0 1 65.89 20.33 30 100 JobInvolvement 0 1 2.73 0.71 1 4 JobLevel 0 1 2.06 1.11 1 5 JobSatisfaction 0 1 2.73 1.10 1 4 MonthlyIncome 0 1 6502.93 4707.96 1009 19999 MonthlyRate 0 1 14313.10 7117.79 2094 26999 NumCompaniesWorked 0 1 2.69 2.50 0 9 PercentSalaryHike 0 1 15.21 3.66 11 25 PerformanceRating 0 1 3.15 0.36 3 4 RelationshipSatisfaction 0 1 2.71 1.08 1 4 StandardHours 0 1 80.00 0.00 80 80 StockOptionLevel 0 1 0.79 0.85 0 3 TotalWorkingYears 0 1 11.28 7.78 0 40 TrainingTimesLastYear 0 1 2.80 1.29 0 6 WorkLifeBalance 0 1 2.76 0.71 1 4 YearsAtCompany 0 1 7.01 6.13 0 40 YearsInCurrentRole 0 1 4.23 3.62 0 18 YearsSinceLastPromotion 0 1 2.19 3.22 0 15 YearsWithCurrManager 0 1 4.12 3.57 0 17 "],
["distances.html", "11.1 Distances", " 11.1 Distances Central to clustering is the concept of distance. Two observations are similar if the distance between their features is relatively small. There are many ways to define distance1, but the two most common are Euclidean and binary. Euclidean distance is the distance between two quantitative vectors: \\(d = \\sqrt{\\sum{(x_i - y_i)^2}}\\). Binary distance is the distance between two binary vectors. It is 1 minus the proportion of shared features.2 In R, the dist(df, method = c(\"euclidean\", \"binary\", ...)) function calculates the distances between observations. When calculating a Euclidean distance, the features should be on similar scales. If they are not, standardize their values as \\((x - \\bar{x})) / sd(x)\\) so that each feature has a mean of 0 and standard deviation of 1.3 A quick way to check if scaling is necesary is the colmeans() function and apply(df, 2, sd). The scale() function is a generic function that scales the columns of a matrix. When calculating a binary distance, the categorical features should be binary. If they are not, create dummy variables. The dummy.data.frame() function from the dummies package converts factor variables into dummy representations.4 In practice, the clustering algorithm will calculate the distances, so you will not use dist(). However, the conditions on scales at binary features still apply. See the options in the Distance Matrix Computation (dist) documentation.↩︎ The binary distance is also called the Jaccard distance. See Wikipedia aricle.↩︎ Not only should the metrics be of the same units, but also the means and standard deviations should be similar.↩︎ Question: What if observations contain both quantitative and categorical features?↩︎ "],
["hca-clustering.html", "11.2 HCA Clustering", " 11.2 HCA Clustering Hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which builds a hierarchy of clusters. One usually presents the HCA results in a dendrogram. The HCA process is: Calculate the distance between each observation with dist(df, method = c(\"euclidean\", \"binary\"). dist() returns an object of class dist. Cluster the distances with hclust(dist, method = c(\"complete\", \"single\", \"average\", \"centroid\"). hclust groups the two closest observations into a cluster. hclust then calculates the cluster distance to the remaining observations. If the shortest distance is between two observations, hclust defines a second cluster, otherwise hclust adds the observation as a new level to the cluster. The process repeats until all observations belong to a single cluster. The “distance” to a cluster requires definition. The “complete” distance is the distance to the furthest member of the cluster. The “single” distance is the distance to the closest member of the cluster. The “average” distance is the average distance to all members of the cluster. The “centroid” distance is the distance between the centroids of each cluster.^[As a rule of thumb, “complete” and “average” tend to produce more balanced trees and are most common. Pruning an unbalanced tree can result in most observations assigned to one cluster and only a few observations assigned to other clusters. This is useful for identifying outliers. hclust() returns a value of class hclust. Evaluate the hclust tree with a dendogram, principal component analysis (PCA), and/or summary statistics. The vertical lines in a dendogram indicate the distance between nodes and their associated cluster. Dendograms are difficult to visualize When the number of features is greater than two.5 “Cut” the hierarchical tree into the desired number of clusters (k) or height h with cutree(hclust, k = NULL, h = NULL). cutree() returns a vector of cluster memberships. Attach this vector back to the original dataframe for visualization and summary statistics. Calculate summary statistics and draw conclusions. Useful summary statistics are typically membership count, and feature averages (or proportions). Example The pokemon dataset contains observations of 800 pokemons6 on 6 dimensions. The data is unlabeled, meaning there is no response variable, just features. The features here are six pokeon ability measures. pokemon &lt;- read_csv(url(&quot;https://assets.datacamp.com/production/course_1815/datasets/Pokemon.csv&quot;)) pokemon$Name &lt;- NULL pokemon$Type1 &lt;- NULL pokemon$Type2 &lt;- NULL pokemon$Total &lt;- NULL pokemon$Generation &lt;- NULL pokemon$Legendary &lt;- NULL head(pokemon) ## # A tibble: 6 x 7 ## Number HitPoints Attack Defense SpecialAttack SpecialDefense Speed ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 45 49 49 65 65 45 ## 2 2 60 62 63 80 80 60 ## 3 3 80 82 83 100 100 80 ## 4 3 80 100 123 122 120 80 ## 5 4 39 52 43 60 50 65 ## 6 5 58 64 58 80 65 80 Before conducting k-means, check whether any preprocessing is required: Are there any NAs? If so, drop these observations, or impute values. Are all of the features comparable? If not, standardize the variables. Are the features multi-nomial? If so, create binary variables. In this case, the means and standard deviations are similar, but I am scaling anyway for the exercise. # Means and SDs colMeans(pokemon[, -c(1)]) ## HitPoints Attack Defense SpecialAttack SpecialDefense ## 69.25875 79.00125 73.84250 72.82000 71.90250 ## Speed ## 68.27750 apply(pokemon[, -c(1)], MARGIN = 2, FUN = sd) ## HitPoints Attack Defense SpecialAttack SpecialDefense ## 25.53467 32.45737 31.18350 32.72229 27.82892 ## Speed ## 29.06047 # Scale the data pokemon.scaled &lt;- scale(pokemon) # Create the full tree hc_model &lt;- hclust(dist(pokemon.scaled), method = &quot;complete&quot;) # Inspect the tree to choose a size. plot(color_branches(as.dendrogram(hc_model), k = 7)) abline(h = 7, col = &quot;red&quot;) The dendogram suggests the optimal number of clusters is seven. Build a cluster with k = 7 means. Attach the cluster assignment vector back to the original dataframe for visualization and/or summary statistics. pokemon &lt;- mutate(pokemon, cluster = cutree(hc_model, k = 7)) # View the resulting model pokemon %&gt;% group_by(cluster) %&gt;% summarise_all(funs(mean(.))) %&gt;% select(-c(2)) %&gt;% knitr::kable(caption = &quot;Cluster Centers&quot;) Table 11.2: Cluster Centers cluster HitPoints Attack Defense SpecialAttack SpecialDefense Speed 1 52.02711 56.78614 54.91867 51.34337 53.05422 51.15663 2 78.06884 85.93478 80.06522 85.77899 85.34420 83.23188 3 83.79268 119.20732 85.53659 117.71951 94.53659 105.04878 4 79.86316 101.75789 114.28421 72.09474 78.25263 54.41053 5 167.41667 70.66667 47.66667 57.75000 72.25000 48.16667 6 20.00000 10.00000 230.00000 10.00000 230.00000 5.00000 7 50.00000 165.00000 35.00000 165.00000 35.00000 150.00000 pokemon %&gt;% gather(key = &quot;Ability&quot;, value = &quot;Score&quot;, c(HitPoints, Attack, Defense, SpecialAttack, SpecialDefense, Speed)) %&gt;% ggplot(aes(x = factor(Ability), y = Score, color = factor(cluster))) + geom_point(aes(group = Number)) Cluster 1 has the lowest values in all features. Cluster 5 has very high HitPoints. Cluster 3 has very high Special Attack. One work-around is to plot just two dimensions at a time.↩︎ More information on the dataset at https://www.kaggle.com/abcsds/pokemon↩︎ "],
["cluster-analysis-with-time-series-data.html", "11.3 Cluster Analysis with Time-Series Data", " 11.3 Cluster Analysis with Time-Series Data Cluster analysis is useful for spacial data, qualitative data, and time-series data. With time series data, the time periods are the features. Typically, this requires the transposition of the data set so that the dates are columns. Otherwise, the same rules apply. "],
["k-means.html", "11.4 K-Means", " 11.4 K-Means K-means clustering is a type of unsupervised machine learning. Unsupervised machine learning searches for structure (homogenous subgroups) within unlabeled data.7 Hierarchical cluster analysis (HCA), and principal component analysis (PCA) are other types of unsupervised machine learning. Unsupervised machine learning contrasts with supervised machine learning. Supervised machine learning makes predictions with labeled data. Decision trees, random forests, and lasso regression are examples of supervised machine learning. Unlabeled data has no defined predefined classification. I.e., unlabeled data has no response variable↩︎ "],
["k-means-algorithm.html", "11.5 K-means Algorithm", " 11.5 K-means Algorithm The K-means clustering algorithm randomly assigns all observations to one of k cluster identifiers. K-means then iteratively calculates the cluster centroids and reassigns the observations to their nearest centroid. The centroid is the mean of the points in the cluster8 The iterations continue until the centroid values stabilize or until the algorithm iterates a set maximum number of times, iter.max. A typical maximum is iter.max = 50. The result is k clusters with the minimum number of total intra-cluster variation. The centroid of cluster \\(c_i \\in C\\) is the mean of the set of cluster observations \\(S_i\\): \\(c_i = \\frac{1}{|S_i|} \\sum_{x_i \\in S_i}{x_i}\\). The nearest centroid is the minimum squared Euclidean distance, \\(\\underset{c_i \\in C}{\\operatorname{arg min}} dist(c_i, x)^2\\).9 Euclidean distance is the distance between two quantitative vectors: \\(dist(c_i, x) = \\sqrt{\\sum{(x_j - c_{i_j})^2}}\\).10 The algorithm will converge to a result, but the result may only be a local optimum. Other random starting centroids may yield a different local optimum. The best set of clusters is the one with the lowest within-cluster sum of squared distances among the cluster members. Common practice is to run the k-means algorithm nstart times and selecting the best result. A typical number of runs is nstart = 20. Learning by Example The general methodology is to 1) prepare the data, 2) choose the optimal number of clusters, 3) run the k-means algorithm, and 4) interpret the results. A good way to learn the methodology is by example. The WisconsinCancer data set contains 30 features of tumors (10 attributes with mean, se, and “worst” values) in n = 569 patients as well as the diagnosis (B = benign, M = malignant). wisc.df &lt;- read_csv(file = &quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1903/datasets/WisconsinCancer.csv&quot;) glimpse(wisc.df) ## Rows: 569 ## Columns: 33 ## $ id &lt;dbl&gt; 842302, 842517, 84300903, 84348301, 8435840... ## $ diagnosis &lt;chr&gt; &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;... ## $ radius_mean &lt;dbl&gt; 17.990, 20.570, 19.690, 11.420, 20.290, 12.... ## $ texture_mean &lt;dbl&gt; 10.38, 17.77, 21.25, 20.38, 14.34, 15.70, 1... ## $ perimeter_mean &lt;dbl&gt; 122.80, 132.90, 130.00, 77.58, 135.10, 82.5... ## $ area_mean &lt;dbl&gt; 1001.0, 1326.0, 1203.0, 386.1, 1297.0, 477.... ## $ smoothness_mean &lt;dbl&gt; 0.11840, 0.08474, 0.10960, 0.14250, 0.10030... ## $ compactness_mean &lt;dbl&gt; 0.27760, 0.07864, 0.15990, 0.28390, 0.13280... ## $ concavity_mean &lt;dbl&gt; 0.30010, 0.08690, 0.19740, 0.24140, 0.19800... ## $ `concave points_mean` &lt;dbl&gt; 0.14710, 0.07017, 0.12790, 0.10520, 0.10430... ## $ symmetry_mean &lt;dbl&gt; 0.2419, 0.1812, 0.2069, 0.2597, 0.1809, 0.2... ## $ fractal_dimension_mean &lt;dbl&gt; 0.07871, 0.05667, 0.05999, 0.09744, 0.05883... ## $ radius_se &lt;dbl&gt; 1.0950, 0.5435, 0.7456, 0.4956, 0.7572, 0.3... ## $ texture_se &lt;dbl&gt; 0.9053, 0.7339, 0.7869, 1.1560, 0.7813, 0.8... ## $ perimeter_se &lt;dbl&gt; 8.589, 3.398, 4.585, 3.445, 5.438, 2.217, 3... ## $ area_se &lt;dbl&gt; 153.40, 74.08, 94.03, 27.23, 94.44, 27.19, ... ## $ smoothness_se &lt;dbl&gt; 0.006399, 0.005225, 0.006150, 0.009110, 0.0... ## $ compactness_se &lt;dbl&gt; 0.049040, 0.013080, 0.040060, 0.074580, 0.0... ## $ concavity_se &lt;dbl&gt; 0.05373, 0.01860, 0.03832, 0.05661, 0.05688... ## $ `concave points_se` &lt;dbl&gt; 0.015870, 0.013400, 0.020580, 0.018670, 0.0... ## $ symmetry_se &lt;dbl&gt; 0.03003, 0.01389, 0.02250, 0.05963, 0.01756... ## $ fractal_dimension_se &lt;dbl&gt; 0.006193, 0.003532, 0.004571, 0.009208, 0.0... ## $ radius_worst &lt;dbl&gt; 25.38, 24.99, 23.57, 14.91, 22.54, 15.47, 2... ## $ texture_worst &lt;dbl&gt; 17.33, 23.41, 25.53, 26.50, 16.67, 23.75, 2... ## $ perimeter_worst &lt;dbl&gt; 184.60, 158.80, 152.50, 98.87, 152.20, 103.... ## $ area_worst &lt;dbl&gt; 2019.0, 1956.0, 1709.0, 567.7, 1575.0, 741.... ## $ smoothness_worst &lt;dbl&gt; 0.1622, 0.1238, 0.1444, 0.2098, 0.1374, 0.1... ## $ compactness_worst &lt;dbl&gt; 0.6656, 0.1866, 0.4245, 0.8663, 0.2050, 0.5... ## $ concavity_worst &lt;dbl&gt; 0.71190, 0.24160, 0.45040, 0.68690, 0.40000... ## $ `concave points_worst` &lt;dbl&gt; 0.26540, 0.18600, 0.24300, 0.25750, 0.16250... ## $ symmetry_worst &lt;dbl&gt; 0.4601, 0.2750, 0.3613, 0.6638, 0.2364, 0.3... ## $ fractal_dimension_worst &lt;dbl&gt; 0.11890, 0.08902, 0.08758, 0.17300, 0.07678... ## $ X33 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... Exclude the diagnosis column from the cluster analysis because it is more of a response variable than a feature. Set it aside for reference though. The data set contains 357 benign tumors (diagnosis = B) and 212 malignant tumors (diagnosis = M). table(wisc.df$diagnosis) ## ## B M ## 357 212 diagnosis &lt;- as.numeric(wisc.df$diagnosis == &quot;M&quot;) Step 1: Prepare Data It is possible to perform k-means on a dataframe, but it is a little easier to extract the feature columns into a matrix. wisc.data &lt;- as.matrix(wisc.df[, c(3:32)]) row.names(wisc.data) &lt;- wisc.df$id Before conducting k-means, check whether any preprocessing is required. If there are NAs, drop the observations or impute values. There are no NAs in this data set. wisc.data &lt;- na.omit(wisc.data) If not all of the features are of comparable scale, standardize the variables as \\((x - \\bar{x}) / sd(x)\\) so that each feature has a mean of 0 and standard deviation of 1. The scale() function scales columns of a matrix. paste(&quot;Means&quot;) ## [1] &quot;Means&quot; summary(colMeans(wisc.data)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0038 0.0681 0.2177 61.8907 15.7337 880.5831 paste(&quot;Std Dev&quot;) ## [1] &quot;Std Dev&quot; summary(apply(X = wisc.data, MARGIN = 2, FUN = sd)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0026 0.0193 0.0727 34.9047 4.1068 569.3570 wisc.data.scaled &lt;- scale(wisc.data) If any of the features multi-nomial, create binary variables. K-means is designed for quantitative data, so it may be better to just not include categorical variables. There are no categorical variables here. Step 2: Choose K Perform k-means clustering with base function kmeans(df, centers, nstart, iter.max). centers is the defined number of clusters. K-means is a random process, so it may produce a different set of clusters each time it runs. nstart sets the number of times to cluster the observations. kmeans() chooses the best set of clusters from the nstart runs. A good value for nstart is 20. It is possible that the clustering iterations do not completely stabilize. If so, you will probably want to cap the number of iterations. iter.max sets the maximum number of times to iterate through the re-assignment process. A good value for iter.max is 50. What is the right number of clusters (centers)? You may have a preference in advance. Or more likely, you need to use judgement from observing the results. There are two common methods for choosing centers: constructing a scree plot or using the silhouette method. The scree plot is a plot of the total within-cluster sum of squared distances resulting from candidate values of centers. The sum of squares decreases as k increases, but at a declining rate. The optimal number of clusters is at the “elbow” in the curve - the point at which the curve flattens.11 kmeans() returns an object of class kmeans, a list in which one of the components is the model sum of squares tot.withinss. In the scree plot below, the elbow may be at k = 2 or k = 3. set.seed(1) wss &lt;- 0 for (k in 1:15) { # within sum of squares wss[k] &lt;- kmeans(wisc.data.scaled, centers = k, nstart = 20, iter.max = 50)$tot.withinss } data.frame(k = 1:15, wss = wss) %&gt;% ggplot(aes(x = k, y = wss)) + geom_line() + geom_point(shape = 21) + scale_x_continuous(breaks = 1:15) + labs(title = &quot;K-means Scree Plot&quot;, x = &quot;Number of Clusters&quot;, y = &quot;Within groups sum of squares&quot;) The silhouette method calculates the within-cluster distance \\(C(i)\\) for each observation, and its distance to the nearest cluster \\(N(i)\\). The silhouette width is \\(S = 1 - C(i) / N(i)\\) for \\(C(i) &lt; N(i)\\) and \\(S = N(i) / C(i) - 1\\) for \\(C(i) &gt; N(i)\\). A value close to 1 means the observation is well-matched to its current cluster; A value near 0 means the observation is on the border between the two clusters; and a value near -1 means the observation is better-matched to the other cluster. The optimal number of clusters is the number that maximizes the total silhouette width. The pam() function from the cluster package returns an object of class pam, a list in which one of the components is the average width silinfo$avg.width. In the silhoette plot below, the maximum silhoette width is at k = 2 with k = 3 a close second. set.seed(1) sil_width &lt;- 0 for (k in 2:15) { sil_width[k] &lt;- pam(wisc.data.scaled, k = k)$silinfo$avg.width } data.frame(k = 1:15, sil_width = sil_width) %&gt;% ggplot(aes(x = k, y = sil_width)) + geom_col() + scale_x_continuous(breaks = 1:15) + labs(title = &quot;K-means Silhoette Plot&quot;, x = &quot;Number of Clusters&quot;, y = &quot;Average Silhoette Width&quot;) The initial cluster assignment is random, so the process may yield different results each time. Use r function set.seed(seed)to create consistent reproducible results. set.seed(1) wisc.k2 &lt;- kmeans(wisc.data.scaled, centers = 2, nstart = 20, iter.max = 50) wisc.df$cluster &lt;- wisc.k2$cluster An intuitive way to interpret the results of k-means models is by plotting the data as a scatter plot and using color to label the samples’ cluster membership. Function fviz_cluster() from the factoextra package illustrates the clusters. Of course, this only works if there are only two features. More likely, create summary statistics of the features grouped by cluster. # wisc.df %&gt;% # gather(key = &quot;Feature&quot;, # value = &quot;Value&quot;, # c(HitPoints, Attack, Defense, # SpecialAttack, SpecialDefense, Speed)) %&gt;% # ggplot(aes(x = factor(Ability), y = Score, color = factor(cluster))) + # geom_point(aes(group = Number)) factoextra::fviz_cluster(wisc.k2, data = wisc.data) Attach the cluster assignment vector back to the original dataframe for visualization and/or summary statistics. Draw conclusions about the clusters by calculating summary statistics of the resulting cluster assigments, typically membership count, and feature averages (or proportions). # View the resulting model knitr::kable(round(wisc.k2$size, 0), caption = &quot;Cluster Size&quot;) Table 11.3: Cluster Size x 380 189 knitr::kable(round(wisc.k2$centers, 0), caption = &quot;Cluster Centers&quot;) Table 11.3: Cluster Centers radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean symmetry_mean fractal_dimension_mean radius_se texture_se perimeter_se area_se smoothness_se compactness_se concavity_se concave points_se symmetry_se fractal_dimension_se radius_worst texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst 0 0 -1 0 0 -1 -1 -1 0 0 0 0 0 0 0 0 0 0 0 0 -1 0 -1 0 0 0 -1 -1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 library(dplyr) wisc.df %&gt;% group_by(diagnosis, cluster) %&gt;% summarise(n = n()) ## # A tibble: 4 x 3 ## # Groups: diagnosis [2] ## diagnosis cluster n ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 B 1 343 ## 2 B 2 14 ## 3 M 1 37 ## 4 M 2 175 wisc.df %&gt;% group_by(diagnosis, cluster) %&gt;% summarise_all(&quot;mean&quot;) ## # A tibble: 4 x 34 ## # Groups: diagnosis [2] ## diagnosis cluster id radius_mean texture_mean perimeter_mean area_mean ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 B 1 2.73e7 12.1 17.9 78.0 463. ## 2 B 2 8.83e6 12.2 17.2 80.8 470. ## 3 M 1 1.10e7 15.0 21.2 97.0 708. ## 4 M 2 4.23e7 18.0 21.7 119. 1036. ## # ... with 27 more variables: smoothness_mean &lt;dbl&gt;, compactness_mean &lt;dbl&gt;, ## # concavity_mean &lt;dbl&gt;, `concave points_mean` &lt;dbl&gt;, symmetry_mean &lt;dbl&gt;, ## # fractal_dimension_mean &lt;dbl&gt;, radius_se &lt;dbl&gt;, texture_se &lt;dbl&gt;, ## # perimeter_se &lt;dbl&gt;, area_se &lt;dbl&gt;, smoothness_se &lt;dbl&gt;, ## # compactness_se &lt;dbl&gt;, concavity_se &lt;dbl&gt;, `concave points_se` &lt;dbl&gt;, ## # symmetry_se &lt;dbl&gt;, fractal_dimension_se &lt;dbl&gt;, radius_worst &lt;dbl&gt;, ## # texture_worst &lt;dbl&gt;, perimeter_worst &lt;dbl&gt;, area_worst &lt;dbl&gt;, ## # smoothness_worst &lt;dbl&gt;, compactness_worst &lt;dbl&gt;, concavity_worst &lt;dbl&gt;, ## # `concave points_worst` &lt;dbl&gt;, symmetry_worst &lt;dbl&gt;, ## # fractal_dimension_worst &lt;dbl&gt;, X33 &lt;dbl&gt; Hence the name “k-means”.↩︎ Euclidean distances are appropriate for quantitative variables. What about categorical variables? This discussion at StackExchange explains that k-modes is suitable for categorical data. It may be okay to convert categorical variables into binary values and treating them as numeric.↩︎ An alternative to Euclidean distance is binary distance, 1 minus the proportion of shared features in a vector of binary variables. See Wikipedia.↩︎ Finding the elbow is a matter of judgement.↩︎ "],
["k-means-vs-hca.html", "11.6 K-Means vs HCA", " 11.6 K-Means vs HCA Hierarchical clustering has some advantages over k-means. It can use any distance method - not just euclidean. The results are stable - k-means can produce different results each time. While they can both be evaluated with the silhouette and elbow plots, hierachical clustering can also be evaluated with a dendogram. But hierarchical clusters has one significant drawback: it is computationally complex compared to k-means. For this last reason, k-means is more common. "]
]
