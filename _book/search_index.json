[
["discrete-variables.html", "Chapter 4 Discrete Variables ", " Chapter 4 Discrete Variables "],
["one-way-tables.html", "4.1 One-Way Tables", " 4.1 One-Way Tables These notes rely on PSU STATS 504 course notes. A one-way frequency table is a frequency table for a single categorical variable. You usually construct a one-way table to test whether the frequency counts differ from a hypothesized distribution using the chi-squared goodness-of-fit test. Here is an example. A researcher crosses tall cut-leaf tomatoes with dwarf potato-leaf tomatoes, then classifies the (\\(n = 1,611\\)) offspring phenotypes. o &lt;- c(926, 288, 293, 104) cell_names &lt;- c(&quot;tall cut-leaf&quot;, &quot;tall potato-leaf&quot;, &quot;dwarf cut-leaf&quot;, &quot;dwarf potato-leaf&quot;) names(o) &lt;- cell_names print(o) ## tall cut-leaf tall potato-leaf dwarf cut-leaf dwarf potato-leaf ## 926 288 293 104 The four phenotypes are expected to occur with relative frequencies 9:3:3:1. pi &lt;- c(9, 3, 3, 1) / (9 + 3 + 3 + 1) print(pi) ## [1] 0.5625 0.1875 0.1875 0.0625 Do the observed phonotype counts conform to the expected proportions? 4.1.1 Chi-Squared Goodness-of-Fit Test The chi-squared goodness-of-fit test, tests whether observed frequency counts \\(O_j\\) of the \\(j \\in (0, 1, \\cdots k)\\) levels of a categorical variable differ from expected frequency counts \\(E_j\\). \\(H_0\\) is \\(O_j = E_j\\). There are two test statistics for this test, Pearson \\(X^2\\) and deviance \\(G^2\\). The sampling distribution of each test statistic approach the chi-squared distribution as \\(n\\) becomes large. It’s a good idea to calculate both test statistics. The Pearson goodness-of-fit statistic is \\[X^2 = \\sum \\frac{(O_j - E_j)^2}{E_j}\\] where \\(O_j = p_j n\\) and \\(E_j = \\pi_j n\\). The deviance statistic is \\[G^2 = 2 \\sum O_j \\log \\left( \\frac{O_j}{E_j} \\right)\\] If the saturated model (the observed data represent the fit of the saturated model, the most complex model possible with the data) proportions \\(p_j\\) equal equal the expected proportions \\(\\pi_j\\), \\(X^2\\) and \\(G^2\\) will equal zero. Large values indicate the data do not agree well with the proposed model. As \\(n \\rightarrow \\infty\\), \\(G^2\\) and \\(X^2\\) approach \\(\\chi_{k-1}^2\\) where \\(k\\) is the number of n-way table cells. You can therefore run a chi-square test of significance using the \\(G^2\\) and \\(X^2\\) test statistics. The chi-square test provides reliable results when at least 80% of \\(E_j &gt;= 5\\). e &lt;- sum(o) * pi names(e) &lt;- cell_names print(e) ## tall cut-leaf tall potato-leaf dwarf cut-leaf dwarf potato-leaf ## 906.1875 302.0625 302.0625 100.6875 data.frame(o, e) %&gt;% rownames_to_column() %&gt;% pivot_longer(cols = -rowname) %&gt;% ggplot(aes(x = as.factor(rowname), y = value, fill = name)) + geom_col(position = position_dodge()) + geom_text(aes(label = round(value, 0)), position = position_dodge(width = 0.9)) + theme_mf() + scale_fill_mf() + labs(title = &quot;Observed vs Expected&quot;, fill = &quot;&quot;, x = &quot;&quot;, y = &quot;&quot;) The \\(X^2\\) statistic is x2 &lt;- sum((o - e)^2 / e) = 1.468722 and the \\(G^2\\) statistic is g2 &lt;- 2 * sum(o * log(o / e)) = 1.4775868, so nearly identical. The chi-sq test p-values are also nearly identical. pchisq(q = x2, df = length(o) - 1, lower.tail = FALSE) ## [1] 0.6895079 pchisq(q = g2, df = length(o) - 1, lower.tail = FALSE) ## [1] 0.6874529 You can also perform the chi-square test of the Pearson test statistic with chisq.test(). chisq.test(o, p = pi) ## ## Chi-squared test for given probabilities ## ## data: o ## X-squared = 1.4687, df = 3, p-value = 0.6895 The p-values based on the \\(\\chi^2\\) distribution with 3 d.f. are about 0.69, so we fail to reject the null hypothesis - the data are consistent with the genetic theory. The \\(\\chi^2\\) value is well outside the \\(\\alpha = 0.05\\) level range of regression. alpha &lt;- 0.05 dof &lt;- length(e) - 1 lrr = -Inf p_val &lt;- pchisq(q = x2, df = length(o) - 1, lower.tail = FALSE) urr = qchisq(p = alpha, df = dof, lower.tail = FALSE) data.frame(chi2 = seq(from = 0, to = 20, by = .1)) %&gt;% mutate(density = dchisq(x = chi2, df = dof)) %&gt;% mutate(rr = ifelse(chi2 &lt; lrr | chi2 &gt; urr, density, 0)) %&gt;% ggplot() + geom_line(aes(x = chi2, y = density), color = mf_pal(12)(12)[12], size = 0.8) + geom_area(aes(x = chi2, y = rr), fill = mf_pal(12)(12)[2], alpha = 0.8) + geom_vline(aes(xintercept = x2), color = mf_pal(12)(12)[11], size = 0.8) + labs(title = bquote(&quot;Chi-Squared Goodness-of-Fit Test&quot;), subtitle = paste0(&quot;Chisq=&quot;, round(x2,2), &quot;, &quot;, &quot;Critical value=&quot;, round(urr,2), &quot;, &quot;, &quot;p-value=&quot;, round(p_val,3), &quot;.&quot; ), x = &quot;chisq&quot;, y = &quot;Density&quot;) + theme(legend.position=&quot;none&quot;) + theme_mf() Inspect the residuals to learn which differences may lead to rejecting the null hypothesis. The Pearson and deviance statistics are sums of individual cell comparisons which can be termed the squared “residuals”, \\(X^2 = \\sum r_j^2\\) and \\(G^2 = \\sum G_j^2\\). The expected value of a \\(\\chi^2\\) random variable is its d.f., \\(k-1\\), and the typical size of any \\(\\chi^2\\) value is be \\((k - 1) / k\\), so for the typical residual should be within 2 \\(\\sqrt{(k - 1) / k}\\). e2_res &lt;- sqrt((o - e)^2 / e) g2_res &lt;- sign(o - e) * sqrt(abs(2 * o * log(o / e))) data.frame(e2_res, g2_res, obs = 1:length(e2_res)) %&gt;% rownames_to_column() %&gt;% pivot_longer(cols = e2_res:g2_res) %&gt;% ggplot(aes(x = obs, y = value, color = name)) + geom_point() + scale_color_mf() Here is a summary of the analysis. data.frame(o, e, e2_res, g2_res) ## o e e2_res g2_res ## tall cut-leaf 926 906.1875 0.6581581 6.328906 ## tall potato-leaf 288 302.0625 0.8091222 -5.240221 ## dwarf cut-leaf 293 302.0625 0.5214343 -4.224967 ## dwarf potato-leaf 104 100.6875 0.3301172 2.594764 If you do not have a theoretical value, but instead want to test whether the data conform to a distribution, the test is nearly the same. Your first step is the estimate the distribution parameter(s). Then you can perform the goodness of fit test, but with degrees of freedom reduced for each estimated parameter. For example, suppose sample \\(n = 100\\) families and count the number of children \\(J\\), and you want to test whether the counts \\(O\\) follow a Poisson distribution, \\(X \\sim Pois(\\lambda)\\). dat &lt;- data.frame(j = 0:5, o = c(19, 26, 29, 13, 10, 3)) print(dat) ## j o ## 1 0 19 ## 2 1 26 ## 3 2 29 ## 4 3 13 ## 5 4 10 ## 6 5 3 The ML estimate for \\(\\lambda\\) is \\[\\hat{\\lambda} = \\frac{j_0 O_0 + j_1 O_1, + \\cdots j_k O_k}{O}\\] lambda_hat &lt;- sum(dat$j * dat$o) / sum(dat$o) print(lambda_hat) ## [1] 1.78 Then the expected values are \\[f(j; \\lambda) = \\frac{e^{-\\hat{\\lambda}} \\hat{\\lambda}^j}{j!}.\\] E &lt;- exp(-lambda_hat) * lambda_hat^dat$j / factorial(dat$j) * sum(dat$o) dat &lt;- cbind(dat, e = E) print(dat) ## j o e ## 1 0 19 16.863815 ## 2 1 26 30.017590 ## 3 2 29 26.715655 ## 4 3 13 15.851289 ## 5 4 10 7.053824 ## 6 5 3 2.511161 Compare the expected values to the observed values with the \\(\\chi^2\\) goodness of fit test. X2 &lt;- sum((dat$o - dat$e)^2 / dat$e) pchisq(q = X2, df = nrow(dat) - 1 - 1) ## [1] 0.4154322 chisq.test(dat$o, p = dat$e / sum(dat$e)) ## Warning in chisq.test(dat$o, p = dat$e/sum(dat$e)): Chi-squared approximation ## may be incorrect ## ## Chi-squared test for given probabilities ## ## data: dat$o ## X-squared = 2.8044, df = 5, p-value = 0.7301 "],
["two-way-tables.html", "4.2 Two-Way Tables", " 4.2 Two-Way Tables These notes rely on PSU STATS 504 course notes. A two-way frequency table is a frequency table for two categorical variables. You usually construct a two-way table to test whether the frequency counts in one categorical variable differ from the other categorical variable using the chi-squared test of independence. If there is a significant difference, then you characterize the effect sizes with measures of association (difference in proportions, relative risk, or odds ratio). Here is an example. A double blind study investigated the therapeutic value of ascorbic acid \\((trt \\in [Placebo, Ascorbic Acid])\\) for treating common colds \\((result \\in [Cold, No Cold])\\) on a sample of \\(n = 279\\) persons. There are two discrete variables each with two levels, hence a two way table. vitc_dat &lt;- matrix( c(31, 17, 109, 122), ncol = 2, dimnames = list( treat = c(&quot;Placebo&quot;, &quot;VitaminC&quot;), resp = c(&quot;Cold&quot;, &quot;NoCold&quot;) ) ) print(vitc_dat) ## resp ## treat Cold NoCold ## Placebo 31 109 ## VitaminC 17 122 prop.table(vitc_dat) ## resp ## treat Cold NoCold ## Placebo 0.1111111 0.390681 ## VitaminC 0.0609319 0.437276 prop.table(vitc_dat, margin = 1) # row pct ## resp ## treat Cold NoCold ## Placebo 0.2214286 0.7785714 ## VitaminC 0.1223022 0.8776978 prop.table(vitc_dat, margin = 2) # col pct ## resp ## treat Cold NoCold ## Placebo 0.6458333 0.4718615 ## VitaminC 0.3541667 0.5281385 Do the observed frequency counts differ from the expected frequency counts under the independence model? (The independence model is a subset of the saturated model where the two explanatory variables are independent.) 4.2.1 Chi-Squared Test of Independence The Pearson goodness-of-fit statistic is \\[X^2 = \\sum_{i,j} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\] where for independent variables \\(E_{ij}\\) equals the marginal probability of row \\(i\\) multiplied by the marginal probability of column \\(j\\). E &lt;- sum(vitc_dat) * prop.table(vitc_dat, 1) * prop.table(vitc_dat, 2) X2 &lt;- sum((vitc_dat - E)^2 / E) print(X2) ## [1] 4.811413 The deviance statistic is \\[G^2 = 2 \\sum_{ij} O_{ij} \\log \\left( \\frac{O_{ij}}{E_{ij}} \\right)\\] G2 &lt;- - 2 * sum(vitc_dat * log(vitc_dat / E)) print(G2) ## [1] 4.871697 The degrees of freedom for \\(E^2\\) and \\(G^2\\) equals the degrees of freedom for the saturated model, \\(I \\times J - 1\\), minus the degrees of freedom from the independence model, \\((I - 1) + (J - 1)\\), which you can solve for \\((I - 1)(J - 1)\\). dof &lt;- (nrow(vitc_dat) - 1) * (ncol(vitc_dat) - 1) print(dof) ## [1] 1 The associated p-values are pchisq(q = G2, df = dof, lower.tail = FALSE) ## [1] 0.02730064 pchisq(q = X2, df = dof, lower.tail = FALSE) ## [1] 0.02827186 The chisq.test() function applies the Yates’s continuity correcton by default to correct for situations with small cell counts by subtracting 0.5 from the observed - expected differences. chisq.test(vitc_dat, correct = FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: vitc_dat ## X-squared = 4.8114, df = 1, p-value = 0.02827 The correction yields more conservative p-values. chisq.test(vitc_dat) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: vitc_dat ## X-squared = 4.1407, df = 1, p-value = 0.04186 The p-values indicate strong evidence for rejecting the independence model. 4.2.2 Difference in Proportions 4.2.3 Relative Risk 4.2.4 Odds Ratio "]
]
