[["analysis-of-variance.html", "Chapter 3 Analysis of Variance 3.1 One-Way ANOVA 3.2 Handling Non-Constant Variance 3.3 References", " Chapter 3 Analysis of Variance These notes are primarily taken from studying PSU STAT-502 and Laerd Statistics. Classic analysis of variance (ANOVA) compares the mean responses to experimental manipulations in controlled environments. However, ANOVA also compares the mean observational responses among independent groups. The conclusions are just not as rigorous for observational studies. 3.1 One-Way ANOVA Use the one-way ANOVA test to compare the mean response of a continuous dependent variable among the groups of a factor variable. (Researchers typically use the independent-samples t-test when a factor has only two groups). The observations must be independent, meaning the data generators cannot influence each other (e.g., same participant in different groups, or participants interact with each other to produce the observed outcome). The ANOVA method decomposes the deviation of observation \\(Y_{ij}\\) around the overall mean \\(\\bar{Y}_{..}\\) into three parts. SS df MS F \\(SS_{Trt} = \\sum{n_i(\\bar{Y}_{i.} - \\bar{Y}_{..})^2}\\) \\(k - 1\\) \\(\\frac{SS_{Trt}}{k - 1}\\) \\(\\frac{MS_{Trt}}{MS_{Err}}\\) \\(SS_{Err} = \\sum(Y_{ij} - \\bar{Y}_{i.})^2\\) \\(N - k\\) \\(\\frac{SS_{Err}}{N - k}\\) \\(SS_{Tot} = \\sum(Y_{ij} - \\bar{Y}_{..})^2\\) \\(N - 1\\) The ANOVA method decomposes the deviation of the \\(j^{th}\\) observation of the \\(i^{th}\\) treatment level around the overall mean \\((Y_{ij} - \\bar{Y}_{..})\\) into the deviation of the observation around its factor level mean (aka treatment mean) \\((Y_{ij} - \\bar{Y}_{i.})\\) (aka, the error \\(\\epsilon_{ij}\\)) plus the deviation of the factor level mean around the overall mean (aka grand mean) \\((\\bar{Y}_{i.} - \\bar{Y}_{..})\\). You can express an ANOVA model as an effects model or a means model. The effects model equates \\(Y_{ij}\\) to the grand mean \\(\\mu\\) plus deviations from the grand mean due to the treatment levels \\(\\tau_i\\) plus deviations around the mean of its factor level \\(\\epsilon_{ij}\\). \\[Y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\\] The cell means model equates \\(Y_{ij}\\) to its factor level mean \\(\\mu_i\\) plus deviations around its factor level mean \\(\\epsilon_{ij}\\). \\[Y_{ij} = \\mu_i + \\epsilon_{ij}\\] The ratio of the variance between treatments to the variance within treatments treatment mean square \\(MS_{Trt}\\) to the error mean square \\(MS_{Err}\\) has an \\(F\\) distribution with \\(k - 1\\) numerator degrees of freedom and \\(N - k\\) denominator degrees of freedom. \\[F = \\frac{MS_{Trt}}{MS_{Err}} = \\frac{SS_{Trt}/(k - 1)}{SS_{Err} / (n - k)}\\] Under \\(H_0\\), both \\(MS_{Trt}\\) and \\(MS_{Err}\\) estimate \\(\\sigma^2\\), the variance common to all \\(k\\) populations. Under the alternative hypothesis, \\(MS_{Trt}\\) estimates \\(\\sigma^2 + \\theta\\), whereas \\(MS_{Err}\\) still estimates \\(\\sigma^2\\). The larger is \\(F\\), the less likely \\(H_0\\) is true. Summarize the results in an ANOVA table. SS df MS F \\(SS_{Trt} = \\sum{n_i(\\bar{Y}_{i.} - \\bar{Y}_{..})^2}\\) \\(k - 1\\) \\(\\frac{SS_{Trt}}{k - 1}\\) \\(\\frac{MS_{Trt}}{MS_{Err}}\\) \\(SS_{Err} = \\sum(Y_{ij} - \\bar{Y}_{i.})^2\\) \\(N - k\\) \\(\\frac{SS_{Err}}{N - k}\\) \\(SS_{Tot} = \\sum(Y_{ij} - \\bar{Y}_{..})^2\\) \\(N - 1\\) The F test does not indicate which populations cause the rejection of \\(H_0\\). Identify which groups differ from the others with a post-hoc test. Post-hoc test options include Tukey, Fishers Least Significant Difference (LSD), Bonferroni, Scheffe, and Dunnett. ANOVA returns reliable results if the following conditions hold: Independence. The sampled observations must be independent within groups. The observations should be from a random sample, or from an experiment using random assignment. Each groups size, n, should be less than 10% of its population size. The groups must also be independent of each other (non-paired). A repeated measures experiment design requires a different test. Normality. Each groups values should be nearly normally distributed. This condition is especially important with small sample sizes.1 Equal Variance. Each groups variance should be rougly equal (homoscedastic groups). This condition is especially important when sample sizes differ between groups. The IQR of the box plot is a good way to visually assess this condition.2 The null hypothesis is \\(H_0: \\mu_1 = \\mu_2 = \\dots = \\mu_k\\) for the \\(k\\) groups. The alternative hypothesis is that at least one group mean differs from the others. Here is an example where you might use an ANOVA test. A study compares the growth of plants using one of three fertilizers and a control group. Dataset greenhouse contains 6 observations per each of the k = 4 treatment levels (N = 24). The figures below show that all three fertilizers produced more growth than the control group. Fertilizers F1 and F3 appear to be about tied for most growth, but it is unclear if the fertilizers are significantly different from each other. The barchart includes a one-standard error error bar. The boxplot may be the better choice for simplicity in generating.3 The boxplot also indicates the variances may differ among the groups. library(ggplot2) library(gridExtra) p1 &lt;- greenhouse %&gt;% group_by(group) %&gt;% summarize(mean_growth = mean(growth), se = sd(growth) / n()) %&gt;% ggplot(aes(x = group, y = mean_growth)) + geom_col(fill = &quot;grey&quot;) + geom_errorbar(aes(ymin = mean_growth - se, ymax = mean_growth + se, width = .2)) + labs(title = &quot;Greenhouse Experiment&quot;, x = &quot;Fertilizer Treatment&quot;, y = &quot;Plant Height (cm)&quot;) + scale_y_continuous(limits = c(0, 35)) p2&lt;-ggplot(data = greenhouse, aes(x = group, y = growth)) + geom_boxplot() + labs(title = &quot;Greenhouse Experiment&quot;, x = &quot;Fertilizer Treatment&quot;, y = &quot;Plant Height (cm)&quot;) + scale_y_continuous(limits = c(0, 35)) grid.arrange(p1, p2, ncol = 2) Conduct a one-way ANOVA of the \\(k = 4\\) factor levels. The ANOVA F-test indicates the factor levels differ (F = 27.5, p &lt; 0.001), so reject \\(H_0\\) that the \\(k = 4\\) factor means are identical. greenhouse.aov &lt;- aov(growth ~ group, data = greenhouse) anova(greenhouse.aov) ## Analysis of Variance Table ## ## Response: growth ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group 3 251.440 83.813 27.465 2.712e-07 *** ## Residuals 20 61.033 3.052 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Before conducting a post-hoc test to determine which factor levels differ, check whether the model conditions hold. The measurements are independent because this is a completely randomized experiment. The individual populations could be assumed normally distributed if \\(n_j &gt;= 30\\), but \\(n_j = 6\\) for each group, so check for normality. The QQ plots below appear to be approximately normal. par(mfrow = c(2, 2)) split(greenhouse$growth, f = greenhouse$group) %&gt;% sapply(function(x) {qqnorm(x); qqline(x)}) ## $Control ## NULL ## ## $F1 ## NULL ## ## $F2 ## NULL ## ## $F3 ## NULL The sample sizes are similar (6 per each of the 4 factor levels), so the equality of sample variances is less critical, but check anyway for demonstration. A rule of them check is to compare group standard deviations. None should be more than double the value of any other. In this case F1 is more than double Control. greenhouse %&gt;% group_by(group) %&gt;% summarize(sd = sd(growth)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 4 x 2 ## group sd ## &lt;fct&gt; &lt;dbl&gt; ## 1 Control 1 ## 2 F1 2.44 ## 3 F2 1.90 ## 4 F3 1.29 Bartletts test of homogeneity fails to reject \\(H_0\\) of homogeity (p = 0.2494). bartlett.test(growth ~ group, data = greenhouse) ## ## Bartlett test of homogeneity of variances ## ## data: growth by group ## Bartlett&#39;s K-squared = 4.1143, df = 3, p-value = 0.2494 Now that the conditions are checked, conduct a post-hoc test to see which groups differ. Here is the Tukey test. As expected, all three fertilizer factor levels differ from the control. F3 differed from F2, but F1 was not significantly different from either F2 or F3. plot(TukeyHSD(greenhouse.aov)) 3.2 Handling Non-Constant Variance The statistical tests for the model conditions (e.g. Bartletts test for homogeneity) are often too sensitive. ANOVA is robust to small violations of the conditions. However, heterogeneity is a common problem in ANOVA. Tranforming the response variable can often remove the heterogeneity. Finding the correct transformation can be challenging, but the Box-Cox procedure can help. The MASS::boxcox() function calculates the profile log-likelihoods for a power transformation of the response variable \\(Y^\\lambda\\). \\(\\lambda\\) \\(Y^\\lambda\\) Transformation 2 \\(Y^2\\) Square 1 \\(Y^1\\) (no transformation) .5 \\(Y^{.5}\\) Square Root 0 \\(\\ln(Y)\\) Log -.5 \\(Y^{-.5}\\) Inverse Square Root -1 \\(Y^{-1}\\) Inverse The Box-Cox procedure does not recommend any particular transformation of the data in this case. library(MASS) boxcox(greenhouse.aov, plotit = TRUE) 3.2.1 Example In a completely randomized design experiment, 20 young pigs are assigned at random among 4 experimental groups, and each group is fed a different diet. The response variable is the pigs weight in kg after consuming the diet for 10 months. Are the mean pig weights the same for all 4 diets? pig &lt;- read.delim(file = &quot;input/pig_weight.txt&quot;, header = TRUE, sep = &quot;,&quot;) pig &lt;- pig %&gt;% gather(key = diet, value = weight) %&gt;% mutate(diet = factor(diet)) glimpse(pig) ## Rows: 20 ## Columns: 2 ## $ diet &lt;fct&gt; Feed.1, Feed.1, Feed.1, Feed.1, Feed.1, Feed.2, Feed.2, Feed... ## $ weight &lt;dbl&gt; 60.8, 57.1, 65.0, 58.7, 61.8, 68.3, 67.7, 74.0, 66.3, 69.9, ... ggplot(data = pig, aes(x = diet, y = weight)) + geom_boxplot() The measurements are independent because this is a completely randomized experiment. The individual populations could be assumed normally distributed if \\(n &gt;= 30\\), but \\(n = 20\\), so we need to check for normality. The sample sizes are similar (5 per each of the 4 factor levels), so the equality of sample variances is less critical, but we can check anyway. First a check of the normality condition. Test for normality by starting with the assumption that the distribution are normal, \\(H_0: normal\\), then falsifying the assumption if sufficient evidence exists. In these normal Q-Q plots, look for substantial deviations from a straight line. These plots looks good. layout(rbind(c(1, 2), c(3, 4))) qqnorm(pig[pig$diet == &quot;Feed.1&quot;,]$weight) qqline(pig[pig$diet == &quot;Feed.1&quot;,]$weight) qqnorm(pig[pig$diet == &quot;Feed.2&quot;,]$weight) qqline(pig[pig$diet == &quot;Feed.2&quot;,]$weight) qqnorm(pig[pig$diet == &quot;Feed.3&quot;,]$weight) qqline(pig[pig$diet == &quot;Feed.3&quot;,]$weight) qqnorm(pig[pig$diet == &quot;Feed.4&quot;,]$weight) qqline(pig[pig$diet == &quot;Feed.4&quot;,]$weight) There are statistical tests for that provide a quantitative evaluation, but the sample sizes are two small for them to be useful. Now check for equal variances with Bartletts test of homogeneity of variances. The p-value is &gt;&gt;.05, so do not reject \\(H_0\\) of equal variances. bartlett.test(weight ~ diet, data = pig) ## ## Bartlett test of homogeneity of variances ## ## data: weight by diet ## Bartlett&#39;s K-squared = 0.46965, df = 3, p-value = 0.9255 Now we are ready for the one-way ANOVA test. The null hypothesis is that all means are equal. The p-value is &lt;.0001, so we reject \\(H_0\\). summary(pig.aov &lt;- aov(weight ~ diet, data = pig)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## diet 3 4703 1567.7 206.7 5.28e-13 *** ## Residuals 16 121 7.6 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Perform a post-hoc test to see which of the groups differ. Here we use Tukeys test. All pairs differed from each other. TukeyHSD(pig.aov) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = weight ~ diet, data = pig) ## ## $diet ## diff lwr upr p adj ## Feed.2-Feed.1 8.56 3.576977 13.543023 0.0008075 ## Feed.3-Feed.1 39.66 34.676977 44.643023 0.0000000 ## Feed.4-Feed.1 25.70 20.716977 30.683023 0.0000000 ## Feed.3-Feed.2 31.10 26.116977 36.083023 0.0000000 ## Feed.4-Feed.2 17.14 12.156977 22.123023 0.0000002 ## Feed.4-Feed.3 -13.96 -18.943023 -8.976977 0.0000030 plot(TukeyHSD(pig.aov)) In a multi-factor experiment each level of m Multi-factor ANOVA (MANOVA) is a method to compare mean responses by treatment factor level of two or more treatments applied in combination. The null hypotheses are \\(H_0: \\mu_{1.} = \\mu_{2.} = \\dots = \\mu_{a.}\\) for the \\(a\\) levels of factor 1, \\(H_0: \\mu_{.1} = \\mu_{.2} = \\dots = \\mu_{.b}\\) for the \\(b\\) levels of factor 2, etc. for all the factors in the experiment, and $H_0: $ no interaction for all the factor interactions. There are two equivalent ways to state the MANOVA model: \\[Y_{ijk} = \\mu_{ij} + \\epsilon_{ijk}\\] In this notation \\(Y_{ijk}\\) refers to the \\(k^{th}\\) observation in the \\(j^{th}\\) level of factor two and the \\(i^{th}\\) level of factor 1. Potentially there could be additional factors. This model formulation decomposes the response into a cell mean and an error term. The second makes the factor effect more explicit and is thus more common: \\[Y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\epsilon_{ijk}\\] $_i 3.2.2 Multiple Variance Comparison F Test 3.2.3 Example A study investigates the relationship between oxygen update and two explanatory variables: smoking, and type of stress test. A sample of \\(n = 27\\) persons, 9 non-smoking, 9 moderately-smoking, and 9 heavy-smoking are divided into three stress tests, bicycle, treadmill, and steps and their oxygen uptake was measured. Is oxygen uptake related to smoking status and type of stress test? Is there an interaction effect between smoking status and type of stress test? library(dplyr) library(ggplot2) library(nortest) # for Anderson-Darling test library(stats) # for anova smoker &lt;- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3) stress &lt;- c(1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3) oxytime &lt;- c(12.8, 13.5, 11.2, 16.2, 18.1, 17.8, 22.6, 19.3, 18.9, 10.9, 11.1, 9.8, 15.5, 13.8, 16.2, 20.1, 21.0, 15.9, 8.7, 9.2, 7.5, 14.7, 13.2, 8.1, 16.2, 16.1, 17.8) oxy &lt;- data.frame(oxytime, smoker, stress) oxy$smoker &lt;- ordered(oxy$smoker, levels = c(1, 2, 3), labels = c(&quot;non-smoker&quot;, &quot;moderate&quot;, &quot;heavy&quot;)) oxy$stress &lt;- factor(oxy$stress, labels = c(&quot;bicycle&quot;, &quot;treadmill&quot;, &quot;steps&quot;)) lm_oxy &lt;- lm(oxytime~smoker+stress+smoker*stress, data = oxy) anova(lm_oxy) ## Analysis of Variance Table ## ## Response: oxytime ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## smoker 2 84.899 42.449 12.8967 0.0003348 *** ## stress 2 298.072 149.036 45.2793 9.473e-08 *** ## smoker:stress 4 2.815 0.704 0.2138 0.9273412 ## Residuals 18 59.247 3.291 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 3.3 References PSU STAT502 SFU BIO710 Use a nonparametric statistical method such as the Kruskal-Wallis test when the population distributions are very non-normal. A general rule of thumb for equal variances is to compare the smallest and largest sample standard deviation. If the ratio of these two sample standard deviations fall within 0.5 to 2, then assume equal variances. More formal tests include the Bartlett test, and Levene test, See [Homogeneity of variance].(http://www.cookbook-r.com/Statistical_analysis/Homogeneity_of_variance/) in Cookbook for R) PSU STATS501 recommends presenting this as a bar chart, but the box plot seems more useful. Which is more common in literature? "]]
