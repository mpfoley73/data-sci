[
["index.html", "My Data Science Notes Intro", " My Data Science Notes Michael Foley 2020-05-20 Intro These notes are pulled from various classes, tutorials, books, etc. and are intended for my own consumption. If you are finding this on the internet, I hope it is useful to you, but you should know that I am just a student and there’s a good chance whatever you’re reading here is mistaken. In fact, that should probably be your null hypothesis… or your prior. Whatever. "],
["probability.html", "Chapter 1 Probability ", " Chapter 1 Probability "],
["principles.html", "1.1 Principles", " 1.1 Principles Here are three rules that come up all the time. \\(Pr(A \\cup B) = Pr(A)+Pr(B) - Pr(AB)\\). This rule generalizes to \\(Pr(A \\cup B \\cup C)=Pr(A)+Pr(B)+Pr(C)-Pr(AB)-Pr(AC)-Pr(BC)+Pr(ABC)\\). \\(Pr(A|B) = \\frac{P(AB)}{P(B)}\\) If A and B are independent, \\(Pr(A \\cap B) = Pr(A)Pr(B)\\), and \\(Pr(A|B)=Pr(A)\\). Uniform distributions on finite sample spaces often reduce to counting the elements of A and the sample space S, a process called combinatorics. Here are three important combinatorial rules. Multiplication Rule. \\(|S|=|S_1 |⋯|S_k|\\). How many outcomes are possible from a sequence of 4 coin flips and 2 rolls of a die? \\(|S|=|S_1| \\cdot |S_2| \\dots |S_6| = 2 \\cdot 2 \\cdot 2 \\cdot 2 \\cdot 6 \\cdot 6 = 288\\). How many subsets are possible from a set of n=10 elements? In each subset, each element is either included or not, so there are \\(2^n = 1024\\) subsets. How many subsets are possible from a set of n=10 elements taken k at a time with replacement? Each experiment has \\(n\\) possible outcomes and is repeated \\(k\\) times, so there are \\(n^k\\) subsets. Permutations. The number of ordered arrangements (permutations) of a set of \\(|S|=n\\) items taken \\(k\\) at a time without replacement has \\(n(n-1) \\dots (n-k+1)\\) subsets because each draw is one of k experiments with decreasing number of possible outcomes. \\[_nP_k = \\frac{n!}{(n-k)!}\\] Notice that if \\(k=0\\) then there is 1 permutation; if \\(k=1\\) then there are \\(n\\) permutations; if \\(k=n\\) then there are \\(n!\\) permutations. How many ways can you distribute 4 jackets among 4 people? \\(_nP_k = \\frac{4!}{(4-4)!} = 4! = 24\\) How many ways can you distribute 4 jackets among 2 people? \\(_nP_k = \\frac{4!}{(4-2)!} = 12\\) Subsets. The number of unordered arrangements (combinations) of a set of \\(|S|=n\\) items taken \\(k\\) at a time without replacement has \\[_nC_k = {n \\choose k} = \\frac{n!}{k!(n-k)!}\\] combinations and is called the binomial coefficient. The binomial coefficient is the number of different subsets. Notice that if k=0 then there is 1 subset; if k=1 then there are n subsets; if k=n then there is 1 subset. The connection with the permutation rule is that there are \\(n!/(n-k)!\\) permutations and each permutation has \\(k!\\) permutations. How many subsets of 7 people can be taken from a set of 12 persons? \\(_{12}C_7 = {12 \\choose 7} = \\frac{12!}{7!(12-7)!} = 792\\) If you are dealt five cards, what is the probability of getting a “full-house” hand containing three kings and two aces (KKKAA)? \\[P(F) = \\frac{{4 \\choose 3} {4 \\choose 2}}{{52 \\choose 5}}\\] Distinguishable permutations. The number of unordered arrangements (distinguishable permutations) of a set of \\(|S|=n\\) items in which \\(n_1\\) are of one type, \\(n_2\\) are of another type, etc., is \\[{n \\choose {n_1, n_2, \\dots, n_k}} = \\frac{n!}{n_{1}! n_{2}! \\dots n_{k}!}\\] How many ordered arrangements are there of the letters in the word PHILIPPINES? There are n=11 objects. \\(|P|=n_1=3\\); \\(|H|=n_2=1\\); \\(|I|=n_3=3\\); \\(|L|=n_4=1\\); \\(|N|=n_5=1\\); \\(|E|=n_6=1\\); \\(|S|=n_7=1\\). \\[{n \\choose {n_1, n_2, \\dots, n_k}} = \\frac{11!}{3! 1! 3! 1! 1! 1! 1!} = 1,108,800\\] How many ways can a research pool of 15 subjects be divided into three equally sized test groups? \\[{n \\choose {n_1, n_2, \\dots, n_k}} = \\frac{15!}{5! 5! 5!} = 756,756\\] "],
["disc-dist.html", "1.2 Discrete Distributions", " 1.2 Discrete Distributions These notes rely heavily on PSU STATS 504 course notes. The most important discrete distributions are the Binomial, Poisson, and Multinomial. Sometimes useful are the related Bernoulli, negative binomial, geometric, and hypergeometric distributions. A discrete random variable \\(X\\) is described by its probability mass function \\(f(x) = P(X = x)\\). The set of \\(x\\) values for which \\(f(x) &gt; 0\\) is called the support. If the distribution depends on unknown parameter(s) \\(\\theta\\) we write it as \\(f(x; \\theta)\\) (frequentist) or \\(f(x | \\theta)\\) (Bayesian). 1.2.1 Bernoulli If \\(X\\) is the result of a trial with two outcomes of probability \\(P(X = 1) = \\pi\\) and \\(P(X = 0) = 1 - \\pi\\), then \\(X\\) is a random variable with a Bernoulli distribution \\[f(x) = \\pi^x (1 - \\pi)^{1 - x}, \\hspace{1cm} x \\in (0, 1)\\] with \\(E(X) = \\pi\\) and \\(Var(X) = \\pi(1 - \\pi)\\). 1.2.2 Binomial If \\(X\\) is the count of successful events in \\(n\\) identical and independent Bernoulli trials of success probability \\(\\pi\\), then \\(X\\) is a random variable with a binomial distribution \\(X \\sim Bin(n,\\pi)\\) \\[f(x;n, \\pi) = \\frac{n!}{x!(n-x)!} \\pi^x (1-\\pi)^{n-x} \\hspace{1cm} x \\in (0, 1, ..., n), \\hspace{2mm} \\pi \\in [0, 1]\\] with \\(E(X)=n\\pi\\) and \\(Var(X) = n\\pi(1-\\pi)\\). Binomial sampling is used to model counts of one level of a categorical variable over a fixed sample size. Here is a simple analysis of data from a Binomial process. Data set dat contains frequencies of high-risk drinkers vs non-high-risk drinkers in a college survey. ## ## No Yes ## 685 630 The MLE of \\(\\pi\\) from the Binomial distribution is the sample mean. x &lt;- sum(dat$high_risk == &quot;Yes&quot;) n &lt;- nrow(dat) p &lt;- x / n print(p) ## [1] 0.4790875 Here is the binomial distribution \\(f(x; \\pi), \\hspace{5mm} x \\in [550, 700]\\). events &lt;- round(seq(from = 550, to = 700, length = 20), 0) density &lt;- dbinom(x = events, prob = p, size = n) prob &lt;- pbinom(q = events, prob = p, size = n, lower.tail = TRUE) df &lt;- data.frame(events, density, prob) ggplot(df, aes(x = factor(events))) + # geom_col(aes(y = density)) + geom_col(aes(y = density), fill = mf_pal()(1), alpha = 0.8) + geom_text( aes(label = round(density, 3), y = density + 0.001), position = position_dodge(0.9), size = 3, vjust = 0 ) + geom_line( data = df, aes(x = as.numeric(factor(events)), y = prob/40), color = mf_pal()(1), size = 1) + scale_y_continuous(sec.axis = sec_axis(~.*40, name = &quot;Cum Prob&quot;)) + theme_mf() + labs(title = &quot;PMF and CDF of Binomial Distribution&quot;, subtitle = &quot;Bin(1315, 0.479).&quot;, x = &quot;Events (x)&quot;, y = &quot;Density&quot;) There are several ways to calculate a confidence interval for \\(\\pi\\). One method is the normal approximation (Wald) interval. \\[\\pi = p \\pm z_{\\alpha /2} \\sqrt{\\frac{p (1 - p)}{n}}\\] alpha &lt;- .05 z &lt;- qnorm(1 - alpha / 2) se &lt;- sqrt(p * (1 - p) / n) p + c(-z*se, z*se) ## [1] 0.4520868 0.5060882 This method is easy to understand and calculate by hand, but its accuracy suffers when \\(np&lt;5\\) or \\(n(1-p)&lt;5\\) and it does not work at all when \\(p = 0\\) or \\(p = 1\\). Option two is the Wilson method. \\[\\frac{p + \\frac{z^2}{2n}}{1 + \\frac{z^2}{n}} \\pm \\frac{z}{1 + \\frac{z^2}{n}} \\sqrt{\\frac{p(1 - p)}{n} + \\frac{z^2}{4n^2}}\\] est &lt;- (p + (z^2)/(2*n)) / (1 + (z^2) / n) pm &lt;- z / (1 + (z^2)/n) * sqrt(p*(1-p)/n + (z^2) / (4*(n^2))) est + c(-pm, pm) ## [1] 0.4521869 0.5061098 This is what prop.test() does when you set correct = FALSE. prop.test(x = x, n = n, correct = FALSE) ## ## 1-sample proportions test without continuity correction ## ## data: x out of n, null probability 0.5 ## X-squared = 2.3004, df = 1, p-value = 0.1293 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.4521869 0.5061098 ## sample estimates: ## p ## 0.4790875 There is a second version of the Wilson interval that applies a “continuity correction” that aligns the “minimum coverage probability”, rather than the “average probability”, with the nominal value. I’ll need to learn what’s inside those quotations at some point. prop.test(x = x, n = n) ## ## 1-sample proportions test with continuity correction ## ## data: x out of n, null probability 0.5 ## X-squared = 2.2175, df = 1, p-value = 0.1365 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.4518087 0.5064898 ## sample estimates: ## p ## 0.4790875 Finally, there is the Clopper-Pearson exact confidence interval. Clopper-Pearson inverts two single-tailed binomial tests at the desired alpha. This is a non-trivial calculation, so there is no easy formula to crank through. Just use the binom.test() function and pray no one asks for an explanation. binom.test(x = x, n = n) ## ## Exact binomial test ## ## data: x and n ## number of successes = 630, number of trials = 1315, p-value = 0.1364 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.4517790 0.5064896 ## sample estimates: ## probability of success ## 0.4790875 The expected probability of no one being a high-risk drinker is \\(f(0;0.479) = \\frac{1315!}{0!(1315-0)!} 0.479^0 (1-0.479)^{1315-0} = 0\\). dbinom(x = 0, size = n, p = p) ## [1] 0 The expected probability of half the population being a high-risk drinker, \\(f(658, 0.479)\\), is impossible to write out, and slow to calculate. pbinom(q = .5*n, size = n, prob = p, lower.tail = FALSE) ## [1] 0.06455096 As n increases for fixed \\(\\pi\\), the binomial distribution approaches normal distribution \\(N(n\\pi, n\\pi(1−\\pi))\\). The normal distribution is a good approximation when \\(n\\) is large. pnorm(q = 0.5, mean = p, sd = se, lower.tail = FALSE) ## [1] 0.06450357 Here are some more examples using smaller sample sizes. The probability 2 out of 10 coin flips are heads if the probability of heads is 0.3: dbinom(x = 2, size = 10, prob = 0.3) ## [1] 0.2334744 Here is a simulation from n = 10,000 random samples of size 10. rbinom() generates a random sample of numbers from the binomial distribution. data.frame(cnt = rbinom(n = 10000, size = 10, prob = 0.3)) %&gt;% count(cnt) %&gt;% ungroup() %&gt;% mutate(pct = n / sum(n), X_eq_x = cnt == 2) %&gt;% ggplot(aes(x = as.factor(cnt), y = n, fill = X_eq_x, label = pct)) + geom_col(alpha = 0.8) + scale_fill_mf() + geom_label(aes(label = round(pct, 2)), size = 3, alpha = .6) + theme_mf() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Binomial Distribution&quot;, subtitle = paste0( &quot;P(X=2) successes in 10 trials when p = 0.3 is &quot;, round(dbinom(2, 10, 0.3), 4), &quot;.&quot; ), x = &quot;Successes&quot;, y = &quot;Count&quot;, caption = &quot;Simulation from n = 10,000 binomial random samples.&quot;) What is the probability of &lt;=2 heads in 10 coin flips where probability of heads is 0.3? The cumulative probability is the sum of the first three bars in the simulation above. Function pbinom() calculates the cumulative binomial probability. pbinom(q = 2, size = 10, prob = 0.3, lower.tail = TRUE) ## [1] 0.3827828 What is the expected number of heads in 25 coin flips if the probability of heads is 0.3? The expected value, \\(\\mu = np\\), is 7.5. Here’s an empirical test from 10,000 samples. mean(rbinom(n = 10000, size = 25, prob = .3)) ## [1] 7.4763 The variance, \\(\\sigma^2 = np (1 - p)\\), is 5.25. Here’s an empirical test. var(rbinom(n = 10000, size = 25, prob = .3)) ## [1] 5.265115 Suppose X and Y are independent random variables distributed \\(X \\sim Bin(10, .6)\\) and \\(Y \\sim Bin(10, .7)\\). What is the probability that either variable is &lt;=4? Let \\(P(A) = P(X&lt;=4)\\) and \\(P(B) = P(Y&lt;=4)\\). Then \\(P(A|B) = P(A) + P(B) - P(AB)\\), and because the events are independent, \\(P(AB) = P(A)P(B)\\). p_a &lt;- pbinom(q = 4, size = 10, prob = 0.6, lower.tail = TRUE) p_b &lt;- pbinom(q = 4, size = 10, prob = 0.7, lower.tail = TRUE) p_a + p_b - (p_a * p_b) ## [1] 0.2057164 Here’s an empirical test. df &lt;- data.frame( x = rbinom(10000, 10, 0.6), y = rbinom(10000, 10, 0.7) ) mean(if_else(df$x &lt;= 4 | df$y &lt;= 4, 1, 0)) ## [1] 0.2078 A couple other points to remember: The Bernoulli distribution is a special case of the binomial with \\(n = 1\\). The binomial distribution assumes independent trials. If you sample without replacement from a finite population, use the hypergeometric distribution. 1.2.3 Poission If \\(X\\) is the number of successes in \\(n\\) (many) trials when the probability of success \\(\\lambda / n\\) is small, then \\(X\\) is a random variable with a Poisson distribution \\(X \\sim Poisson(\\lambda)\\) \\[f(x;\\lambda) = \\frac{e^{-\\lambda} \\lambda^x}{x!} \\hspace{1cm} x \\in (0, 1, ...), \\hspace{2mm} \\lambda &gt; 0\\] with \\(E(X)=\\lambda\\) and \\(Var(X) = \\lambda\\). The Poisson likelihood function is \\[L(\\lambda; x) = \\prod_{i=1}^N f(x_i; \\lambda) = \\prod_{i=1}^N \\frac{e^{-\\lambda} \\lambda^x_i}{x_i !} = \\frac{e^{-n \\lambda} \\lambda^{\\sum x_i}}{\\prod x_i}.\\] The Poisson loglikelihood function is \\[l(\\lambda; x) = \\sum_{i=1}^N x_i \\log \\lambda - n \\lambda.\\] One can show that the loglikelihood function is maximized at \\[\\hat{\\lambda} = \\sum_{i=1}^N x_i / n.\\] Thus, for a Poisson sample, the MLE for \\(\\lambda\\) is just the sample mean. Poisson sampling is used to model counts of events that occur randomly over a fixed period of time. Here is a simple analysis of data from a Poisson process. Data set dat contains frequencies of goal counts during the first round matches of the 2002 World Cup. ## goals freq ## 1 0 23 ## 2 1 37 ## 3 2 20 ## 4 3 11 ## 5 4 2 ## 6 5 1 ## 7 6 0 ## 8 7 0 ## 9 8 1 The MLE of \\(\\lambda\\) from the Poisson distribution is the sample mean. lambda &lt;- weighted.mean(dat$goals, dat$freq) print(lambda) ## [1] 1.378947 The 0.95 CI is \\(\\lambda \\pm z_{.05/2} \\sqrt{\\lambda / n}\\) n &lt;- sum(dat$freq) z &lt;- qnorm(0.975) se &lt;- sqrt(lambda / n) paste0(&quot;[&quot;, round(lambda - z*se, 2), &quot;, &quot;, round(lambda + z*se, 2),&quot;]&quot;) ## [1] &quot;[1.14, 1.62]&quot; The expected probability of scoring 2 goals in a match is \\(\\frac{e^{-1.38} 1.38^2}{2!} = 0.239\\). dpois(x = 2, lambda = lambda) ## [1] 0.2394397 events &lt;- 0:10 density &lt;- dpois(x = events, lambda = 3) prob &lt;- ppois(q = events, lambda = 3, lower.tail = TRUE) df &lt;- data.frame(events, density, prob) ggplot(df, aes(x = factor(events), y = density)) + geom_col() + geom_text( aes(label = round(density, 3), y = density + 0.01), position = position_dodge(0.9), size = 3, vjust = 0 ) + geom_line( data = df, aes(x = events, y = prob/4), size = 1) + scale_y_continuous(sec.axis = sec_axis(~.*4, name = &quot;Cum Prob&quot;)) + theme_mf() + scale_fill_mf() + labs(title = &quot;PMF and CDF of Poisson Distribution&quot;, subtitle = &quot;Poisson(3).&quot;, x = &quot;Events (x)&quot;, y = &quot;Density&quot;) The expected probability of scoring 2 to 4 goals in a match is sum(dpois(x = c(2:4), lambda = lambda)) ## [1] 0.3874391 Or, using the cumulative probability distribution, ppois(q = 4, lambda = lambda) - ppois(q = 1, lambda = lambda) ## [1] 0.3874391 How well does the Poisson distribution fit the 2002 World Cup data? dat %&gt;% mutate(pred = n * dpois(x = goals, lambda = lambda)) %&gt;% rename(obs = freq) %&gt;% pivot_longer(cols = -goals) %&gt;% ggplot(aes(x = goals, y = value, color = name)) + geom_point() + theme_mf() + scale_color_mf() + geom_smooth(se = FALSE) + labs( title = &quot;Poisson Dist: Observed vs Expected&quot;, color = &quot;&quot;, y = &quot;frequencey&quot; ) It fits the data pretty good! \\(Poison(\\lambda) \\rightarrow Bin(n, \\pi)\\) when \\(n\\pi = \\lambda\\) and \\(n \\rightarrow \\infty\\) and \\(\\pi \\rightarrow 0\\). Because the Poisson is limit of the \\(Bin(n, \\pi)\\), it is useful as an approximation to the binomial when \\(n\\) is large (\\(n&gt;=20\\)) and \\(\\pi\\) small (\\(p&lt;=0.05\\)). For example, suppose a baseball player has a p=.03 chance of hitting a homerun. What is the probability of X&gt;=20 homeruns in 500 at-bats? This is a binomial process because the sample size is fixed. pbinom(q = 20, size = 500, prob = 0.03, lower.tail = FALSE) ## [1] 0.07979678 But \\(n\\) is large and \\(\\pi\\) is small, so the Poission distribution will work well too. ppois(q = 20, lambda = 0.03 * 500, lower.tail = FALSE) ## [1] 0.08297091 What is the distribution of successes from a sample of n = 50 when the probability of success is p = .03? n = 500 p = 0.03 x = 0:30 data.frame( events = x, Poisson = dpois(x = x, lambda = p * n), Binomial = dbinom(x = x, size = n, p = p) ) %&gt;% pivot_longer(cols = -events) %&gt;% ggplot(aes(x = events, y = value, color = name)) + geom_point() + theme_mf() + scale_color_mf() + labs(title = &quot;Poisson(15) vs. Bin(500, .03)&quot;, subtitle = &quot;Poisson approximation to binomial.&quot;, x = &quot;Events&quot;, y = &quot;Density&quot;, color = &quot;&quot;) When the observed variance is greater than \\(\\lambda\\) (overdispersion), the Negative Binomial distribution can be used instead of Poisson. Suppose the probability that a drug produces a certain side effect is p = = 0.1% and n = 1,000 patients in a clinical trial receive the drug. What is the probability 0 people experience the side effect? The expected value is np, 1. The probability of measuring 0 when the expected value is 1 is dpois(x = 0, lambda = 1000 * .001) = 0.3678794. 1.2.4 Multinomial If \\(X = (X_1, X_2, \\cdots, X_k)\\) are the counts of successful events in \\(n\\) identical and independent trials of success probabilities \\(\\pi = (\\pi_1, \\pi_2, \\cdots, \\pi_k)\\), then \\(X\\) is a random variable with a multinomial distribution \\(X \\sim Mult(n,\\pi)\\) \\[f(x; n, \\pi) = \\frac{n!}{x_{1}! x_{2}! \\cdots x_{k}!} \\pi^{x_1} \\pi^{x_2} \\cdots \\pi^{x_k} \\hspace{1cm} x \\in \\{0, 1, ..., n \\}, \\hspace{2mm} \\pi \\in [0, 1]\\] with expected values vector \\(E(X_j) = n\\pi_j\\) and covariance matrix \\[Var(X) = \\begin{bmatrix}n\\pi_{1}(1-\\pi_{1}) &amp; -n\\pi_{1}\\pi_{2} &amp; \\cdots &amp; -n\\pi_{1}\\pi_{k}\\\\ -n\\pi_{1}\\pi_{2} &amp; n\\pi_{2}(1-\\pi_{2}) &amp; \\cdots &amp; -n\\pi_{2}\\pi_{k}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ -n\\pi_{1}\\pi_{k} &amp; -n\\pi_{2}\\pi_{k} &amp; \\cdots &amp; n\\pi_{k}(1-\\pi_{k}) \\end{bmatrix}\\] so \\(Var(X_j) = n \\pi_j (1 - \\pi_j)\\) and \\(cov(X_j, X_k) = -n \\pi_j \\pi_k\\). The individual components of a multinomial random vector are binomial and have a binomial distribution, \\(X_i = Bin(n, \\pi_i)\\). Binomial is a special case of multinomial for k = 2. Suppose a city population is 20% black, 15% Hispanic, and 65% other. From a random sample of \\(n = 12\\) persons, what is the probability of 4 black and 8 other? \\[f(x;\\pi) = \\frac{12!}{4! 0! 8!} (0.20)^4 (0.15)^0 (0.65)^8 = 0.0252\\] Function dmultinom() calculates the multinomial probability. dmultinom(x = c(4, 0, 8), prob = c(0.20, 0.15, 0.65)) ## [1] 0.025 To calculate the probability of &lt;= 1 black, combine Hispanic and other, then sum the probability of black = 1 and black = 2. \\[f(x;\\pi) = \\frac{12!}{0! 12!} (0.20)^0 (0.80)^{12} + \\frac{12!}{1! 11!} (0.20)^1 (0.80)^{11} = 0.2748\\] dmultinom(x = c(0, 12), prob = c(0.20, 0.80)) + dmultinom(x = c(1, 11), prob = c(0.20, 0.80)) ## [1] 0.27 1.2.5 Negative-Binomial If \\(X\\) is the count of failure events ocurring prior to reaching \\(r\\) successful events in a sequence of Bernouli trias of success probability \\(p\\), then \\(X\\) is a random variable with a negative-binomial distribution \\(X \\sim NB(r, p)\\). The probability of \\(X = x\\) failures prior to \\(r\\) successes is \\[f(x;r, p) = {{x + r - 1} \\choose {r - 1}} p^r (1-p)^{x}.\\] with \\(E(X) = r (1 - p) / p\\) and \\(Var(X) = r (1-p) / p^2\\). When the data has overdispersion, model the data with the negative-binomial distribution instead of Poission. Examples An oil company has a \\(p = 0.20\\) chance of striking oil when drilling a well. What is the probability the company drills \\(x + r = 7\\) wells to strike oil \\(r = 3\\) times? Note that the question is formulated as counting total events, \\(x + r = 7\\), so translate it to total failed events, \\(x = 4\\). \\[f(x;r, p) = {{4 + 3 - 1} \\choose {3 - 1}} (0.20)^3 (1 - 0.20)^4 = 0.049.\\] Function dnbinom() calculates the negative-binomial probability. Parameter x equals the number of failures, \\(x - r\\). dnbinom(x = 4, size = 3, prob = 0.2) ## [1] 0.049 The expected number of failures prior to 3 successes is \\(E(X) = 3 (1 - 0.20) / 0.20 = 12\\) with variance \\(Var(X) = 3 (1 - 0.20) / 0.20^2 = 60\\). Confirm this with a simulation from n = 10,000 random samples using rnbinom(). my_dat &lt;- rnbinom(n = 10000, size = 3, prob = 0.20) mean(my_dat) ## [1] 12 var(my_dat) ## [1] 61 1.2.6 Geometric If \\(X\\) is the count of Bernoulli trials of success probability \\(p\\) required to achieve the first successful event, then \\(X\\) is a random variable with a geometric distribution \\(X \\sim G(p)\\). The probability of \\(X = x\\) trials is \\[f(x; p) = p(1-p)^{x-1}.\\] with \\(E(X)=\\frac{{n}}{{p}}\\) and \\(Var(X) = \\frac{(1-p)}{p^2}\\). The probability of \\(X&lt;=n\\) trials is \\[F(X=n) = 1 - (1-p)^n.\\] Examples What is the probability a marketer encounters x = 3 people on the street who did not attend a sporting event before the first success if the population probability is p = 0.20? \\[f(4; 0.20) = 0.20(1-0.20)^{4-1} = 0.102.\\] Function dgeom() calculates the geometric distribution probability. Parameter x is the number of failures, not the number of trials. dgeom(x = 3, prob = 0.20) ## [1] 0.1 data.frame(cnt = rgeom(n = 10000, prob = 0.20)) %&gt;% count(cnt) %&gt;% top_n(n = 15, wt = n) %&gt;% ungroup() %&gt;% mutate(pct = round(n / sum(n), 3), X_eq_x = cnt == 3) %&gt;% ggplot(aes(x = as.factor(cnt), y = n, fill = X_eq_x, label = pct)) + geom_col(alpha = 0.8) + scale_fill_mf() + geom_text(size = 3) + theme_mf() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Distribution of trials prior to first success&quot;, subtitle = paste(&quot;P(X = 3) | X ~ G(.2) = &quot;, round(dgeom(2, .2), 3)), x = &quot;Unsuccessful trials&quot;, y = &quot;Count&quot;, caption = &quot;simulation of n = 10,000 samples from geometric dist.&quot;) What is the probability the marketer fails to find someone who attended a game in x &lt;= 5 trials before finding someone who attended a game on the sixth trial when the population probability is p = 0.20? p = 0.20 n = 5 # exact pgeom(q = n, prob = p, lower.tail = TRUE) ## [1] 0.74 # simulated mean(rgeom(n = 10000, prob = p) &lt;= n) ## [1] 0.74 What is the probability the marketer fails to find someone who attended a game on x &gt;= 5 trials before finding someone who attended a game on the next trial? p = 0.20 n = 5 # exact pgeom(q = n, prob = p, lower.tail = FALSE) ## [1] 0.26 # simulated mean(rgeom(n = 10000, prob = p) &gt; n) ## [1] 0.26 The expected number of trials to achieve the first success is 1 / 0.20 = 5, Var(X) = (1 - 0.20) / 0.20^2 = 20? p = 0.20 # mean # exact 1 / p ## [1] 5 # simulated mean(rgeom(n = 10000, prob = p)) + 1 ## [1] 5 # Variance # exact (1 - p) / p^2 ## [1] 20 # simulated var(rgeom(n = 100000, prob = p)) ## [1] 20 1.2.7 Hypergeometric If \\(X\\) is the count of successful events in a sample of size \\(n\\) without replacement from a population of size \\(N\\) containing \\(K\\) successes and \\(N-K\\) non-successes, then \\(X\\) is a random variable with a hypergeometric distribution \\[f(x|N,K,n) = \\frac{{{K}\\choose{k}}{{N-K}\\choose{n-k}}}{{N}\\choose{n}}.\\] with \\(E(X) = n\\frac{K}{N}\\) and \\(Var(X) = n \\frac{K}{N} \\cdot \\frac{N-n}{N} \\cdot \\frac{N-K}{N-1}\\). The formula follows from the frequency table of the possible outcomes. Sampled Not Sampled Total success k K-k K non-success n-k (N-K)-(n-k) N-K Total n N-n N If \\(X\\) is the count of successful events in a sample of size \\(k\\) without replacement from a population containing \\(M\\) successes and \\(N\\) non-successes, then \\(X\\) is a random variable with a hypergeometric distribution \\[f(x|m,n,k) = \\frac{{{m}\\choose{x}}{{n}\\choose{k-x}}}{{m+n}\\choose{k}}.\\] with \\(E(X)=k\\frac{m}{m+n}\\) and \\(Var(X) = k\\frac{m}{m+n}\\cdot\\frac{m+n-k}{m+n}\\cdot\\frac{n}{m+n-1}\\). phyper returns the cumulative probability (percentile) p at the specified value (quantile) q. qhyper returns the value (quantile) q at the specified cumulative probability (percentile) p. Example What is the probability of selecting \\(X = 14\\) red marbles from a sample of \\(k = 20\\) taken from an urn containing \\(m = 70\\) red marbles and \\(n = 30\\) green marbles? Function dhyper() calculates the hypergeometric probability. x = 14 m = 70 n = 30 k = 20 dhyper(x = x, m = m, n = n, k = k) ## [1] 0.21 The expected value is 14 and variance is 3.39. The hypergeometric random variable is similar to the binomial random variable except that it applies to situations of sampling without replacement from a small population. As the population size increases, sampling without replacement converges to sampling with replacement, and the hypergeometric distribution converges to the binomial. What if the total population size is 250? 500? 1000? 1.2.8 Gamma If \\(X\\) is the interval until the \\(\\alpha^{th}\\) successful event when the average interval is \\(\\theta\\), then \\(X\\) is a random variable with a gamma distribution \\(X \\sim \\Gamma(\\alpha, \\theta)\\). The probability of an interval of \\(X = x\\) is \\[f(x; \\alpha, \\theta) = \\frac{1}{\\Gamma(\\alpha)\\theta^\\alpha}x^{\\alpha-1}e^{-x/\\theta}.\\] where \\(\\Gamma(\\alpha) = (1 - \\alpha)!\\) with \\(E(X) = \\alpha \\theta\\) and \\(Var(X) = \\alpha \\theta^2\\). Examples On average, someone sends a money order once per 15 minutes (\\(\\theta = .25\\)). What is the probability someone sends \\(\\alpha = 10\\) money orders in less than \\(x = 3\\) hours?* theta = 0.25 alpha = 10 pgamma(q = 3, shape = alpha, scale = 0.25) ## [1] 0.76 data.frame(x = 0:1000 / 100, prob = pgamma(q = 0:1000 / 100, shape = alpha, scale = theta, lower.tail = TRUE)) %&gt;% mutate(Interval = ifelse(x &gt;= 0 &amp; x &lt;= 3, &quot;0 to 3&quot;, &quot;other&quot;)) %&gt;% ggplot(aes(x = x, y = prob, fill = Interval)) + geom_area(alpha = 0.9) + theme_mf() + scale_fill_mf() + labs(title = &quot;X ~ Gam(alpha = 10, theta = .25)&quot;, subtitle = &quot;Probability of 10 events in X hours when the mean time to an event is .25 hours.&quot;, x = &quot;Interval (x)&quot;, y = &quot;pgamma&quot;) "],
["cont-dist.html", "1.3 Continuous Distributions", " 1.3 Continuous Distributions 1.3.1 Normal Random variable \\(X\\) is distributed \\(X \\sim N(\\mu, \\sigma^2)\\) if \\[f(X)=\\frac{{1}}{{\\sigma \\sqrt{{2\\pi}}}}e^{-.5(\\frac{{x-\\mu}}{{\\sigma}})^2}\\]. Example IQ scores are distributed \\(X \\sim N(100, 16^2\\). What is the probability a randomly selected person’s IQ is &lt;90? my_mean = 100 my_sd = 16 my_x = 90 # exact pnorm(q = my_x, mean = my_mean, sd = my_sd, lower.tail = TRUE) ## [1] 0.27 # simulated mean(rnorm(n = 10000, mean = my_mean, sd = my_sd) &lt;= my_x) ## [1] 0.26 library(dplyr) library(ggplot2) data.frame(x = 0:1500 / 10, prob = pnorm(q = 0:1500 / 10, mean = my_mean, sd = my_sd, lower.tail = TRUE)) %&gt;% mutate(cdf = ifelse(x &gt; 0 &amp; x &lt;= my_x, prob, 0)) %&gt;% ggplot() + geom_line(aes(x = x, y = prob)) + geom_area(aes(x = x, y = cdf), alpha = 0.3) + labs(title = bquote(&#39;X~N(&#39;~mu==.(my_mean)~&#39;,&#39;~sigma^{2}==.(my_sd)^{2}~&#39;)&#39;), subtitle = bquote(&#39;P(X&lt;=&#39;~.(my_x)~&#39;) when mean is&#39;~.(my_mean)~&#39; and variance is&#39;~.(my_sd)^{2}~&#39;.&#39;), x = &quot;x&quot;, y = &quot;Probability&quot;) 1.3.1.1 Example IQ scores are distributed \\(X \\sim N(100, 16^2\\). What is the probability a randomly selected person’s IQ is &gt;140? my_mean = 100 my_sd = 16 my_x = 140 # exact pnorm(q = my_x, mean = my_mean, sd = my_sd, lower.tail = FALSE) ## [1] 0.0062 # simulated mean(rnorm(n = 10000, mean = my_mean, sd = my_sd) &gt; my_x) ## [1] 0.0052 library(dplyr) library(ggplot2) data.frame(x = 0:1500 / 10, prob = pnorm(q = 0:1500 / 10, mean = my_mean, sd = my_sd, lower.tail = TRUE)) %&gt;% mutate(cdf = ifelse(x &gt; my_x &amp; x &lt; 1000, prob, 0)) %&gt;% ggplot() + geom_line(aes(x = x, y = prob)) + geom_area(aes(x = x, y = cdf), alpha = 0.3) + labs(title = bquote(&#39;X~N(&#39;~mu==.(my_mean)~&#39;,&#39;~sigma^{2}==.(my_sd)^{2}~&#39;)&#39;), subtitle = bquote(&#39;P(X&lt;=&#39;~.(my_x)~&#39;) when mean is&#39;~.(my_mean)~&#39; and variance is&#39;~.(my_sd)^{2}~&#39;.&#39;), x = &quot;x&quot;, y = &quot;Probability&quot;) 1.3.1.2 Example IQ scores are distributed \\(X \\sim N(100, 16^2\\). What is the probability a randomly selected person’s IQ is between 92 and 114? my_mean = 100 my_sd = 16 my_x_l = 92 my_x_h = 114 # exact pnorm(q = my_x_h, mean = my_mean, sd = my_sd, lower.tail = TRUE) - pnorm(q = my_x_l, mean = my_mean, sd = my_sd, lower.tail = TRUE) ## [1] 0.5 library(dplyr) library(ggplot2) data.frame(x = 0:1500 / 10, prob = pnorm(q = 0:1500 / 10, mean = my_mean, sd = my_sd, lower.tail = TRUE)) %&gt;% mutate(cdf = ifelse(x &gt; my_x_l &amp; x &lt;= my_x_h, prob, 0)) %&gt;% ggplot() + geom_line(aes(x = x, y = prob)) + geom_area(aes(x = x, y = cdf), alpha = 0.3) + labs(title = bquote(&#39;X~N(&#39;~mu==.(my_mean)~&#39;,&#39;~sigma^{2}==.(my_sd)^{2}~&#39;)&#39;), subtitle = bquote(&#39;P(X&lt;=&#39;~.(my_x)~&#39;) when mean is&#39;~.(my_mean)~&#39; and variance is&#39;~.(my_sd)^{2}~&#39;.&#39;), x = &quot;x&quot;, y = &quot;Probability&quot;) 1.3.1.3 Example Class scores are distributed \\(X \\sim N(70, 10^2\\). If the instructor wants to give A’s to &gt;=85th percentile and B’s to 75th-85th percentile, what are the cutoffs? my_mean = 70 my_sd = 10 my_pct_l = .75 my_pct_h = .85 qnorm(p = my_pct_l, mean = my_mean, sd = my_sd, lower.tail = TRUE) ## [1] 77 qnorm(p = my_pct_h, mean = my_mean, sd = my_sd, lower.tail = TRUE) ## [1] 80 library(dplyr) library(ggplot2) data.frame(x = 0:1000 / 10, prob = pnorm(q = 0:1000 / 10, mean = my_mean, sd = my_sd, lower.tail = TRUE)) %&gt;% mutate(cdf = ifelse(prob &gt; my_pct_l &amp; prob &lt;= my_pct_h, prob, 0)) %&gt;% ggplot() + geom_line(aes(x = x, y = prob)) + geom_area(aes(x = x, y = cdf), alpha = 0.3) + labs(title = bquote(&#39;X~N(&#39;~mu==.(my_mean)~&#39;,&#39;~sigma^{2}==.(my_sd)^{2}~&#39;)&#39;), subtitle = bquote(&#39;P(X&lt;=x) = [&#39;~.(my_pct_l)~&#39;,&#39;~.(my_pct_h)~&#39;] when mean is&#39;~.(my_mean)~&#39; and variance is&#39;~.(my_sd)^{2}~&#39;.&#39;), x = &quot;x&quot;, y = &quot;Probability&quot;) 1.3.1.4 Normal Approximation to Binomial The CLT implies that certain distributions can be approximated by the normal distribution. The binomial distribution \\(X \\sim B(n,p)\\) is approximately normal with mean \\(\\mu = n p\\) and variance \\(\\sigma^2=np(1-p)\\). The approximation is useful when the expected number of successes and failures is at least 5: \\(np&gt;=5\\) and \\(n(1-p)&gt;=5\\). 1.3.1.5 Example A measure requires p&gt;=50% popular to pass. A sample of n=1,000 yields x=460 approvals. What is the probability that the overall population approves, P(X)&gt;0.5? my_x = 460 my_p = 0.50 my_n = 1000 my_mean = my_p * my_n my_sd = round(sqrt(my_n * my_p * (1 - my_p)), 1) # Exact binomial pbinom(q = my_x, size = my_n, prob = my_p, lower.tail = TRUE) ## [1] 0.0062 # Normal approximation pnorm(q = my_x, mean = my_p * my_n, sd = sqrt(my_n * my_p * (1 - my_p)), lower.tail = TRUE) ## [1] 0.0057 library(dplyr) library(ggplot2) library(tidyr) data.frame(x = 400:600, Normal = pnorm(q = 400:600, mean = my_p * my_n, sd = sqrt(my_n * my_p * (1 - my_p)), lower.tail = TRUE), Binomial = pbinom(q = 400:600, size = my_n, prob = my_p, lower.tail = TRUE)) %&gt;% gather(key = &quot;Distribution&quot;, value = &quot;cdf&quot;, c(-x)) %&gt;% ggplot(aes(x = x, y = cdf, color = Distribution)) + geom_line() + labs(title = bquote(&#39;X~B(n=&#39;~.(my_n)~&#39;, p=&#39;~.(my_p)~&#39;), &#39;~&#39;X~N(&#39;~mu==.(my_mean)~&#39;,&#39;~sigma^{2}==.(my_sd)^{2}~&#39;)&#39;), subtitle = &quot;Normal approximation to the binomial&quot;, x = &quot;x&quot;, y = &quot;Probability&quot;) The Poisson distribution \\(x~P(\\lambda)\\) is approximately normal with mean \\(\\mu = \\lambda\\) and variance \\(\\sigma^2 = \\lambda\\), for large values of \\(\\lambda\\). 1.3.1.6 Example The annual number of earthquakes registering at least 2.5 on the Richter Scale and having an epicenter within 40 miles of downtown Memphis follows a Poisson distribution with mean \\(\\lambda=6.5\\). What is the probability that at least \\(x&gt;=9\\)* such earthquakes will strike next year?* my_x = 9 my_lambda = 6.5 my_sd = round(sqrt(my_lambda), 2) # Exact Poisson ppois(q = my_x - 1, lambda = my_lambda, lower.tail = FALSE) ## [1] 0.21 # Normal approximation pnorm(q = my_x - 0.5, mean = my_lambda, sd = my_sd, lower.tail = FALSE) ## [1] 0.22 library(dplyr) library(ggplot2) library(tidyr) data.frame(x = 0:200 / 10, Normal = pnorm(q = 0:200 / 10, mean = my_lambda, sd = my_sd, lower.tail = TRUE), Poisson = ppois(q = 0:200 / 10, lambda = my_lambda, lower.tail = TRUE)) %&gt;% gather(key = &quot;Distribution&quot;, value = &quot;cdf&quot;, c(-x)) %&gt;% ggplot(aes(x = x, y = cdf, color = Distribution)) + geom_line() + labs(title = bquote(&#39;X~P(&#39;~lambda~&#39;=&#39;~.(my_lambda)~&#39;), &#39;~&#39;X~N(&#39;~mu==.(my_lambda)~&#39;,&#39;~sigma^{2}==.(my_lambda)~&#39;)&#39;), subtitle = &quot;Normal approximation to the Poisson&quot;, x = &quot;x&quot;, y = &quot;Probability&quot;) 1.3.1.7 From Sample to Population Suppose a person’s blood pressure typically measures 160?20 mm. If one takes n=5 blood pressure readings, what is the probability the average will be &lt;=150? my_mu = 160 my_sigma = 20 my_n = 5 my_x = 150 my_se = round(my_sigma / sqrt(my_n), 1) pnorm(q = my_x, mean = my_mu, sd = my_sigma / sqrt(my_n), lower.tail = TRUE) ## [1] 0.13 library(dplyr) library(ggplot2) data.frame(x = 1000:2000 / 10, prob = pnorm(q = 1000:2000 / 10, mean = my_mu, sd = my_sigma / sqrt(my_n), lower.tail = TRUE)) %&gt;% mutate(cdf = ifelse(x &gt; 0 &amp; x &lt;= my_x, prob, 0)) %&gt;% ggplot() + geom_line(aes(x = x, y = prob)) + geom_area(aes(x = x, y = cdf), alpha = 0.3) + labs(title = bquote(&#39;X~N(&#39;~mu==.(my_mu)~&#39;,&#39;~sigma^{2}==.(my_se)^{2}~&#39;)&#39;), subtitle = bquote(&#39;P(X&lt;=&#39;~.(my_x)~&#39;) when mean is&#39;~.(my_mu)~&#39; and variance is&#39;~sigma~&#39;/sqrt(n)&#39;~.(my_se)^{2}~&#39;.&#39;), x = &quot;x&quot;, y = &quot;Probability&quot;) knitr::include_app(&quot;https://mpfoley73.shinyapps.io/shiny_dist/&quot;, height = &quot;600px&quot;) "],
["join-distributions.html", "1.4 Join Distributions", " 1.4 Join Distributions "],
["likelihood.html", "1.5 Likelihood", " 1.5 Likelihood The likelihood function is the likelihood of a parameter \\(\\theta\\) given an observed value of the random variable \\(X\\). The likelihood function is identical to the probability distribution function, except that it reverses which variable is considered fixed. E.g., the binomial probability distribution expresses the probability that \\(X = x\\) given the success probability \\(\\theta = \\pi\\). \\[f(x|\\pi) = \\frac{n!}{x!(n-x)!} \\pi^x (1-\\pi)^{n-x}.\\] The corresponding likelihood function expresses the probability that \\(\\pi = p\\) given the observed value \\(x\\). \\[L(p|x) = \\frac{n!}{x!(n-x)!} p^x (1-p)^{n-x}.\\] You usually want to know the value of \\(\\theta\\) at the maximum of the likelihood function. When taking derivatives, any multiplicative constant is irrevelant and can be discarded. So for the binomial distribution, the likelihood function for \\(\\pi\\) may instead be expressed as \\[L(p|x) \\propto p^x (1-p)^{n-x}\\] Calculating the maximum is usually simplified using the log-likelihood, \\(l(\\theta|x) = \\log L(\\theta|x)\\). For the binomial distribution, \\(l(p|x) = x \\log p + (n - x) \\log (1 - p)\\). Frequently you derive loglikelihood from a sample. The overall likelihood is the product of the individual likelihoods, and the overall loglikelihood is the log of the overall likelihood. \\[l(\\theta|x) = \\log \\prod_{i=1}^n f(x_i|\\theta)\\] Here are plots of the binomial log-likelihood of \\(pi\\) for several values of \\(X\\) from a sample of size \\(n = 5\\). As the total sample size \\(n\\) grows, the loglikelihood function becomes more sharply peaked around its maximum, and becomes nearly quadratic (i.e. a parabola, if there is a single parameter). Here is the same plot with \\(n = 500\\). The value of \\(\\theta\\) that maximizes \\(l\\) (and \\(L\\)) is the maximum-likelihood estimator (MLE) of \\(\\theta\\), \\(\\hat{\\theta}\\). E.g., suppose you have an experiment of \\(n = 5\\) Bernoulli trials \\(\\left(X \\sim Bin(5, \\pi) \\right)\\) with and \\(X = 3\\) successful events. A plot of \\(L(p|x) = p^3(1 - p)^2\\) shows the MLE is at \\(p = 0.6\\). This approach is called maximum-likelihood estimation. MLE usually involves setting the derivatives to zero and solving for \\(theta\\). "],
["discrete-analysis.html", "Chapter 2 Categorical Analysis - Nonmodel", " Chapter 2 Categorical Analysis - Nonmodel This section describes interactions or associations between two or three categorical variables mostly via single summary statistics and with significance testing. This non-model based analysis does not handle more complicated situations such as simultaneous effects of multiple variables, or mixtures of categorical and continuous variables. "],
["chi-square-test.html", "2.1 Chi-Square Test", " 2.1 Chi-Square Test These notes rely on PSU STAT 500, Wikipedia, and Disha M. The chi-square test compares observed categorical variable frequency counts \\(O\\) with their expected values \\(E\\). The test statistic \\(X^2 = \\sum (O - E)^2 / E\\) is distributed \\(\\chi^2\\). \\(H_0: O = E\\) and \\(H_a\\) is at least one pair of frequency counts differ. The chi-square test relies on the central limit theorem, so it is valid for independent, normally distributed samples, typically affirmed with at least 5 successes and failures in each cell. There a small variations in the chi-square for its various applications. The chi-square goodness-of-fit test tests whether observed frequency counts \\(O_j\\) of the \\(j \\in (0, 1, \\cdots k)\\) levels of a single categorical variable differ from expected frequency counts \\(E_j\\). \\(H_0\\) is \\(O_j = E_j\\). The chi-square independence test tests whether observed joint frequency counts \\(O_{ij}\\) of the \\(i \\in (0, 1, \\cdots I)\\) levels of categorical variable \\(Y\\) and the \\(j \\in (0, 1, \\cdots J)\\) levels of categorical variable \\(Z\\) differ from expected frequency counts \\(E_{ij}\\) under the independence model where \\(\\pi_{ij} = \\pi_{i+} \\pi_{+j}\\), the joint densities. \\(H_0\\) is \\(O_{ij} = E_{ij}\\). The chi-square homogeneity test tests whether frequency counts of the \\(R\\) levels of a categorical variable are distributed identically across \\(C\\) different populations. "],
["one-way-tables.html", "2.2 One-Way Tables", " 2.2 One-Way Tables These notes rely on PSU STATS 504 course notes. A one-way table is a frequency table for a single categorical variable. You usually construct a one-way table to test whether the frequency counts differ from a hypothesized distribution using the chi-square goodness-of-fit test. You may also simply want to construct a confidence interval around a proportion. Here is an example. A researcher crosses tall cut-leaf tomatoes with dwarf potato-leaf tomatoes, then classifies the (\\(n = 1,611\\)) offspring phenotypes. o &lt;- c(926, 288, 293, 104) cell_names &lt;- c(&quot;tall cut-leaf&quot;, &quot;tall potato-leaf&quot;, &quot;dwarf cut-leaf&quot;, &quot;dwarf potato-leaf&quot;) names(o) &lt;- cell_names print(o) ## tall cut-leaf tall potato-leaf dwarf cut-leaf dwarf potato-leaf ## 926 288 293 104 The four phenotypes are expected to occur with relative frequencies 9:3:3:1. pi &lt;- c(9, 3, 3, 1) / (9 + 3 + 3 + 1) print(pi) ## [1] 0.562 0.188 0.188 0.062 e &lt;- sum(o) * pi names(e) &lt;- cell_names print(e) ## tall cut-leaf tall potato-leaf dwarf cut-leaf dwarf potato-leaf ## 906 302 302 101 data.frame(O = o, E = e) %&gt;% rownames_to_column(var = &quot;i&quot;) %&gt;% pivot_longer(cols = -i, values_to = &quot;freq&quot;) %&gt;% group_by(name) %&gt;% mutate(pct = freq / sum(freq)) %&gt;% ungroup() %&gt;% ggplot(aes(x = i, y = freq, fill = name, label = paste0(round(freq, 0), &quot;\\n&quot;, scales::percent(pct, accuracy = 0.1))) ) + geom_col(position = position_dodge()) + geom_text(position = position_dodge(width = 0.9), size = 2.8) + theme_mf() + scale_fill_mf() + labs(title = &quot;Observed vs Expected&quot;, fill = &quot;&quot;) Do the observed phenotype counts conform to the expected proportions? This is a goodness-of-fit question because you are comparing frequencies from a single categorical variable to a set of hypothesized frequencies. 2.2.1 Chi-Square Goodness-of-Fit Test The chi-square goodness-of-fit test tests whether observed frequency counts \\(O_j\\) of the \\(J\\) levels of a categorical variable differ from expected frequency counts \\(E_j\\) in a sample. \\(H_0\\) is \\(O_j = E_j\\). There are two possible test statistics for this test, Pearson \\(X^2\\) and deviance \\(G^2\\). The sampling distributions of \\(X^2\\) and \\(G^2\\) approach the \\(\\chi_{J-1}^2\\) as the sample size \\(n \\rightarrow \\infty\\). It’s a good idea to calculate both test statistics. The Pearson goodness-of-fit statistic is \\[X^2 = \\sum \\frac{(O_j - E_j)^2}{E_j}\\] where \\(O_j = p_j n\\) and \\(E_j = \\pi_j n\\). There is a variation of the \\(X^2\\) statistic that corrects for small cell counts by subtracting 0.5 from each cell, the Yates Continuity Correction. \\[X^2 = \\sum \\frac{(O_j - E_j - 0.5)^2}{E_j}\\] The deviance statistic, aka likelihood-ratio chi-square test statistic, is \\[G^2 = 2 \\sum O_j \\log \\left[ \\frac{O_j}{E_j} \\right]\\] If the saturated model (the observed data represent the fit of the saturated model, the most complex model possible with the data) proportions \\(p_j\\) equal equal the expected proportions \\(\\pi_j\\), \\(X^2\\) and \\(G^2\\) will equal zero. Large values indicate the data do not agree well with the proposed model. You can perform a chi-square test of significance with the \\(G^2\\) and \\(X^2\\) test statistics with \\(dof\\) degrees of freedom (d.f.). The chi-square test is reliable when at least 80% of \\(E_j &gt;= 5\\). Calculate \\(X^2\\) as x2 &lt;- sum((o - e)^2 / e) = 1.47 and the \\(G^2\\) as g2 &lt;- 2 * sum(o * log(o / e)) = 1.48. The degrees of freedom are length(o) - 1 = 3. The chi-sq test p-values are nearly identical. pchisq(q = x2, df = dof, lower.tail = FALSE) ## [1] 0.69 pchisq(q = g2, df = dof, lower.tail = FALSE) ## [1] 0.69 chisq.test() performs the chi-square test of the Pearson test statistic. chisq.test(o, p = pi) ## ## Chi-squared test for given probabilities ## ## data: o ## X-squared = 1, df = 3, p-value = 0.7 The p-values based on the \\(\\chi^2\\) distribution with 3 d.f. are about 0.69, so the test fails to reject the null hypothesis that the observed frequencies are consistent with the theory. The plot of the chi-squared distribution shows \\(X^2\\) well outside the \\(\\alpha = 0.05\\) range of rejection. alpha &lt;- 0.05 dof &lt;- length(e) - 1 lrr = -Inf p_val &lt;- pchisq(q = x2, df = length(o) - 1, lower.tail = FALSE) urr = qchisq(p = alpha, df = dof, lower.tail = FALSE) data.frame(chi2 = seq(from = 0, to = 20, by = .1)) %&gt;% mutate(density = dchisq(x = chi2, df = dof)) %&gt;% mutate(rr = ifelse(chi2 &lt; lrr | chi2 &gt; urr, density, 0)) %&gt;% ggplot() + geom_line(aes(x = chi2, y = density), color = mf_pal(12)(12)[12], size = 0.8) + geom_area(aes(x = chi2, y = rr), fill = mf_pal(12)(12)[2], alpha = 0.8) + geom_vline(aes(xintercept = x2), color = mf_pal(12)(12)[11], size = 0.8) + labs(title = bquote(&quot;Chi-Square Goodness-of-Fit Test&quot;), subtitle = paste0(&quot;X^2=&quot;, round(x2,2), &quot;, &quot;, &quot;Critical value=&quot;, round(urr,2), &quot;, &quot;, &quot;p-value=&quot;, round(p_val,3), &quot;.&quot; ), x = &quot;chisq&quot;, y = &quot;Density&quot;) + theme(legend.position=&quot;none&quot;) + theme_mf() If you reject \\(H_0\\), you can inspect the residuals to learn which differences may have lead to rejecting the rejection. \\(X^2\\) and \\(G^2\\) are sums of squared cell comparisons, or “residuals”. The expected value of a \\(\\chi^2\\) random variable is its d.f., \\(k - 1\\), so the average residual size is \\((k - 1) / k\\). The typical residual should be within 2 \\(\\sqrt{(k - 1) / k}\\). e2_res &lt;- sqrt((o - e)^2 / e) g2_res &lt;- sign(o - e) * sqrt(abs(2 * o * log(o / e))) data.frame(e2_res) %&gt;% rownames_to_column() %&gt;% # pivot_longer(cols = e2_res:g2_res) %&gt;% ggplot(aes(x = rowname, y = e2_res)) + geom_point(size = 3, color = mf_pal(12)(12)[2], alpha = 0.8) + theme_mf() + labs(title = &quot;X^2 Residuals by Cell&quot;, color = &quot;&quot;, x = &quot;&quot;, y = &quot;&quot;) If you want to test whether the data conform to a particular distribution instead of some set of theoretical values, the test is nearly the same except for an adjustment to the d.f. Your first step is the estimate the distribution’s parameter(s). Then you perform the goodness of fit test, but with degrees of freedom reduced for each estimated parameter. For example, suppose you sample \\(n = 100\\) families and count the number of children. The count of children should be a Poisson random variable, \\(J \\sim Pois(\\lambda)\\). dat &lt;- data.frame(j = 0:5, o = c(19, 26, 29, 13, 10, 3)) The ML estimate for \\(\\lambda\\) is \\[\\hat{\\lambda} = \\frac{j_0 O_0 + j_1 O_1, + \\cdots j_k O_k}{O}\\] lambda_hat &lt;- sum(dat$j * dat$o) / sum(dat$o) print(lambda_hat) ## [1] 1.8 The probabilities for each possible count are \\[f(j; \\lambda) = \\frac{e^{-\\hat{\\lambda}} \\hat{\\lambda}^j}{j!}.\\] f &lt;- exp(-lambda_hat) * lambda_hat^dat$j / factorial(dat$j) E &lt;- f * sum(dat$o) dat &lt;- cbind(dat, e = E) dat %&gt;% rename(pois = e) %&gt;% pivot_longer(cols = -j, values_to = &quot;freq&quot;) %&gt;% group_by(name) %&gt;% mutate(pct = freq / sum(freq)) %&gt;% ungroup() %&gt;% ggplot(aes(x = fct_inseq(as.factor(j)), y = freq, fill = name, label = paste0(round(freq, 0), &quot;\\n&quot;, scales::percent(pct, accuracy = 0.1))) ) + geom_col(position = position_dodge()) + geom_text(position = position_dodge(width = 0.9), size = 2.8) + theme_mf() + scale_fill_mf() + labs(title = &quot;Observed vs Expected&quot;, fill = &quot;&quot;, x = &quot;children in family&quot;) Compare the expected values to the observed values with the \\(\\chi^2\\) goodness of fit test. In this case, \\(df = 6 - 1 - 1\\) because the estimated paramater \\(\\lambda\\) reduces d.f. by 1. (X2 &lt;- sum((dat$o - dat$e)^2 / dat$e)) ## [1] 2.8 (dof &lt;- nrow(dat) - 1 - 1) ## [1] 4 pchisq(q = X2, df = dof) ## [1] 0.42 Be careful of this adjustment to the d.f. because chisq.test() does not take this into account, and you cannot override the d.f.. chisq.test(dat$o, p = dat$e / sum(dat$e)) ## Warning in chisq.test(dat$o, p = dat$e/sum(dat$e)): Chi-squared approximation ## may be incorrect ## ## Chi-squared test for given probabilities ## ## data: dat$o ## X-squared = 3, df = 5, p-value = 0.7 2.2.2 Proportion Test A special case of the one-way table is the \\(2 \\times 1\\) table for a binomial random variable. When you calculate a single proportion \\(p\\), you can compare it to a hypothesized \\(\\pi_0\\), or create a confidence interval around the estimate. Suppose a company claims to resolve at least 70% of maintenance requests within 24 hours. In a random sample of \\(n = 50\\) repair requests, the company resolves \\(O_1 = 33\\) (\\(p_1 = 66\\%)\\) within 24 hours. At a 5% level of significance, is the maintenance company’s claim valid? o &lt;- c(33, 17) n &lt;- sum(o) cell_names &lt;- c(&quot;resolved&quot;, &quot;not resolved&quot;) names(o) &lt;- cell_names print(o) ## resolved not resolved ## 33 17 The null hypothesis is that the maintenance company resolves \\(\\pi_0 = 0.70\\) of requests within 24 hours, \\(H_0: \\pi = \\pi_0\\) with alternative hypothesis \\(H_a: \\pi &lt; \\pi_0\\). This is a left-tailed test with an \\(\\alpha = 0.05\\) level of significance. pi_0 &lt;- 0.70 alpha &lt;- 0.05 The sample is independently drawn without replacement from &lt;10% of the population (by assumption) and there were &gt;=5 successes, so you can use the Clopper-Pearson exact binomial test. Clopper-Pearson inverts two single-tailed binomial tests at the desired alpha. binom.test(x = o, p = pi_0, alternative = &quot;less&quot;, conf.level = 1 - alpha) ## ## Exact binomial test ## ## data: o ## number of successes = 33, number of trials = 50, p-value = 0.3 ## alternative hypothesis: true probability of success is less than 0.7 ## 95 percent confidence interval: ## 0.00 0.77 ## sample estimates: ## probability of success ## 0.66 There is insufficient evidence (p = 0.3161) to reject \\(H_0\\) that true probability of success is less than 0.7. x &lt;- c(0:50) p_x &lt;- dbinom(x = x, size = n, prob = pi_0) observed &lt;- factor(if_else(x == o[1], 1, 0)) data.frame(x, p_x, observed) %&gt;% ggplot(aes(x = x, y = p_x, fill = observed)) + geom_col() + theme_mf() + scale_fill_mf() + labs(title = &quot;Exact Binomial&quot;) There were &gt;=5 failures, &gt;=30 observations, and the measured probability of success was within (.2,.80), so you can also use the Wald normal approximation method where \\(\\pi = p \\pm z_{\\alpha/2} SE\\) and \\(Z = (p - \\pi_0) / SE\\) where \\(SE = \\sqrt{\\pi_0 (1 - \\pi_0) / n}\\). p &lt;- o[1] / sum(o) se &lt;- sqrt(pi_0 * (1 - pi_0) / sum(o)) z &lt;- (p - pi_0) / se pnorm(q = p, mean = pi_0, sd = se, lower.tail = TRUE) ## resolved ## 0.27 Again, there is insufficient evidence (p = 0.2685) to reject \\(H_0\\) that true probability of success is less than 0.7. The 95% CI around the measured p = 0.66 is z_alpha &lt;- qnorm(0.95, mean = p, sd = se, lower.tail = FALSE) c(0, p + z_alpha * se) ## resolved ## 0.0 0.7 "],
["two-way-tables.html", "2.3 Two-Way Tables", " 2.3 Two-Way Tables These notes rely on PSU STATS 504 course notes. A two-way frequency table is a frequency table for two categorical variables. You usually construct a two-way table to test whether the frequency counts in one categorical variable differ from the other categorical variable using the chi-square independence test. If there is a significant difference (i.e., the variables are related), then describe the relationship with an analysis of the residuals, calculations of measures of association (difference in proportions, relative risk, or odds ratio), and partition tests. Here are three case studies that illustrate the concepts. The first is a simple 2x2 table. The second is a 3x2 table that extends some of the concepts. The third is a 2x4 table where one factor is ordinal. Study 1: “Vitamin C” 2x2 Table. A double blind study investigated whether vitamin C prevents common colds on a sample of n = 279 persons. This study has two categorical variables each with two levels, a 2x2 two way table. vitc_o &lt;- matrix( c(31, 17, 109, 122), ncol = 2, dimnames = list( treat = c(&quot;Placebo&quot;, &quot;VitaminC&quot;), resp = c(&quot;Cold&quot;, &quot;NoCold&quot;) ) ) vitc_o %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot; &quot;) %&gt;% janitor::adorn_totals(where = c(&quot;row&quot;, &quot;col&quot;)) ## Cold NoCold Total ## Placebo 31 109 140 ## VitaminC 17 122 139 ## Total 48 231 279 Study 2: “Smoking” 3x2 Table. An analysis classifies n = 5375 high school students by their smoking behavior and the smoking behavior of their parents. smoke_o &lt;- matrix( c(400, 416, 188, 1380, 1823, 1168), ncol = 2, dimnames = list( parents = c(&quot;Both&quot;, &quot;One&quot;, &quot;Neither&quot;), student = c(&quot;Smoker&quot;, &quot;Non-smoker&quot;)) ) smoke_o %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot; &quot;) %&gt;% janitor::adorn_totals(where = c(&quot;row&quot;, &quot;col&quot;)) ## Smoker Non.smoker Total ## Both 400 1380 1780 ## One 416 1823 2239 ## Neither 188 1168 1356 ## Total 1004 4371 5375 Study 3: “CHD” Ordinal Table. A study of classified n = 1329 patients by cholesterol level and whether they had been diagnosed with coronary heart disease (CHD). # tribble() is a little easier. chd_o &lt;- tribble( ~L_0_199, ~L_200_219, ~L_220_259, ~L_260p, 12, 8, 31, 41, 307, 246, 439, 245 ) %&gt;% as.matrix() rownames(chd_o) &lt;- c(&quot;CHD&quot;, &quot;No CHD&quot;) chd_o %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot; &quot;) %&gt;% janitor::adorn_totals(where = c(&quot;row&quot;, &quot;col&quot;)) ## L_0_199 L_200_219 L_220_259 L_260p Total ## CHD 12 8 31 41 92 ## No CHD 307 246 439 245 1237 ## Total 319 254 470 286 1329 2.3.1 Chi-Square Independence Test The chi-square independence test tests whether observed joint frequency counts \\(O_{ij}\\) differ from expected frequency counts \\(E_{ij}\\) under the independence model (the model of independent explanatory variables, \\(\\pi_{ij} = \\pi_{i+} \\pi_{+j}\\). \\(H_0\\) is \\(O_{ij} = E_{ij}\\). There are two possible test statistics for this test, Pearson \\(X^2\\) (and the continuity adjusted \\(X^2\\)), and deviance \\(G^2\\). As \\(n \\rightarrow \\infty\\) their sampling distributions approach \\(\\chi_{df}^2\\) with degrees of freedom (df) equal to the saturated model df \\(I \\times J - 1\\) minus the independence model df \\((I - 1) + (J - 1)\\), which you can algebraically solve for \\(df = (I - 1)(J - 1)\\). The Pearson goodness-of-fit statistic is \\[X^2 = \\sum \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\] where \\(O_{ij}\\) is the observed count, and \\(E_{ij}\\) is the product of the row and column marginal probabilities. For the Vitamin C study, \\(X^2\\) is vitc_e &lt;- sum(vitc_o) * prop.table(vitc_o, 1) * prop.table(vitc_o, 2) X2 &lt;- sum((vitc_o - vitc_e)^2 / vitc_e) print(X2) ## [1] 4.8 and the deviance statistic is \\[G^2 = 2 \\sum_{ij} O_{ij} \\log \\left( \\frac{O_{ij}}{E_{ij}} \\right)\\] G2 &lt;- - 2 * sum(vitc_o * log(vitc_o / vitc_e)) print(G2) ## [1] 4.9 \\(X^2\\) and \\(G^2\\) increase with the disagreement between the saturated model proportions \\(p_{ij}\\) and the independence model proportions \\(\\pi_{ij}\\). The degrees of freedom is vitc_dof &lt;- (nrow(vitc_o) - 1) * (ncol(vitc_o) - 1) print(vitc_dof) ## [1] 1 The associated p-values are pchisq(q = G2, df = vitc_dof, lower.tail = FALSE) ## [1] 0.027 pchisq(q = X2, df = vitc_dof, lower.tail = FALSE) ## [1] 0.028 The chisq.test() function applies the Yates continuity correcton by default to correct for situations with small cell counts. The Yates continuity correction subtracts 0.5 from the \\(O_{ij} - E_{ij}\\) differences. Set correct = FALSE to suppress Yates. vitc_chisq_test &lt;- chisq.test(vitc_o, correct = FALSE) print(vitc_chisq_test) ## ## Pearson&#39;s Chi-squared test ## ## data: vitc_o ## X-squared = 5, df = 1, p-value = 0.03 The Yates correction yields more conservative p-values. chisq.test(vitc_o) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: vitc_o ## X-squared = 4, df = 1, p-value = 0.04 These p-values are evidence for rejecting the independence model. Here is the chi-square test applied to the CHD data. Recall this data set is 4x2, so the degrees of freedom are \\((4-1)(2-1) = 3\\). The Yates continuity correction does not apply to data other than 2x2, so the correct = c(TRUE, FALSE) has no effect in chisq.test(). (chd_chisq_test &lt;- chisq.test(chd_o)) ## ## Pearson&#39;s Chi-squared test ## ## data: chd_o ## X-squared = 35, df = 3, p-value = 0.0000001 The p-value is very low, so reject the null hypothesis of independence. This demonstrates that a relationship exists between cholesterol and CHD. Now you should describe that relationship by evaluating the (i) residuals, (ii) measures of association, and (iii) partitioning chi-square. 2.3.2 Residuals Analysis If the chi-squared independence test rejects \\(H_0\\) of identical frequency distributions, the next step is to identify which cells may be driving the lack of fit. The Pearson residuals in the two-way table are \\[r_{ij} = \\frac{O_{ij} - E_{ij}}{\\sqrt{E_{ij}}}\\] where \\(X^2 = \\sum{r_{ij}}\\). The \\(r_{ij}\\) values have a normal distribution with mean 0, but with unequal variances. The standardized Pearson residual for a two-way table is \\[r_{ij} = \\frac{O_{ij} - E_{ij}}{\\sqrt{E_{ij}(1 - p_{i+})(1 - p_{+j})}}\\] and the \\(r_{ij}\\) values do have a \\(\\sim N(0, 1)\\) distribution. \\(r_{ij}^2 &gt; 4\\) is a sign of lack of fit. The chissq.test() object includes residuals that match the manual calculation. (vitc_o - vitc_e) / sqrt(vitc_e) ## resp ## treat Cold NoCold ## Placebo -1.4 0.64 ## VitaminC 1.4 -0.64 vitc_chisq_test$residuals ## resp ## treat Cold NoCold ## Placebo 1.4 -0.64 ## VitaminC -1.4 0.64 It also includes stdres that match the manual standardized calculation. (well, no it doesn’t, but I don’t know what my mistake is.) (vitc_e - vitc_o) / sqrt(vitc_e * (1 - prop.table(vitc_o, margin = 1)) * (1 - prop.table(vitc_o, margin = 2)) ) ## resp ## treat Cold NoCold ## Placebo 2.7 -1.9 ## VitaminC -1.9 2.7 vitc_chisq_test$stdres ## resp ## treat Cold NoCold ## Placebo 2.2 -2.2 ## VitaminC -2.2 2.2 Here are the squared Pearson residuals for the CHD data. The squared Pearson residuals for CHD 0-199, 200-219, and 260+ are greater than 4, and seem to be driving the lack of independence. chd_chisq_test$residuals^2 ## L_0_199 L_200_219 L_220_259 L_260p ## CHD 4.60 5.22 0.0725 22.7 ## No CHD 0.34 0.39 0.0054 1.7 2.3.3 Difference in Proportions The difference in proportions measure is the difference in the probabilities of characteristic \\(Z\\) conditioned on two groups \\(Y = 1\\) and \\(Y = 2\\): \\(\\delta = \\pi_{1|1} - \\pi_{1|2}\\). In social sciences and epidemiology \\(\\pi_{1|1}\\) and \\(\\pi_{1|2}\\) are sometimes referred to as “risk” values. The point estimate for \\(\\delta\\) is \\(r = p_{1|1} - p_{1|2}\\). Under the normal approximation method, the sampling distribution of the difference in population proportions has a normal distribution centered at \\(d\\) with variance \\(Var(\\delta)\\). The point estimate for \\(Var(\\delta)\\) is \\(Var(d)\\). \\[Var(d) = \\frac{p_{1|1} (1 - p_{1|1})}{n_{1+}} + \\frac{p_{1|2} (1 - p_{1|2})}{n_{2+}}\\] In the vitamin C acid example, \\(\\delta\\) is the difference in the row conditional frequencies. p &lt;- prop.table(vitc_o, margin = 1) d &lt;- p[2, 1] - p[1, 1] print(d) ## [1] -0.099 The variance is var_d &lt;- (p[2, 1])*(1 - p[2, 1]) / sum(vitc_o[2, ]) + (p[1, 1])*(1 - p[1, 1]) / sum(vitc_o[1, ]) print(var_d) ## [1] 0.002 The 95% CI is d + c(-1, 1) * qnorm(.975) * sqrt(var_d) ## [1] -0.187 -0.011 This is how prop.test() without the continuity correction calculates the confidence interval. (prop.test.result &lt;- prop.test(vitc_o, correct = FALSE)) ## ## 2-sample test for equality of proportions without continuity ## correction ## ## data: vitc_o ## X-squared = 5, df = 1, p-value = 0.03 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## 0.011 0.187 ## sample estimates: ## prop 1 prop 2 ## 0.22 0.12 lcl &lt;- -round(prop.test.result$conf.int[2], 3) ucl &lt;- -round(prop.test.result$conf.int[1], 3) data.frame(d_i = -300:300 / 1000) %&gt;% mutate(density = dnorm(x = d_i, mean = d, sd = sqrt(var_d))) %&gt;% mutate(rr = ifelse(d_i &lt; lcl | d_i &gt; ucl, density, 0)) %&gt;% ggplot() + geom_line(aes(x = d_i, y = density)) + geom_area(aes(x = d_i, y = rr, fill = mf_pal(12)(12)[2]), alpha = 0.8) + geom_vline(aes(xintercept = d), color = &quot;blue&quot;) + theme_mf() + labs(title = bquote(&quot;Difference in Proportions Confidence Interval&quot;), subtitle = paste0( &quot;d = &quot;, round(d, 3) ), x = &quot;d&quot;, y = &quot;Density&quot;) + theme(legend.position=&quot;none&quot;) The normal approximation method applies when the central limit theorem conditions hold: the sample is independently drawn (random sampling without replacement from \\(n &lt; 10\\%\\) of the population in observational studies, or random assignment in experiments), there are at least \\(n_i p_i &gt;= 5\\) successes and \\(n_i (1 - p_i) &gt;= 5\\) failures for each group, the sample sizes are both \\(&gt;=30\\), and the probability of success for each group is not extreme, \\((0.2, 0.8)\\). Test \\(H_0: d = \\delta_0\\) for some hypothesized population \\(\\delta\\) (usually 0) with test statistic \\[Z = \\frac{d - \\delta_0}{se_{d}}\\] where \\[se_{d} = \\sqrt{p (1 - p) \\left( \\frac{1}{n_{1+}} + \\frac{1}{n_{2+}} \\right)}\\] approximates \\(se_{\\delta_0}\\) where \\(p\\) is the pooled proportion \\[p = \\frac{n_{11} + n_{21}}{n_{1+} + n_{2+}}.\\] p_pool &lt;- (vitc_o[1, 1] + vitc_o[2, 1]) / sum(vitc_o) se_d &lt;- sqrt(p_pool * (1 - p_pool) * (1 / sum(vitc_o[1, ]) + 1 / sum(vitc_o[2, ]))) z &lt;- (d - 0) / se_d pnorm(z) * 2 ## [1] 0.028 lrr = qnorm(p = .05/2, mean = 0, sd = se_d, lower.tail = TRUE) urr = qnorm(p = .05/2, mean = 0, sd = se_d, lower.tail = FALSE) data.frame(d_i = -300:300 / 1000) %&gt;% mutate(density = dnorm(x = d_i, mean = 0, sd = se_d)) %&gt;% mutate(rr = ifelse(d_i &lt; lrr | d_i &gt; urr, density, 0)) %&gt;% ggplot() + geom_line(aes(x = d_i, y = density)) + geom_area(aes(x = d_i, y = rr, fill = mf_pal(12)(12)[2]), alpha = 0.8) + geom_vline(aes(xintercept = d), color = &quot;blue&quot;) + geom_vline(aes(xintercept = 0), color = &quot;black&quot;) + theme_mf() + labs(title = &quot;Hypothesis Test of Difference in Proportions&quot;, subtitle = paste0( &quot;d = &quot;, round(d, 3), &quot; (Z = &quot;, round(z, 2), &quot;, p = &quot;, round(pnorm(z) * 2, 4), &quot;).&quot; ), x = &quot;d&quot;, y = &quot;Density&quot;) + theme(legend.position=&quot;none&quot;) The null hypothesis \\(H_0: \\delta_0 = 0\\) is equivalent to saying that two variables are independent, \\(\\pi_{1|1} = \\pi_{1|2}\\), so you can also use the \\(\\chi^2\\) or \\(G^2\\) test for independence in a 2 × 2. That’s what prop.test() is doing. The square of the z-statistic is algebraically equal to \\(\\chi^2\\). The two-sided test comparing \\(Z\\) to a \\(N(0, 1)\\) is identical to comparing \\(\\chi^2\\) to a chi-square distribution with df = 1. Compare the \\(Z^2\\) to the output from prop.test(). z^2 ## [1] 4.8 prop.test.result$statistic ## X-squared ## 4.8 The difference in proportions is easy to interpret, but when \\(Z = 1\\) is a rare event, the individual probabilities \\(\\pi_{1|1}\\) and \\(\\pi_{1|2}\\) are both small and \\(\\delta\\) is nearly zero even when the effect is strong. In the CHD study, two of the conditional probabilities of CHD within the four cholesterol groups are similar, 0-199 (0.038) and 200-219 (.031). round(prop.table(chd_o, margin = 2), 3) ## L_0_199 L_200_219 L_220_259 L_260p ## CHD 0.038 0.031 0.066 0.14 ## No CHD 0.962 0.969 0.934 0.86 Is the difference in these proportions statistically signficant? You can test this with the difference in proportions test or a chisq test. prop.test(t(chd_o[, c(1:2)])) ## ## 2-sample test for equality of proportions with continuity correction ## ## data: t(chd_o[, c(1:2)]) ## X-squared = 0.03, df = 1, p-value = 0.9 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## -0.027 0.040 ## sample estimates: ## prop 1 prop 2 ## 0.038 0.031 chisq.test(t(chd_o[, c(1:2)])) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: t(chd_o[, c(1:2)]) ## X-squared = 0.03, df = 1, p-value = 0.9 You could go on to try other pairwise tests to establish which levels differ from the others. 2.3.4 Relative Risk The relative risk measure is the ratio of the probabilities of characteristic \\(Z\\) conditioned on two groups \\(Y = 1\\) and \\(Y = 2\\): \\(\\rho = \\pi_{1|1} / \\pi_{1|2}\\). In social sciences and epidemiology \\(\\rho\\) is sometimes referred to as the “relative risk”. The point estimate for \\(\\rho\\) is \\(r = p_{1|1} / p_{1|2}\\). Because \\(\\rho\\) is non-negative, a normal approximation for \\(\\log \\rho\\) has a less skewed distribution than \\(\\rho\\). The approximate variance of \\(\\log \\rho\\) is \\[Var(\\log \\rho) = \\frac{1 - \\pi_{11}/\\pi_{1+}}{n_{1+}\\pi_{11}/\\pi_{1+}} + \\frac{1 - \\pi_{21}/\\pi_{2+}}{n_{2+}\\pi_{21}/\\pi_{2+}}\\] and is estimated by \\[Var(\\log r) = \\left( \\frac{1}{n_{11}} - \\frac{1}{n_{1+}} \\right) + \\left( \\frac{1}{n_{21}} - \\frac{1}{n_{2+}} \\right)\\] In the vitamin C acid example, \\(r\\) is the ratio of the row conditional frequencies. vitc_prop &lt;- prop.table(vitc_o, margin = 1) vitc_risk &lt;- vitc_prop[2, 1] / vitc_prop[1, 1] print(vitc_risk) ## [1] 0.55 The variance is vitc_risk_var &lt;- 1 / vitc_o[1, 1] - 1 / sum(vitc_o[1, ]) + 1 / vitc_o[2, 1] - 1 / sum(vitc_o[2, ]) print(vitc_risk_var) ## [1] 0.077 The 95% CI is exp(log(vitc_risk) + c(-1, 1) * qnorm(.975) * sqrt(vitc_risk_var)) ## [1] 0.32 0.95 Thus, at 0.05 level, you can reject the independence model. People taking vitamin C are half as likely to catch a cold. In the CHD study, you could summarize the relationship between CHD and cholesterol level by a set of three relative risks using 0-199 as the baseline: 200–219 versus 0–199, 220–259 versus 0–199, and 260+ versus 0–199. (chd_prop &lt;- prop.table(chd_o, margin = 2)) ## L_0_199 L_200_219 L_220_259 L_260p ## CHD 0.038 0.031 0.066 0.14 ## No CHD 0.962 0.969 0.934 0.86 (chd_risk &lt;- chd_prop[1, ] / chd_prop[1, 1]) ## L_0_199 L_200_219 L_220_259 L_260p ## 1.00 0.84 1.75 3.81 2.3.5 Odds Ratio The odds ratio is the most commonly used measure of association. It is also a natural parameter for many of the log-linear and logistic models. The odds is the ratio of probabilities of “success” and “failure”. When conditioned on a variable, the odds ratio is \\[\\theta = \\frac{\\pi_{1|1} / \\pi_{2|1}} {\\pi_{1|2} / \\pi_{2|2}}\\] and is estimated by the sample frequencies \\[\\hat{\\theta} = \\frac{n_{11} n_{22}} {n_{12} n_{21}}\\] The log-odds ratio has a better normal approximation than the odds ratio, so define the confidence interval on the log scale. \\[Var(\\log \\hat{\\theta}) = \\frac{1}{n_{11}} + \\frac{1}{n_{12}} + \\frac{1}{n_{21}} + \\frac{1}{n_{22}}\\] For the Vitamin C example, the odds of getting a cold after taking a placebo pill are \\(0.22 / 0.78 = 0.28\\) and the odds of getting a cold after taking Vitamin C are \\(0.12 / 0.88 = 0.14\\). vitc_odds &lt;- vitc_prop[, 1] / vitc_prop[, 2] print(vitc_odds) ## Placebo VitaminC ## 0.28 0.14 The odds of getting a cold given vitamin C are \\(0.14 / 0.28 = 0.49\\) times the odds of getting cold given a placebo. vitc_theta_hat &lt;- vitc_odds[2] / vitc_odds[1] print(vitc_theta_hat) ## VitaminC ## 0.49 with variance (var_vitc_theta_hat &lt;- sum(1 / vitc_o)) ## [1] 0.11 The 95% CI is z_alpha &lt;- qnorm(p = 0.975) exp(log(vitc_theta_hat) + c(-1, 1) * z_alpha * sqrt(var_vitc_theta_hat)) ## [1] 0.26 0.93 Keep in mind the following properties of odds ratios. You can convert an odds pack to probabilities by solving \\(\\pi / (1 - \\pi)\\) for \\(\\pi = odds / (1 + odds)\\). If two variables are independent, then the conditional probabilities \\(\\pi_{1|1}\\) and \\(\\pi_{1|2}\\) will be equal and therefore the odds ratio will equal 1. If \\(\\pi_{1|1} &gt; \\pi_{1|2}\\) then the odds ratio will be \\(1 &lt; \\theta &lt; \\infty\\). If \\(\\pi_{1|1} &lt; \\pi_{1|2}\\) then the odds ratio will be \\(0 &lt; \\theta &lt; 1\\). the sample odds ratio will equal \\(0\\) or \\(\\infty\\) if any \\(n_{ij} = 0\\). If you have any empty cells add 1/2 to each cell count. 2.3.6 Partitioning Chi-Square Besides looking at the residuals or the measures of association, another way to describe the effects is to form a sequence of smaller tables by combining or collapsing rows and/or columns in a meaningful way. For the smoking study, you might ask whether a student is more likely to smoke if either parent smokes. Collapse the first two rows (1 parent smokes, both parents smoke) and run the chi-squared test. smoke_clps_1 &lt;- rbind(smoke_o[1, ] + smoke_o[2, ], smoke_o[3, ]) smoke_clps_1_theta &lt;- (smoke_clps_1[1, 1] / smoke_clps_1[1, 2]) / (smoke_clps_1[2, 1] / smoke_clps_1[2, 2]) (smoke_clps_1_chisq &lt;- chisq.test(smoke_clps_1)) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: smoke_clps_1 ## X-squared = 27, df = 1, p-value = 0.0000002 The estimated odds a student smokes if at least one parent smokes is 1.58 (X^2 = 27.3, p = 0. Or you may ask, whether among students with at least one smoking parent, there is a difference between those with one smoking parent and those with two smoking parents. Answer this by running a chi-square test on the first two rows of the data table, discarding the row where neither parent smokes. smoke_clps_2_theta &lt;- (smoke_o[1, 1] / smoke_o[1, 2]) / (smoke_o[2, 1] / smoke_o[2, 2]) (smoke_clps_2_chisq &lt;- chisq.test(smoke_o[c(1:2), ])) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: smoke_o[c(1:2), ] ## X-squared = 9, df = 1, p-value = 0.003 The estimated odds a student smokes if both parents smoke compared to one parent is 1.27 (X^2 = 9, p = 0. 2.3.7 Correlation When classification is ordinal, there may exist a linear trend among the levels of the characteristics. Measure the linear relationship with Pearson’s correlation coefficient, or its nonparametric alternatives, Spearman’s correlation coefficient and Kendall’s tau. In the CHD study, the four levels of cholesterol (0-199, 200-219, 220-259, and 260+) may be treated as ordinal data. You can also treat the presence of heart disease as ordinal. The Pearson correlation, \\[r = \\frac{cov(X, Y)}{s_X s_Y}\\] is #dim=dim(table) rbar &lt;- sum(margin.table(chd_o, 1) * c(1, 2)) / sum(chd_o) rdif &lt;- c(1, 2) - rbar cbar &lt;- sum(margin.table(chd_o, 2) * c(1, 2, 3, 4)) / sum(chd_o) cdif &lt;- c(1, 2, 3, 4) - cbar ssr &lt;- sum(margin.table(chd_o, 1) * (rdif^2)) ssc &lt;- sum(margin.table(chd_o, 2) * (cdif^2)) ssrc &lt;- sum(t(chd_o * rdif) * cdif) pcor &lt;- ssrc / (sqrt(ssr * ssc)) pcor ## [1] -0.14 M2 &lt;- (sum(chd_o) - 1) * pcor^2 M2 ## [1] 26 "],
["k-way-tables.html", "2.4 K-Way Tables", " 2.4 K-Way Tables These notes rely on PSU STATS 504 course notes. A k-way table has k independent variables. Each cell in the k-way table has joint probability \\(\\pi_{ij..k}\\). You can collapse a k-way table along a dimension to create a marginal table and the resulting joint probabilities in the marginal table are referred to as marginal distributions. Alternatively, you can consider a single level of a dimension, creating a conditional distribution. Here are two case studies to illustrate the concepts. Study 1: Death Penalty. Data is collected for n = 326 murder cases along three dimensions: defendant’s color, victim’s color, and whether or not the defendant received the death penalty. deathp &lt;- array(c(19, 132, 11, 52, 0, 9, 6, 97), dim=c(2,2,2)) dimnames(deathp) &lt;- list( DeathPen = c(&quot;yes&quot;,&quot;no&quot;), Defendant=c(&quot;white&quot;,&quot;black&quot;), Victim=c(&quot;white&quot;,&quot;black&quot;) ) ftable(deathp, row.vars = c(&quot;Defendant&quot;, &quot;Victim&quot;), col.vars = &quot;DeathPen&quot;) ## DeathPen yes no ## Defendant Victim ## white white 19 132 ## black 0 9 ## black white 11 52 ## black 6 97 Study 2: Boy Scouts. Data is collected for n = 800 Boy Scouts along three dimensions: socioeconomic status, scout status, and delinquency status. scout &lt;- expand.grid( delinquent = c(&quot;no&quot;,&quot;yes&quot;), scout = c(&quot;no&quot;, &quot;yes&quot;), SES = c(&quot;low&quot;, &quot;med&quot;,&quot;high&quot;) ) scout &lt;- cbind(scout, count = c(169,42,43,11,132,20,104,14,59,2,196,8)) scout_ct &lt;- xtabs(count ~ ., scout) ftable(scout_ct, row.vars = c(&quot;SES&quot;, &quot;scout&quot;), col.vars = &quot;delinquent&quot;) ## delinquent no yes ## SES scout ## low no 169 42 ## yes 43 11 ## med no 132 20 ## yes 104 14 ## high no 59 2 ## yes 196 8 2.4.1 Odds Ratio In the DeathP study, the marginal odds ratio is 1.18, meaning the odds of death penalty for a white defendant are 1.18 times as high as they are for a black defendant. deathp_m &lt;- margin.table(deathp, margin = c(1, 2)) deathp_p &lt;- prop.table(deathp_m, margin = 2) deathp_odds &lt;- deathp_p[1, ] / deathp_p[2, ] deathp_or &lt;- deathp_odds[1] / deathp_odds[2] print(deathp_or) ## white ## 1.2 The conditional odds ratio given the victim is white is 0.68, and 0.79 given the victim is black, meaning the odds of death penalty for a white defendant are 0.68 times as high as they are for a black defendent if the victim is white, and 0.79 times as high as they are for a black defendent if the victim is black. deathp_m &lt;- margin.table(deathp[, , 1], margin = c(2, 1)) deathp_p &lt;- prop.table(deathp_m, margin = 2) deathp_odds &lt;- deathp_p[1, ] / deathp_p[2, ] deathp_or &lt;- deathp_odds[1] / deathp_odds[2] print(deathp_or) ## yes ## 0.68 deathp_m &lt;- margin.table(deathp[, , 2], margin = c(2, 1)) + 0.5 deathp_p &lt;- prop.table(deathp_m, margin = 2) deathp_odds &lt;- deathp_p[1, ] / deathp_p[2, ] deathp_or &lt;- deathp_odds[1] / deathp_odds[2] print(deathp_or) ## yes ## 0.79 Notice above that the second margin table had a zero in one cell. To get an odds ratio in this case, the convention is to add 0.5 to all cells. Interesting that the marginal odds of a white defendent receiving the death penalty are &gt;1, but the conditional odds of a white defendent receiving the death penalty given the race of the victim are &lt;1. Simpson’s paradox is the phenomenon that a pair of variables can have marginal association and partial (conditional) associations in opposite direction. Another way to think about this is that the nature and direction of association changes due to presence or absence of a third (possibly confounding) variable. 2.4.2 Chi-Square Independence Test There are several ways to think about “independence” when dealing with a k-way table. Mutual independence. All variables are independent from each other, (A, B, C). Joint independence. Two variables are jointly independent of the third, (AB, C). Marginal independence. Two variables are independent if you ignore the third, (A, B). Conditional independence Two variables are independent given the third, (AC, BC). Homogeneous associations Conditional (partial) odds-ratios are not related on the value of the third, (AB, AC, BC). Under the assumption that the model of independence is true, once you know the marginal probability values, we have enough information to estimate all unknown cell probabilities. Because each of the marginal probability vectors must add up to one, the number of free parameters in the model is (I − 1) + (J − 1) + (K −I ). This is exactly like the two-way table 2.4.2.1 Mutual Independence The simplest model is that ALL variables are independent of one another (A, B, C). The joint probabilities are equal to the product of the marginal probabilities, \\(\\pi_{ijk} = \\pi_i \\pi_j \\pi_k\\). In terms of odds ratios, the model (A, B, C) implies that the odds ratios in the marginal tables A × B, B × C, and A × C are equal to 1. The chi-square independence test tests whether observed joint frequency counts \\(O_{ijk}\\) differ from expected frequency counts \\(E_{ijk}\\) under the independence model \\(\\pi_{ijk} = \\pi_{i+} \\pi_{+j} \\pi_{k+}\\). \\(H_0\\) is \\(O_{ijk} = E_{ijk}\\). This is essentially a one-way table chi-squre goodness-of-fit test. For the Boy Scouts study, \\(X^2\\) and its associated p-value is scout_e &lt;- array(NA, dim(scout_ct)) for (i in 1:dim(scout_ct)[1]) { for (j in 1:dim(scout_ct)[2]) { for (k in 1:dim(scout_ct)[3]) { scout_e[i,j,k] &lt;- ( margin.table(scout_ct, 3)[k] * margin.table(scout_ct, 2)[j] * margin.table(scout_ct, 1)[i]) / (sum(scout_ct))^2 } } } scout_df &lt;- (prod(dim(scout_ct)) - 1) - sum(dim(scout_ct) - 1) scout_x2 &lt;- sum((scout_ct - scout_e)^2 / scout_e) print(scout_x2) ## [1] 215 pchisq(scout_x2, df = scout_df, lower.tail = FALSE) ## [1] 0.00000000000000000000000000000000000000000079 You can also get X^2 this way, although longer code. x &lt;- scout %&gt;% group_by(delinquent) %&gt;% summarize(n = sum(count)) %&gt;% ungroup() %&gt;% mutate(p = n / sum(n)) p_d &lt;- x$p names(p_d) &lt;- x$delinquent x &lt;- scout %&gt;% group_by(scout) %&gt;% summarize(n = sum(count)) %&gt;% ungroup() %&gt;% mutate(p = n / sum(n)) p_b &lt;- x$p names(p_b) &lt;- x$scout x &lt;- scout %&gt;% group_by(SES) %&gt;% summarize(n = sum(count)) %&gt;% ungroup() %&gt;% mutate(p = n / sum(n)) p_s &lt;- x$p names(p_s) &lt;- x$SES scout_e &lt;- scout %&gt;% mutate(e = as.numeric(p_d[delinquent] * p_b[scout] * p_s[SES] * sum(count))) x2 &lt;- sum((scout_e$count - scout_e$e)^2 / scout_e$e) print(x2) ## [1] 215 The deviance statistic is scout_g2 &lt;- 2 * sum(scout_ct * log(scout_ct / scout_e$e)) print(scout_g2) ## [1] 219 pchisq(scout_g2, df = scout_df, lower.tail = FALSE) ## [1] 0.00000000000000000000000000000000000000000013 Safe to say the mutual independence model does not fit. chisq.test() does not test the complete independence model for k-way tables. Instead, conduct tests on all \\({3 \\choose 2} = 3\\) embedded 2-way tables. For the Boy Scouts study, you can conduct a series of 2-way chi-sq tests. SES*scout: (scout_ses_scout &lt;- margin.table(scout_ct, c(3, 2))) ## scout ## SES no yes ## low 211 54 ## med 152 118 ## high 61 204 chisq.test(scout_ses_scout) ## ## Pearson&#39;s Chi-squared test ## ## data: scout_ses_scout ## X-squared = 172, df = 2, p-value &lt;0.0000000000000002 SES*delinquent (scout_ses_delinquent &lt;- margin.table(scout_ct, c(3, 1))) ## delinquent ## SES no yes ## low 212 53 ## med 236 34 ## high 255 10 chisq.test(scout_ses_delinquent) ## ## Pearson&#39;s Chi-squared test ## ## data: scout_ses_delinquent ## X-squared = 33, df = 2, p-value = 0.00000007 delinquent*scout (scout_delinquent_scout &lt;- margin.table(scout_ct, c(1, 2))) ## scout ## delinquent no yes ## no 360 343 ## yes 64 33 chisq.test(scout_delinquent_scout) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: scout_delinquent_scout ## X-squared = 7, df = 1, p-value = 0.009 The odds ratio for scout delinquency vs non-scout delinquency is 0.54 with 95% CI (0.347, 0.845), so reject the null hypothesis that boy scout status and delinquent status are independent of one another (OR = 1), and thus that boy scout status, delinquent status, and socioeconomic status are not mutually independent. (scout_or &lt;- vcd::oddsratio(scout_delinquent_scout, log = FALSE)) ## odds ratios for delinquent and scout ## ## [1] 0.54 confint(scout_or) ## 2.5 % 97.5 % ## no:yes/no:yes 0.35 0.84 2.4.2.2 Joint Independence The joint independence model is that two variables are jointly independent of a third (AB, C). The joint probabilities are equal to the product of the AB marginal probability and the C marginal probability, \\(\\pi_{ijk} = \\pi_{ij} \\pi_k\\). The degrees of freedom in the associated chi-sq test are the number of free parameters in the saturated model minus the number of free parameters in the joint independence model. \\(df = (IJK-1) - ((IJ-1) + (K-1)) = (IJ-1)(K-1)\\). From the Boy Scouts study, suppose juvenile delinquency is the response variable. Test the null hypothesis that juvenile delinquency is independent of boy scout status and socioeconomic status. scout_d_bs &lt;- ftable( scout_ct, row.vars = c(&quot;SES&quot;, &quot;scout&quot;), col.vars = &quot;delinquent&quot; ) print(scout_d_bs) ## delinquent no yes ## SES scout ## low no 169 42 ## yes 43 11 ## med no 132 20 ## yes 104 14 ## high no 59 2 ## yes 196 8 chisq.test(scout_d_bs) ## ## Pearson&#39;s Chi-squared test ## ## data: scout_d_bs ## X-squared = 33, df = 5, p-value = 0.000004 Notice how the contingency table is constructed to not indicate whether SES and scout are related. 2.4.2.3 Marginal Independence The marginal independence model is that two variables are independent while ignoring the third (A, B). The joint probabilities are equal to the product of the two marginal probabilities, \\(\\pi_{ij} = \\pi_{i} \\pi_j\\), just like a 2-way table. The degrees of freedom in the associated chi-sq test are the number of free parameters in the saturated model minus the number of free parameters in the marginal independence model. \\(df = (I - 1) + (J - 1) = (I-1)(J-1)\\). From the Boy Scouts study, suppose juvenile delinquency is the response variable. Test the null hypothesis that juvenile delinquency is independent of boy scout status and socioeconomic status. ct &lt;- xtabs(count ~ scout + SES, scout) print(ct) ## SES ## scout low med high ## no 211 152 61 ## yes 54 118 204 chisq.test(ct) ## ## Pearson&#39;s Chi-squared test ## ## data: ct ## X-squared = 172, df = 2, p-value &lt;0.0000000000000002 2.4.2.4 Conditional Independence The conditional independence model is that two variables are independent given a third (AB, AC). The joint probabilities are equal to the product of the conditional probabilities (B|A and C|A) and the marginal probability of A, \\(\\pi_{ijk} = \\pi_{j|i}\\pi_{k|i}\\pi_{i++}\\). You can test for conditional independence (AB, AC) by individually testing the independence (B, C) for each level of A. \\(X^2\\) and \\(G^2\\) are the sum of the individual values and the degrees of freedom are \\(I(J-1)(K-1)\\). A better way to do it is with the Cochran-Mantel-Haenszel Test. The degrees of freedom in the associated chi-sq test are the number of free parameters in the saturated model minus the number of free parameters in the joint independence model. \\(df = (IJK-1) - ((IJ-1) + (K-1)) = (IJ-1)(K-1)\\). From the Boy Scouts study, the section on joint independence found that scouting and SES were jointly independent of delinquency, so it must be that either scouting is independent of delinquency, or SES is independent of delinquency, or both are independent of delinquency. The marginal test below establishes the independance of (scout, delinquent). ct &lt;- xtabs(count ~ scout + delinquent, scout) print(ct) ## delinquent ## scout no yes ## no 360 64 ## yes 343 33 chisq.test(ct, correct = FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: ct ## X-squared = 7, df = 1, p-value = 0.006 The odds ratio 0.54 is a strong negative relationship between boy scout status and delinquency - boy scouts are 46% less likely (on the odds scale) to be delinquent than non-boy scouts. ct &lt;- xtabs(count ~ scout + delinquent, scout) p &lt;- prop.table(ct, margin = 1) (or &lt;- (p[2, 2] / p[2, 1]) / (p[1, 2] / p[1, 1])) ## [1] 0.54 However, one may ask is scouting and delinquency conditionally independent given SES? You can answer this question with three chi-square tests, one for each level of SES. ct &lt;- xtabs(count ~ scout + delinquent, scout[scout$SES == &quot;low&quot;, ]) print(ct) ## delinquent ## scout no yes ## no 169 42 ## yes 43 11 chisq.test(ct, correct = FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: ct ## X-squared = 0.006, df = 1, p-value = 0.9 ct &lt;- xtabs(count ~ scout + delinquent, scout[scout$SES == &quot;med&quot;, ]) print(ct) ## delinquent ## scout no yes ## no 132 20 ## yes 104 14 chisq.test(ct, correct = FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: ct ## X-squared = 0.1, df = 1, p-value = 0.8 ct &lt;- xtabs(count ~ scout + delinquent, scout[scout$SES == &quot;high&quot;, ]) print(ct) ## delinquent ## scout no yes ## no 59 2 ## yes 196 8 chisq.test(ct, correct = FALSE) ## Warning in chisq.test(ct, correct = FALSE): Chi-squared approximation may be ## incorrect ## ## Pearson&#39;s Chi-squared test ## ## data: ct ## X-squared = 0.05, df = 1, p-value = 0.8 You can add up the three \\(X^2\\) values and run a chi-sq test with \\(df = 3\\). p = 0.984 indicating that the conditional independence model fits extremely well. pchisq(0.0058145 + 0.10098 + 0.053447, df = 3, lower.tail = FALSE) ## [1] 0.98 The other way of testing for independence is the Cochran-Mantel-Haenszel test. The Cochran-Mantel-Haenszel (CMH) test statistic \\[M^2 = \\frac{\\left[ \\sum_k{(n_{11k} - \\mu_{11k})}\\right]^2}{\\sum_k{Var(n_{11k})}}\\] For the Boy Scouts study, the test is mantelhaen.test(scout_ct) ## ## Mantel-Haenszel chi-squared test without continuity correction ## ## data: scout_ct ## Mantel-Haenszel X-squared = 0.008, df = 1, p-value = 0.9 ## alternative hypothesis: true common odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.6 1.6 ## sample estimates: ## common odds ratio ## 0.98 X2= 0.008 (df = 1, p-value = 0.9287) indicating the conditional independence model is a good fit for this data, so do not reject the null hypothesis. 2.4.2.5 Homogenous Associations Whereas the conditional independence model, (AB, AC) requires the BC odds ratios at each level of A to equal 1, the homogeneous association model, (AB, AC, BC), requires the BC odds ratios at each level of A to be identical, but not necessarily equal to 1. The Breslow-Day statistic is \\[X^2 = \\sum_{ijk}{\\frac{\\left(O_{ijk} - E_{ijk}\\right)^2}{E_{ijk}}}\\] DescTools::BreslowDayTest(scout_ct) ## ## Breslow-Day test on Homogeneity of Odds Ratios ## ## data: scout_ct ## X-squared = 0.2, df = 2, p-value = 0.9 "],
["continuous-analysis.html", "Chapter 3 Continuous Variable Analysis", " Chapter 3 Continuous Variable Analysis 3.0.1 Correlation Correlation measures the strength and direction of association between two variables. There are three common correlation tests: the Pearson product moment (Pearson’s r), Spearman’s rank-order (Spearman’s rho), and Kendall’s tau (Kendall’s tau). Use the Pearson’s r if both variables are quantitative (interval or ratio), normally distributed, and the relationship is linear with homoscedastic residuals. The Spearman’s rho and Kendal’s tao correlations are non-parametric measures, so they are valid for both quantitative and ordinal variables and do not carry the normality and homoscedasticity conditions. However, non-parametric tests have less statistical power than parametric tests, so only use these correlations if Pearson does not apply. 3.0.1.1 Pearson’s r Pearson’s \\(r\\) \\[r = \\frac{\\sum{(X_i - \\bar{X})(Y_i - \\bar{Y})}}{\\sqrt{\\sum{(X_i - \\bar{X})^2 \\sum{(Y_i - \\bar{Y})^2}}}} = \\frac{cov(X,Y)}{s_X s_Y}\\] estimates the population correlation \\(\\rho\\). Pearson’s \\(r\\) ranges from \\(-1\\) (perfect negative linear relationship) to \\(+1\\) (perfect positive linear relationship, and \\(r = 0\\) when there is no linear relationship. A correlation in the range \\((.1, .3)\\) is condidered small, \\((.3, .5)\\) medium, and \\((.5, 1.0)\\) large. Pearson’s \\(r\\) only applies if the variables are interval or ratio, normally distributed, linearly related, there are minimal outliers, and the residuals are homoscedastic. Test \\(H_0: \\rho = 0\\) with test statistic \\[T = r \\sqrt{\\frac{n-2}{1-r^2}}.\\] The nascard data set consists of \\(n = 898\\) races from 1975 - 2003. nascard &lt;- read.fwf( file = url(&quot;http://jse.amstat.org/datasets/nascard.dat.txt&quot;), widths = c(5, 6, 4, 4, 4, 5, 9, 4, 11, 30), col.names = c(&#39;series_race&#39;, &#39;year&#39;, &#39;race_year&#39;, &#39;finish_pos&#39;, &#39;start_pos&#39;, &#39;laps_comp&#39;, &#39;winnings&#39;, &#39;num_cars&#39;,&#39;car_make&#39;, &#39;driver&#39;) ) nascard_sr1 &lt;- nascard[nascard$series_race == 1,] glimpse(nascard_sr1) ## Rows: 35 ## Columns: 10 ## $ series_race &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... ## $ year &lt;dbl&gt; 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1... ## $ race_year &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... ## $ finish_pos &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, ... ## $ start_pos &lt;dbl&gt; 1, 2, 25, 27, 22, 35, 3, 21, 33, 10, 17, 24, 6, 28, 18,... ## $ laps_comp &lt;dbl&gt; 191, 191, 184, 183, 178, 175, 172, 168, 167, 166, 166, ... ## $ winnings &lt;dbl&gt; 12035, 8135, 6535, 5035, 3835, 2885, 3185, 2485, 2335, ... ## $ num_cars &lt;dbl&gt; 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35,... ## $ car_make &lt;fct&gt; Matador , Mercury , Chevrolet , Dodge , For... ## $ driver &lt;fct&gt; BobbyAllison , DavidPearson ... In race 1 of 1975 \\((n = 35)\\), what was the correlation between the driver’s finishing position and prize? Explore the relationship with a scatterplot. As expected, there is a negative relationship between finish position and winnings, but it is non-linear. However, 1/winnings may be linearly related to finish position. p1 &lt;- ggplot(data = nascard_sr1, aes(x = finish_pos, y = winnings)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_mf() + labs(title = &quot;Pearson&#39;s Rho&quot;, subtitle = &quot;&quot;) p2 &lt;- ggplot(data = nascard_sr1, aes(x = finish_pos, y = 1/winnings)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_mf() + labs(title = &quot;Pearson&#39;s Rho&quot;, subtitle = &quot;y = 1 / winnings&quot;) gridExtra::grid.arrange(p1, p2, nrow = 1) (*I tried several variable tranformations and chose the one producing the highest \\(R^2\\) that also had a normal distribution in each variable.) #summary(lm(winnings ~ finish_pos, data = nascard_sr1)) # r2 = .5474 #summary(lm(log(winnings) ~ finish_pos, data = nascard_sr1)) # r2 = .9015 nascard_lm &lt;- lm(1/winnings ~ finish_pos, data = nascard_sr1) # r2 = .9509 #summary(lm(winnings ~ log(finish_pos), data = nascard_sr1)) # r2 = .883 #summary(lm(log(winnings) ~ log(finish_pos), data = nascard_sr1)) # r2 = .9766 #summary(lm(winnings ~ log(finish_pos), data = nascard_sr1)) # r2 = .883 coef(nascard_lm) ## (Intercept) finish_pos ## -0.000018 0.000045 Finish position and prize are ratio variables, so the Pearson’s r applies. Check whether each variable is normally distributed. p1 &lt;- ggplot(nascard_sr1, aes(sample = finish_pos)) + stat_qq() + stat_qq_line() + theme_mf() + labs(title = &quot;Q-Q Plots&quot;, subtitle = &quot;Finish Position&quot;) p2 &lt;- ggplot(nascard_sr1, aes(sample = winnings)) + stat_qq() + stat_qq_line() + theme_mf() + labs(title = &quot;&quot;, subtitle = &quot;Winnings&quot;) gridExtra::grid.arrange(p1, p2, nrow = 1) The normal distribution plots look good. You can also use the Anderson-Darling statistical test. nortest::ad.test(nascard_sr1$finish_pos) ## ## Anderson-Darling normality test ## ## data: nascard_sr1$finish_pos ## A = 0.4, p-value = 0.4 nortest::ad.test(1/nascard_sr1$winnings) ## ## Anderson-Darling normality test ## ## data: 1/nascard_sr1$winnings ## A = 0.3, p-value = 0.5 Both fail to reject the normality null hypothesis. Pearson’s \\(r\\) is x &lt;- nascard_sr1$finish_pos y &lt;- 1/nascard_sr1$winnings (r = sum((x - mean(x)) * (y - mean(y))) / sqrt(sum((x - mean(x))^2) * sum((y - mean(y))^2))) ## [1] 0.98 Test \\(H_0: \\rho = 0\\) with test statistic \\(T\\). n &lt;- nrow(nascard_sr1) (t = r * sqrt((n - 2) / (1 - r^2))) ## [1] 25 pt(q = t, df = n - 2, lower.tail = FALSE) * 2 ## [1] 0.000000000000000000000035 \\(P(T&gt;.9752) &lt; .0001\\), so reject \\(H_0\\) that the correlation is zero. cor.test() performs these calculations. Specify method = \"pearson\". cor.test( x = nascard_sr1$finish_pos, y = 1/nascard_sr1$winnings, alternative = &quot;two.sided&quot;, method = &quot;pearson&quot; ) ## ## Pearson&#39;s product-moment correlation ## ## data: nascard_sr1$finish_pos and 1/nascard_sr1$winnings ## t = 25, df = 33, p-value &lt;0.0000000000000002 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.95 0.99 ## sample estimates: ## cor ## 0.98 3.0.1.2 Spearman’s Rho Spearman’s \\(\\rho\\) is the Pearson’s r applied to the sample variable ranks. Let \\((X_i, Y_i)\\) be the ranks of the \\(n\\) sample pairs with mean ranks \\(\\bar{X} = \\bar{Y} = (n+1)/2\\). Spearman’s rho is \\[\\hat{\\rho} = \\frac{\\sum{(X_i - \\bar{X})(Y_i - \\bar{Y})}}{\\sqrt{\\sum{(X_i - \\bar{X})^2 \\sum{(Y_i - \\bar{Y})^2}}}}\\] Spearman’s rho is a non-parametric test, so there is no associated confidence interval. From the nascard study, what was the correlation between the driver’s starting position start_pos and finishing position finish_pos? From the scatterplot, there does not appear to be much of a relationship. nascard_sr1 %&gt;% ggplot(aes(x = start_pos, y = finish_pos)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_mf() + labs(title = &quot;Spearman&#39;s Rho&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Both star_pos and finish_pos are ordinal variables, so use Spearman’s rho instead of Pearson. Normally, you replace the variable values with their ranks, but in this case, the values are their ranks. After that, Spearman’s \\(\\rho\\) is the same as Pearson’s r. x &lt;- rank(nascard_sr1$start_pos) y &lt;- rank(nascard_sr1$finish_pos) (rho = sum((x - mean(x)) * (y - mean(y))) / sqrt(sum((x - mean(x))^2) * sum((y - mean(y))^2))) ## [1] -0.038 Test \\(H_0: \\rho = 0\\) with test statistic \\(T\\). n &lt;- nrow(nascard_sr1) (t = rho * sqrt((n - 2) / (1 - rho^2))) ## [1] -0.22 pt(q = abs(t), df = n - 2, lower.tail = FALSE) * 2 ## [1] 0.83 \\(P(T&gt;.2206) = .8268\\), so do not reject \\(H_0\\) that the correlation is zero. cor.test() performs these calculations. Specify method = \"spearman\". cor.test( x = nascard_sr1$start_pos, y = nascard_sr1$finish_pos, alternative = &quot;two.sided&quot;, method = &quot;spearman&quot; ) ## ## Spearman&#39;s rank correlation rho ## ## data: nascard_sr1$start_pos and nascard_sr1$finish_pos ## S = 7414, p-value = 0.8 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## -0.038 3.0.1.3 Kendall’s Tau The Kendall Rank correlation coefficient (Kendall’s tau) measures the relationship between ranked variables. (see also Statistics How To). Let \\((X_i, Y_i)\\) be the ranks of the \\(n\\) sample pairs. For each \\(X_i\\), count \\(Y &gt; X_i\\). The total count is \\(k\\). Kendall’s tau is \\[\\hat{\\tau} = \\frac{4k}{n(n-1)} -1\\] If \\(X\\) and \\(Y\\) have the same rank orders, \\(\\hat{\\tau} = 1\\), and if they have opposite rank orders \\(\\hat{\\tau} = -1\\). Test \\(H_0: \\tau = 0\\) with test statistic \\[Z = \\hat{\\tau} \\sqrt{\\frac{9n(n - 1)}{2(2n + 5)}}.\\] Kendall’s tau is a non-parametric test, so there is no associated confidence interval. From the nascard study, what was the correlation between the driver’s starting position start_pos and finishing position finish_pos? Spearman’s rho was \\(\\hat{\\rho} = -0.038\\). For Kendall’s tau, for each observation, count the number of other observations where both start_pos and finish_pos is larger. The sum is \\(k = 293\\). Then calculate Kendall’s \\(\\tau\\) is \\(\\hat{\\tau} = \\frac{4 * 293}{35(35-1)} -1 = -.0151\\). n &lt;- nrow(nascard_sr1) k &lt;- 0 for (i in 1:n) { for(j in i:n) { k = k + if_else( nascard_sr1[j, ]$start_pos &gt; nascard_sr1[i, ]$start_pos &amp; nascard_sr1[j, ]$finish_pos &gt; nascard_sr1[i, ]$finish_pos, 1, 0) } } (k) ## [1] 293 (tau = 4.0*k / (n * (n - 1)) - 1) ## [1] -0.015 Test \\(H_0: \\tau = 0\\) with test statistic \\(T\\). n &lt;- nrow(nascard_sr1) (t = tau * sqrt(9 * n * (n-1) / (2 * (2*n + 5)))) ## [1] -0.13 pt(q = abs(t), df = n - 2, lower.tail = FALSE) * 2 ## [1] 0.9 \\(P(|T| &gt; 0.128) = .8991\\), so do not reject \\(H_0\\) that the correlation is zero. cor.test() performs these calculations. Specify method = \"kendall\". cor.test( x = nascard_sr1$start_pos, y = nascard_sr1$finish_pos, alternative = &quot;two.sided&quot;, method = &quot;kendall&quot; ) ## ## Kendall&#39;s rank correlation tau ## ## data: nascard_sr1$start_pos and nascard_sr1$finish_pos ## T = 293, p-value = 0.9 ## alternative hypothesis: true tau is not equal to 0 ## sample estimates: ## tau ## -0.015 Here is a fun way to tie Pearson, Spearman, and Kendal together: Calculate Spearman’s rho and Kendall’s tau for all 898 races in the nascard data set and calculate the Pearson correlation of their values! nascard_smry &lt;- nest(nascard, -c(series_race, year)) %&gt;% mutate( spearman = map(data, ~ cor.test(.$start_pos, .$finish_pos, alternative = &quot;two.sided&quot;, method = &quot;spearman&quot;)), spearman_est = unlist(map(spearman, ~ .$estimate)), spearman_p = unlist(map(spearman, ~ .$p.value)), kendall = map(data, ~ cor.test(.$start_pos, .$finish_pos, alternative = &quot;two.sided&quot;, method = &quot;kendall&quot;)), kendall_est = unlist(map(kendall, ~ .$estimate)), kendall_p = unlist(map(kendall, ~ .$p.value)) ) nascard_smry2 &lt;- nascard_smry %&gt;% group_by(year) %&gt;% summarize( spearman_r = mean(spearman_est), kendall_tau = mean(kendall_est) ) %&gt;% pivot_longer(cols = c(spearman_r, kendall_tau), names_to = &quot;method&quot;, values_to = &quot;estimate&quot;) ggplot(nascard_smry2, aes(x = year, color = method)) + geom_line(aes(y = estimate)) + theme_mf() + scale_color_mf() + labs(title = &quot;Average Rank Correlations vs Year&quot;, color = &quot;&quot;) In terms of significance, the two tests usually return the same results. table( ifelse(nascard_smry$spearman_p &lt; .05, &quot;sig&quot;, &quot;insig&quot;), ifelse(nascard_smry$kendall_p &lt; .05, &quot;sig&quot;, &quot;insig&quot;) ) ## ## insig sig ## insig 344 18 ## sig 11 525 Calculate the Pearson coefficient of the relationship between Spearman’s rho and Kendall’s tau. ggplot(data = nascard_smry, aes(x = kendall_est, y = spearman_est)) + geom_point() + geom_smooth(method = lm, se = FALSE) + theme_mf() + labs(title = &quot;Pearson&#39;s r of Spearman&#39;s rho vs Kendall&#39;s tau&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; The scatterplot with fitted line has r-squared equal to the Pearson coefficient squared. cor.test( x = nascard_smry$spearman_est, y = nascard_smry$kendall_est, alternative = &quot;two.sided&quot;, method = &quot;pearson&quot; )$estimate^2 ## cor ## 0.98 summary(lm(kendall_est ~ spearman_est, data = nascard_smry))$r.squared ## [1] 0.98 "],
["experiment-design.html", "Chapter 4 Experiment Design", " Chapter 4 Experiment Design These notes are structured from the PSU STAT 503 course. "],
["single-factor.html", "4.1 Single Factor", " 4.1 Single Factor "],
["blocking.html", "4.2 Blocking", " 4.2 Blocking "],
["nested.html", "4.3 Nested", " 4.3 Nested "],
["split-plot.html", "4.4 Split Plot", " 4.4 Split Plot "],
["part-2-supervised-machine-learning.html", "PART 2: Supervised Machine Learning", " PART 2: Supervised Machine Learning Machine learning (ML) develops algorithms to identify patterns in data (unsupervised ML) or make predictions and inferences (supervised ML). Supervised ML trains the machine to learn from prior examples to predict either a categorical outcome (classification) or a numeric outcome (regression), or to infer the relationships between the outcome and its explanatory variables. Two early forms of supervised ML are linear regression (OLS) and generalized linear models (GLM) (Poisson and logistic regression). These methods have been improved with advanced linear methods, including stepwise selection, regularization (ridge, lasso, elastic net), principal components regression, and partial least squares. With greater computing capacity, non-linear models are now in use, including polynomial regression, step functions, splines, and generalized additive models (GAM). Decision trees (bagging, random forests, and, boosting) are additional options for regression and classification, and support vector machines is an additional option for classification. "],
["ordinary-least-squares.html", "Chapter 5 Ordinary Least Squares", " Chapter 5 Ordinary Least Squares library(caret) library(Metrics) library(tidyverse) library(corrplot) library(gridExtra) library(car) # for avPlots library(AppliedPredictiveModeling) library(e1071) # for skewness() library(purrr) # for map() library(broom) # for augment() These notes cover linear regression. "],
["linear-regression-model.html", "5.1 Linear Regression Model", " 5.1 Linear Regression Model The population regression model \\(E(Y) = X \\beta\\) summarizes the trend between the predictors and the mean responses. The individual responses vary about the population regression, \\(y_i = X_i \\beta + \\epsilon_i\\) with assumed mean structure \\(y_i \\sim N(\\mu_i, \\sigma^2)\\) and assumed constant variance \\(\\sigma^2\\). Equivalently, the model presumes a linear relationship between \\(y\\) and \\(X\\) with residuals \\(\\epsilon\\) that are independent normal random variables with mean zero and constant variance \\(\\sigma^2\\). Estimate the population regression model coefficients as \\(\\hat{y} = X \\hat{\\beta}\\), and the population variance as \\(\\hat{\\sigma}^2\\). The most common method of estimating the \\(\\beta\\) coefficients and \\(\\sigma\\) is ordinary least squares (OLS). OLS minimizes the sum of squared residuals from a random sample. The individual predicted values vary about the actual value, \\(e_i = y_i - \\hat{y}_i\\), where \\(\\hat{y}_i = X_i \\hat{\\beta}\\). The OLS model is the best linear unbiased estimator (BLUE) if the residuals are independent random variables normally distributed with mean zero and constant variance \\(\\sigma^2\\). Recall these conditions with the LINE pneumonic: Linear, Independent, Normal, and Equal. Linearity. The explanatory variables are each linearly related to the response variable: \\(E(\\epsilon | X_j) = 0\\). Independence. The residuals are unrelated to each other. Independence is violated when repeated measurements are taken, or when there is a temporal component in the model. Normality. The residuals are normally distributed: \\(\\epsilon|X \\sim N(0, \\sigma^2I)\\). Equal Variances. The variance of the residuals is constant (homoscedasticity): \\(E(\\epsilon \\epsilon&#39; | X) = \\sigma^2I\\) Additionally, you should make sure you model has “little” or no multicollinearity among the variables. "],
["parameter-estimation.html", "5.2 Parameter Estimation", " 5.2 Parameter Estimation There are two model parameters to estimate: \\(\\hat{\\beta}\\) estimates the coefficient vector \\(\\beta\\), and \\(\\hat{\\sigma}\\) estimates the variance of the residuals along the regression line. Derive the coefficient estimators by minimizing the sum of squared residuals \\(SSE = (y - X \\hat{\\beta})&#39; (y - X \\hat{\\beta})\\). The result is \\[\\hat{\\beta} = (X&#39;X)^{-1}X&#39;y.\\] The residual standard error (RSE) estimates the sample deviation around the population regression line. (Think of each value of \\(X\\) along the regression line as a subpopulation with mean \\(y_i\\) and variance \\(\\sigma^2\\). This variance is assumed to be the same for all \\(X\\).) \\[\\hat{\\sigma} = \\sqrt{(n-k-1)^{-1} e&#39;e}.\\] The standard error for the coefficient estimators is the square root of the error variance divided by \\((X&#39;X)\\). \\[SE(\\hat{\\beta}) = \\sqrt{\\hat{\\sigma}^2 (X&#39;X)^{-1}}.\\] 5.2.0.1 Example Dataset mtcars contains response variable fuel consumption mpg and 10 aspects of automobile design and performance for 32 automobiles. What is the relationship between the response variable and its predictors? d &lt;- mtcars %&gt;% mutate(vs = factor(vs, labels = c(&quot;V&quot;, &quot;S&quot;)), am = factor(am, labels = c(&quot;automatic&quot;, &quot;manual&quot;)), cyl = ordered(cyl), gear = ordered(gear), carb = ordered(carb)) glimpse(d) ## Rows: 32 ## Columns: 11 ## $ mpg &lt;dbl&gt; 21, 21, 23, 21, 19, 18, 14, 24, 23, 19, 18, 16, 17, 15, 10, 10... ## $ cyl &lt;ord&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4,... ## $ disp &lt;dbl&gt; 160, 160, 108, 258, 360, 225, 360, 147, 141, 168, 168, 276, 27... ## $ hp &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, ... ## $ drat &lt;dbl&gt; 3.9, 3.9, 3.8, 3.1, 3.1, 2.8, 3.2, 3.7, 3.9, 3.9, 3.9, 3.1, 3.... ## $ wt &lt;dbl&gt; 2.6, 2.9, 2.3, 3.2, 3.4, 3.5, 3.6, 3.2, 3.1, 3.4, 3.4, 4.1, 3.... ## $ qsec &lt;dbl&gt; 16, 17, 19, 19, 17, 20, 16, 20, 23, 18, 19, 17, 18, 18, 18, 18... ## $ vs &lt;fct&gt; V, V, S, S, V, S, V, S, S, S, S, V, V, V, V, V, V, S, S, S, S,... ## $ am &lt;fct&gt; manual, manual, manual, automatic, automatic, automatic, autom... ## $ gear &lt;ord&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3,... ## $ carb &lt;ord&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1,... The data consists of 32 observations. A scatterplot matrix of the numeric variables shows the strongest individual association with mpg is from wt (corr = -0.87) followed by disp (corr = -0.85) and hp (corr = -0.78), drat is moderately correlated with mpg (corr = 0.68), and qsec is weakly correlated with mpg (corr = 0.42). corrplot(cor(subset(d, select = c(mpg, disp, hp, drat, wt, qsec))), type = &quot;upper&quot;, method = &quot;number&quot;) Many of the Predictor variables are strongly correlated with each other. Boxplots of the categorical variables shows differences in levels, although ordinal variables gear and and carb do not have a monotonic relationshiop with mpg. p_list &lt;- list() for(i in c(&quot;cyl&quot;, &quot;vs&quot;, &quot;am&quot;, &quot;gear&quot;, &quot;carb&quot;)) { p &lt;- ggplot(d, aes_string(x = i, y = &quot;mpg&quot;)) + geom_boxplot() p_list &lt;- c(p_list, list(p)) } do.call(&quot;grid.arrange&quot;, c(p_list, ncol = 2)) I’ll drop the gear and carb predictors, and fit a population model to the remaining predictors. m &lt;- lm(mpg ~ ., data = d[,1:9]) summary(m) ## ## Call: ## lm(formula = mpg ~ ., data = d[, 1:9]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.998 -1.355 -0.311 1.199 4.110 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 19.54098 14.14642 1.38 0.181 ## cyl.L 0.34256 2.76483 0.12 0.903 ## cyl.Q 1.38843 1.11210 1.25 0.225 ## disp 0.00669 0.01351 0.49 0.626 ## hp -0.02914 0.01718 -1.70 0.104 ## drat 0.58806 1.50311 0.39 0.699 ## wt -3.15525 1.42023 -2.22 0.037 * ## qsec 0.52324 0.69013 0.76 0.456 ## vsS 1.23780 2.10606 0.59 0.563 ## ammanual 3.00091 1.85340 1.62 0.120 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.5 on 22 degrees of freedom ## Multiple R-squared: 0.877, Adjusted R-squared: 0.826 ## F-statistic: 17.4 on 9 and 22 DF, p-value: 0.0000000481 The summary() function shows \\(\\hat{\\beta}\\) as Estimate, \\(SE({\\hat{\\beta}})\\) as Std. Error, and \\(\\hat{\\sigma}\\) as Residual standard error. You can verify this by manually peforming these calculations using matrix algebra (see matrix algebra in r notes at R for Dummies). Here are the coefficient estimators, \\(\\hat{\\beta} = (X&#39;X)^{-1}X&#39;y\\). X &lt;- model.matrix(m) y &lt;- d$mpg beta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% y round(beta_hat, 5) ## [,1] ## (Intercept) 19.5410 ## cyl.L 0.3426 ## cyl.Q 1.3884 ## disp 0.0067 ## hp -0.0291 ## drat 0.5881 ## wt -3.1553 ## qsec 0.5232 ## vsS 1.2378 ## ammanual 3.0009 Here is the residual standard error, \\(\\hat{\\sigma} = \\sqrt{(n-k-1)^{-1} \\hat{e}&#39;\\hat{e}}\\). n &lt;- nrow(X) k &lt;- ncol(X) - 1 # exclude the intercept term y_hat &lt;- X %*% beta_hat sse &lt;- sum((y - y_hat)^2) rse &lt;- sqrt(sse / (n - k - 1)) cat(&quot;Residual standard error: &quot;, round(rse, 3), &quot; on &quot;, (n - k - 1), &quot; degrees of freedom.&quot;) ## Residual standard error: 2.5 on 22 degrees of freedom. Use the residual standard errors to derive the standard errors of the coefficients, \\(SE(\\hat{\\beta}) = \\sqrt{\\hat{\\sigma}^2 (X&#39;X)^{-1}}\\). se_beta_hat &lt;- sqrt(diag(rse^2 * solve(t(X) %*% X))) matrix(round(se_beta_hat, 5), dimnames = list(names(se_beta_hat), &quot;Std. Error&quot;)) ## Std. Error ## (Intercept) 14.146 ## cyl.L 2.765 ## cyl.Q 1.112 ## disp 0.014 ## hp 0.017 ## drat 1.503 ## wt 1.420 ## qsec 0.690 ## vsS 2.106 ## ammanual 1.853 "],
["model-assumptions.html", "5.3 Model Assumptions", " 5.3 Model Assumptions The linear regression model assumes the relationship between the predictors and the response is linear with the residuals that are independent random variables normally distributed with mean zero and constant variance. Additionally, you will want to check for multicollinearity in the predictors because it can produce unreliable coefficient estimates and predicted values. Use a residuals vs fits plot \\(\\left( e \\sim \\hat{Y} \\right)\\) to detect non-linearity and unequal error variances, including outliers. The polynomial trend line should show that the residuals vary around \\(e = 0\\) in a straight line (linearity). The variance should be of constant width (especially no fan shape at the low or high ends). Use a residuals normal probability plot to compares the theoretical percentiles of the normal distribution versus the observed sample percentiles. It should be approximately linear. A scale-location plot \\(\\sqrt{e / sd(e)} \\sim \\hat{y}\\) checks the homogeneity of variance of the residuals (homoscedasticity). The square root of the absolute value of the residuals should be spread equally along a horizontal line. A residuals vs leverage plot identifies influential observations. A plot of the standardized residuals vs the leverage should fall within the 95% probability band. par(mfrow = c(2, 2)) plot(m, labels.id = NULL) 5.3.1 Linearity The explanatory variables should each be linearly related to the response variable: \\(E(\\epsilon | X_j) = 0\\). A good way to test this condition is with a residuals vs fitted values plot. A curved pattern in the residuals indicates a curvature in the relationship between the response and the predictor that is not explained by our model. A linear model does not adequately describe the relationship between the predictor and the response. Test for linearity four ways: Residuals vs fits plot \\((e \\sim \\hat{Y})\\) should bounce randomly around 0. Observed vs fits plot \\((Y \\sim \\hat{Y})\\) should be symmetric along the 45-degree line. Each \\((Y \\sim X_j )\\) plot should have correlation \\(\\rho \\sim 1\\). Each \\((e \\sim X_j)\\) plot should exhibit no pattern. If the linearity condition fails, change the functional form of the model with non-linear transformations of the explanatory variables. A common way to do this is with Box-Cox transformations. \\[w_t = \\begin{cases} \\begin{array}{l} log(y_t) \\quad \\quad \\lambda = 0 \\\\ (y_t^\\lambda - 1) / \\lambda \\quad \\text{otherwise} \\end{array} \\end{cases}\\] \\(\\lambda\\) can take any value, but values near the following yield familiar transformations. \\(\\lambda = 1\\) yields no substantive transformation. \\(\\lambda = 0.5\\) is a square root plus linear transformation. \\(\\lambda = 0.333\\) is a cube root plus linear transformation. \\(\\lambda = 0\\) is a natural log transformation. \\(\\lambda = -1\\) is an inverse transformation. A common source of non-linearity in a model is skewed response or independent variables (see discussion here). mtcars has some skewed variables. tmp &lt;- map(mtcars, skewness) %&gt;% unlist() %&gt;% as.data.frame() %&gt;% rownames_to_column() colnames(tmp) &lt;- c(&quot;IV&quot;, &quot;skew&quot;) ggplot(tmp, aes(x = order(IV, skew), y = skew)) + geom_col() df &lt;- mtcars m &lt;- lm(mpg ~ hp, data = df) par(mfrow = c(2, 2)) plot(m) plot(m$model$mpg, m$fitted.values) abline(0, 1) cor(df$mpg, df$hp) ## [1] -0.78 postResample(pred = m$fitted.values, obs = m$model$mpg) ## RMSE Rsquared MAE ## 3.7 0.6 2.9 bc &lt;- BoxCoxTrans(mtcars$hp) df$hp_bc &lt;- predict(bc, mtcars$hp) m_bc &lt;- lm(mpg ~ hp_bc, data = df) plot(m_bc) plot(m_bc$model$mpg, m_bc$fitted.values) abline(0, 1) cor(df$mpg, df$hp_bc) ## [1] -0.85 postResample(pred = m_bc$fitted.values, obs = m_bc$model$mpg) ## RMSE Rsquared MAE ## 3.14 0.72 2.41 # Which vars are skewed? map(mtcars, skewness) ## $mpg ## [1] 0.61 ## ## $cyl ## [1] -0.17 ## ## $disp ## [1] 0.38 ## ## $hp ## [1] 0.73 ## ## $drat ## [1] 0.27 ## ## $wt ## [1] 0.42 ## ## $qsec ## [1] 0.37 ## ## $vs ## [1] 0.24 ## ## $am ## [1] 0.36 ## ## $gear ## [1] 0.53 ## ## $carb ## [1] 1.1 # Benchmark model: mpg ~ hp d0 &lt;- mtcars m0 &lt;- lm(mpg ~ hp, data = d0) d0 &lt;- augment(m0, d0) d0.cor &lt;- round(cor(d0$mpg, d0$hp), 2) # Benchmark diagnostics p0a &lt;- ggplot(d0, aes(x = hp, y = mpg)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;Response vs IV&quot;, subtitle = paste0(&quot;Correlation ~ 1? (rho = &quot;, d0.cor, &quot;)&quot;)) p0b &lt;- ggplot(d0, aes(x = .fitted, y = .resid)) + geom_point() + geom_abline(intercept = 0, slope = 0) + labs(title = &quot;Residuals vs Fits&quot;, subtitle = &quot;Random scatter?&quot;) p0c &lt;- ggplot(d0, aes(x = .fitted, y = mpg)) + geom_point() + geom_abline(intercept = 0, slope = 1) + expand_limits(x = c(0, 35), y = c(0, 35)) + labs(title = &quot;Observed vs Fits&quot;, subtitle = &quot;Symmetric along 45 degree line?&quot;) p0d &lt;- ggplot(d0, aes(x = hp, y = .resid)) + geom_point() + geom_abline(intercept = 0, slope = 0) + labs(title = &quot;Residuals vs IV&quot;, subtitle = &quot;Random scatter?&quot;) grid.arrange(p0a, p0b, p0c, p0d, nrow = 2) ## `geom_smooth()` using formula &#39;y ~ x&#39; # Benchmark performance postResample(pred = m0$fitted.values, obs = m0$model$mpg) ## RMSE Rsquared MAE ## 3.7 0.6 2.9 # Box-Cox transform hp d1 &lt;- mtcars bc &lt;- BoxCoxTrans(d1$hp) d1$hp_bc &lt;- predict(bc, d1$hp) m1 &lt;- lm(mpg ~ hp_bc, data = d1) d1 &lt;- augment(m1, d1) d1.cor &lt;- round(cor(d1$mpg, d1$hp_bc), 2) p1a &lt;- ggplot(d1, aes(x = hp_bc, y = mpg)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;Response vs IV&quot;, subtitle = paste0(&quot;Correlation ~ 1? (rho = &quot;, d1.cor, &quot;)&quot;)) p1b &lt;- ggplot(d1, aes(x = .fitted, y = .resid)) + geom_point() + geom_abline(intercept = 0, slope = 0) + labs(title = &quot;Residuals vs Fits&quot;, subtitle = &quot;Random scatter?&quot;) p1c &lt;- ggplot(d1, aes(x = .fitted, y = mpg)) + geom_point() + geom_abline(intercept = 0, slope = 1) + expand_limits(x = c(0, 35), y = c(0, 35)) + labs(title = &quot;Observed vs Fits&quot;, subtitle = &quot;Symmetric along 45 degree line?&quot;) p1d &lt;- ggplot(d1, aes(x = hp, y = .resid)) + geom_point() + geom_abline(intercept = 0, slope = 0) + labs(title = &quot;Residuals vs IV&quot;, subtitle = &quot;Random scatter?&quot;) grid.arrange(p1a, p1b, p1c, p1d, nrow = 2) ## `geom_smooth()` using formula &#39;y ~ x&#39; postResample(pred = m1$fitted.values, obs = m1$model$mpg) ## RMSE Rsquared MAE ## 3.14 0.72 2.41 5.3.2 Multicollinearity The multicollinearity condition is violated when two or more of the predictors in a regression model are correlated. Muticollinearity can occur for structural reasons, as when one variable is a transformation of another variable, or for data reasons, as occurs in observational studies. Multicollinearity is a problem because it inflates the variances of the estimated coefficients, resulting in larger confidence intervals. When predictor variables are correlated, the precision of the estimated regression coefficients decreases with each added correlated predictor variable. The usual interpretation of a slope coefficient as the change in the mean response for each additional unit increase in the predictor when all the other predictors are held constant breaks down because changing one predictor necessarily changes the others. A residuals vs fits plot \\((\\epsilon \\sim \\hat{Y})\\) should have correlation \\(\\rho \\sim 0\\). A correlation matrix is helpful for picking out the correlation strengths. A good rule of thumb is correlation coefficients should be less than 0.80. However, this test may not work when a variable is correlated with a function of other variables. A model with multicollinearity may have a significant F-test with insignificant individual slope estimator t-tests. Another way to detect multicollinearity is by calculating variance inflation factors. The predictor variance \\(Var(\\hat{\\beta_k})\\) increases by a factor \\[VIF_k = \\frac{1}{1 - R_k^2}\\] where \\(R_k^2\\) is the \\(R^2\\) of a regression of the \\(k^{th}\\) predictor on the remaining predictors. A \\(VIF_k\\) of \\(1\\) indicates no inflation (no corellation). A \\(VIF_k &gt;= 4\\) warrants investigation. A \\(VIF_k &gt;= 10\\) requires correction. 5.3.2.1 Example Does the model mpg ~ . exhibit multicollinearity? The correlation matrix above (and presented again below) has several correlated covariates. disp is strongly correlated with wt (r = 0.89) and hp (r = 0.79). m &lt;- lm(mpg ~ ., data = mtcars) corrplot(cor(subset(d, select = c(mpg, disp, hp, drat, wt, qsec))), type = &quot;upper&quot;, method = &quot;number&quot;) Calculate the VIFs. round(vif(m), 2) ## cyl disp hp drat wt qsec vs am gear carb ## 15.4 21.6 9.8 3.4 15.2 7.5 5.0 4.6 5.4 7.9 There are two predictors with VIFs greater than 10, cyl (GVIF = 21.36) and disp (GVIF = 13.76). One way to address multicollinearity is removing one or more of the violating predictors from the regression model. Try removing cyl. vif(m &lt;-lm(mpg ~ . - cyl, data = d[,1:9])) ## disp hp drat wt qsec vs am ## 9.9 5.4 2.8 7.6 6.0 4.2 3.5 summary(m) ## ## Call: ## lm(formula = mpg ~ . - cyl, data = d[, 1:9]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.407 -1.469 -0.282 1.142 4.537 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.4980 12.4804 1.00 0.3266 ## disp 0.0137 0.0114 1.21 0.2382 ## hp -0.0228 0.0153 -1.50 0.1478 ## drat 0.9553 1.4074 0.68 0.5038 ## wt -3.9497 1.2626 -3.13 0.0046 ** ## qsec 0.8715 0.6133 1.42 0.1682 ## vsS 0.5902 1.8330 0.32 0.7503 ## ammanual 3.0240 1.6684 1.81 0.0824 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.5 on 24 degrees of freedom ## Multiple R-squared: 0.867, Adjusted R-squared: 0.829 ## F-statistic: 22.4 on 7 and 24 DF, p-value: 0.00000000453 Removing cyl reduced the VIFs of the other variables below 10. disp is still right up there (VIF = 9.87), so it may be worth dropping it from the model too. The model summary still shows that there is only one significant (at .05 level a significance) variable (wt, p = .00457). What if I drop disp too? vif(m &lt;- lm(mpg ~ . - cyl - disp, data = d[,1:9])) ## hp drat wt qsec vs am ## 5.1 2.7 5.1 5.8 4.1 3.3 summary(m) ## ## Call: ## lm(formula = mpg ~ . - cyl - disp, data = d[, 1:9]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.369 -1.721 -0.253 1.099 4.603 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 16.1410 12.2232 1.32 0.1986 ## hp -0.0180 0.0149 -1.21 0.2380 ## drat 0.6205 1.3926 0.45 0.6597 ## wt -3.0751 1.0446 -2.94 0.0069 ** ## qsec 0.7347 0.6084 1.21 0.2385 ## vsS 0.2045 1.8217 0.11 0.9115 ## ammanual 2.5653 1.6397 1.56 0.1303 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.5 on 25 degrees of freedom ## Multiple R-squared: 0.859, Adjusted R-squared: 0.825 ## F-statistic: 25.4 on 6 and 25 DF, p-value: 0.00000000169 The model is not improved, so keep disp. m &lt;-lm(mpg ~ . - cyl, data = d[,1:9]) If the multicollinearity occurs because you are using a polynomial regression model, center the predictor variables (subtract their means). 5.3.2.2 Example Data set exerimmun (exerimun.txt) contains observations of immunoglobin in blood (a measure of immunity) and maximal oxygen uptake (a measure of exercise level) for \\(n = 30\\) individuals. igg = amount of immunoglobin in blood (mg) oxygent = maximal oxygen uptake (ml/kg) How does exercise affect the immune system? #exerimmun &lt;- read_tsv(file = &quot;./Data/exerimmun.txt&quot;) exerimmun &lt;- tribble( ~igg, ~oxygen, 881, 34.6, 1290, 45, 2147, 62.3, 1909, 58.9, 1282, 42.5, 1530, 44.3, 2067, 67.9, 1982, 58.5, 1019, 35.6, 1651, 49.6, 752, 33, 1687, 52, 1782, 61.4, 1529, 50.2, 969, 34.1, 1660, 52.5, 2121, 69.9, 1382, 38.8, 1714, 50.6, 1959, 69.4, 1158, 37.4, 965, 35.1, 1456, 43, 1273, 44.1, 1418, 49.8, 1743, 54.4, 1997, 68.5, 2177, 69.5, 1965, 63, 1264, 43.2 ) head(exerimmun) ## # A tibble: 6 x 2 ## igg oxygen ## &lt;dbl&gt; &lt;dbl&gt; ## 1 881 34.6 ## 2 1290 45 ## 3 2147 62.3 ## 4 1909 58.9 ## 5 1282 42.5 ## 6 1530 44.3 The scatterplot oxygen ~ igg shows some curvature. Formulate a quadratic polynomial regression function, \\(igg_i = \\beta_0 + \\beta_1 oxygen_i + \\beta_2 oxygen_i^2 + \\epsilon_i\\) where the error terms are assumed to be independent, and normally distributed with equal variance. ggplot(exerimmun, aes(y = igg, x = oxygen)) + geom_point() + geom_smooth(method = lm, formula = y ~ poly(x, 2), se = FALSE) + labs(title = &quot;Immunoglobin in Blood&quot;) The formulated regression fits the data well (\\(adj R^2 = .933\\)), but the terms oxygen and oxygen^2 are strongly correlated. m_blood &lt;- lm(igg ~ poly(oxygen, 2), data = exerimmun) summary(m_blood) ## ## Call: ## lm(formula = igg ~ poly(oxygen, 2), data = exerimmun) ## ## Residuals: ## Min 1Q Median 3Q Max ## -185.37 -82.13 1.05 66.01 227.38 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1557.6 19.4 80.16 &lt;0.0000000000000002 *** ## poly(oxygen, 2)1 2114.7 106.4 19.87 &lt;0.0000000000000002 *** ## poly(oxygen, 2)2 -360.8 106.4 -3.39 0.0022 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 106 on 27 degrees of freedom ## Multiple R-squared: 0.938, Adjusted R-squared: 0.933 ## F-statistic: 203 on 2 and 27 DF, p-value: &lt;0.0000000000000002 cor(exerimmun$oxygen, exerimmun$oxygen^2) ## [1] 0.99 Remove the structural multicollinearity by centering the predictors. You can scale the predictors with scale(), but be careful to scale new data when predicting new observations with predict(newdata=)! Whenever possible, perform the transformation right in the model. m_blood &lt;- lm(igg ~ I(oxygen - mean(exerimmun$oxygen)) + I((oxygen - mean(exerimmun$oxygen))^2), data = exerimmun) summary(m_blood) ## ## Call: ## lm(formula = igg ~ I(oxygen - mean(exerimmun$oxygen)) + I((oxygen - ## mean(exerimmun$oxygen))^2), data = exerimmun) ## ## Residuals: ## Min 1Q Median 3Q Max ## -185.37 -82.13 1.05 66.01 227.38 ## ## Coefficients: ## Estimate Std. Error t value ## (Intercept) 1632.196 29.349 55.61 ## I(oxygen - mean(exerimmun$oxygen)) 34.000 1.689 20.13 ## I((oxygen - mean(exerimmun$oxygen))^2) -0.536 0.158 -3.39 ## Pr(&gt;|t|) ## (Intercept) &lt;0.0000000000000002 *** ## I(oxygen - mean(exerimmun$oxygen)) &lt;0.0000000000000002 *** ## I((oxygen - mean(exerimmun$oxygen))^2) 0.0022 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 106 on 27 degrees of freedom ## Multiple R-squared: 0.938, Adjusted R-squared: 0.933 ## F-statistic: 203 on 2 and 27 DF, p-value: &lt;0.0000000000000002 The estimated intercept coefficient \\(\\hat{\\beta}_0 = 1632\\) means a person whose maximal oxygen uptake is \\(50.64\\) ml/kg (the mean value) is predicted to have \\(1632\\) mg of immunoglobin in his blood. The estimated coefficient \\(\\hat{\\beta}_1 = 34.0\\) means a person whose maximal oxygen uptake is near \\(50.64\\) ml/kg is predicted to increase by 34.0 mg for every 1 ml/kg increase in maximal oxygen uptake. By performing all transformations in the model, it is straightforward to perform predictions. Here is the predicted value of immunoglobin when maximal oxygen uptake = 90.00 ml/kg. predict(m_blood, newdata = data.frame(oxygen = 90), interval = &quot;prediction&quot;) ## fit lwr upr ## 1 2140 1640 2640 5.3.3 Normality A normal probability plot or a normal quantile plot should have values near the line with no bow-shaped deviations. A histogram should be normally distributed. A residuals vs fits plot \\((\\epsilon \\sim \\hat{Y})\\) should be randomly scattered around 0. Sometimes the normality check fails when linearity assumption does not hold, so check for linearity first. Parameter estimation is not sensitive to this condition, but prediction intervals are. 5.3.4 Equal Variances The residuals should be the same size at both low and high values of the response variable. A residuals vs fits plot \\((\\epsilon \\sim \\hat{Y})\\) should have random scatter in a band of constant width around 0, and no fan shape at the low and high ends. All tests and intervals are sensitive to this condition. "],
["prediction.html", "5.4 Prediction", " 5.4 Prediction The standard error in the expected value of \\(\\hat{y}\\) at some new set of predictors \\(X_n\\) is \\[SE(\\mu_\\hat{y}) = \\sqrt{\\hat{\\sigma}^2 (X_n (X&#39;X)^{-1} X_n&#39;)}.\\] The standard error increases the further \\(X_n\\) is from \\(\\bar{X}\\). If \\(X_n = \\bar{X}\\), the equation reduces to \\(SE(\\mu_\\hat{y}) = \\sigma / \\sqrt{n}\\). If \\(n\\) is large, or the predictor values are spread out, \\(SE(\\mu_\\hat{y})\\) will be relatively small. The \\((1 - \\alpha)\\%\\) confidence interval is \\(\\hat{y} \\pm t_{\\alpha / 2} SE(\\mu_\\hat{y})\\). The standard error in the predicted value of \\(\\hat{y}\\) at some \\(X_{new}\\) is \\[SE(\\hat{y}) = SE(\\mu_\\hat{y})^2 + \\sqrt{\\hat{\\sigma}^2}.\\] Notice the standard error for a predicted value is always greater than the standard error of the expected value. The \\((1 - \\alpha)\\%\\) prediction interval is \\(\\hat{y} \\pm t_{\\alpha / 2} SE(\\hat{y})\\). 5.4.0.1 Example What is the expected value of mpg if the predictor values equal their mean values? R performs this calucation with the predict() function with parameter interval = confidence. m &lt;-lm(mpg ~ ., data = d[,1:9]) X_new &lt;- data.frame(Const = 1, cyl = factor(round(mean(as.numeric(as.character(d$cyl))),0), levels = levels(d$cyl)), disp = mean(d$disp), hp = mean(d$hp), drat = mean(d$drat), wt = mean(d$wt), qsec = mean(d$qsec), vs = factor(&quot;S&quot;, levels = levels(d$vs)), am = factor(&quot;manual&quot;, levels = levels(d$am))) predict.lm(object = m, newdata = X_new, interval = &quot;confidence&quot;) ## fit lwr upr ## 1 21 17 25 You can verify this by manually calculating \\(SE(\\mu_\\hat{y}) = \\sqrt{\\hat{\\sigma}^2 (X_{new} (X&#39;X)^{-1} X_{new}&#39;)}\\) using matrix algebra. X2 &lt;- lapply(data.frame(model.matrix(m)), mean) %&gt;% unlist() %&gt;% t() X2[2] &lt;- contr.poly(3)[2,1] # cyl linear X2[3] &lt;- contr.poly(3)[2,2] # cyl quadratic X2[9] &lt;- 1 X2[10] &lt;- 1 y_exp &lt;- sum(m$coefficients * as.numeric(X2)) se_y_exp &lt;- as.numeric(sqrt(rse^2 * X2 %*% solve(t(X) %*% X) %*% t(X2))) t_crit &lt;- qt(p = .05 / 2, df = n - k - 1, lower.tail = FALSE) me &lt;- t_crit * se_y_exp cat(&quot;fit: &quot;, round(y_exp, 6), &quot;, 95% CI: (&quot;, round(y_exp - me, 6), &quot;, &quot;, round(y_exp + me, 6), &quot;)&quot;) ## fit: 21 , 95% CI: ( 17 , 25 ) 5.4.0.2 Example What is the predicted value of mpg if the predictor values equal their mean values? R performs this calucation with the predict() with parameter interval = prediction. predict.lm(object = m, newdata = X_new, interval = &quot;prediction&quot;) ## fit lwr upr ## 1 21 15 28 se_y_hat &lt;- sqrt(rse^2 + se_y_exp^2) me &lt;- t_crit * se_y_hat cat(&quot;fit: &quot;, round(y_exp, 6), &quot;, 95% CI: (&quot;, round(y_exp - me, 6), &quot;, &quot;, round(y_exp + me, 6), &quot;)&quot;) ## fit: 21 , 95% CI: ( 15 , 28 ) "],
["inference.html", "5.5 Inference", " 5.5 Inference Draw conclusions about the significance of the coefficient estimates with the t-test and/or F-test. 5.5.1 t-Test By assumption, the residuals are normally distributed, so the Z-test statistic could evaluate the parameter estimators, \\[Z = \\frac{\\hat{\\beta} - \\beta_0}{\\sqrt{\\sigma^2 (X&#39;X)^{-1}}}\\] where \\(\\beta_0\\) is the null-hypothesized value, usually 0. \\(\\sigma\\) is unknown, but \\(\\frac{\\hat{\\sigma}^2 (n - k)}{\\sigma^2} \\sim \\chi^2\\). The ratio of the normal distribution divided by the adjusted chi-square \\(\\sqrt{\\chi^2 / (n - k)}\\) is t-distributed, \\[t = \\frac{\\hat{\\beta} - \\beta_0}{\\sqrt{\\hat{\\sigma}^2 (X&#39;X)^{-1}}} = \\frac{\\hat{\\beta} - \\beta_0}{SE(\\hat{\\beta})}\\] The \\((1 - \\alpha)\\) confidence intervals are \\(CI = \\hat{\\beta} \\pm t_{\\alpha / 2, df} SE(\\hat{\\beta})\\) with p-value equaling the probability of measuring a \\(t\\) of that extreme, \\(p = P(t &gt; |t|)\\). For a one-tail test, divide the reported p-value by two. The \\(SE(\\hat{\\beta})\\) decreases with 1) a better fitting regression line (smaller \\(\\hat{\\sigma}^2\\)), 2) greater variation in the predictor (larger \\(X&#39;X\\)), and 3) larger sample size (larger n). 5.5.1.1 Example Define a 95% confidence interval around the slope parameters. The summary() output shows the t values and probabilities in the t value and Pr(&gt;|t|) columns. You can verify this manually using matrix algebra for \\(t = \\frac{(\\hat{\\beta} - \\beta_1)}{SE(\\hat{\\beta})}\\) with \\(\\beta_1 = 0\\). The \\((1 - \\alpha)\\) confidence interval is \\(CI = \\hat{\\beta} \\pm t_{\\alpha / 2, df} SE(\\hat{\\beta})\\). The table below gathers the parameter estimators and t-test results. t &lt;- beta_hat / se_beta_hat p_value &lt;- pt(q = abs(t), df = n - k - 1, lower.tail = FALSE) * 2 t_crit &lt;- qt(p = .05 / 2, df = n - k - 1, lower.tail = FALSE) lcl = beta_hat - t_crit * se_beta_hat ucl = beta_hat + t_crit * se_beta_hat data.frame(beta = round(beta_hat, 4), se = round(se_beta_hat, 4), t = round(t, 4), p = round(p_value, 4), lcl = round(lcl,4), ucl = round(ucl, 4)) ## beta se t p lcl ucl ## (Intercept) 19.5410 14.146 1.38 0.181 -9.797 48.8789 ## cyl.L 0.3426 2.765 0.12 0.902 -5.391 6.0765 ## cyl.Q 1.3884 1.112 1.25 0.225 -0.918 3.6948 ## disp 0.0067 0.013 0.49 0.625 -0.021 0.0347 ## hp -0.0291 0.017 -1.70 0.104 -0.065 0.0065 ## drat 0.5881 1.503 0.39 0.699 -2.529 3.7053 ## wt -3.1552 1.420 -2.22 0.037 -6.101 -0.2099 ## qsec 0.5232 0.690 0.76 0.456 -0.908 1.9545 ## vsS 1.2378 2.106 0.59 0.563 -3.130 5.6055 ## ammanual 3.0009 1.853 1.62 0.120 -0.843 6.8446 5.5.2 F-Test The F-test for the model is a test of the null hypothesis that none of the independent variables linearly predict the dependent variable, that is, the model parameters are jointly zero: \\(H_0 : \\beta_1 = \\ldots = \\beta_k = 0\\). The regression mean sum of squares \\(MSR = \\frac{(\\hat{y} - \\bar{y})&#39;(\\hat{y} - \\bar{y})}{k-1}\\) and the error mean sum of squares \\(MSE = \\frac{\\hat{\\epsilon}&#39;\\hat{\\epsilon}}{n-k}\\) are each chi-square variables. Their ratio has an F distribution with \\(k - 1\\) numerator degrees of freedom and \\(n - k\\) denominator degrees of freedom. The F statistic can also be expressed in terms of the coefficient of correlation \\(R^2 = \\frac{MSR}{MST}\\). \\[F(k - 1, n - k) = \\frac{MSR}{MSE} = \\frac{R^2}{1 - R^2} \\frac{n-k}{k-1}\\] MSE is \\(\\sigma^2\\). If \\(H_0\\) is true, that is, there is no relationship between the predictors and the response, then \\(MSR\\) is also equal to \\(\\sigma^2\\), so \\(F = 1\\). As \\(R^2 \\rightarrow 1\\), \\(F \\rightarrow \\infty\\), and as \\(R^2 \\rightarrow 0\\), \\(F \\rightarrow 0\\). F increases with \\(n\\) and decreases with \\(k\\). 5.5.2.1 Example What is the probability that all parameters are jointly equal to zero? The F-statistic is presented at the bottom of the summary() function. You can verify this manually. ssr &lt;- sum((m$fitted.values - mean(d$mpg))^2) sse &lt;- sum(m$residuals^2) sst &lt;- sum((m$mpg - mean(d$mpg))^2) msr &lt;- ssr / k mse &lt;- sse / (n - k - 1) f = msr / mse p_value &lt;- pf(q = f, df1 = k, df2 = n - k - 1, lower.tail = FALSE) cat(&quot;F-statistic: &quot;, round(f, 4), &quot; on 3 and 65 DF, p-value: &quot;, p_value) ## F-statistic: 17 on 3 and 65 DF, p-value: 0.000000048 There is sufficient evidence \\((F = 17.35, P &lt; .0001)\\) to reject \\(H_0\\) that the parameter estimators are jointly equal to zero. The aov function calculates the sequential sum of squares. The regression sum of squares SSR for mpg ~ cyl is 824.8. Adding disp to the model increases SSR by 57.6. Adding hp to the model increases SSR by 18.5. It would seem that hp does not improve the model. summary(aov(m)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## cyl 2 825 412 65.26 0.00000000056 *** ## disp 1 58 58 9.12 0.0063 ** ## hp 1 19 19 2.93 0.1011 ## drat 1 12 12 1.89 0.1836 ## wt 1 56 56 8.83 0.0070 ** ## qsec 1 2 2 0.24 0.6282 ## vs 1 0 0 0.05 0.8289 ## am 1 17 17 2.62 0.1197 ## Residuals 22 139 6 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Order matters. Had we started with disp, then added hp we would find both estimators were significant. summary(aov(lm(mpg ~ disp + hp + drat + wt + qsec + vs + am + cyl, data = d))) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## disp 1 809 809 128.00 0.00000000012 *** ## hp 1 34 34 5.33 0.031 * ## drat 1 30 30 4.77 0.040 * ## wt 1 71 71 11.16 0.003 ** ## qsec 1 13 13 2.01 0.170 ## vs 1 0 0 0.04 0.852 ## am 1 20 20 3.24 0.086 . ## cyl 2 10 5 0.82 0.451 ## Residuals 22 139 6 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],
["interpretation.html", "5.6 Interpretation", " 5.6 Interpretation A plot of the standardized coefficients shows the relative importance of each variable. The distance the coefficients are from zero shows how much a change in a standard deviation of the regressor changes the mean of the predicted value. The CI shows the precision. The plot shows not only which variables are significant, but also which are important. d_sc &lt;- d %&gt;% mutate_at(c(&quot;mpg&quot;, &quot;disp&quot;, &quot;hp&quot;, &quot;drat&quot;, &quot;wt&quot;, &quot;qsec&quot;), scale) m_sc &lt;- lm(mpg ~ ., d_sc[,1:9]) lm_summary &lt;- summary(m_sc)$coefficients df &lt;- data.frame(Features = rownames(lm_summary), Estimate = lm_summary[,&#39;Estimate&#39;], std_error = lm_summary[,&#39;Std. Error&#39;]) df$lower = df$Estimate - qt(.05/2, m_sc$df.residual) * df$std_error df$upper = df$Estimate + qt(.05/2, m_sc$df.residual) * df$std_error df &lt;- df[df$Features != &quot;(Intercept)&quot;,] ggplot(df) + geom_vline(xintercept = 0, linetype = 4) + geom_point(aes(x = Estimate, y = Features)) + geom_segment(aes(y = Features, yend = Features, x=lower, xend=upper), arrow = arrow(angle=90, ends=&#39;both&#39;, length = unit(0.1, &#39;cm&#39;))) + scale_x_continuous(&quot;Standardized Weight&quot;) + labs(title = &quot;Model Feature Importance&quot;) The added variable plot shows the bivariate relationship between \\(Y\\) and \\(X_i\\) after accounting for the other variables. For example, the partial regression plots of y ~ x1 + x2 + x3 would plot the residuals of y ~ x2 + x3 vs x1, and so on. library(car) avPlots(m) "],
["model-validation.html", "5.7 Model Validation", " 5.7 Model Validation Evaluate predictive accuracy by training the model on a training data set and testing on a test data set. 5.7.1 Accuracy Metrics The most common measures of model fit are R-squared, RMSE, RSE, MAE, Adjusted R-squared, AIC, AICc, BIC, and Mallow’s Cp. 5.7.1.1 R-Squared The coefficient of determination (R-squared) is the percent of total variation in the response variable that is explained by the regression line. \\[R^2 = 1 - \\frac{SSE}{SST}\\] where \\(SSE = \\sum_{i=1}^n{(y_i - \\hat{y}_i)^2}\\) is the sum squared differences between the predicted and observed value, \\(SST = \\sum_{i = 1}^n{(y_i - \\bar{y})^2}\\) is the sum of squared differences between the observed and overall mean value, and \\(RSS = \\sum_{i=1}^n{(\\hat{y}_i - \\bar{y})^2}\\) is the sum of squared differences between the predicted and overall mean “no-relationship line” value. At the extremes, \\(R^2 = 1\\) means all data points fall perfectly on the regression line - the predictors account for all variation in the response; \\(R^2 = 0\\) means the regression line is horizontal at \\(\\bar{y}\\) - the predictors account for none of the variation in the response. In the simple case of a single predictor variable, \\(R^2\\) equals the squared correlation between \\(x\\) and \\(y\\), \\(r = Cor(x,y)\\). ssr &lt;- sum((m$fitted.values - mean(d$mpg))^2) sse &lt;- sum(m$residuals^2) sst &lt;- sum((d$mpg - mean(d$mpg))^2) (r2 &lt;- ssr / sst) ## [1] 0.88 (r2 &lt;- 1 - sse / sst) ## [1] 0.88 summary(m)$r.squared ## [1] 0.88 \\(R^2\\) is also equal to the correlation between the fitted value and observed values, \\(R^2 = Cor(Y, \\hat{Y})^2\\). cor(m$fitted.values, d$mpg)^2 ## [1] 0.88 R-squared is proportional to the the variance in the response, SST. Given a constant percentage error in predictions, a test set with relatively low variation in the reponse will have a lower R-squared. Conversely, test sets with large variation, e.g., housing data with home sale ranging from $60K to $2M may have a large R-squared despite average prediction errors of &gt;$10K. A close variant of R-squared is the non-parametric Spearman’s rank correlation. This statistic is the correlation of the ranks of the response and the predicted values. It is used when the model goal is ranking. 5.7.1.2 RMSE The root mean squared error (RMSE) is the average prediction error (square root of mean squared error). \\[RMSE = \\sqrt{\\frac{\\sum_{i=1}^n{(y_i - \\hat{y}_i)^2}}{n}}\\] sqrt(mean((d$mpg - m$fitted.values)^2)) ## [1] 2.1 The rmse() function from the Metrics package, and the postResample() function in caret calculate RMSE. rmse(actual = d$mpg, predicted = m$fitted.values) ## [1] 2.1 postResample(pred = m$fitted.values, obs = d$mpg)[1] ## RMSE ## 2.1 The mean squared error of a model with theoretical residual of mean zero and constant variance \\(\\sigma^2\\) can be decomposed into the model’s bias and the model’s variance: \\[E[MSE] = \\sigma^2 + Bias^2 + Var.\\] A model that predicts the response closely will have low bias, but be relatively sensitive to the training data and thus have high variance. A model that predicts the response conservatively (e.g., a simple mean) will have large bias, but be relatively insensitive to nuances in the training data. Here is an example of a simulated sine wave. A model predicting the mean value at the upper and lower levels has low variance, but high bias, and a model of an actual sine wave has low bias and high variance. This is referred to as the variance-bias trade-off. 5.7.1.3 RSE The residual standard error (RSE, or model sigma \\(\\hat{\\sigma}\\)) is an estimate of the standard deviation of \\(\\epsilon\\). It is roughly the average amount the response deviates from the true regression line. \\[\\sigma = \\sqrt{\\frac{\\sum_{i=1}^n{(y_i - \\hat{y}_i)^2}}{n-k-1}}\\] sqrt(sum((d$mpg - m$fitted.values)^2) / (n - k - 1)) ## [1] 2.5 # sd is sqrt(sse / (n-1)), sigma = sqrt(sse / (n - k - 1)) sd(m$residuals) * sqrt((n - 1) / (n - k - 1)) ## [1] 2.5 summary(m)$sigma ## [1] 2.5 sigma(m) ## [1] 2.5 5.7.1.4 MAE The mean absolute error (MAE) is the average absolute prediction arror. It is less sensitive to outliers. \\[MAE = \\frac{\\sum_{i=1}^n{|y_i - \\hat{y}_i|}}{n}\\] sum(abs(d$mpg - m$fitted.values)) / n ## [1] 1.7 The postResample() function in caret conveniently calculates all three. postResample(pred = m$fitted.values, obs = d$mpg) ## RMSE Rsquared MAE ## 2.08 0.88 1.70 defaultSummary(data = data.frame(obs = d$mpg, pred = m$fitted.values), model = m) ## RMSE Rsquared MAE ## 2.08 0.88 1.70 apply(as.matrix(m$fitted.values), 2, postResample, obs = d$mpg) ## [,1] ## RMSE 2.08 ## Rsquared 0.88 ## MAE 1.70 These metrics are good for evaluating a model, but less useful for comparing models. The problem is that they tend to improve with additional variables added to the model, even if the improvement is not significant. The following metrics aid model comparison by penalizing added variables. 5.7.1.5 Adjusted R-squared The adjusted R-squared (\\(\\bar{R}^2\\)) penalizes the R-squared metric for increasing number of predictors. \\[\\bar{R}^2 = 1 - \\frac{SSE}{SST} \\cdot \\frac{n-1}{n-k-1}\\] (adj_r2 &lt;- 1 - sse/sst * (n - 1) / (n - k - 1)) ## [1] 0.83 summary(m)$adj.r.squared ## [1] 0.83 5.7.1.6 AIC Akaike’s Information Criteria (AIC) is a penalization metric. The lower the AIC, the better the model. AIC(m) ## [1] 160 5.7.1.7 AICc AICc corrects AIC for small sample sizes. AIC(m) + (2 * k * (k + 1)) / (n - k - 1) ## [1] 168 5.7.1.8 BIC The Basiean information criteria (BIC) is like AIC, but with a stronger penalty for additional variables. BIC(m) ## [1] 176 5.7.1.9 Mallows Cp Mallows Cp is a variant of AIC. 5.7.1.9.1 Example Compare the full model to a model without cyl. The glance() function from the broom package calculates many validation metrics. Here are the validation stats for the full model and then the reduced model. library(broom) glance(m) %&gt;% select(adj.r.squared, sigma, AIC, BIC, p.value) ## # A tibble: 1 x 5 ## adj.r.squared sigma AIC BIC p.value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.826 2.51 160. 176. 0.0000000481 glance(lm(mpg ~ . - cyl, d[, 1:9])) %&gt;% select(adj.r.squared, sigma, AIC, BIC, p.value) ## # A tibble: 1 x 5 ## adj.r.squared sigma AIC BIC p.value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.829 2.50 158. 171. 0.00000000453 The ajusted R2 increased and AIC and BIC decreased, meaning the full model is less efficient at explaining the variability in the response value. The residual standard error sigma is smaller for the reduced model. Finally, the F statistic p-value is smaller for the reduced model, meaning the reduced model is statistically more significant. Note that these regression metrics are all internal measures, that is they have been computed on the training dataset, not the test dataset. 5.7.2 Cross-Validation Cross-validation is a set of methods for measuring the performance of a predictive model on a test dataset. The main measures of prediction performance are R2, RMSE and MAE. 5.7.2.1 Validation Set To perform validation set cross validation, randomly split the data into a training data set and a test data set. Fit models to the training data set, then predict values with the validation set. The model that produces the best prediction performance is the preferred model. The caret package provides useful methods for cross-validation. 5.7.2.1.1 Example library(caret) set.seed(123) train_idx &lt;- createDataPartition(y = d$mpg, p = 0.80, list = FALSE) d.train &lt;- d[train_idx, ] d.test &lt;- d[-train_idx, ] Build the model using d.train, make predictions, then calculate the R2, RMSE, and MAE. Use the train() function from the caret package. Use method = \"none\" to simply fit the model to the entire data set. set.seed(123) m1 &lt;- train(mpg ~ ., data = d.train[, 1:9], method = &quot;lm&quot;, trControl = trainControl(method = &quot;none&quot;)) print(m1) ## Linear Regression ## ## 28 samples ## 8 predictor ## ## No pre-processing ## Resampling: None postResample(pred = predict(m1, newdata = d.test), obs = d.test$mpg) ## RMSE Rsquared MAE ## 3.10 0.96 2.45 The validation set method is only useful when you have a large data set to partition. A second disadvantage is that building a model on a fraction of the data leaves out information. The test error will vary with which observations are included in the training set. 5.7.2.2 LOOCV Leave one out cross validation (LOOCV) works by successively modeling with training sets leaving out one data point, then averaging the prediction errors. set.seed(123) m2 &lt;- train(mpg ~ ., data = d.train[, 1:9], method = &quot;lm&quot;, trControl = trainControl(method = &quot;LOOCV&quot;)) print(m2) ## Linear Regression ## ## 28 samples ## 8 predictor ## ## No pre-processing ## Resampling: Leave-One-Out Cross-Validation ## Summary of sample sizes: 27, 27, 27, 27, 27, 27, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 2.8 0.76 2.3 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE postResample(pred = predict(m2, newdata = d.test), obs = d.test$mpg) ## RMSE Rsquared MAE ## 3.10 0.96 2.45 This method isn’t perfect either. It repeats as many times as there are data points, so the execution time may be long. LOOCV is also sensitive to outliers. 5.7.2.3 K-fold Cross-Validation K-fold cross-validation splits the dataset into k folds (subsets), then uses k-1 of the folds for a training set and the remaining fold for a test set, then repeats for all permutations of k taken k-1 at a time. E.g., 3-fold cross-validation will partition the data into sets A, B, and C, then create train/test splits of [AB, C], [AC, B], and [BC, A]. K-fold cross-validation is less computationally expensive than LOOCV, and often yields more accurate test error rate estimates. What is the right value of k? The lower is k the more biased the estimates; the higher is k the larger the estimate variability. At the extremes k = 2 is the validation set method, and k = n is the LOOCV method. In practice, one typically performs k-fold cross-validation using k = 5 or k = 10 because these values have been empirically shown to balence bias and variance. set.seed(123) m3 &lt;- train(mpg ~ ., data = d.train[, 1:9], method = &quot;lm&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 5)) print(m3) ## Linear Regression ## ## 28 samples ## 8 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 22, 22, 23, 22, 23 ## Resampling results: ## ## RMSE Rsquared MAE ## 3 0.85 2.6 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE postResample(pred = predict(m3, newdata = d.test), obs = d.test$mpg) ## RMSE Rsquared MAE ## 3.10 0.96 2.45 5.7.2.4 Repeated K-fold CV You can also perform k-fold cross-validation multiple times and average the results. Specify method = \"repeatedcv\" and repeats = 3 in the trainControl object for three repeats. set.seed(123) m4 &lt;- train(mpg ~ ., data = d.train[, 1:9], method = &quot;lm&quot;, trControl = trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 3)) print(m4) ## Linear Regression ## ## 28 samples ## 8 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold, repeated 3 times) ## Summary of sample sizes: 22, 22, 23, 22, 23, 23, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 3.1 0.81 2.7 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE postResample(pred = predict(m4, newdata = d.test), obs = d.test$mpg) ## RMSE Rsquared MAE ## 3.10 0.96 2.45 5.7.2.5 Bootstrapping Bootstrapping randomly selects a sample of n observations with replacement from the original dataset to evaluate the model. The procedure is repeated many times. Specify method = \"boot\" and number = 100 to perform 100 bootstrap samples. set.seed(123) m5 &lt;- train(mpg ~ ., data = d.train[, 1:9], method = &quot;lm&quot;, trControl = trainControl(method = &quot;boot&quot;, number = 100)) ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit ## may be misleading ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit ## may be misleading ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit ## may be misleading print(m5) ## Linear Regression ## ## 28 samples ## 8 predictor ## ## No pre-processing ## Resampling: Bootstrapped (100 reps) ## Summary of sample sizes: 28, 28, 28, 28, 28, 28, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 3.9 0.64 3.2 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE postResample(pred = predict(m5, newdata = d.test), obs = d.test$mpg) ## RMSE Rsquared MAE ## 3.10 0.96 2.45 5.7.3 Gain Curve For supervised learning purposes, a visual way to evaluate a regression model is with the gain curve. This visualization compares a predictive model score to an actual outcome (either binary (0/1) or continuous). The gain curve plot measures how well the model score sorts the data compared to the true outcome value. The x-axis is the fraction of items seen when sorted by score, and the y-axis is the cumulative summed true outcome when sorted by score. For comparison, GainCurvePlot also plots the “wizard curve”: the gain curve when the data is sorted according to its true outcome. A relative Gini score close to 1 means the model sorts responses well. library(WVPlots) d$fitted &lt;- m$fitted.values GainCurvePlot(d, xvar = &quot;fitted&quot;, truthVar = &quot;mpg&quot;, title = &quot;Model Gain Curve&quot;) "],
["reference.html", "5.8 Reference", " 5.8 Reference Penn State University, STAT 501, Lesson 12: Multicollinearity &amp; Other Regression Pitfalls. https://newonlinecourses.science.psu.edu/stat501/lesson/12. STHDA. Bootstrap Resampling Essentials in R. http://www.sthda.com/english/articles/38-regression-model-validation/156-bootstrap-resampling-essentials-in-r/ Molnar, Christoph. “Interpretable machine learning. A Guide for Making Black Box Models Explainable”, 2019. https://christophm.github.io/interpretable-ml-book/. "],
["generalized-linear-models.html", "Chapter 6 Generalized Linear Models", " Chapter 6 Generalized Linear Models These notes are primarily from PSU STAT 504 which uses Alan Agresti’s Categorical Data Analysis (Agresti 2013). I also reviewed PSU STAT 501, DataCamp’s Generalized Linar Models in R, DataCamp’s Multiple and Logistic Regression, and **Interpretable machine learning*\"** (Molnar 2020). The linear regression model, \\(E(Y|X) = X \\beta\\), structured as \\(y_i = X_i \\beta + \\epsilon_i\\) where \\(X_i \\beta = \\mu_i\\), assumes the response is a linear function of the predictors and the residuals are independent random variables normally distributed with mean zero and constant variance, \\(\\epsilon \\sim N \\left(0, \\sigma^2 \\right)\\). This implies that given some set of predictors, the response is normally distributed about its expected value, \\(y_i \\sim N \\left(\\mu_i, \\sigma^2 \\right)\\). However, there are many situations where this assumption of normality fails. Generalized linear models (GLMs) are a generalization of the linear regression model that addresses non-normal response distributions. The response given a set of predictors will not have a normal distribution if its underlying data-generating process is binomial or multinomial (proportions), Poisson (counts), or exponential (time-to-event). In these situations a regular linear regression can predict proportions outside [0, 100] or counts or times that are negative. GLMs solve this problem by modeling a function of the expected value of \\(y\\), \\(f(E(Y|X)) = X \\beta\\). There are three components to a GLM: the random component is the probability distribution of the response variable (normal, binomial, Poisson, etc.); the systematic component is the explanatory variables \\(X\\beta\\); and the link function \\(\\eta\\) specifies the link between random and systematic components, converting the response range to \\([-\\infty, +\\infty]\\). Linear regression is thus a special case of GLM where link function is the identity function, \\(f(E(Y|X)) = E(Y|X)\\). For a logistic regression, where the data generating process is binomial, the link function is \\[f(E(Y|X)) = \\ln \\left( \\frac{E(Y|X)}{1 - E(Y|X)} \\right) = \\ln \\left( \\frac{\\pi}{1 - \\pi} \\right) = logit(\\pi)\\] where \\(\\pi\\) is the event probability. (As an aside, you have probably heard of the related “probit” regression. The probit regression link function is \\(f(E(Y|X)) = \\Phi^{-1}(E(Y|X)) = \\Phi^{-1}(\\pi)\\). The difference between the logistic and probit link function is theoretical, and the practical significance is slight. You can probably safely ignore probit). For a Poisson regression, the link function is \\[f(E(Y|X)) = \\ln (E(Y|X)) = \\ln(\\lambda)\\] where \\(\\lambda\\) is the expected event rate. For an exponential regression, the link function is \\[f(E(Y|X) = -E(Y|X) = -\\lambda\\] where \\(\\lambda\\) is the expected time to event. GLM uses maximum likelihood estimation (MLE) rather than ordinary least squares (OLS) to estimate the parameters, and thus relies on large-sample approximations. In R, specify a GLM just like an linear model, but with the glm() function, specifying the distribution with the family parameter. family = \"gaussian\": linear regression family = \"binomial\": logistic regression family = binomial(link = \"probit\"): probit regression family = \"poisson\": Poisson regression References "],
["logistic-regression.html", "6.1 Logistic Regression", " 6.1 Logistic Regression Logistic regression estimates the probability of a particular level of a categorical response variable given a set of predictors. The response levels can be binary, nominal (multiple categories), or ordinal (multiple levels). The binary logistic regression model is \\[y = logit(\\pi) = \\ln \\left( \\frac{\\pi}{1 - \\pi} \\right) = X \\beta\\] where \\(\\pi\\) is the event probability. The model predicts the log odds of the response variable. The maximum likelihood estimator maximizes the likelihood function \\[L(\\beta; y, X) = \\prod_{i=1}^n \\pi_i^{y_i}(1 - \\pi_i)^{(1-y_i)} = \\prod_{i=1}^n\\frac{\\exp(y_i X_i \\beta)}{1 + \\exp(X_i \\beta)}.\\] There is no closed-form solution, so GLM estimates coefficients with interatively reweighted least squares. Here is a case study to illustrate the points. Dataset donner contains observations of 45 members of the Donner party with response variable (surv) an explanatory variables age and sex. glimpse(donner) ## Rows: 45 ## Columns: 3 ## $ age &lt;dbl&gt; 23, 40, 40, 30, 28, 40, 45, 62, 65, 45, 25, 28, 28, 23, 22, 23... ## $ sex &lt;fct&gt; M, F, M, M, M, M, F, M, M, F, F, M, M, M, F, F, M, F, F, M, F,... ## $ surv &lt;fct&gt; Died, Lived, Lived, Died, Died, Died, Died, Died, Died, Died, ... donner %&gt;% mutate(surv = as.numeric(surv)-1) %&gt;% ggplot(aes(x = age, y = surv, color = sex)) + geom_jitter() + geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;binomial&quot;), se = FALSE) + theme_mf() + labs(title = &quot;Donner Party Survivorship&quot;, color = &quot;&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Fit a logistic regression \\(SURV = SEX + AGE + SEX : AGE\\). m &lt;- glm(surv ~ sex*age, data = donner, family = binomial(link = logit)) summary(m) ## ## Call: ## glm(formula = surv ~ sex * age, family = binomial(link = logit), ## data = donner) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.228 -0.939 -0.555 0.779 1.700 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 7.2464 3.2052 2.26 0.024 * ## sexM -6.9280 3.3989 -2.04 0.042 * ## age -0.1941 0.0874 -2.22 0.026 * ## sexM:age 0.1616 0.0943 1.71 0.086 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 61.827 on 44 degrees of freedom ## Residual deviance: 47.346 on 41 degrees of freedom ## AIC: 55.35 ## ## Number of Fisher Scoring iterations: 5 The “z value” in the Coefficients table is the Wald z statistic, \\(z = \\hat{\\beta} / SE(\\hat{\\beta})\\), which if squared is the Wald chi-squared statistic, \\(z^2\\). The p.value is the area to the right of \\(z^2\\) in the \\(\\chi_1^2\\) density curve: m %&gt;% tidy() %&gt;% mutate( z = estimate / std.error, p_z2 = pchisq(z^2, df = 1, lower.tail = FALSE) ) %&gt;% select(term, estimate, z, p_z2) ## # A tibble: 4 x 4 ## term estimate z p_z2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 7.25 2.26 0.0238 ## 2 sexM -6.93 -2.04 0.0415 ## 3 age -0.194 -2.22 0.0264 ## 4 sexM:age 0.162 1.71 0.0865 Below the Coefficients table, the “dispersion parameter” refers to overdispersion, a common issue with GLM. For a logistic regression, the response variable should be distributed \\(y_i \\sim Bin(n_i, \\pi_i)\\) with \\(\\mu_i = n_i \\pi_i\\) and \\(\\sigma^2 = \\pi (1 - \\pi)\\). Overdispersion means the data shows evidence of variance greater than \\(\\sigma^2\\). “Fisher scoring” is a method for ML estimation. Logistic regression uses an iterative procedure to fit the model, so this section indicates whether the algorithm converged. The null deviance is the likelihood ratio \\(G^2 = 61.827\\) of the intercept-only model. The residual deviance is the likelihood ratio \\(G^2 = 47.346\\) after including all model covariates. \\(G^2\\) is large, so reject the null hypothesis of no age and sex effects. The ANOVA table shows the change in deviance from adding each variable successively to the model. anova(m) ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: surv ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev ## NULL 44 61.8 ## sex 1 4.54 43 57.3 ## age 1 6.03 42 51.3 ## sex:age 1 3.91 41 47.3 glance(m) ## # A tibble: 1 x 7 ## null.deviance df.null logLik AIC BIC deviance df.residual ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 61.8 44 -23.7 55.3 62.6 47.3 41 Plug in values to interpret the model. The log odds of a 24 year-old female surviving is \\(\\hat{y} = 2.59\\). The log odds of a 24 year-old male surviving is \\(\\hat{y} = -0.46\\). coef(m)[&quot;(Intercept)&quot;] + coef(m)[&quot;sexM&quot;]*0 + coef(m)[&quot;age&quot;]*24 + coef(m)[&quot;sexM:age&quot;]*0*24 coef(m)[&quot;(Intercept)&quot;] + coef(m)[&quot;sexM&quot;]*1 + coef(m)[&quot;age&quot;]*24 + coef(m)[&quot;sexM:age&quot;]*1*24 # Or use predict() (lo_f &lt;- predict(m, newdata = data.frame(sex = &quot;F&quot;, age = 24))) (lo_m &lt;- predict(m, newdata = data.frame(sex = &quot;M&quot;, age = 24))) Log odds are not easy to interpet. Exponentiate the log odds to get the odds. \\[odds(\\hat{y}) = \\exp (\\hat{y}) = \\frac{\\pi}{1 - \\pi}.\\] The odds of a 24 year-old female surviving is \\(\\exp(\\hat{y}) = 13.31\\). The odds of a 24 year-old male surviving is \\(\\exp(\\hat{y}) = 0.63\\). exp(lo_f) exp(lo_m) Solve for \\(\\pi\\) to get the probability. \\[\\pi = \\frac{\\exp (\\hat{y})}{1 + \\exp (\\hat{y})}\\] The probability of a 24 year-old female surviving is \\(\\pi = 0.93\\). The probability of a female of average age surviving is \\(\\pi = 0.39\\). The predict() function for a logistic model returns log-odds, but can also return \\(\\pi\\) by specifying parameter type = \"response\". exp(lo_f) / (1 + exp(lo_f)) exp(lo_m) / (1 + exp(lo_m)) # Or use predict(..., type = &quot;response&quot;) (p_f &lt;- predict(m, newdata = data.frame(sex = &quot;F&quot;, age =24), type = &quot;response&quot;)) (p_m &lt;- predict(m, newdata = data.frame(sex = &quot;M&quot;, age =24), type = &quot;response&quot;)) Interpret the coefficient estimates using the odds ratio, the ratio of the odds before and after an increment to the predictors. The odds ratio is how much the odds would be multiplied after a \\(\\delta = X_1 - X_0\\) unit increase in \\(X\\). \\[\\theta = \\frac{\\pi / (1 - \\pi) |_{X = X_1}}{\\pi / (1 - \\pi) |_{X = X_0}} = \\frac{\\exp (X_1 \\hat{\\beta})}{\\exp (X_0 \\hat{\\beta})} = \\exp ((X_1-X_0) \\hat{\\beta}) = \\exp (\\delta \\hat{\\beta})\\] The odds of a female surviving are multiplied by a factor of \\(\\exp(1 \\cdot (-0.19)) = 0.824\\) per additional year of age (or the odds fall by \\(1 - 0.824 = 17.6\\%\\)). The odds of a male surviving are multiplied by a factor of \\(\\exp(1 \\cdot (-0.161-0.19)) = 0.968\\) per additional year of age. exp(1 * (coef(m)[&quot;age&quot;] + 0*coef(m)[&quot;sexM:age&quot;])) # female exp(1 * (coef(m)[&quot;age&quot;] + 1*coef(m)[&quot;sexM:age&quot;])) # male oddsratio::or_glm() calculates the odds ratio from an increment in the predictor values. oddsratio::or_glm(donner, m, incr = list(age = 1)) ## # A tibble: 3 x 5 ## predictor oddsratio `CI_low (2.5)` `CI_high (97.5)` increment ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 sexM 0.001 0 0.243 Indicator variable ## 2 age 0.824 0.652 0.945 1 ## 3 sexM:age 1.18 1.00 1.50 Indicator variable The predicted values can also be expressed as the probabilities \\(\\pi\\). This produces the familiar signmoidal shape of the binary relationship. augment(m, type.predict = &quot;response&quot;) %&gt;% ggplot(aes(x = age)) + geom_point(aes(y = surv)) + geom_line(aes(y = .fitted+1)) + theme_mf() + labs(x = &quot;AGE&quot;, y = &quot;Probability of SURVIVE&quot;, title = &quot;Binary Fitted Line Plot&quot;) Evaluate a logistic regression using a Gain curve or ROC curve. In the gain curve, the x-axis is the fraction of items seen when sorted by the predicted value, and the y-axis is the cumulative summed true outcome. The “wizard” curve is the gain curve when the data is sorted by the true outcome. If the model’s gain curve is close to the wizard curve, then the model predicted the response variable well. The grey area is the “gain” over a random prediction. 20 of the 45 members of the Donner party survived. The gain curve encountered 10 survivors (50%) within the first 12 observations (27%). It encountered all 20 survivors on the 37th observation. The bottom of the grey area is the outcome of a random model. Only half the survivors would be observed within 50% of the observations. The top of the grey area is the outcome of the perfect model, the “wizard curve”. Half the survivors would be observed in 10/45=22% of the observations. options(yardstick.event_first = FALSE) # set the second level as success augment(m, type.predict = &quot;response&quot;) %&gt;% yardstick::gain_curve(surv, .fitted) %&gt;% autoplot() + labs(title = &quot;Gain Curve&quot;) The ROC (Receiver Operating Characteristics) curve plots sensitivity vs specificity at different cut-off values for the probability, ranging cut-off from 0 to 1. options(yardstick.event_first = FALSE) # set the second level as success augment(m, type.predict = &quot;response&quot;) %&gt;% yardstick::roc_curve(surv, .fitted) %&gt;% autoplot() + labs(title = &quot;ROC Curve&quot;) "],
["multinomial-logistic-regression.html", "6.2 Multinomial Logistic Regression", " 6.2 Multinomial Logistic Regression The following notes rely on the [PSU STAT 504 course notes](https://online.stat.psu.edu/stat504/node/171/. Multinomial logistic regression models the odds the multinomial response variable \\(Y \\sim Mult(n, \\pi)\\) is in level \\(j\\) relative to baseline category \\(j^*\\) for all pairs of categories as a function of \\(k\\) explanatory variables, \\(X = (X_1, X_2, ... X_k)\\). \\[\\log \\left( \\frac{\\pi_{ij}}{\\pi_{ij^*}} \\right) = x_i^T \\beta_j, \\hspace{5mm} j \\ne j^2\\] Interpet the \\(k^{th}\\) element of \\(\\beta_j\\) as the increase in log-odds of falling a response in category \\(j\\) relative to category \\(j^*\\) resulting from a one-unit increase in the \\(k^{th}\\) predictor term, holding the other terms constant. Multinomial model is a type of GLM. Here is an example using multinomial logistic regression. A researcher classified the stomach contents of \\(n = 219\\) alligators according to \\(r = 5\\) categories (fish, Inv., Rept, Bird, Other) as a function of covariates Lake, Sex, and Size.. gator_dat &lt;- tribble( ~profile, ~Gender, ~Size, ~Lake, ~Fish, ~Invertebrate, ~Reptile, ~Bird, ~Other, &quot;1&quot;, &quot;f&quot;, &quot;&lt;2.3&quot;, &quot;george&quot;, 3, 9, 1, 0, 1, &quot;2&quot;, &quot;m&quot;, &quot;&lt;2.3&quot;, &quot;george&quot;, 13, 10, 0, 2, 2, &quot;3&quot;, &quot;f&quot;, &quot;&gt;2.3&quot;, &quot;george&quot;, 8, 1, 0, 0, 1, &quot;4&quot;, &quot;m&quot;, &quot;&gt;2.3&quot;, &quot;george&quot;, 9, 0, 0, 1, 2, &quot;5&quot;, &quot;f&quot;, &quot;&lt;2.3&quot;, &quot;hancock&quot;, 16, 3, 2, 2, 3, &quot;6&quot;, &quot;m&quot;, &quot;&lt;2.3&quot;, &quot;hancock&quot;, 7, 1, 0, 0, 5, &quot;7&quot;, &quot;f&quot;, &quot;&gt;2.3&quot;, &quot;hancock&quot;, 3, 0, 1, 2, 3, &quot;8&quot;, &quot;m&quot;, &quot;&gt;2.3&quot;, &quot;hancock&quot;, 4, 0, 0, 1, 2, &quot;9&quot;, &quot;f&quot;, &quot;&lt;2.3&quot;, &quot;oklawaha&quot;, 3, 9, 1, 0, 2, &quot;10&quot;, &quot;m&quot;, &quot;&lt;2.3&quot;, &quot;oklawaha&quot;, 2, 2, 0, 0, 1, &quot;11&quot;, &quot;f&quot;, &quot;&gt;2.3&quot;, &quot;oklawaha&quot;, 0, 1, 0, 1, 0, &quot;12&quot;, &quot;m&quot;, &quot;&gt;2.3&quot;, &quot;oklawaha&quot;, 13, 7, 6, 0, 0, &quot;13&quot;, &quot;f&quot;, &quot;&lt;2.3&quot;, &quot;trafford&quot;, 2, 4, 1, 1, 4, &quot;14&quot;, &quot;m&quot;, &quot;&lt;2.3&quot;, &quot;trafford&quot;, 3, 7, 1, 0, 1, &quot;15&quot;, &quot;f&quot;, &quot;&gt;2.3&quot;, &quot;trafford&quot;, 0, 1, 0, 0, 0, &quot;16&quot;, &quot;m&quot;, &quot;&gt;2.3&quot;, &quot;trafford&quot;, 8, 6, 6, 3, 5 ) gator_dat &lt;- gator_dat %&gt;% mutate( Gender = as_factor(Gender), Lake = fct_relevel(Lake, &quot;hancock&quot;), Size = as_factor(Size) ) There are 4 equations to estimate: \\[\\log \\left( \\frac{\\pi_j} {\\pi_{j^*}} \\right) = \\beta X\\] where \\(\\pi_{j^*}\\) is the probability of fish, the baseline category. Run a multivariate logistic regression model with VGAM::vglm(). library(VGAM) vglm() fits 4 logit models. gator_vglm &lt;- vglm( cbind(Bird,Invertebrate,Reptile,Other,Fish) ~ Lake + Size + Gender, data = gator_dat, family = multinomial ) summary(gator_vglm) ## ## Call: ## vglm(formula = cbind(Bird, Invertebrate, Reptile, Other, Fish) ~ ## Lake + Size + Gender, family = multinomial, data = gator_dat) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## log(mu[,1]/mu[,5]) -1.199 -0.548 -0.2242 0.368 3.48 ## log(mu[,2]/mu[,5]) -1.322 -0.461 0.0105 0.381 1.87 ## log(mu[,3]/mu[,5]) -0.703 -0.575 -0.3551 0.261 2.06 ## log(mu[,4]/mu[,5]) -1.694 -0.289 -0.1081 1.124 1.37 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept):1 -1.857 0.581 -3.19 0.0014 ** ## (Intercept):2 -1.611 0.551 -2.93 0.0034 ** ## (Intercept):3 -2.287 0.657 -3.48 0.0005 *** ## (Intercept):4 -0.664 0.380 -1.75 0.0806 . ## Lakegeorge:1 -0.575 0.795 -0.72 0.4694 ## Lakegeorge:2 1.781 0.623 2.86 0.0043 ** ## Lakegeorge:3 -1.129 1.193 -0.95 0.3437 ## Lakegeorge:4 -0.767 0.569 -1.35 0.1776 ## Lakeoklawaha:1 -1.126 1.192 -0.94 0.3451 ## Lakeoklawaha:2 2.694 0.669 4.02 0.000057 *** ## Lakeoklawaha:3 1.401 0.810 1.73 0.0839 . ## Lakeoklawaha:4 -0.741 0.742 -1.00 0.3184 ## Laketrafford:1 0.662 0.846 0.78 0.4341 ## Laketrafford:2 2.936 0.687 4.27 0.000019 *** ## Laketrafford:3 1.932 0.825 2.34 0.0193 * ## Laketrafford:4 0.791 0.588 1.35 0.1784 ## Size&gt;2.3:1 0.730 0.652 1.12 0.2629 ## Size&gt;2.3:2 -1.336 0.411 -3.25 0.0012 ** ## Size&gt;2.3:3 0.557 0.647 0.86 0.3890 ## Size&gt;2.3:4 -0.291 0.460 -0.63 0.5275 ## Genderm:1 -0.606 0.689 -0.88 0.3787 ## Genderm:2 -0.463 0.396 -1.17 0.2418 ## Genderm:3 -0.628 0.685 -0.92 0.3598 ## Genderm:4 -0.253 0.466 -0.54 0.5881 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Names of linear predictors: log(mu[,1]/mu[,5]), log(mu[,2]/mu[,5]), ## log(mu[,3]/mu[,5]), log(mu[,4]/mu[,5]) ## ## Residual deviance: 50 on 40 degrees of freedom ## ## Log-likelihood: -73 on 40 degrees of freedom ## ## Number of Fisher scoring iterations: 5 ## ## No Hauck-Donner effect found in any of the estimates ## ## ## Reference group is level 5 of the response The residual deviance is 50.2637 on 40 degrees of freedom. Residual deviance tests the current model fit versus the saturated model. The saturated model, which fits a separate multinomial distribution to each of the 16 profiles (unique combinations of lake, sex and size), has 16 × 4 = 64 parameters. The current model has an intercept, three lake coefficients, one sex coefficient and one size coefficient for each of the four logit equations, for a total of 24 parameters. Therefore, the overall fit statistics have 64 − 24 = 40 degrees of freedom. E &lt;- data.frame(fitted(gator_vglm) * rowSums(gator_dat[, 5:9])) O &lt;- gator_dat %&gt;% select(Bird, Invertebrate, Reptile, Other, Fish) + .000001 (g2 &lt;- 2 * sum(O * log(O / E))) ## [1] 50 indicates the model fits okay, but not great. The Residual Deviance of 50.26 with 40 df from the table above output is reasonable, with p-value of 0.1282 and the statistics/df is close to 1 that is 1.256. "],
["ordinal-logistic-regression.html", "6.3 Ordinal Logistic Regression", " 6.3 Ordinal Logistic Regression These notes rely on UVA, PSU STAT 504 class notes, and Laerd Statistics. The ordinal logistic regression model is \\[logit[P(Y \\le j)] = \\log \\left[ \\frac{P(Y \\le j)}{P(Y \\gt j)} \\right] = \\alpha_j - \\beta X, \\hspace{5mm} j \\in [1, J-1]\\] where \\(j \\in [1, J-1]\\) are the levels of the ordinal outcome variable \\(Y\\). The proportional odds model assumes there is a common set of slope parameters \\(\\beta\\) for the predictors. The ordinal outcomes are distinguished by the \\(J-1\\) intercepts \\(\\alpha_j\\). The benchmark level is \\(J\\). Technically, the model could be written \\(logit[P(Y \\le j)] = \\alpha_j + \\zeta X\\), replacing beta with zeta because the model fits \\(\\alpha_j - \\beta X\\) instead of \\(\\alpha_j + \\beta X\\). Suppose you want to model the probability a respondent holds a political ideology [“Socialist”, “Liberal”, “Moderate”, “Conservative”, “Libertarian”] given their party affiliation [“Republican”, “Democrat”]. table(ideology) ## ideo ## party Socialist Liberal Moderate Conservative Libertarian ## Rep 30 46 148 84 99 ## Dem 80 81 171 41 55 6.3.1 Assumptions Ordinal regression makes four assumptions about the underlying data. One is that the response variable is ordinal (duh). The second is that the explanatory variables are continuous or categorical. You can include ordinal variables, but you need to treat them either as continous or categorical. Third, there is no multicollinearity. Fourth, the odds are proportional, meaning each independent variable has an identical effect at each cumulative split of the ordinal dependent variable. Test for proportionality with a full likelihood ratio test comparing the fitted location model to a model with varying location parameters. This test can sometimes flag violations that do not exist, so can also run separate binomial logistic regressions on cumulative dichotomous dependent variables to further determine if this assumption is met. 6.3.2 Modeling Fit a proportional odds logistic regression. pom &lt;- MASS::polr(ideo ~ party, data = ideology) summary(pom) ## Call: ## MASS::polr(formula = ideo ~ party, data = ideology) ## ## Coefficients: ## Value Std. Error t value ## partyDem -0.975 0.129 -7.54 ## ## Intercepts: ## Value Std. Error t value ## Socialist|Liberal -2.469 0.132 -18.736 ## Liberal|Moderate -1.475 0.109 -13.531 ## Moderate|Conservative 0.237 0.094 2.516 ## Conservative|Libertarian 1.070 0.104 10.292 ## ## Residual Deviance: 2474.98 ## AIC: 2484.98 The log-odds a Democrat identifies as Socialist vs &gt;Socialist, or equivalently, the log-odds a Democrat identifies as &lt;=Socialist vs &gt;=Liberal is \\[logit[P(Y \\le 1)] = -2.4690 - (-0.9745)(1) = -1.4945\\] which translates into an odds of \\[odds(Y&lt;=1) = exp(logit[P(Y \\le 1)]) = \\frac{exp(-2.469)}{exp(-0.9745)} = 0.2244\\] It is the same for Republicans, except multiply the slope coefficient by zero. \\[logit[P(Y \\le 1)] = -2.4690 - (-0.9745)(0) = -2.4690\\] \\[odds(Y&lt;=1) = exp(logit[P(Y \\le 1)]) = \\frac{exp(-2.469)}{exp(0)} = -2.4690\\] The “proportional odds” part of the proportional odds model is that the ratios of the \\(J - 1\\) odds are identical for each level of the predictors. The numerators are always the same, and the denominators differ only by the exponent of the slope coefficient, \\(-0.9745\\). For all \\(j \\in [1, J-1]\\), the odds a Democrat’s ideology is \\(\\le j\\) vs \\(&gt;j\\) is \\(exp(-0.9745) = 2.6498\\) times that of a Republican’s odds. You can translate the cumulative odds to cumulative probabilities by taking the ratio \\(\\pi = exp(odds) / (1 + exp(odds))\\). The probability a Democrat identifies as &lt;=Socialist vs &gt;Socialist is \\[P(Y \\le 1) = \\frac{exp(-1.4945)} {(1 + exp(-1.4945))} = 0.183.\\] The individual probabilities are just the successive differences in the cumulative probabilities. The log odds a Democrat identifies as &lt;=Liberal vs &gt;Liberal are \\(logit[P(Y \\lt 2)] = -1.4745 - (-0.9745)(1) = -0.500\\), which translates into a probability of \\(P(Y \\le 2) = exp(-0.5) / (1 + exp(-0.5)) = 0.378\\). The probability a Democrat identifies as Liberal is the difference in adjacent cumulative probabilities, \\(P(Y \\le 2) - P(Y \\le 1) = 0.378 = 0.183 = 0.194\\). This is how the model to predicts the level probabilities. x &lt;- predict(pom, newdata = data.frame(party = c(&quot;Dem&quot;, &quot;Rep&quot;)), type = &quot;probs&quot;) rownames(x) &lt;- c(&quot;Dem&quot;, &quot;Rep&quot;) print(x) ## Socialist Liberal Moderate Conservative Libertarian ## Dem 0.183 0.19 0.39 0.11 0.11 ## Rep 0.078 0.11 0.37 0.19 0.26 Always check the assumption of proportional odds. One way to do this is by comparing the proportional odds model with a multinomial logit model, also called an unconstrained baseline logit model. The multinomial logit model models unordered responses and fits a slope to each level of the \\(J – 1\\) responses. The proportional odds model is nested in the multinomial model, so you can use a likelihood ratio test to see if the models are statistically different. mlm &lt;- nnet::multinom(ideo ~ party, data = ideology) ## # weights: 15 (8 variable) ## initial value 1343.880657 ## iter 10 value 1239.866743 ## final value 1235.648615 ## converged Calculate the difference in the deviance test statistics \\(D = -2 loglik(\\beta)\\). G &lt;- -2 * (logLik(pom)[1] - logLik(mlm)[1]) pchisq(G, df = length(pom$zeta) - 1, lower.tail = FALSE) ## [1] 0.3 The p-value is high, so do not reject the null hypothesis that the proportional odds model fits differently than the more complex multinomial logit model. 6.3.3 Case Study The General Social Survey for year 1972, 1973, and 1974 surveyed caucasian Christians about their attitudes att toward abortion. Respondents were classified by years of education edu and religious group att. abort %&gt;% pivot_wider(names_from = att, values_from = cnt) ## # A tibble: 27 x 6 ## year rel edu Neg Mix Pos ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1972 Prot Low 9 12 48 ## 2 1972 Prot Med 13 43 197 ## 3 1972 Prot High 4 9 139 ## 4 1972 SProt Low 9 17 30 ## 5 1972 SProt Med 6 10 97 ## 6 1972 SProt High 1 8 68 ## 7 1972 Cath Low 14 12 32 ## 8 1972 Cath Med 18 50 131 ## 9 1972 Cath High 8 13 64 ## 10 1973 Prot Low 4 16 59 ## # ... with 17 more rows Fit a proportional-odds cumulative logit model with just main effects. There are two main effects for year, two for rel, and two for edu, plus two logit equations for the response for a total of eight parameters. abort_mdl &lt;- MASS::polr(att ~ year + rel + edu, data = abort, weights = cnt) summary(abort_mdl) ## Call: ## MASS::polr(formula = att ~ year + rel + edu, data = abort, weights = cnt) ## ## Coefficients: ## Value Std. Error t value ## year1973 0.221 0.105 2.11 ## year1974 0.233 0.106 2.20 ## relSProt -0.249 0.113 -2.19 ## relCath -0.796 0.100 -7.93 ## eduMed 0.717 0.109 6.59 ## eduHigh 1.127 0.128 8.79 ## ## Intercepts: ## Value Std. Error t value ## Neg|Mix -2.33 0.14 -17.27 ## Mix|Pos -0.79 0.12 -6.49 ## ## Residual Deviance: 4059.32 ## AIC: 4075.32 All predictors are significant. Now fit the saturated model. abort_mdl_sat &lt;- MASS::polr(att ~ year*rel*edu, weights = cnt, data = abort) summary(abort_mdl_sat) ## Call: ## MASS::polr(formula = att ~ year * rel * edu, data = abort, weights = cnt) ## ## Coefficients: ## Value Std. Error t value ## year1973 0.3422 0.365 0.9370 ## year1974 -0.0144 0.361 -0.0398 ## relSProt -0.6274 0.366 -1.7141 ## relCath -0.7238 0.369 -1.9620 ## eduMed 0.5036 0.301 1.6743 ## eduHigh 1.6007 0.390 4.1068 ## year1973:relSProt 0.1093 0.524 0.2088 ## year1974:relSProt 0.4387 0.521 0.8421 ## year1973:relCath 0.3895 0.539 0.7228 ## year1974:relCath 0.8696 0.559 1.5543 ## year1973:eduMed 0.2805 0.441 0.6365 ## year1974:eduMed 0.5487 0.430 1.2761 ## year1973:eduHigh -0.5961 0.540 -1.1031 ## year1974:eduHigh -0.1175 0.541 -0.2171 ## relSProt:eduMed 1.1421 0.479 2.3849 ## relCath:eduMed 0.1275 0.424 0.3004 ## relSProt:eduHigh 0.3039 0.586 0.5186 ## relCath:eduHigh -0.5623 0.532 -1.0572 ## year1973:relSProt:eduMed -1.2710 0.667 -1.9069 ## year1974:relSProt:eduMed -1.3331 0.668 -1.9952 ## year1973:relCath:eduMed -0.8360 0.629 -1.3291 ## year1974:relCath:eduMed -0.8973 0.647 -1.3879 ## year1973:relSProt:eduHigh 0.9047 0.878 1.0309 ## year1974:relSProt:eduHigh -0.4321 0.805 -0.5369 ## year1973:relCath:eduHigh -0.2265 0.749 -0.3021 ## year1974:relCath:eduHigh -0.9896 0.766 -1.2915 ## ## Intercepts: ## Value Std. Error t value ## Neg|Mix -2.311 0.267 -8.647 ## Mix|Pos -0.761 0.261 -2.922 ## ## Residual Deviance: 4018.27 ## AIC: 4074.27 Compare the two models. anova(abort_mdl, abort_mdl_sat) ## Likelihood ratio tests of ordinal regression models ## ## Response: att ## Model Resid. df Resid. Dev Test Df LR stat. Pr(Chi) ## 1 year + rel + edu 3229 4059 ## 2 year * rel * edu 3209 4018 1 vs 2 20 41 0.0037 The likelihood ratio test indicates the main-effects model fits poorly in comparison to the saturated model (LR = 41.0, df = 20, p &lt; 0.01). From the table of coefficients,the effects of religion and education appear to be much more powerful than the year, so consider modeling an interaction between rel and edu. This is also what the stepwise AIC algorithm recommends. summary(abort_mdl_step) ## Call: ## MASS::polr(formula = att ~ year + rel + edu + rel:edu, data = abort, ## weights = cnt) ## ## Coefficients: ## Value Std. Error t value ## year1973 0.228 0.105 2.171 ## year1974 0.241 0.106 2.271 ## relSProt -0.450 0.213 -2.110 ## relCath -0.348 0.223 -1.556 ## eduMed 0.750 0.177 4.230 ## eduHigh 1.369 0.219 6.244 ## relSProt:eduMed 0.253 0.267 0.946 ## relCath:eduMed -0.389 0.260 -1.496 ## relSProt:eduHigh 0.385 0.336 1.146 ## relCath:eduHigh -0.944 0.307 -3.079 ## ## Intercepts: ## Value Std. Error t value ## Neg|Mix -2.26 0.17 -13.28 ## Mix|Pos -0.72 0.16 -4.49 ## ## Residual Deviance: 4040.44 ## AIC: 4064.44 Compare the model with the rel:edu interaction to the saturated model. anova(abort_mdl_step, abort_mdl_sat) ## Likelihood ratio tests of ordinal regression models ## ## Response: att ## Model Resid. df Resid. Dev Test Df LR stat. Pr(Chi) ## 1 year + rel + edu + rel:edu 3225 4040 ## 2 year * rel * edu 3209 4018 1 vs 2 16 22 0.14 Great, this time they are not significantly different (LR = 22.2, df = 16, p = 0.138). Interpret the results. Positive coefficients mean attitudes toward legalizing abortion are more positive relative to the reference year, 1972. The odds of supporting legalization in 1973 compared to 1972 were \\(exp(0.2281) = 1.26\\). The odds for 1974 vs 1972 were \\(exp(0.2410) = 1.27\\), so attitudes toward abortion became more positive from 1972 to 1973, but remained nearly unchanged from 1973 to 1974. Among Protestants (reference religious group), increasing education is associated with more positive attitudes toward abortion legalization. The odds of a Protestant with medium education vs low education supporting legalization are \\(exp(0.7504) = 2.12\\). Among Southern Protestants, odds are \\(exp(0.7504 + 0.2526) = 2.73\\). Therefore, the estimated effects of education for Southern Protestants are in the same direction as for Protestants but are somewhat larger. Note, however, that the interaction effect coefficient is not not significantly different from zero, so the effect of education among Protestants and Southern Protestants is not significantly different. Among Catholics, the medium vs low education odds are \\(exp(0.7504- 0.3892) = 1.44\\). And the high vs low education odds are \\(exp(1.3689 - 0.9442) = 1.53\\). Increasing education is still associated with more positive attitudes, but the effects are smaller than they are among Protestants and Southern Protestants. Example Summarization We used logistic regression to investigate whether groups with the Christian religion might moderate the effects of education on attitudes toward abortion legalization. For Protestants, higher education education was associated with higher, significant, increase of odds of a more positive attitude toward abortion legalization, b = 0.7504, SE = 0.1774, OR = 2.12, p &lt; .01. There was a significant interaction for Catholics at high levels of education, b = -0.9442, SE = 0.3066, p &lt; .01, relative to the Protestant reference group, but no significant interaction at medium education, and no interaction at all for Southern Protestants relative to the reference group. The figure above graphs the interaction, showing the change in the expected probability of positive attitude by education level for Protestant, Southern Protestant, and Catholic religious groups. Overall, the significant interaction for Catholics at high levels of education suggests that education has a different relationship to attitudes toward abortion depending on the individual’s religious group, but the difference between Protestant and Southern Protestant is minimal. "],
["poisson-regression.html", "6.4 Poisson Regression", " 6.4 Poisson Regression Poisson models count data, like “traffic tickets per day”, or “website hits per day”. The response is an expected rate or intensity. For count data, specify the generalized model, this time with family = poisson or family = quasipoisson. Recall that the probability of achieving a count \\(y\\) when the expected rate is \\(\\lambda\\) is distributed \\[P(Y = y|\\lambda) = \\frac{e^{-\\lambda} \\lambda^y}{y!}.\\] The poisson regression model is \\[\\lambda = \\exp(X \\beta).\\] You can solve this for \\(y\\) to get \\[y = X\\beta = \\ln(\\lambda).\\] That is, the model predicts the log of the response rate. For a sample of size n, the likelihood function is \\[L(\\beta; y, X) = \\prod_{i=1}^n \\frac{e^{-\\exp({X_i\\beta})}\\exp({X_i\\beta})^{y_i}}{y_i!}.\\] The log-likelihood is \\[l(\\beta) = \\sum_{i=1}^n (y_i X_i \\beta - \\sum_{i=1}^n\\exp(X_i\\beta) - \\sum_{i=1}^n\\log(y_i!).\\] Maximizing the log-likelihood has no closed-form solution, so the coefficient estimates are found through interatively reweighted least squares. Poisson processes assume the variance of the response variable equals its mean. “Equals” means the mean and variance are of a similar order of magnitude. If that assumption does not hold, use the quasi-poisson. Use Poisson regression for large datasets. If the predicted counts are much greater than zero (&gt;30), the linear regression will work fine. Whereas RMSE is not useful for logistic models, it is a good metric in Poisson. Dataset fire contains response variable injuries counting the number of injuries during the month and one explanatory variable, the month mo. fire &lt;- read_csv(file = &quot;C:/Users/mpfol/OneDrive/Documents/Data Science/Data/CivilInjury_0.csv&quot;) ## Parsed with column specification: ## cols( ## ID = col_double(), ## `Injury Date` = col_datetime(format = &quot;&quot;), ## `Total Injuries` = col_double() ## ) fire &lt;- fire %&gt;% mutate(mo = as.POSIXlt(`Injury Date`)$mon + 1) %&gt;% rename(dt = `Injury Date`, injuries = `Total Injuries`) str(fire) ## tibble [300 x 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ ID : num [1:300] 1 2 3 4 5 6 7 8 9 10 ... ## $ dt : POSIXct[1:300], format: &quot;2005-01-10&quot; &quot;2005-01-11&quot; ... ## $ injuries: num [1:300] 1 1 1 5 2 1 1 1 1 1 ... ## $ mo : num [1:300] 1 1 1 1 1 1 2 2 2 4 ... In a situation like this where there the relationship is bivariate, start with a visualization. ggplot(fire, aes(x = mo, y = injuries)) + geom_jitter() + geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;poisson&quot;)) + labs(title = &quot;Injuries by Month&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Fit a poisson regression in R using glm(formula, data, family = poisson). But first, check whether the mean and variance of injuries are the same magnitude? If not, then use family = quasipoisson. mean(fire$injuries) ## [1] 1.4 var(fire$injuries) ## [1] 1 They are of the same magnitude, so fit the regression with family = poisson. m2 &lt;- glm(injuries ~ mo, family = poisson, data = fire) summary(m2) ## ## Call: ## glm(formula = injuries ~ mo, family = poisson, data = fire) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.399 -0.347 -0.303 -0.250 4.318 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.2280 0.1048 2.18 0.03 * ## mo 0.0122 0.0140 0.87 0.38 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 139.87 on 299 degrees of freedom ## Residual deviance: 139.11 on 298 degrees of freedom ## AIC: 792.1 ## ## Number of Fisher Scoring iterations: 5 The predicted value \\(\\hat{y}\\) is the estimated log of the response variable, \\[\\hat{y} = X \\hat{\\beta} = \\ln (\\lambda).\\] Suppose mo is January (mo = ), then the log ofinjuries` is \\(\\hat{y} = 0.323787\\). Or, more intuitively, the expected count of injuries is \\(\\exp(0.323787) = 1.38\\) predict(m2, newdata = data.frame(mo=1)) ## 1 ## 0.24 predict(m2, newdata = data.frame(mo=1), type = &quot;response&quot;) ## 1 ## 1.3 Here is a plot of the predicted counts in red. augment(m2, type.predict = &quot;response&quot;) %&gt;% ggplot(aes(x = mo, y = injuries)) + geom_point() + geom_point(aes(y = .fitted), color = &quot;red&quot;) + scale_y_continuous(limits = c(0, NA)) + labs(x = &quot;Month&quot;, y = &quot;Injuries&quot;, title = &quot;Poisson Fitted Line Plot&quot;) Evaluate a logistic model fit with an analysis of deviance. (perf &lt;- glance(m2)) ## # A tibble: 1 x 7 ## null.deviance df.null logLik AIC BIC deviance df.residual ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 140. 299 -394. 792. 799. 139. 298 (pseudoR2 &lt;- 1 - perf$deviance / perf$null.deviance) ## [1] 0.0054 The deviance of the null model (no regressors) is 139.9. The deviance of the full model is 132.2. The psuedo-R2 is very low at .05. How about the RMSE? RMSE(pred = predict(m2, type = &quot;response&quot;), obs = fire$injuries) ## [1] 1 The average prediction error is about 0.99. That’s almost as much as the variance of injuries - i.e., just predicting the mean of injuries would be almost as good! Use the GainCurvePlot() function to plot the gain curve. augment(m2, type.predict = &quot;response&quot;) %&gt;% ggplot(aes(x = injuries, y = .fitted)) + geom_point() + geom_smooth(method =&quot;lm&quot;) + labs(x = &quot;Actual&quot;, y = &quot;Predicted&quot;, title = &quot;Poisson Fitted vs Actual&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; augment(m2) %&gt;% data.frame() %&gt;% GainCurvePlot(xvar = &quot;.fitted&quot;, truthVar = &quot;injuries&quot;, title = &quot;Poisson Model&quot;) It seems that mo was a poor predictor of injuries. "],
["multivariate-statistical-analysis.html", "Chapter 7 Multivariate Statistical Analysis", " Chapter 7 Multivariate Statistical Analysis These notes are structured from the PSU STAT 504 course. "],
["background.html", "7.1 Background", " 7.1 Background "],
["manova.html", "7.2 MANOVA", " 7.2 MANOVA "],
["repeated-measures.html", "7.3 Repeated Measures", " 7.3 Repeated Measures "],
["lda.html", "7.4 LDA", " 7.4 LDA Linear Discriminant Analysis (LDA) is a supervised machine learning classification (binary or multimonial) and dimension reduction method. LDA finds linear combinations of variables that best “discriminate” the response classes. One approach (Welch) to LDA assumes the predictor variables are continuous random variables normally distributed and with equal variance. You will typically scale the data to meet these conditions. For a response variable of \\(k\\) levels, LDA produces \\(k-1\\) discriminants using Bayes Theorem. \\[Pr[Y = C_l | X] = \\frac{P[Y = C_l] P[X | Y = C_l]}{\\sum_{l=1}^C Pr[Y = C_l] Pr[X | Y = C_l]}\\] The probability that \\(Y\\) equals class level \\(C_l\\) given the predictors \\(X\\) equals the prior probability of \\(Y\\) multiplied by the probability of observing \\(X\\) if \\(Y = C_l\\) divided by the sum of all priors and probabilities of \\(X\\) given the priors. The predicted value for any \\(X\\) is just the \\(C_l\\) with the maximimum probability. One way to calculate the probabilities is by assuming \\(X\\) has a multivariate normal distribution with means \\(\\mu_l\\) and common variance \\(\\Sigma\\). Then the linear discriminant function group \\(l\\) is \\[X&#39;\\Sigma^{-1}\\mu_l - 0.5 \\mu_l^{&#39;}\\Sigma^{-1}\\mu_l + \\log(Pr[Y = C_l])\\] The theoretical means and covariance matrix is estimated by the sample mean \\(\\mu = \\bar{x}_l\\) and covariance \\(\\Sigma = S\\), and the population predictors \\(X\\) are replaced with the sample predictors \\(u\\). Another approach (Fisher) to LDA is to find a linear combination of predictors that maximizes the between-group covariance matrix \\(B\\) relative to the within-group covariance matrix \\(W\\). \\[\\frac{b&#39;Bb}{b&#39;Wb}\\] The solution to this optimization problem is the eigenvector corresponding to the largest eigenvalue of \\(W^{-1}B\\). This vector is a linear discrminant. Solving for two-group setting gives the discriminant function \\(S^{-1}(\\bar{x}_1 - \\bar{x}_2)\\) where \\(S^{-1}\\) is the inverse of the covariance matrix of the data and \\(\\bar{x}_1\\) and \\(\\bar{x}_2\\) are the means of each predictor in response groups 1 and 2. In practice, a new sample, \\(u\\), is projected onto the discriminant function as \\(uS^{-1}(\\bar{x}_1 - \\bar{x}_2)\\), which returns a discriminant score. A new sample is then classified into group 1 if the sample is closer to the group 1 mean than the group 2 mean in the projection: \\[\\left| b&#39;(u - \\bar{x}_1) \\right| - \\left| b&#39;(u - \\bar{x}_2) \\right| &lt; 0\\] In general, the model requires \\(CP + P(P + 1)/2\\) parameters with \\(P\\) predictors and \\(C\\) classes. The value of the extra parameters in LDA models is that the between-predictor correlations are explicitly handled by the model. This should provide some advantage to LDA over logistic regression when there are substantial correlations, although both models will break down when the multicollinearity becomes extreme. Fisher’s formulation is intuitive, easy to solve mathematically, and, unlike Welch’s approach, involves no assumptions about the underlying distributions of the data. In practice, it is best to center and scale predictors and remove near-zero-variance predictors. If the matrix is still not invertible, use PLS or regularization. "],
["pca.html", "7.5 PCA", " 7.5 PCA "],
["factor-analysis.html", "7.6 Factor Analysis", " 7.6 Factor Analysis "],
["canonical-correlation.html", "7.7 Canonical Correlation", " 7.7 Canonical Correlation "],
["cluster-analysis.html", "7.8 Cluster Analysis", " 7.8 Cluster Analysis This section is based on PSU STAT 504 and PSU STAT 508. "],
["classification.html", "Chapter 8 Classification", " Chapter 8 Classification "],
["regularization.html", "Chapter 9 Regularization", " Chapter 9 Regularization "],
["decision-trees.html", "Chapter 10 Decision Trees", " Chapter 10 Decision Trees Decision trees, also known as classification and regression tree (CART) models, are tree-based methods for supervised machine learning. Simple classification trees and regression trees are easy to use and interpret, but are not competitive with the best machine learning methods. However, they form the foundation for bagged trees, random forests, and boosted trees models, which although less interpretable, are very accurate. CART models segment the predictor space into \\(K\\) non-overlapping terminal nodes (leaves), \\(A_1, A_2, \\dots, A_K\\). Each node is described by a set of rules which can be used to predict new responses. The predicted value \\(\\hat{y}\\) for each node is the mode (classification), or mean (regression). CART models define the nodes through a top-down greedy process called recursive binary splitting. The process is top-down because it begins at the top of the tree with all observations in a single region and successively splits the predictor space. It is greedy because at each splitting step, the best split is made at that particular step without consideration to subsequent splits. The best split is the predictor variable and cutpoint that minimizes a cost function. For a regression tree, the most common cost function is the sum of squared residuals, \\[RSS = \\sum_{k=1}^K\\sum_{i \\in A_k}{\\left(y_i - \\hat{y}_{A_k} \\right)^2}.\\] For a classification tree, the most common cost functions are the Gini index, \\[G = \\sum_{c=1}^C{\\hat{p}_{kc}(1 - \\hat{p}_{kc})},\\] or the entropy \\[D = - \\sum_{c=1}^C{\\hat{p}_{kc} \\log \\hat{p}_{kc}}\\] where \\(\\hat{p}_{kc}\\) is the proportion of training observations in node \\(k\\) node that are class \\(c\\). A completely pure node in a binary tree will have \\(\\hat{p} \\in [0, 1]\\) and \\(G = D = 0\\). A completely impure node in a binary tree will have \\(\\hat{p} = 0.5\\) and \\(G = 0.5^2 \\cdot 2 = 0.25\\) and \\(D = -(0.5 \\log(0.5)) \\cdot 2 = 0.69\\). CART repeats the splitting process for each of the child nodes until a stopping criterion is satisfied, usually when no node size surpasses a predefined maximum, or continued splitting does not improve the model significantly. CART may also impose a minimum number of observations in each node. The resulting tree likely over-fits the training data and therefore does not generalize well to test data, so CART prunes the tree, minimizing the cross-validated prediction error. Rather than cross-validating every possible subtree to find the one with minimum error, CART uses cost-complexity pruning. Cost-complexity is the tradeoff between error (cost) and tree size (complexity) where the tradeoff is quantified with cost-complexity parameter \\(c_p\\). In the equation below, the cost complexity of the tree \\(R_{c_p}(T)\\) is the sum of its risk (error) plus a “cost complexity” factor \\(c_p\\) multiple of the tree size \\(|T|\\). \\[R_{c_p}(T) = R(T) + c_p|T|\\] \\(c_p\\) can take on any value from \\([0..\\infty]\\), but it turns out there is an optimal tree for ranges of \\(c_p\\) values, so there are only a finite set of interesting values for \\(c_p\\) (James et al. 2013) (Therneau and Atkinson 2019) (Kuhn and Johnson 2016). A parametric algorithm identifies the interesting \\(c_p\\) values and their associated pruned trees, \\(T_{c_p}\\). CART uses cross-validation to determine which \\(c_p\\) is optimal. References "],
["classification-tree.html", "10.1 Classification Tree", " 10.1 Classification Tree A simple classification tree is rarely performed on its own; the bagged, random forest, and gradient boosting methods build on this logic. However, it is good to start here to build understanding. I’ll learn by example. Using the ISLR::OJ data set, I will predict which brand of orange juice, Citrus Hill (CH) or Minute Maid = (MM), customers Purchase using from the 17 feature variables. Load the libraries and data. library(ISLR) # OJ dataset library(rpart) # classification and regression trees library(caret) # modeling workflow library(rpart.plot) # better formatted plots than the ones in rpart library(plotROC) # ROC curves library(ROCR) library(tidyverse) library(skimr) # neat alternative to glance &amp; summary oj_dat &lt;- OJ #skim_with(numeric = list(p0 = NULL, p25 = NULL, p50 = NULL, p75 = NULL, # p100 = NULL, hist = NULL)) #skim(oj_dat) I’ll split oj_dat (n = 1,070) into oj_train (80%, n = 857) and oj_test (20%, n = 213). I’ll fit a simple decision tree with oj_train, then later a bagged tree, a random forest, and a gradient boosting tree. I’ll compare their predictive performance with oj_test. set.seed(12345) partition &lt;- createDataPartition(y = oj_dat$Purchase, p = 0.8, list = FALSE) oj_train &lt;- oj_dat[partition, ] oj_test &lt;- oj_dat[-partition, ] Function rpart::rpart() builds a full tree, minimizing the Gini index \\(G\\) by default (parms = list(split = \"gini\")), until the stopping criterion is satisfied. The default stopping criterion is only attempt a split if the current node as at least minsplit = 20 observations, only accept a split if each of the two resulting nodes have at least minbucket = round(minsplit/3) observations, and only accept a split if the resulting overall fit improves by cp = 0.01 (i.e., \\(\\Delta G &lt;= 0.01\\)). set.seed(123) oj_model_1 &lt;- rpart( formula = Purchase ~ ., data = oj_train, method = &quot;class&quot; # &quot;class&quot; for classification, &quot;anova&quot; for regression ) print(oj_model_1) ## n= 857 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 857 330 CH (0.610 0.390) ## 2) LoyalCH&gt;=0.48 537 94 CH (0.825 0.175) ## 4) LoyalCH&gt;=0.76 271 13 CH (0.952 0.048) * ## 5) LoyalCH&lt; 0.76 266 81 CH (0.695 0.305) ## 10) PriceDiff&gt;=-0.16 226 50 CH (0.779 0.221) * ## 11) PriceDiff&lt; -0.16 40 9 MM (0.225 0.775) * ## 3) LoyalCH&lt; 0.48 320 80 MM (0.250 0.750) ## 6) LoyalCH&gt;=0.28 146 58 MM (0.397 0.603) ## 12) SalePriceMM&gt;=2 71 31 CH (0.563 0.437) * ## 13) SalePriceMM&lt; 2 75 18 MM (0.240 0.760) * ## 7) LoyalCH&lt; 0.28 174 22 MM (0.126 0.874) * The output starts with the root node. The predicted class at the root is CH and this prediction produces 334 errors on the 857 observations for a success rate of 0.61026838 and an error rate of 0.38973162. The child nodes of node “x” are labeled 2x) and 2x+1), so the child nodes of 1) are 2) and 3), and the child nodes of 2) are 4) and 5). Terminal nodes are labeled with an asterisk (*). Surprisingly, only 3 of the 17 features were used the in full tree: LoyalCH (Customer brand loyalty for CH), PriceDiff (relative price of MM over CH), and SalePriceMM (absolute price of MM). The first split is at LoyalCH = 0.48285. Here is what the full (unpruned) tree looks like. rpart.plot(oj_model_1, yesno = TRUE) The boxes show the node classification (based on mode), the proportion of observations that are not CH, and the proportion of observations included in the node. rpart() not only grew the full tree, it identified the set of cost complexity parameters, and measured the model performance of each corresponding tree using cross-validation. printcp() displays the candidate \\(c_p\\) values. You can use this table to decide how to prune the tree. printcp(oj_model_1) ## ## Classification tree: ## rpart(formula = Purchase ~ ., data = oj_train, method = &quot;class&quot;) ## ## Variables actually used in tree construction: ## [1] LoyalCH PriceDiff SalePriceMM ## ## Root node error: 334/857 = 0 ## ## n= 857 ## ## CP nsplit rel error xerror xstd ## 1 0 0 1 1 0 ## 2 0 1 1 1 0 ## 3 0 3 0 0 0 ## 4 0 5 0 0 0 There are 4 \\(c_p\\) values in this model. The model with the smallest complexity parameter allows the most splits (nsplit). The highest complexity parameter corresponds to a tree with just a root node. rel error is the error rate relative to the root node. The root node absolute error is 0.38973162 (the proportion of MM), so its rel error is 0.38973162/0.38973162 = 1.0. That means the absolute error of the full tree (at CP = 0.01) is 0.42814 * 0.38973162 = 0.1669. You can verify that by calculating the error rate of the predicted values: data.frame(pred = predict(oj_model_1, newdata = oj_train, type = &quot;class&quot;)) %&gt;% mutate(obs = oj_train$Purchase, err = if_else(pred != obs, 1, 0)) %&gt;% summarize(mean_err = mean(err)) ## mean_err ## 1 0.17 Finishing the CP table tour, xerror is the relative cross-validated error rate and xstd is its standard error. If you want the lowest possible error, then prune to the tree with the smallest relative CV error (xerror) (\\(c_p\\) = 0.01, CV error = 0.18). If you want to balance predictive power with simplicity, prune to the smallest tree within 1 SE of the one with the smallest relative error. The CP table is not super-helpful for finding that tree. I’ll add a column to find it. oj_model_1$cptable %&gt;% data.frame() %&gt;% mutate(min_xerror_idx = which.min(oj_model_1$cptable[, &quot;xerror&quot;]), rownum = row_number(), xerror_cap = oj_model_1$cptable[min_xerror_idx, &quot;xerror&quot;] + oj_model_1$cptable[min_xerror_idx, &quot;xstd&quot;], eval = case_when(rownum == min_xerror_idx ~ &quot;min xerror&quot;, xerror &lt; xerror_cap ~ &quot;under cap&quot;, TRUE ~ &quot;&quot;)) %&gt;% select(-rownum, -min_xerror_idx) ## CP nsplit rel.error xerror xstd xerror_cap eval ## 1 0.479 0 1.00 1.00 0.043 0.5 ## 2 0.033 1 0.52 0.54 0.036 0.5 ## 3 0.013 3 0.46 0.47 0.034 0.5 under cap ## 4 0.010 5 0.43 0.46 0.034 0.5 min xerror The simplest tree using the 1-SE rule is $c_p = 0.01347305, CV error = 0.18). Fortunately, plotcp() presents a nice graphical representation of the relationship between xerror and cp. plotcp(oj_model_1, upper = &quot;splits&quot;) The dashed line is set at the minimum xerror + xstd. The top axis shows the number of splits in the tree. I’m not sure why the CP values are not the same as in the table (they are close, but not the same). The figure suggests I should prune to 5 or 3 splits. I see this curve never really hits a minimum - it is still decreasing at 5 splits. The default tuning parameter value cp = 0.01 may be too small, so I’ll set it to cp = 0.001 and start over. set.seed(123) oj_model_1b &lt;- rpart( formula = Purchase ~ ., data = oj_train, method = &quot;class&quot;, cp = 0.001 ) print(oj_model_1b) ## n= 857 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 857 330 CH (0.610 0.390) ## 2) LoyalCH&gt;=0.48 537 94 CH (0.825 0.175) ## 4) LoyalCH&gt;=0.76 271 13 CH (0.952 0.048) * ## 5) LoyalCH&lt; 0.76 266 81 CH (0.695 0.305) ## 10) PriceDiff&gt;=-0.16 226 50 CH (0.779 0.221) ## 20) ListPriceDiff&gt;=0.26 115 11 CH (0.904 0.096) * ## 21) ListPriceDiff&lt; 0.26 111 39 CH (0.649 0.351) ## 42) PriceMM&gt;=2.2 19 2 CH (0.895 0.105) * ## 43) PriceMM&lt; 2.2 92 37 CH (0.598 0.402) ## 86) DiscCH&gt;=0.12 7 0 CH (1.000 0.000) * ## 87) DiscCH&lt; 0.12 85 37 CH (0.565 0.435) ## 174) ListPriceDiff&gt;=0.22 45 15 CH (0.667 0.333) * ## 175) ListPriceDiff&lt; 0.22 40 18 MM (0.450 0.550) ## 350) LoyalCH&gt;=0.53 28 13 CH (0.536 0.464) ## 700) WeekofPurchase&lt; 2.7e+02 21 8 CH (0.619 0.381) * ## 701) WeekofPurchase&gt;=2.7e+02 7 2 MM (0.286 0.714) * ## 351) LoyalCH&lt; 0.53 12 3 MM (0.250 0.750) * ## 11) PriceDiff&lt; -0.16 40 9 MM (0.225 0.775) * ## 3) LoyalCH&lt; 0.48 320 80 MM (0.250 0.750) ## 6) LoyalCH&gt;=0.28 146 58 MM (0.397 0.603) ## 12) SalePriceMM&gt;=2 71 31 CH (0.563 0.437) ## 24) LoyalCH&lt; 0.3 7 0 CH (1.000 0.000) * ## 25) LoyalCH&gt;=0.3 64 31 CH (0.516 0.484) ## 50) WeekofPurchase&gt;=2.5e+02 52 22 CH (0.577 0.423) ## 100) PriceCH&lt; 1.9 35 11 CH (0.686 0.314) ## 200) StoreID&lt; 1.5 9 1 CH (0.889 0.111) * ## 201) StoreID&gt;=1.5 26 10 CH (0.615 0.385) ## 402) LoyalCH&lt; 0.41 17 4 CH (0.765 0.235) * ## 403) LoyalCH&gt;=0.41 9 3 MM (0.333 0.667) * ## 101) PriceCH&gt;=1.9 17 6 MM (0.353 0.647) * ## 51) WeekofPurchase&lt; 2.5e+02 12 3 MM (0.250 0.750) * ## 13) SalePriceMM&lt; 2 75 18 MM (0.240 0.760) ## 26) SpecialCH&gt;=0.5 14 6 CH (0.571 0.429) * ## 27) SpecialCH&lt; 0.5 61 10 MM (0.164 0.836) * ## 7) LoyalCH&lt; 0.28 174 22 MM (0.126 0.874) ## 14) LoyalCH&gt;=0.035 117 21 MM (0.179 0.821) ## 28) WeekofPurchase&lt; 2.7e+02 104 21 MM (0.202 0.798) ## 56) PriceCH&gt;=1.9 20 9 MM (0.450 0.550) ## 112) WeekofPurchase&gt;=2.5e+02 12 5 CH (0.583 0.417) * ## 113) WeekofPurchase&lt; 2.5e+02 8 2 MM (0.250 0.750) * ## 57) PriceCH&lt; 1.9 84 12 MM (0.143 0.857) * ## 29) WeekofPurchase&gt;=2.7e+02 13 0 MM (0.000 1.000) * ## 15) LoyalCH&lt; 0.035 57 1 MM (0.018 0.982) * This is a much larger tree. Did I find a cp value that produces a local min? plotcp(oj_model_1b, upper = &quot;splits&quot;) Yes, the min is at CP = 0.011 with 5 splits. The min + 1 SE is at CP = 0.021 with 3 splits. I’ll prune the tree to 3 splits. oj_model_1b_pruned &lt;- prune( oj_model_1b, cp = oj_model_1b$cptable[oj_model_1b$cptable[, 2] == 3, &quot;CP&quot;] ) rpart.plot(oj_model_1b_pruned, yesno = TRUE) The most “important” indicator of Purchase appears to be LoyalCH. From the rpart vignette (page 12), “An overall measure of variable importance is the sum of the goodness of split measures for each split for which it was the primary variable, plus goodness (adjusted agreement) for all splits in which it was a surrogate.” Surrogates refer to alternative features for a node to handle missing data. For each split, CART evaluates a variety of alternative “surrogate” splits to use when the feature value for the primary split is NA. Surrogate splits are splits that produce results similar to the original split. A variable’s importance is the sum of the improvement in the overall Gini (or RMSE) measure produced by the nodes in which it appears. Here is the variable importance for this model. oj_model_1b_pruned$variable.importance ## LoyalCH PriceDiff SalePriceMM StoreID WeekofPurchase ## 150.2 20.8 11.6 10.0 8.4 ## DiscMM PriceMM PctDiscMM PriceCH SalePriceCH ## 7.1 7.1 6.3 3.1 1.0 oj_model_1b_pruned$variable.importance %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;Feature&quot;) %&gt;% rename(Overall = &#39;.&#39;) %&gt;% ggplot(aes(x = fct_reorder(Feature, Overall), y = Overall)) + geom_pointrange(aes(ymin = 0, ymax = Overall), color = &quot;cadetblue&quot;, size = .3) + theme_minimal() + coord_flip() + labs(x = &quot;&quot;, y = &quot;&quot;, title = &quot;Variable Importance with Simple Classication&quot;) LoyalCH is by far the most important variable, as expected from its position at the top of the tree, and one level down. You can see how the surrogates appear in the model with the summary() function. summary(oj_model_1b_pruned) ## Call: ## rpart(formula = Purchase ~ ., data = oj_train, method = &quot;class&quot;, ## cp = 0.001) ## n= 857 ## ## CP nsplit rel error xerror xstd ## 1 0.479 0 1.00 1.00 0.043 ## 2 0.033 1 0.52 0.54 0.036 ## 3 0.013 3 0.46 0.47 0.034 ## ## Variable importance ## LoyalCH PriceDiff SalePriceMM StoreID WeekofPurchase ## 67 9 5 4 4 ## DiscMM PriceMM PctDiscMM PriceCH ## 3 3 3 1 ## ## Node number 1: 857 observations, complexity param=0.48 ## predicted class=CH expected loss=0.39 P(node) =1 ## class counts: 523 334 ## probabilities: 0.610 0.390 ## left son=2 (537 obs) right son=3 (320 obs) ## Primary splits: ## LoyalCH &lt; 0.48 to the right, improve=130, (0 missing) ## StoreID &lt; 3.5 to the right, improve= 40, (0 missing) ## PriceDiff &lt; 0.015 to the right, improve= 24, (0 missing) ## ListPriceDiff &lt; 0.26 to the right, improve= 23, (0 missing) ## SalePriceMM &lt; 1.8 to the right, improve= 20, (0 missing) ## Surrogate splits: ## StoreID &lt; 3.5 to the right, agree=0.65, adj=0.053, (0 split) ## PriceMM &lt; 1.9 to the right, agree=0.64, adj=0.031, (0 split) ## WeekofPurchase &lt; 230 to the right, agree=0.63, adj=0.016, (0 split) ## DiscMM &lt; 0.77 to the left, agree=0.63, adj=0.006, (0 split) ## SalePriceMM &lt; 1.4 to the right, agree=0.63, adj=0.006, (0 split) ## ## Node number 2: 537 observations, complexity param=0.033 ## predicted class=CH expected loss=0.18 P(node) =0.63 ## class counts: 443 94 ## probabilities: 0.825 0.175 ## left son=4 (271 obs) right son=5 (266 obs) ## Primary splits: ## LoyalCH &lt; 0.76 to the right, improve=18.0, (0 missing) ## PriceDiff &lt; 0.015 to the right, improve=15.0, (0 missing) ## SalePriceMM &lt; 1.8 to the right, improve=14.0, (0 missing) ## ListPriceDiff &lt; 0.26 to the right, improve=11.0, (0 missing) ## DiscMM &lt; 0.15 to the left, improve= 7.8, (0 missing) ## Surrogate splits: ## WeekofPurchase &lt; 260 to the right, agree=0.59, adj=0.18, (0 split) ## PriceCH &lt; 1.8 to the right, agree=0.59, adj=0.17, (0 split) ## StoreID &lt; 3.5 to the right, agree=0.59, adj=0.16, (0 split) ## PriceMM &lt; 2 to the right, agree=0.59, adj=0.16, (0 split) ## SalePriceMM &lt; 2 to the right, agree=0.59, adj=0.16, (0 split) ## ## Node number 3: 320 observations ## predicted class=MM expected loss=0.25 P(node) =0.37 ## class counts: 80 240 ## probabilities: 0.250 0.750 ## ## Node number 4: 271 observations ## predicted class=CH expected loss=0.048 P(node) =0.32 ## class counts: 258 13 ## probabilities: 0.952 0.048 ## ## Node number 5: 266 observations, complexity param=0.033 ## predicted class=CH expected loss=0.3 P(node) =0.31 ## class counts: 185 81 ## probabilities: 0.695 0.305 ## left son=10 (226 obs) right son=11 (40 obs) ## Primary splits: ## PriceDiff &lt; -0.16 to the right, improve=21, (0 missing) ## ListPriceDiff &lt; 0.24 to the right, improve=21, (0 missing) ## SalePriceMM &lt; 1.8 to the right, improve=17, (0 missing) ## DiscMM &lt; 0.15 to the left, improve=10, (0 missing) ## PctDiscMM &lt; 0.073 to the left, improve=10, (0 missing) ## Surrogate splits: ## SalePriceMM &lt; 1.6 to the right, agree=0.91, adj=0.38, (0 split) ## DiscMM &lt; 0.57 to the left, agree=0.90, adj=0.30, (0 split) ## PctDiscMM &lt; 0.26 to the left, agree=0.90, adj=0.30, (0 split) ## WeekofPurchase &lt; 270 to the left, agree=0.87, adj=0.15, (0 split) ## SalePriceCH &lt; 2.1 to the left, agree=0.86, adj=0.05, (0 split) ## ## Node number 10: 226 observations ## predicted class=CH expected loss=0.22 P(node) =0.26 ## class counts: 176 50 ## probabilities: 0.779 0.221 ## ## Node number 11: 40 observations ## predicted class=MM expected loss=0.22 P(node) =0.047 ## class counts: 9 31 ## probabilities: 0.225 0.775 The last step is to make predictions on the validation data set. For a classification tree, set argument type = \"class\". oj_model_1b_preds &lt;- predict(oj_model_1b_pruned, oj_test, type = &quot;class&quot;) I’ll evaluate the predictions and record the accuracy (correct classification percentage) for comparison to other models. Two ways to evaluate the model are the confusion matrix, and the ROC curve. 10.1.1 Confusion Matrix Print the confusion matrix with caret::confusionMatrix() to see how well does this model performs against the test data set. oj_model_1b_cm &lt;- confusionMatrix(data = oj_model_1b_preds, reference = oj_test$Purchase) oj_model_1b_cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 113 13 ## MM 17 70 ## ## Accuracy : 0.859 ## 95% CI : (0.805, 0.903) ## No Information Rate : 0.61 ## P-Value [Acc &gt; NIR] : 0.00000000000000126 ## ## Kappa : 0.706 ## ## Mcnemar&#39;s Test P-Value : 0.584 ## ## Sensitivity : 0.869 ## Specificity : 0.843 ## Pos Pred Value : 0.897 ## Neg Pred Value : 0.805 ## Prevalence : 0.610 ## Detection Rate : 0.531 ## Detection Prevalence : 0.592 ## Balanced Accuracy : 0.856 ## ## &#39;Positive&#39; Class : CH ## The confusion matrix is at the top. It also includes a lot of statistics. It’s worth getting familiar with the stats. The model accuracy and 95% CI are calculated from the binomial test. binom.test(x = 113 + 70, n = 213) ## ## Exact binomial test ## ## data: 113 + 70 and 213 ## number of successes = 183, number of trials = 213, p-value ## &lt;0.0000000000000002 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.81 0.90 ## sample estimates: ## probability of success ## 0.86 The “No Information Rate” (NIR) statistic is the class rate for the largest class. In this case CH is the largest class, so NIR = 130/213 = 0.6103. “P-Value [Acc &gt; NIR]” is the binomial test that the model accuracy is significantly better than the NIR (i.e., significantly better than just always guessing CH). binom.test(x = 113 + 70, n = 213, p = 130/213, alternative = &quot;greater&quot;) ## ## Exact binomial test ## ## data: 113 + 70 and 213 ## number of successes = 183, number of trials = 213, p-value = ## 0.000000000000001 ## alternative hypothesis: true probability of success is greater than 0.61 ## 95 percent confidence interval: ## 0.81 1.00 ## sample estimates: ## probability of success ## 0.86 The “Accuracy” statistic indicates the model predicts 0.8590 of the observations correctly. That’s good, but less impressive when you consider the prevalence of CH is 0.6103 - you could achieve 61% accuracy just by predicting CH every time. A measure that controls for the prevalence is Cohen’s kappa statistic. The kappa statistic is explained here. It compares the accuracy to the accuracy of a “random system”. It is defined as \\[\\kappa = \\frac{Acc - RA}{1-RA}\\] where \\[RA = \\frac{ActFalse \\times PredFalse + ActTrue \\times PredTrue}{Total \\times Total}\\] is the hypotheical probability of a chance agreement. ActFalse will be the number of “MM” (13 + 70 = 83) and actual true will be the number of “CH” (113 + 17 = 130). The predicted counts are table(oj_model_1b_preds) ## oj_model_1b_preds ## CH MM ## 126 87 So, \\(RA = (83*87 + 130*126) / 213^2 = 0.5202\\) and \\(\\kappa = (0.8592 - 0.5202)/(1 - 0.5202) = 0.7064\\). The kappa statistic varies from 0 to 1 where 0 means accurate predictions occur merely by chance, and 1 means the predictions are in perfect agreement with the observations. In this case, a kappa statistic of 0.7064 is “substantial”. See chart here. The other measures from the confusionMatrix() output are various proportions and you can remind yourself of their definitions in the documentation with ?confusionMatrix. Visuals are almost always helpful. Here is a plot of the confusion matrix. plot(oj_test$Purchase, oj_model_1b_preds, main = &quot;Simple Classification: Predicted vs. Actual&quot;, xlab = &quot;Actual&quot;, ylab = &quot;Predicted&quot;) By the way, how does the validation set accuracy () oj_model_1b_train_preds &lt;- predict(oj_model_1b_pruned, oj_train, type = &quot;class&quot;) oj_model_1b_train_cm &lt;- confusionMatrix(data = oj_model_1b_train_preds, reference = oj_train$Purchase) oj_model_1b_train_cm$overall ## Accuracy ## 0.822637106184364030880828977387864142656326 ## Kappa ## 0.632311348714850951502342013554880395531654 ## AccuracyLower ## 0.795383992748886825552290247287601232528687 ## AccuracyUpper ## 0.847649652472277748138651531917275860905647 ## AccuracyNull ## 0.610268378063010485945483196701388806104660 ## AccuracyPValue ## 0.000000000000000000000000000000000000000019 ## McnemarPValue ## 0.042583955841125085972631580943925655446947 The accuracy on the training data set was a little lower than on the test data set. I though it would be higher, not lower. 10.1.2 ROC Curve Another measure of accuracy is the ROC (receiver operating characteristics) curve (Fawcett 2005). The ROC curve is a plot of the true positive rate (TPR, sensitivity) versus the false positive rate (FPR, 1 - specificity) for a set of thresholds. By default, the threshold for predicting the default classification is 0.50, but it could be any threshold. The ROC curves varies the thresholds. (I’ll use the geom_roc geom from plotROC. data.frame(M = predict(oj_model_1b_pruned, oj_test, &quot;prob&quot;)[, 1], D = if_else(oj_test$Purchase == &quot;CH&quot;, 1, 0)) %&gt;% ggplot() + geom_roc(aes(m = M, d = D), hjust = -0.4, vjust = 1.5, linealpha = 0.6, labelsize = 3, n.cuts = 10) + geom_abline(intercept = 0, slope = 1, linetype = 2) + coord_equal() + theme_minimal() + labs(x = &quot;FPR&quot;, y = &quot;TPR&quot;, title = &quot;Model 1b ROC Curve&quot;, subtitle = &quot;Pruned model using rpart&quot;, caption = &quot;Data: ISLM OJ data set.&quot;) You can also use prediction() and plot.prediction() from the ROCR package. pred &lt;- prediction(predict(oj_model_1b_pruned, newdata = oj_test, type = &quot;prob&quot;)[, 2], oj_test$Purchase) plot(performance(pred, &quot;tpr&quot;, &quot;fpr&quot;)) abline(0, 1, lty = 2) Hmm, not quite the same… A few points on the ROC space are helpful for understanding how to use it. The lower left point (0, 0) is the result of always predicting “negative” or in this case “MM” if “CH” is taken as the default class. Sure, your false positive rate is zero, but since you never predict a positive, your true positive rate is also zero. The upper right point (1, 1) is the results of always predicting “positive” (or “CH” here). You catch all the positives, but you miss all the negatives. The upper left point (0, 1) is the result of perfect accuracy. You catch all the positives and all the negatives. The lower right point (1, 0) is the result of perfect imbecility. You made the exact wrong prediction every time. The 45 degree diagonal is the result of randomly guessing positive (CH) X percent of the time. If you guess positive 90% of the time and the prevalence is 50%, your TPR will be 90% and your FPR will also be 90%, etc. From the last bullet, it is evident that any point below and to the right of the 45 degree diagonal represents an instance where the model would have been better off just predicting entirely one way or the other. The goal is for all nodes to bunch up in the upper left. Points to the left of the diagonal with a low TPR can be thought of as “conservative” predicters - they only make positive (CH) predictions with strong evidence. Points to the left of the diagnonal with a high TPR can be thought of as “liberal” predicters - they make positive (CH) predictions with weak evidence. 10.1.3 Caret Approach I can also fit the model with caret::train(). There are two ways to tune hyperparameters in train(): set the number of tuning parameter values to consider by setting tuneLength, or set particular values to consider for each parameter by defining a tuneGrid. I’ll build the model using 10-fold cross-validation to optimize the hyperparameter CP. If you don’t have any idea what the tuning parameter ought to look like, use tuneLength to get close, then fine-tune with tuneGrid. That’s what I’ll do. I’ll create a training control object that I can re-use in other model builds. oj_trControl = trainControl( method = &quot;cv&quot;, # k-fold cross validation number = 10, # 10 folds savePredictions = &quot;final&quot;, # save predictions for the optimal tuning parameter classProbs = TRUE # return class probabilities in addition to predicted values # summaryFunction = twoClassSummary # computes sensitivity, specificity and the area under the ROC curve. ) Now fit the model. set.seed(1234) oj_model_2 = train( Purchase ~ ., data = oj_train, method = &quot;rpart&quot;, tuneLength = 5, metric = &quot;Accuracy&quot;, trControl = oj_trControl ) caret built a full tree using rpart’s default parameters: gini splitting index, at least 20 observations in a node in order to consider splitting it, and at least 6 observations in each node. Caret then calculated the accuracy for each candidate value of \\(\\alpha\\). Here is the results. print(oj_model_2) ## CART ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... ## Resampling results across tuning parameters: ## ## cp Accuracy Kappa ## 0.006 0.81 0.59 ## 0.009 0.81 0.59 ## 0.013 0.81 0.59 ## 0.033 0.78 0.54 ## 0.479 0.66 0.18 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.009. The second cp (0.008982036) produced the highest accuracy. I can drill into the best value of cp using a tuning grid. I’ll try that now. set.seed(1234) oj_model_3 = train( Purchase ~ ., data = oj_train, method = &quot;rpart&quot;, tuneGrid = expand.grid(cp = seq(from = 0.001, to = 0.010, length = 11)), metric=&#39;Accuracy&#39;, trControl = oj_trControl ) print(oj_model_3) ## CART ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... ## Resampling results across tuning parameters: ## ## cp Accuracy Kappa ## 0.0010 0.80 0.58 ## 0.0019 0.80 0.58 ## 0.0028 0.80 0.58 ## 0.0037 0.81 0.60 ## 0.0046 0.80 0.59 ## 0.0055 0.81 0.59 ## 0.0064 0.81 0.59 ## 0.0073 0.81 0.60 ## 0.0082 0.81 0.60 ## 0.0091 0.81 0.59 ## 0.0100 0.81 0.60 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.0082. The beset model is at cp = 0.009. Here are the rules in the final model. oj_model_3$finalModel ## n= 857 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 857 330 CH (0.610 0.390) ## 2) LoyalCH&gt;=0.48 537 94 CH (0.825 0.175) ## 4) LoyalCH&gt;=0.76 271 13 CH (0.952 0.048) * ## 5) LoyalCH&lt; 0.76 266 81 CH (0.695 0.305) ## 10) PriceDiff&gt;=-0.16 226 50 CH (0.779 0.221) * ## 11) PriceDiff&lt; -0.16 40 9 MM (0.225 0.775) * ## 3) LoyalCH&lt; 0.48 320 80 MM (0.250 0.750) ## 6) LoyalCH&gt;=0.28 146 58 MM (0.397 0.603) ## 12) SalePriceMM&gt;=2 71 31 CH (0.563 0.437) ## 24) LoyalCH&lt; 0.3 7 0 CH (1.000 0.000) * ## 25) LoyalCH&gt;=0.3 64 31 CH (0.516 0.484) ## 50) WeekofPurchase&gt;=2.5e+02 52 22 CH (0.577 0.423) ## 100) PriceCH&lt; 1.9 35 11 CH (0.686 0.314) * ## 101) PriceCH&gt;=1.9 17 6 MM (0.353 0.647) * ## 51) WeekofPurchase&lt; 2.5e+02 12 3 MM (0.250 0.750) * ## 13) SalePriceMM&lt; 2 75 18 MM (0.240 0.760) * ## 7) LoyalCH&lt; 0.28 174 22 MM (0.126 0.874) * Here is the tree. rpart.plot(oj_model_3$finalModel) Here is the ROC curve. library(plotROC) ggplot(oj_model_3$pred) + geom_roc( aes( m = MM, d = factor(obs, levels = c(&quot;CH&quot;, &quot;MM&quot;)) ), hjust = -0.4, vjust = 1.5 ) + coord_equal() ## Warning in verify_d(data$d): D not labeled 0/1, assuming CH = 0 and MM = 1! Here are the cross-validated Accuracy for each candidate cp value. plot(oj_model_3) Evaluate the model by making predictions with the test data set. oj_model_3_preds &lt;- predict(oj_model_3, oj_test, type = &quot;raw&quot;) The confusion matrix shows the true positives and true negatives. oj_model_3_cm &lt;- confusionMatrix( data = oj_model_3_preds, reference = oj_test$Purchase ) oj_model_3_cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 115 18 ## MM 15 65 ## ## Accuracy : 0.845 ## 95% CI : (0.789, 0.891) ## No Information Rate : 0.61 ## P-Value [Acc &gt; NIR] : 0.0000000000000631 ## ## Kappa : 0.672 ## ## Mcnemar&#39;s Test P-Value : 0.728 ## ## Sensitivity : 0.885 ## Specificity : 0.783 ## Pos Pred Value : 0.865 ## Neg Pred Value : 0.812 ## Prevalence : 0.610 ## Detection Rate : 0.540 ## Detection Prevalence : 0.624 ## Balanced Accuracy : 0.834 ## ## &#39;Positive&#39; Class : CH ## The accuracy metric is the slightly worse than in my previous model. Here is a graphical representation of the confusion matrix. plot(oj_test$Purchase, oj_model_3_preds, main = &quot;Simple Classification: Predicted vs. Actual&quot;, xlab = &quot;Actual&quot;, ylab = &quot;Predicted&quot;) Finally, here is the variable importance plot. plot(varImp(oj_model_3), main=&quot;Variable Importance with Simple Classication&quot;) Looks like the manual effort faired best. Here is a summary the accuracy rates of the three models. rbind(data.frame(model = &quot;Manual Class&quot;, Acc = round(oj_model_1b_cm$overall[&quot;Accuracy&quot;], 5)), data.frame(model = &quot;Caret w/tuneGrid&quot;, Acc = round(oj_model_3_cm$overall[&quot;Accuracy&quot;], 5)) ) ## model Acc ## Accuracy Manual Class 0.86 ## Accuracy1 Caret w/tuneGrid 0.85 References "],
["regression-trees.html", "10.2 Regression Trees", " 10.2 Regression Trees A simple regression tree is built in a manner similar to a simple classificatioon tree, and like the simple classification tree, it is rarely invoked on its own; the bagged, random forest, and gradient boosting methods build on this logic. I’ll learn by example again. Using the ISLR::Carseats data set, I will predict Sales using from the 10 feature variables. Load the data. carseats_dat &lt;- Carseats #skim_with(numeric = list(p0 = NULL, p25 = NULL, p50 = NULL, p75 = NULL, # p100 = NULL, hist = NULL)) #skim(carseats_dat) I’ll split careseats_dat (n = 400) into carseats_train (80%, n = 321) and carseats_test (20%, n = 79). I’ll fit a simple decision tree with carseats_train, then later a bagged tree, a random forest, and a gradient boosting tree. I’ll compare their predictive performance with carseats_test. set.seed(12345) partition &lt;- createDataPartition(y = carseats_dat$Sales, p = 0.8, list = FALSE) carseats_train &lt;- carseats_dat[partition, ] carseats_test &lt;- carseats_dat[-partition, ] The first step is to build a full tree, then perform k-fold cross-validation to help select the optimal cost complexity (cp). The only difference here is the rpart() parameter method = \"anova\" to produce a regression tree. set.seed(1234) carseats_model_1 &lt;- rpart( formula = Sales ~ ., data = carseats_train, method = &quot;anova&quot;, xval = 10, model = TRUE # to plot splits with factor variables. ) print(carseats_model_1) ## n= 321 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 321 2600 7.5 ## 2) ShelveLoc=Bad,Medium 251 1500 6.8 ## 4) Price&gt;=1.1e+02 168 720 6.0 ## 8) ShelveLoc=Bad 50 170 4.7 ## 16) Population&lt; 2e+02 20 48 3.6 * ## 17) Population&gt;=2e+02 30 81 5.4 * ## 9) ShelveLoc=Medium 118 430 6.5 ## 18) Advertising&lt; 12 88 290 6.1 ## 36) CompPrice&lt; 1.4e+02 69 190 5.8 ## 72) Price&gt;=1.3e+02 16 51 4.5 * ## 73) Price&lt; 1.3e+02 53 110 6.2 * ## 37) CompPrice&gt;=1.4e+02 19 58 7.4 * ## 19) Advertising&gt;=12 30 83 7.8 * ## 5) Price&lt; 1.1e+02 83 440 8.4 ## 10) Age&gt;=64 32 150 6.9 ## 20) Price&gt;=85 25 67 6.2 ## 40) ShelveLoc=Bad 9 18 4.8 * ## 41) ShelveLoc=Medium 16 21 6.9 * ## 21) Price&lt; 85 7 20 9.6 * ## 11) Age&lt; 64 51 180 9.3 ## 22) Income&lt; 58 12 28 7.7 * ## 23) Income&gt;=58 39 120 9.7 ## 46) Age&gt;=50 14 21 8.5 * ## 47) Age&lt; 50 25 60 10.0 * ## 3) ShelveLoc=Good 70 420 10.0 ## 6) Price&gt;=1.1e+02 49 240 9.4 ## 12) Advertising&lt; 14 41 160 8.9 ## 24) Age&gt;=61 17 53 7.8 * ## 25) Age&lt; 61 24 69 9.8 * ## 13) Advertising&gt;=14 8 13 12.0 * ## 7) Price&lt; 1.1e+02 21 61 12.0 * The output starts with the root node. The predicted Sales at the root is the mean Sales for the training data set, 7.535950 (values are $000s). The deviance at the root is the SSE, 2567.768. The child nodes of node “x” are labeled 2x) and 2x+1), so the child nodes of 1) are 2) and 3), and the child nodes of 2) are 4) and 5). Terminal nodes are labeled with an asterisk (*). The first split is at ShelveLoc = [Bad, Medium] vs Good. Here is what the full (unpruned) tree looks like. rpart.plot(carseats_model_1, yesno = TRUE) The boxes show the node predicted value (mean) and the proportion of observations that are in the node (or child nodes). rpart() not only grew the full tree, it also used cross-validation to test the performance of the possible complexity hyperparameters. printcp() displays the candidate cp values. You can use this table to decide how to prune the tree. printcp(carseats_model_1) ## ## Regression tree: ## rpart(formula = Sales ~ ., data = carseats_train, method = &quot;anova&quot;, ## model = TRUE, xval = 10) ## ## Variables actually used in tree construction: ## [1] Advertising Age CompPrice Income Population Price ## [7] ShelveLoc ## ## Root node error: 2568/321 = 8 ## ## n= 321 ## ## CP nsplit rel error xerror xstd ## 1 0 0 1 1 0 ## 2 0 1 1 1 0 ## 3 0 2 1 1 0 ## 4 0 3 1 1 0 ## 5 0 4 1 1 0 ## 6 0 5 0 1 0 ## 7 0 6 0 1 0 ## 8 0 7 0 1 0 ## 9 0 8 0 1 0 ## 10 0 9 0 1 0 ## 11 0 10 0 1 0 ## 12 0 11 0 1 0 ## 13 0 12 0 1 0 ## 14 0 13 0 1 0 ## 15 0 14 0 1 0 ## 16 0 15 0 1 0 There are 16 possible cp values in this model. The model with the smallest complexity parameter allows the most splits (nsplit). The highest complexity parameter corresponds to a tree with just a root node. rel error is the SSE relative to the root node. The root node SSE is 2567.76800, so its rel error is 2567.76800/2567.76800 = 1.0. That means the absolute error of the full tree (at CP = 0.01) is 0.30963 * 2567.76800 = 795.058. You can verify that by calculating the SSE of the model predicted values: data.frame(pred = predict(carseats_model_1, newdata = carseats_train)) %&gt;% mutate(obs = carseats_train$Sales, sq_err = (obs - pred)^2) %&gt;% summarize(sse = sum(sq_err)) ## sse ## 1 795 Finishing the CP table tour, xerror is the cross-validated SSE and xstd is its standard error. If you want the lowest possible error, then prune to the tree with the smallest relative SSE (xerror). If you want to balance predictive power with simplicity, prune to the smallest tree within 1 SE of the one with the smallest relative SSE. The CP table is not super-helpful for finding that tree. I’ll add a column to find it. carseats_model_1$cptable %&gt;% data.frame() %&gt;% mutate(min_xerror_idx = which.min(carseats_model_1$cptable[, &quot;xerror&quot;]), rownum = row_number(), xerror_cap = carseats_model_1$cptable[min_xerror_idx, &quot;xerror&quot;] + carseats_model_1$cptable[min_xerror_idx, &quot;xstd&quot;], eval = case_when(rownum == min_xerror_idx ~ &quot;min xerror&quot;, xerror &lt; xerror_cap ~ &quot;under cap&quot;, TRUE ~ &quot;&quot;)) %&gt;% select(-rownum, -min_xerror_idx) ## CP nsplit rel.error xerror xstd xerror_cap eval ## 1 0.263 0 1.00 1.01 0.077 0.59 ## 2 0.121 1 0.74 0.75 0.059 0.59 ## 3 0.046 2 0.62 0.65 0.051 0.59 ## 4 0.045 3 0.57 0.67 0.052 0.59 ## 5 0.042 4 0.52 0.66 0.051 0.59 ## 6 0.026 5 0.48 0.62 0.049 0.59 ## 7 0.026 6 0.46 0.62 0.048 0.59 ## 8 0.024 7 0.43 0.62 0.048 0.59 ## 9 0.015 8 0.41 0.58 0.042 0.59 under cap ## 10 0.015 9 0.39 0.56 0.041 0.59 under cap ## 11 0.015 10 0.38 0.56 0.041 0.59 under cap ## 12 0.014 11 0.36 0.56 0.041 0.59 under cap ## 13 0.014 12 0.35 0.56 0.038 0.59 min xerror ## 14 0.014 13 0.33 0.56 0.038 0.59 under cap ## 15 0.011 14 0.32 0.57 0.039 0.59 under cap ## 16 0.010 15 0.31 0.57 0.038 0.59 under cap Okay, so the simplest tree is the one with CP = 0.01544139 (8 splits). Fortunately, plotcp() presents a nice graphical representation of the relationship between xerror and cp. plotcp(carseats_model_1, upper = &quot;splits&quot;) The dashed line is set at the minimum xerror + xstd. The top axis shows the number of splits in the tree. I’m not sure why the CP values are not the same as in the table (they are close, but not the same). The smallest relative error is at 0.01, but the maximum CP below the dashed line (one standard deviation above the mimimum error) is at CP = .019 (8 splits). Use the prune() function to prune the tree by specifying the associated cost-complexity cp. carseats_model_1_pruned &lt;- prune( carseats_model_1, cp = carseats_model_1$cptable[carseats_model_1$cptable[, 2] == 8, &quot;CP&quot;] ) rpart.plot(carseats_model_1_pruned, yesno = TRUE) The most “important” indicator of Sales is ShelveLoc. Here are the importance values from the model. carseats_model_1_pruned$variable.importance %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;Feature&quot;) %&gt;% rename(Overall = &#39;.&#39;) %&gt;% ggplot(aes(x = fct_reorder(Feature, Overall), y = Overall)) + geom_pointrange(aes(ymin = 0, ymax = Overall), color = &quot;cadetblue&quot;, size = .3) + theme_minimal() + coord_flip() + labs(x = &quot;&quot;, y = &quot;&quot;, title = &quot;Variable Importance with Simple Regression&quot;) The most important indicator of Sales is ShelveLoc, then Price, then Age, all of which appear in the final model. CompPrice was also important. The last step is to make predictions on the validation data set. The root mean squared error (\\(RMSE = \\sqrt{(1/2) \\sum{(actual - pred)^2}})\\) and mean absolute error (\\(MAE = (1/n) \\sum{|actual - pred|}\\)) are the two most common measures of predictive accuracy. The key difference is that RMSE punishes large errors more harshly. For a regression tree, set argument type = \"vector\" (or do not specify at all). carseats_model_1_preds &lt;- predict( carseats_model_1_pruned, carseats_test, type = &quot;vector&quot; ) carseats_model_1_pruned_rmse &lt;- RMSE( pred = carseats_model_1_preds, obs = carseats_test$Sales ) carseats_model_1_pruned_rmse ## [1] 2.4 The pruning process leads to an average prediction error of 2.39 in the test data set. Not too bad considering the standard deviation of Sales is 2.8. Here is a predicted vs actual plot. plot(carseats_test$Sales, carseats_model_1_preds, main = &quot;Simple Regression: Predicted vs. Actual&quot;, xlab = &quot;Actual&quot;, ylab = &quot;Predicted&quot;) abline(0, 1) The 6 possible predicted values do a decent job of binning the observations. 10.2.1 Caret Approach I can also fit the model with caret::train(), specifying method = \"rpart\". I’ll build the model using 10-fold cross-validation to optimize the hyperparameter CP. carseats_trControl = trainControl( method = &quot;cv&quot;, # k-fold cross validation number = 10, # 10 folds savePredictions = &quot;final&quot; # save predictions for the optimal tuning parameter ) I’ll let the model look for the best CP tuning parameter with tuneLength to get close, then fine-tune with tuneGrid. set.seed(1234) carseats_model_2 = train( Sales ~ ., data = carseats_train, method = &quot;rpart&quot;, # for classification tree tuneLength = 5, # choose up to 5 combinations of tuning parameters (cp) metric = &quot;RMSE&quot;, # evaluate hyperparamter combinations with RMSE trControl = carseats_trControl ) ## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : ## There were missing values in resampled performance measures. print(carseats_model_2) ## CART ## ## 321 samples ## 10 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... ## Resampling results across tuning parameters: ## ## cp RMSE Rsquared MAE ## 0.042 2.2 0.41 1.8 ## 0.045 2.2 0.38 1.8 ## 0.046 2.3 0.37 1.8 ## 0.121 2.4 0.29 1.9 ## 0.263 2.7 0.19 2.2 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was cp = 0.042. The first cp (0.04167149) produced the smallest RMSE. I can drill into the best value of cp using a tuning grid. I’ll try that now. myGrid &lt;- expand.grid(cp = seq(from = 0, to = 0.1, by = 0.01)) carseats_model_3 = train( Sales ~ ., data = carseats_train, method = &quot;rpart&quot;, # for classification tree tuneGrid = myGrid, # choose up to 5 combinations of tuning parameters (cp) metric = &quot;RMSE&quot;, # evaluate hyperparamter combinations with RMSE trControl = carseats_trControl ) print(carseats_model_3) ## CART ## ## 321 samples ## 10 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 289, 288, 289, ... ## Resampling results across tuning parameters: ## ## cp RMSE Rsquared MAE ## 0.00 2.1 0.46 1.7 ## 0.01 2.2 0.43 1.8 ## 0.02 2.2 0.39 1.8 ## 0.03 2.2 0.41 1.8 ## 0.04 2.3 0.37 1.8 ## 0.05 2.3 0.34 1.8 ## 0.06 2.2 0.37 1.8 ## 0.07 2.3 0.37 1.8 ## 0.08 2.3 0.37 1.8 ## 0.09 2.3 0.37 1.8 ## 0.10 2.3 0.37 1.8 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was cp = 0. It looks like the best performing tree is the unpruned one. plot(carseats_model_3) Lets’s see the final model. rpart.plot(carseats_model_3$finalModel) What were the most important variables? plot(varImp(carseats_model_3), main=&quot;Variable Importance with Simple Regression&quot;) Evaluate the model by making predictions with the test data set. carseats_model_3_preds &lt;- predict(carseats_model_3, carseats_test, type = &quot;raw&quot;) data.frame(Actual = carseats_test$Sales, Predicted = carseats_model_3_preds) %&gt;% ggplot(aes(x = Actual, y = Predicted)) + geom_point() + geom_smooth() + geom_abline(slope = 1, intercept = 0) + scale_y_continuous(limits = c(0, 15)) + labs(title = &quot;Simple Regression: Predicted vs. Actual&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Looks like the model over-estimates at the low end and undestimates at the high end. Calculate the test data set RMSE. carseats_model_3_pruned_rmse &lt;- RMSE( pred = carseats_model_3_preds, obs = carseats_test$Sales ) carseats_model_3_pruned_rmse ## [1] 2.3 Caret faired better in this model. Here is a summary the RMSE values of the two models. rbind(data.frame(model = &quot;Manual ANOVA&quot;, RMSE = round(carseats_model_1_pruned_rmse, 5)), data.frame(model = &quot;Caret&quot;, RMSE = round(carseats_model_3_pruned_rmse, 5)) ) ## model RMSE ## 1 Manual ANOVA 2.4 ## 2 Caret 2.3 "],
["bagging.html", "10.3 Bagging", " 10.3 Bagging Bootstrap aggregation, or bagging, is a general-purpose procedure for reducing the variance of a statistical learning method. The algorithm constructs B regression trees using B bootstrapped training sets, and averages the resulting predictions. These trees are grown deep, and are not pruned. Hence each individual tree has high variance, but low bias. Averaging these B trees reduces the variance. For classification trees, bagging takes the “majority vote” for the prediction. Use a value of B sufficiently large that the error has settled down. To test the model accuracy, the out-of-bag observations are predicted from the models that do not use them. If B/3 of observations are in-bag, there are B/3 predictions per observation. These predictions are averaged for the test prediction. Again, for classification trees, a majority vote is taken. The downside to bagging is that it improves accuracy at the expense of interpretability. There is no longer a single tree to interpret, so it is no longer clear which variables are more important than others. Bagged trees are a special case of random forests, so see the next section for an example. "],
["random-forests.html", "10.4 Random Forests", " 10.4 Random Forests Random forests improve bagged trees by way of a small tweak that de-correlates the trees. As in bagging, the algorithm builds a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of mtry predictors is chosen as split candidates from the full set of p predictors. A fresh sample of mtry predictors is taken at each split. Typically \\(mtry \\sim \\sqrt{b}\\). Bagged trees are thus a special case of random forests where mtry = p. 10.4.0.1 Bagging Classification Example Again using the OJ data set to predict Purchase, this time I’ll use the bagging method by specifying method = \"treebag\". I’ll use tuneLength = 5 and not worry about tuneGrid anymore. Caret has no hyperparameters to tune with this model. oj.bag = train(Purchase ~ ., data = oj_train, method = &quot;treebag&quot;, # for bagging tuneLength = 5, # choose up to 5 combinations of tuning parameters metric = &quot;ROC&quot;, # evaluate hyperparamter combinations with ROC trControl = trainControl( method = &quot;cv&quot;, # k-fold cross validation number = 10, # k=10 folds savePredictions = &quot;final&quot;, # save predictions for the optimal tuning parameters classProbs = TRUE, # return class probabilities in addition to predicted values summaryFunction = twoClassSummary # for binary response variable ) ) oj.bag ## Bagged CART ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 771, 772, 771, 771, 771, 772, ... ## Resampling results: ## ## ROC Sens Spec ## 0.85 0.82 0.72 #plot(oj.bag$) oj.pred &lt;- predict(oj.bag, oj_test, type = &quot;raw&quot;) plot(oj_test$Purchase, oj.pred, main = &quot;Bagging Classification: Predicted vs. Actual&quot;, xlab = &quot;Actual&quot;, ylab = &quot;Predicted&quot;) (oj.conf &lt;- confusionMatrix(data = oj.pred, reference = oj_test$Purchase)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 110 17 ## MM 20 66 ## ## Accuracy : 0.826 ## 95% CI : (0.769, 0.875) ## No Information Rate : 0.61 ## P-Value [Acc &gt; NIR] : 0.00000000000712 ## ## Kappa : 0.637 ## ## Mcnemar&#39;s Test P-Value : 0.742 ## ## Sensitivity : 0.846 ## Specificity : 0.795 ## Pos Pred Value : 0.866 ## Neg Pred Value : 0.767 ## Prevalence : 0.610 ## Detection Rate : 0.516 ## Detection Prevalence : 0.596 ## Balanced Accuracy : 0.821 ## ## &#39;Positive&#39; Class : CH ## oj.bag.acc &lt;- as.numeric(oj.conf$overall[1]) rm(oj.pred) rm(oj.conf) #plot(oj.bag$, oj.bag$finalModel$y) plot(varImp(oj.bag), main=&quot;Variable Importance with Simple Classication&quot;) 10.4.0.2 Random Forest Classification Example Now I’ll try it with the random forest method by specifying method = \"ranger\". I’ll stick with tuneLength = 5. Caret tunes three hyperparameters: mtry: number of randomly selected predictors. Default is sqrt(p). splitrule: splitting rule. For classification, options are “gini” (default) and “extratrees”. min.node.size: minimal node size. Default is 1 for classification. oj.frst = train(Purchase ~ ., data = oj_train, method = &quot;ranger&quot;, # for random forest tuneLength = 5, # choose up to 5 combinations of tuning parameters metric = &quot;ROC&quot;, # evaluate hyperparamter combinations with ROC trControl = trainControl( method = &quot;cv&quot;, # k-fold cross validation number = 10, # 10 folds savePredictions = &quot;final&quot;, # save predictions for the optimal tuning parameter1 classProbs = TRUE, # return class probabilities in addition to predicted values summaryFunction = twoClassSummary # for binary response variable ) ) oj.frst ## Random Forest ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 772, 771, 772, 770, 772, 772, ... ## Resampling results across tuning parameters: ## ## mtry splitrule ROC Sens Spec ## 2 gini 0.86 0.87 0.69 ## 2 extratrees 0.85 0.88 0.63 ## 5 gini 0.87 0.85 0.72 ## 5 extratrees 0.86 0.86 0.69 ## 9 gini 0.87 0.84 0.73 ## 9 extratrees 0.87 0.85 0.69 ## 13 gini 0.86 0.83 0.74 ## 13 extratrees 0.87 0.83 0.71 ## 17 gini 0.86 0.81 0.74 ## 17 extratrees 0.86 0.83 0.71 ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 1 ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were mtry = 9, splitrule = gini ## and min.node.size = 1. plot(oj.frst) oj.pred &lt;- predict(oj.frst, oj_test, type = &quot;raw&quot;) plot(oj_test$Purchase, oj.pred, main = &quot;Random Forest Classification: Predicted vs. Actual&quot;, xlab = &quot;Actual&quot;, ylab = &quot;Predicted&quot;) (oj.conf &lt;- confusionMatrix(data = oj.pred, reference = oj_test$Purchase)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 109 16 ## MM 21 67 ## ## Accuracy : 0.826 ## 95% CI : (0.769, 0.875) ## No Information Rate : 0.61 ## P-Value [Acc &gt; NIR] : 0.00000000000712 ## ## Kappa : 0.639 ## ## Mcnemar&#39;s Test P-Value : 0.511 ## ## Sensitivity : 0.838 ## Specificity : 0.807 ## Pos Pred Value : 0.872 ## Neg Pred Value : 0.761 ## Prevalence : 0.610 ## Detection Rate : 0.512 ## Detection Prevalence : 0.587 ## Balanced Accuracy : 0.823 ## ## &#39;Positive&#39; Class : CH ## oj.frst.acc &lt;- as.numeric(oj.conf$overall[1]) rm(oj.pred) rm(oj.conf) #plot(oj.bag$, oj.bag$finalModel$y) #plot(varImp(oj.frst), main=&quot;Variable Importance with Simple Classication&quot;) The model algorithm explains “ROC was used to select the optimal model using the largest value. The final values used for the model were mtry = 9, splitrule = extratrees and min.node.size = 1.” You can see the results of tuning grid combinations in the associated plot of ROC AUC vs mtry grouped by splitting rule. The bagging (accuracy = 0.80751) and random forest (accuracy = 0.81690) models faired pretty well, but the manual classification tree is still in first place. There’s still gradient boosting to investigate! rbind(data.frame(model = &quot;Manual Class&quot;, Accuracy = round(oj_model_1b_cm$overall[&quot;Accuracy&quot;], 5)), data.frame(model = &quot;Caret w.tuneGrid&quot;, Accuracy = round(oj_model_3_cm$overall[&quot;Accuracy&quot;], 5)), data.frame(model = &quot;Bagging&quot;, Accuracy = round(oj.bag.acc, 5)), data.frame(model = &quot;Random Forest&quot;, Accuracy = round(oj.frst.acc, 5)) ) %&gt;% arrange(desc(Accuracy)) ## model Accuracy ## 1 Manual Class 0.86 ## 2 Caret w.tuneGrid 0.85 ## 3 Bagging 0.83 ## 4 Random Forest 0.83 10.4.0.3 Bagging Regression Example Again using the Carseats data set to predict Sales, this time I’ll use the bagging method by specifying method = \"treebag\". I’ll use tuneLength = 5 and not worry about tuneGrid anymore. Caret has no hyperparameters to tune with this model. carseats.bag = train(Sales ~ ., data = carseats_train, method = &quot;treebag&quot;, # for bagging tuneLength = 5, # choose up to 5 combinations of tuning parameters metric = &quot;RMSE&quot;, # evaluate hyperparamter combinations with RMSE trControl = trainControl( method = &quot;cv&quot;, # k-fold cross validation number = 10, # 10 folds savePredictions = &quot;final&quot; # save predictions for the optimal tuning parameter1 ) ) carseats.bag ## Bagged CART ## ## 321 samples ## 10 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 288, 289, 289, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 1.7 0.65 1.4 #plot(carseats.bag$finalModel) carseats.pred &lt;- predict(carseats.bag, carseats_test, type = &quot;raw&quot;) plot(carseats_test$Sales, carseats.pred, main = &quot;Bagging Regression: Predicted vs. Actual&quot;, xlab = &quot;Actual&quot;, ylab = &quot;Predicted&quot;) abline(0, 1) (carseats.bag.rmse &lt;- RMSE(pred = carseats.pred, obs = carseats_test$Sales)) ## [1] 1.9 rm(carseats.pred) plot(varImp(carseats.bag), main=&quot;Variable Importance with Regression Bagging&quot;) 10.4.0.4 Random Forest Regression Example Now I’ll try it with the random forest method by specifying method = \"ranger\". I’ll stick with tuneLength = 5. Caret tunes three hyperparameters: mtry: number of randomly selected predictors splitrule: splitting rule. For regression, options are “variance” (default), “extratrees”, and “maxstat”. min.node.size: minimal node size carseats.frst = train(Sales ~ ., data = carseats_train, method = &quot;ranger&quot;, # for random forest tuneLength = 5, # choose up to 5 combinations of tuning parameters metric = &quot;RMSE&quot;, # evaluate hyperparamter combinations with RMSE trControl = trainControl( method = &quot;cv&quot;, # k-fold cross validation number = 10, # 10 folds savePredictions = &quot;final&quot; # save predictions for the optimal tuning parameter1 ) ) carseats.frst ## Random Forest ## ## 321 samples ## 10 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 289, 289, 288, ... ## Resampling results across tuning parameters: ## ## mtry splitrule RMSE Rsquared MAE ## 2 variance 1.8 0.70 1.4 ## 2 extratrees 1.9 0.65 1.5 ## 4 variance 1.6 0.73 1.3 ## 4 extratrees 1.7 0.69 1.4 ## 6 variance 1.5 0.73 1.2 ## 6 extratrees 1.6 0.70 1.3 ## 8 variance 1.5 0.72 1.2 ## 8 extratrees 1.6 0.71 1.3 ## 11 variance 1.6 0.72 1.2 ## 11 extratrees 1.6 0.71 1.3 ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were mtry = 6, splitrule = variance ## and min.node.size = 5. plot(carseats.frst) carseats.pred &lt;- predict(carseats.frst, carseats_test, type = &quot;raw&quot;) plot(carseats_test$Sales, carseats.pred, main = &quot;Random Forest Regression: Predicted vs. Actual&quot;, xlab = &quot;Actual&quot;, ylab = &quot;Predicted&quot;) abline(0, 1) (carseats.frst.rmse &lt;- RMSE(pred = carseats.pred, obs = carseats_test$Sales)) ## [1] 1.8 rm(carseats.pred) #plot(varImp(carseats.frst), main=&quot;Variable Importance with Regression Random Forest&quot;) The model algorithm explains “RMSE was used to select the optimal model using the smallest value. The final values used for the model were mtry = 11, splitrule = variance and min.node.size = 5.” You can see the results of tuning grid combinations in the associated plot of ROC AUC vs mtry grouped by splitting rule. The bagging and random forest models faired very well - they took over the first and second place! rbind(data.frame(model = &quot;Manual ANOVA&quot;, RMSE = round(carseats_model_1_pruned_rmse, 5)), data.frame(model = &quot;ANOVA w.tuneGrid&quot;, RMSE = round(carseats_model_3_pruned_rmse, 5)), data.frame(model = &quot;Bagging&quot;, RMSE = round(carseats.bag.rmse, 5)), data.frame(model = &quot;Random Forest&quot;, RMSE = round(carseats.frst.rmse, 5)) ) %&gt;% arrange(RMSE) ## model RMSE ## 1 Random Forest 1.8 ## 2 Bagging 1.9 ## 3 ANOVA w.tuneGrid 2.3 ## 4 Manual ANOVA 2.4 "],
["gradient-boosting.html", "10.5 Gradient Boosting", " 10.5 Gradient Boosting Boosting is a method to improve (boost) the week learners sequentially and increase the model accuracy with a combined model. There are several boosting algorithms. One of the earliest was AdaBoost (adaptive boost). A more recent innovation is gradient boosting. Adaboost creates a single split tree (decision stump) then weights the observations by how well the initial tree performed, putting more weight on the difficult observations. It then creates a second tree using the weights so that it focuses on the difficult observations. Observations that are difficult to classify receive increasing larger weights until the algorithm identifies a model that correctly classifies them. The final model returns predictions that are a majority vode. (I think Adaboost applies only to classification problems, not regressions). Gradient boosting generalizes the AdaBoost method, so that the object is to minimize a loss function. In the case of classification problems, the loss function is the log-loss; for regression problems, the loss function is mean squared error. The regression trees are addative, so that the successive models can be added together to correct the residuals in the earlier models. Gradient boosting constructs its trees in a “greedy” manner, meaning it chooses the best splits based on purity scores like Gini or minimizing the loss. It is common to constrain the weak learners by setting maximum tree size parameters. Gradient boosting continues until it reaches maximum number of trees or an acceptible error level. This can result in overfitting, so it is common to employ regularization methods that penalize aspects of the model. Tree Constraints. In general the more constrained the tree, the more trees need to be grown. Parameters to optimize include number of trees, tree depth, number of nodes, minimmum observations per split, and minimum improvement to loss. Learning Rate. Each successive tree can be weighted to slow down the learning rate. Decreasing the learning rate increases the number of required trees. Common growth rates are 0.1 to 0.3. The gradient boosting algorithm fits a shallow tree \\(T_1\\) to the data, \\(M_1 = T_1\\). Then it fits a tree \\(T_2\\) to the residuals and adds a weighted sum of the tree to the original tree as \\(M_2 = M_1 + \\gamma T_2\\). For regularized boosting, include a learning rate factor \\(\\eta \\in (0..1)\\), \\(M_2 = M_1 + \\eta \\gamma T_2\\). A larger \\(\\eta\\) produces faster learning, but risks overfitting. The process repeats until the residuals are small enough, or until it reaches the maximum iterations. Because overfitting is a risk, use cross-validation to select the appropriate number of trees (the number of trees producing the lowest RMSE). 10.5.0.1 Gradient Boosting Classification Example Again using the OJ data set to predict Purchase, this time I’ll use the gradient boosting method by specifying method = \"gbm\". I’ll use tuneLength = 5 and not worry about tuneGrid anymore. Caret tunes the following hyperparameters (see modelLookup(\"gbm\")). n.trees: number of boosting iterations interaction.depth: maximum tree depth shrinkage: shrinkage n.minobsinnode: mimimum terminal node size oj.gbm &lt;- train(Purchase ~ ., data = oj_train, method = &quot;gbm&quot;, # for bagged tree tuneLength = 5, # choose up to 5 combinations of tuning parameters metric = &quot;ROC&quot;, # evaluate hyperparamter combinations with ROC trControl = trainControl( method = &quot;cv&quot;, # k-fold cross validation number = 10, # 10 folds savePredictions = &quot;final&quot;, # save predictions for the optimal tuning parameter1 classProbs = TRUE, # return class probabilities in addition to predicted values summaryFunction = twoClassSummary # for binary response variable ) ) ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2789 nan 0.1000 0.0273 ## 2 1.2286 nan 0.1000 0.0245 ## 3 1.1929 nan 0.1000 0.0175 ## 4 1.1613 nan 0.1000 0.0148 ## 5 1.1263 nan 0.1000 0.0146 ## 6 1.0991 nan 0.1000 0.0105 ## 7 1.0752 nan 0.1000 0.0102 ## 8 1.0579 nan 0.1000 0.0087 ## 9 1.0433 nan 0.1000 0.0047 ## 10 1.0280 nan 0.1000 0.0082 ## 20 0.9233 nan 0.1000 0.0026 ## 40 0.8226 nan 0.1000 0.0010 ## 60 0.7809 nan 0.1000 -0.0001 ## 80 0.7595 nan 0.1000 -0.0002 ## 100 0.7506 nan 0.1000 -0.0008 ## 120 0.7407 nan 0.1000 -0.0005 ## 140 0.7317 nan 0.1000 -0.0005 ## 160 0.7277 nan 0.1000 -0.0009 ## 180 0.7232 nan 0.1000 -0.0004 ## 200 0.7181 nan 0.1000 -0.0007 ## 220 0.7115 nan 0.1000 -0.0008 ## 240 0.7096 nan 0.1000 -0.0010 ## 250 0.7081 nan 0.1000 -0.0015 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2695 nan 0.1000 0.0319 ## 2 1.2150 nan 0.1000 0.0260 ## 3 1.1702 nan 0.1000 0.0225 ## 4 1.1260 nan 0.1000 0.0186 ## 5 1.0913 nan 0.1000 0.0147 ## 6 1.0586 nan 0.1000 0.0160 ## 7 1.0276 nan 0.1000 0.0146 ## 8 1.0045 nan 0.1000 0.0109 ## 9 0.9836 nan 0.1000 0.0099 ## 10 0.9624 nan 0.1000 0.0068 ## 20 0.8337 nan 0.1000 0.0027 ## 40 0.7525 nan 0.1000 -0.0005 ## 60 0.7240 nan 0.1000 -0.0005 ## 80 0.7063 nan 0.1000 -0.0006 ## 100 0.6879 nan 0.1000 -0.0011 ## 120 0.6751 nan 0.1000 -0.0018 ## 140 0.6605 nan 0.1000 -0.0012 ## 160 0.6477 nan 0.1000 -0.0013 ## 180 0.6359 nan 0.1000 -0.0010 ## 200 0.6274 nan 0.1000 -0.0018 ## 220 0.6166 nan 0.1000 -0.0005 ## 240 0.6078 nan 0.1000 -0.0011 ## 250 0.6014 nan 0.1000 -0.0019 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2548 nan 0.1000 0.0377 ## 2 1.1905 nan 0.1000 0.0294 ## 3 1.1343 nan 0.1000 0.0258 ## 4 1.0935 nan 0.1000 0.0180 ## 5 1.0529 nan 0.1000 0.0168 ## 6 1.0172 nan 0.1000 0.0159 ## 7 0.9824 nan 0.1000 0.0151 ## 8 0.9534 nan 0.1000 0.0127 ## 9 0.9277 nan 0.1000 0.0109 ## 10 0.9066 nan 0.1000 0.0088 ## 20 0.7870 nan 0.1000 0.0023 ## 40 0.7150 nan 0.1000 -0.0008 ## 60 0.6799 nan 0.1000 -0.0023 ## 80 0.6520 nan 0.1000 -0.0012 ## 100 0.6298 nan 0.1000 -0.0005 ## 120 0.6117 nan 0.1000 -0.0024 ## 140 0.5973 nan 0.1000 -0.0016 ## 160 0.5849 nan 0.1000 -0.0023 ## 180 0.5670 nan 0.1000 -0.0015 ## 200 0.5548 nan 0.1000 -0.0006 ## 220 0.5440 nan 0.1000 -0.0024 ## 240 0.5290 nan 0.1000 -0.0020 ## 250 0.5228 nan 0.1000 -0.0016 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2554 nan 0.1000 0.0399 ## 2 1.1847 nan 0.1000 0.0346 ## 3 1.1321 nan 0.1000 0.0199 ## 4 1.0823 nan 0.1000 0.0224 ## 5 1.0392 nan 0.1000 0.0208 ## 6 1.0067 nan 0.1000 0.0145 ## 7 0.9768 nan 0.1000 0.0139 ## 8 0.9462 nan 0.1000 0.0123 ## 9 0.9238 nan 0.1000 0.0095 ## 10 0.8966 nan 0.1000 0.0090 ## 20 0.7681 nan 0.1000 0.0007 ## 40 0.6937 nan 0.1000 -0.0004 ## 60 0.6552 nan 0.1000 -0.0017 ## 80 0.6202 nan 0.1000 -0.0018 ## 100 0.5887 nan 0.1000 -0.0027 ## 120 0.5653 nan 0.1000 -0.0012 ## 140 0.5434 nan 0.1000 -0.0017 ## 160 0.5275 nan 0.1000 -0.0008 ## 180 0.5068 nan 0.1000 -0.0012 ## 200 0.4935 nan 0.1000 -0.0016 ## 220 0.4801 nan 0.1000 -0.0018 ## 240 0.4665 nan 0.1000 -0.0010 ## 250 0.4603 nan 0.1000 -0.0012 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2513 nan 0.1000 0.0375 ## 2 1.1795 nan 0.1000 0.0320 ## 3 1.1264 nan 0.1000 0.0254 ## 4 1.0742 nan 0.1000 0.0225 ## 5 1.0282 nan 0.1000 0.0196 ## 6 0.9888 nan 0.1000 0.0177 ## 7 0.9547 nan 0.1000 0.0136 ## 8 0.9303 nan 0.1000 0.0103 ## 9 0.9008 nan 0.1000 0.0121 ## 10 0.8803 nan 0.1000 0.0073 ## 20 0.7563 nan 0.1000 0.0003 ## 40 0.6715 nan 0.1000 -0.0012 ## 60 0.6253 nan 0.1000 -0.0016 ## 80 0.5868 nan 0.1000 -0.0021 ## 100 0.5538 nan 0.1000 -0.0015 ## 120 0.5285 nan 0.1000 -0.0034 ## 140 0.5070 nan 0.1000 -0.0025 ## 160 0.4872 nan 0.1000 -0.0012 ## 180 0.4736 nan 0.1000 -0.0023 ## 200 0.4566 nan 0.1000 -0.0015 ## 220 0.4407 nan 0.1000 -0.0011 ## 240 0.4262 nan 0.1000 -0.0013 ## 250 0.4186 nan 0.1000 -0.0024 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2743 nan 0.1000 0.0307 ## 2 1.2220 nan 0.1000 0.0240 ## 3 1.1885 nan 0.1000 0.0165 ## 4 1.1522 nan 0.1000 0.0177 ## 5 1.1186 nan 0.1000 0.0136 ## 6 1.0912 nan 0.1000 0.0111 ## 7 1.0693 nan 0.1000 0.0106 ## 8 1.0492 nan 0.1000 0.0089 ## 9 1.0309 nan 0.1000 0.0093 ## 10 1.0172 nan 0.1000 0.0069 ## 20 0.9206 nan 0.1000 0.0030 ## 40 0.8357 nan 0.1000 -0.0002 ## 60 0.7936 nan 0.1000 -0.0000 ## 80 0.7764 nan 0.1000 -0.0009 ## 100 0.7682 nan 0.1000 -0.0004 ## 120 0.7620 nan 0.1000 -0.0008 ## 140 0.7582 nan 0.1000 -0.0011 ## 160 0.7536 nan 0.1000 -0.0005 ## 180 0.7501 nan 0.1000 -0.0006 ## 200 0.7448 nan 0.1000 -0.0008 ## 220 0.7409 nan 0.1000 -0.0006 ## 240 0.7385 nan 0.1000 -0.0011 ## 250 0.7368 nan 0.1000 -0.0007 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2697 nan 0.1000 0.0323 ## 2 1.2121 nan 0.1000 0.0276 ## 3 1.1636 nan 0.1000 0.0251 ## 4 1.1220 nan 0.1000 0.0166 ## 5 1.0826 nan 0.1000 0.0131 ## 6 1.0537 nan 0.1000 0.0134 ## 7 1.0269 nan 0.1000 0.0104 ## 8 1.0061 nan 0.1000 0.0084 ## 9 0.9858 nan 0.1000 0.0082 ## 10 0.9678 nan 0.1000 0.0066 ## 20 0.8429 nan 0.1000 0.0024 ## 40 0.7685 nan 0.1000 -0.0010 ## 60 0.7422 nan 0.1000 -0.0006 ## 80 0.7228 nan 0.1000 -0.0009 ## 100 0.7073 nan 0.1000 -0.0013 ## 120 0.6937 nan 0.1000 -0.0024 ## 140 0.6836 nan 0.1000 -0.0014 ## 160 0.6703 nan 0.1000 -0.0022 ## 180 0.6607 nan 0.1000 -0.0009 ## 200 0.6529 nan 0.1000 -0.0011 ## 220 0.6438 nan 0.1000 -0.0017 ## 240 0.6370 nan 0.1000 -0.0015 ## 250 0.6311 nan 0.1000 -0.0011 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2569 nan 0.1000 0.0361 ## 2 1.1946 nan 0.1000 0.0301 ## 3 1.1386 nan 0.1000 0.0266 ## 4 1.0954 nan 0.1000 0.0205 ## 5 1.0524 nan 0.1000 0.0204 ## 6 1.0186 nan 0.1000 0.0149 ## 7 0.9847 nan 0.1000 0.0126 ## 8 0.9618 nan 0.1000 0.0086 ## 9 0.9344 nan 0.1000 0.0114 ## 10 0.9135 nan 0.1000 0.0095 ## 20 0.8003 nan 0.1000 0.0027 ## 40 0.7353 nan 0.1000 -0.0011 ## 60 0.7042 nan 0.1000 -0.0027 ## 80 0.6800 nan 0.1000 -0.0017 ## 100 0.6602 nan 0.1000 -0.0008 ## 120 0.6393 nan 0.1000 -0.0017 ## 140 0.6231 nan 0.1000 -0.0016 ## 160 0.6077 nan 0.1000 -0.0028 ## 180 0.5977 nan 0.1000 -0.0012 ## 200 0.5863 nan 0.1000 -0.0014 ## 220 0.5749 nan 0.1000 -0.0013 ## 240 0.5618 nan 0.1000 -0.0022 ## 250 0.5577 nan 0.1000 -0.0022 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2495 nan 0.1000 0.0419 ## 2 1.1884 nan 0.1000 0.0265 ## 3 1.1279 nan 0.1000 0.0248 ## 4 1.0820 nan 0.1000 0.0208 ## 5 1.0426 nan 0.1000 0.0166 ## 6 1.0089 nan 0.1000 0.0146 ## 7 0.9799 nan 0.1000 0.0126 ## 8 0.9474 nan 0.1000 0.0140 ## 9 0.9225 nan 0.1000 0.0079 ## 10 0.9066 nan 0.1000 0.0048 ## 20 0.7815 nan 0.1000 0.0014 ## 40 0.7028 nan 0.1000 -0.0019 ## 60 0.6661 nan 0.1000 -0.0011 ## 80 0.6386 nan 0.1000 -0.0006 ## 100 0.6075 nan 0.1000 -0.0005 ## 120 0.5861 nan 0.1000 -0.0019 ## 140 0.5674 nan 0.1000 -0.0020 ## 160 0.5467 nan 0.1000 -0.0016 ## 180 0.5318 nan 0.1000 -0.0020 ## 200 0.5200 nan 0.1000 -0.0025 ## 220 0.5050 nan 0.1000 -0.0009 ## 240 0.4930 nan 0.1000 -0.0020 ## 250 0.4883 nan 0.1000 -0.0016 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2558 nan 0.1000 0.0336 ## 2 1.1856 nan 0.1000 0.0326 ## 3 1.1172 nan 0.1000 0.0305 ## 4 1.0713 nan 0.1000 0.0222 ## 5 1.0313 nan 0.1000 0.0171 ## 6 0.9965 nan 0.1000 0.0164 ## 7 0.9613 nan 0.1000 0.0156 ## 8 0.9354 nan 0.1000 0.0103 ## 9 0.9089 nan 0.1000 0.0111 ## 10 0.8859 nan 0.1000 0.0059 ## 20 0.7690 nan 0.1000 0.0006 ## 40 0.6889 nan 0.1000 -0.0004 ## 60 0.6452 nan 0.1000 -0.0021 ## 80 0.6127 nan 0.1000 -0.0021 ## 100 0.5811 nan 0.1000 -0.0032 ## 120 0.5557 nan 0.1000 -0.0011 ## 140 0.5332 nan 0.1000 -0.0012 ## 160 0.5118 nan 0.1000 -0.0014 ## 180 0.4879 nan 0.1000 -0.0012 ## 200 0.4737 nan 0.1000 -0.0020 ## 220 0.4591 nan 0.1000 -0.0024 ## 240 0.4460 nan 0.1000 -0.0030 ## 250 0.4386 nan 0.1000 -0.0013 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2717 nan 0.1000 0.0320 ## 2 1.2209 nan 0.1000 0.0250 ## 3 1.1751 nan 0.1000 0.0208 ## 4 1.1404 nan 0.1000 0.0149 ## 5 1.1056 nan 0.1000 0.0119 ## 6 1.0782 nan 0.1000 0.0125 ## 7 1.0569 nan 0.1000 0.0081 ## 8 1.0356 nan 0.1000 0.0098 ## 9 1.0176 nan 0.1000 0.0080 ## 10 1.0042 nan 0.1000 0.0066 ## 20 0.9023 nan 0.1000 0.0021 ## 40 0.8156 nan 0.1000 0.0006 ## 60 0.7793 nan 0.1000 0.0004 ## 80 0.7609 nan 0.1000 -0.0014 ## 100 0.7514 nan 0.1000 -0.0006 ## 120 0.7448 nan 0.1000 -0.0005 ## 140 0.7406 nan 0.1000 -0.0008 ## 160 0.7353 nan 0.1000 -0.0007 ## 180 0.7329 nan 0.1000 -0.0008 ## 200 0.7281 nan 0.1000 -0.0014 ## 220 0.7239 nan 0.1000 -0.0008 ## 240 0.7211 nan 0.1000 -0.0011 ## 250 0.7203 nan 0.1000 -0.0010 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2647 nan 0.1000 0.0368 ## 2 1.2060 nan 0.1000 0.0275 ## 3 1.1558 nan 0.1000 0.0232 ## 4 1.1207 nan 0.1000 0.0178 ## 5 1.0787 nan 0.1000 0.0172 ## 6 1.0478 nan 0.1000 0.0141 ## 7 1.0178 nan 0.1000 0.0117 ## 8 0.9963 nan 0.1000 0.0099 ## 9 0.9749 nan 0.1000 0.0107 ## 10 0.9514 nan 0.1000 0.0095 ## 20 0.8341 nan 0.1000 0.0029 ## 40 0.7601 nan 0.1000 -0.0015 ## 60 0.7335 nan 0.1000 -0.0010 ## 80 0.7131 nan 0.1000 -0.0011 ## 100 0.7006 nan 0.1000 -0.0018 ## 120 0.6886 nan 0.1000 -0.0008 ## 140 0.6740 nan 0.1000 -0.0014 ## 160 0.6628 nan 0.1000 -0.0017 ## 180 0.6522 nan 0.1000 -0.0010 ## 200 0.6423 nan 0.1000 -0.0005 ## 220 0.6353 nan 0.1000 -0.0020 ## 240 0.6251 nan 0.1000 -0.0017 ## 250 0.6211 nan 0.1000 -0.0013 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2533 nan 0.1000 0.0369 ## 2 1.1914 nan 0.1000 0.0304 ## 3 1.1304 nan 0.1000 0.0280 ## 4 1.0808 nan 0.1000 0.0214 ## 5 1.0411 nan 0.1000 0.0177 ## 6 1.0095 nan 0.1000 0.0131 ## 7 0.9791 nan 0.1000 0.0118 ## 8 0.9487 nan 0.1000 0.0146 ## 9 0.9234 nan 0.1000 0.0114 ## 10 0.9050 nan 0.1000 0.0081 ## 20 0.7868 nan 0.1000 -0.0018 ## 40 0.7190 nan 0.1000 -0.0002 ## 60 0.6929 nan 0.1000 -0.0013 ## 80 0.6706 nan 0.1000 -0.0012 ## 100 0.6511 nan 0.1000 -0.0008 ## 120 0.6313 nan 0.1000 -0.0028 ## 140 0.6161 nan 0.1000 -0.0010 ## 160 0.6016 nan 0.1000 -0.0019 ## 180 0.5875 nan 0.1000 -0.0013 ## 200 0.5754 nan 0.1000 -0.0021 ## 220 0.5613 nan 0.1000 -0.0011 ## 240 0.5456 nan 0.1000 -0.0014 ## 250 0.5423 nan 0.1000 -0.0016 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2526 nan 0.1000 0.0406 ## 2 1.1845 nan 0.1000 0.0346 ## 3 1.1309 nan 0.1000 0.0246 ## 4 1.0809 nan 0.1000 0.0250 ## 5 1.0380 nan 0.1000 0.0205 ## 6 1.0017 nan 0.1000 0.0164 ## 7 0.9661 nan 0.1000 0.0146 ## 8 0.9372 nan 0.1000 0.0130 ## 9 0.9115 nan 0.1000 0.0105 ## 10 0.8921 nan 0.1000 0.0084 ## 20 0.7698 nan 0.1000 0.0014 ## 40 0.6902 nan 0.1000 -0.0016 ## 60 0.6501 nan 0.1000 -0.0019 ## 80 0.6200 nan 0.1000 -0.0006 ## 100 0.5995 nan 0.1000 -0.0026 ## 120 0.5766 nan 0.1000 -0.0019 ## 140 0.5576 nan 0.1000 -0.0020 ## 160 0.5428 nan 0.1000 -0.0027 ## 180 0.5276 nan 0.1000 -0.0026 ## 200 0.5091 nan 0.1000 -0.0011 ## 220 0.4954 nan 0.1000 -0.0014 ## 240 0.4801 nan 0.1000 -0.0028 ## 250 0.4748 nan 0.1000 -0.0021 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2578 nan 0.1000 0.0379 ## 2 1.1876 nan 0.1000 0.0342 ## 3 1.1214 nan 0.1000 0.0297 ## 4 1.0650 nan 0.1000 0.0254 ## 5 1.0182 nan 0.1000 0.0188 ## 6 0.9812 nan 0.1000 0.0172 ## 7 0.9484 nan 0.1000 0.0116 ## 8 0.9182 nan 0.1000 0.0116 ## 9 0.8929 nan 0.1000 0.0068 ## 10 0.8704 nan 0.1000 0.0085 ## 20 0.7522 nan 0.1000 0.0002 ## 40 0.6778 nan 0.1000 -0.0019 ## 60 0.6318 nan 0.1000 -0.0015 ## 80 0.5982 nan 0.1000 -0.0011 ## 100 0.5669 nan 0.1000 -0.0032 ## 120 0.5451 nan 0.1000 -0.0012 ## 140 0.5224 nan 0.1000 -0.0002 ## 160 0.5005 nan 0.1000 -0.0027 ## 180 0.4834 nan 0.1000 -0.0015 ## 200 0.4693 nan 0.1000 -0.0009 ## 220 0.4530 nan 0.1000 -0.0016 ## 240 0.4419 nan 0.1000 -0.0035 ## 250 0.4324 nan 0.1000 -0.0016 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2761 nan 0.1000 0.0283 ## 2 1.2243 nan 0.1000 0.0231 ## 3 1.1816 nan 0.1000 0.0192 ## 4 1.1477 nan 0.1000 0.0158 ## 5 1.1174 nan 0.1000 0.0122 ## 6 1.0930 nan 0.1000 0.0116 ## 7 1.0704 nan 0.1000 0.0102 ## 8 1.0499 nan 0.1000 0.0083 ## 9 1.0340 nan 0.1000 0.0078 ## 10 1.0167 nan 0.1000 0.0082 ## 20 0.9152 nan 0.1000 0.0021 ## 40 0.8226 nan 0.1000 0.0007 ## 60 0.7895 nan 0.1000 0.0000 ## 80 0.7690 nan 0.1000 -0.0008 ## 100 0.7612 nan 0.1000 -0.0010 ## 120 0.7541 nan 0.1000 -0.0004 ## 140 0.7491 nan 0.1000 -0.0012 ## 160 0.7443 nan 0.1000 -0.0006 ## 180 0.7405 nan 0.1000 -0.0009 ## 200 0.7369 nan 0.1000 -0.0009 ## 220 0.7329 nan 0.1000 -0.0007 ## 240 0.7287 nan 0.1000 -0.0014 ## 250 0.7268 nan 0.1000 -0.0011 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2642 nan 0.1000 0.0359 ## 2 1.2066 nan 0.1000 0.0279 ## 3 1.1624 nan 0.1000 0.0237 ## 4 1.1213 nan 0.1000 0.0202 ## 5 1.0853 nan 0.1000 0.0162 ## 6 1.0560 nan 0.1000 0.0116 ## 7 1.0275 nan 0.1000 0.0125 ## 8 1.0040 nan 0.1000 0.0109 ## 9 0.9823 nan 0.1000 0.0077 ## 10 0.9612 nan 0.1000 0.0105 ## 20 0.8409 nan 0.1000 0.0026 ## 40 0.7578 nan 0.1000 -0.0007 ## 60 0.7294 nan 0.1000 -0.0007 ## 80 0.7095 nan 0.1000 -0.0026 ## 100 0.6965 nan 0.1000 -0.0012 ## 120 0.6857 nan 0.1000 -0.0022 ## 140 0.6751 nan 0.1000 -0.0004 ## 160 0.6650 nan 0.1000 -0.0018 ## 180 0.6581 nan 0.1000 -0.0017 ## 200 0.6520 nan 0.1000 -0.0009 ## 220 0.6436 nan 0.1000 -0.0011 ## 240 0.6351 nan 0.1000 -0.0009 ## 250 0.6312 nan 0.1000 -0.0016 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2577 nan 0.1000 0.0367 ## 2 1.1908 nan 0.1000 0.0288 ## 3 1.1314 nan 0.1000 0.0257 ## 4 1.0832 nan 0.1000 0.0231 ## 5 1.0473 nan 0.1000 0.0147 ## 6 1.0104 nan 0.1000 0.0174 ## 7 0.9743 nan 0.1000 0.0139 ## 8 0.9460 nan 0.1000 0.0113 ## 9 0.9236 nan 0.1000 0.0105 ## 10 0.9026 nan 0.1000 0.0077 ## 20 0.7942 nan 0.1000 0.0009 ## 40 0.7292 nan 0.1000 -0.0018 ## 60 0.6933 nan 0.1000 -0.0013 ## 80 0.6650 nan 0.1000 -0.0008 ## 100 0.6458 nan 0.1000 -0.0020 ## 120 0.6252 nan 0.1000 -0.0018 ## 140 0.6106 nan 0.1000 -0.0010 ## 160 0.5953 nan 0.1000 -0.0009 ## 180 0.5810 nan 0.1000 -0.0014 ## 200 0.5683 nan 0.1000 -0.0016 ## 220 0.5544 nan 0.1000 -0.0009 ## 240 0.5425 nan 0.1000 -0.0010 ## 250 0.5367 nan 0.1000 -0.0012 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2542 nan 0.1000 0.0373 ## 2 1.1874 nan 0.1000 0.0310 ## 3 1.1277 nan 0.1000 0.0279 ## 4 1.0765 nan 0.1000 0.0231 ## 5 1.0393 nan 0.1000 0.0179 ## 6 1.0012 nan 0.1000 0.0141 ## 7 0.9658 nan 0.1000 0.0139 ## 8 0.9421 nan 0.1000 0.0101 ## 9 0.9187 nan 0.1000 0.0088 ## 10 0.8966 nan 0.1000 0.0095 ## 20 0.7715 nan 0.1000 0.0019 ## 40 0.6966 nan 0.1000 -0.0017 ## 60 0.6516 nan 0.1000 -0.0010 ## 80 0.6206 nan 0.1000 -0.0015 ## 100 0.5991 nan 0.1000 -0.0028 ## 120 0.5818 nan 0.1000 -0.0019 ## 140 0.5644 nan 0.1000 -0.0018 ## 160 0.5476 nan 0.1000 -0.0016 ## 180 0.5329 nan 0.1000 -0.0016 ## 200 0.5212 nan 0.1000 -0.0016 ## 220 0.5055 nan 0.1000 -0.0032 ## 240 0.4926 nan 0.1000 -0.0013 ## 250 0.4850 nan 0.1000 -0.0006 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2523 nan 0.1000 0.0397 ## 2 1.1844 nan 0.1000 0.0301 ## 3 1.1269 nan 0.1000 0.0263 ## 4 1.0773 nan 0.1000 0.0216 ## 5 1.0310 nan 0.1000 0.0211 ## 6 0.9902 nan 0.1000 0.0176 ## 7 0.9582 nan 0.1000 0.0119 ## 8 0.9338 nan 0.1000 0.0106 ## 9 0.9054 nan 0.1000 0.0120 ## 10 0.8869 nan 0.1000 0.0083 ## 20 0.7620 nan 0.1000 -0.0001 ## 40 0.6734 nan 0.1000 -0.0010 ## 60 0.6333 nan 0.1000 -0.0004 ## 80 0.5982 nan 0.1000 -0.0023 ## 100 0.5685 nan 0.1000 -0.0013 ## 120 0.5436 nan 0.1000 -0.0024 ## 140 0.5196 nan 0.1000 -0.0012 ## 160 0.5010 nan 0.1000 -0.0031 ## 180 0.4832 nan 0.1000 -0.0032 ## 200 0.4677 nan 0.1000 -0.0014 ## 220 0.4504 nan 0.1000 -0.0016 ## 240 0.4333 nan 0.1000 -0.0013 ## 250 0.4273 nan 0.1000 -0.0022 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2726 nan 0.1000 0.0321 ## 2 1.2186 nan 0.1000 0.0259 ## 3 1.1770 nan 0.1000 0.0190 ## 4 1.1350 nan 0.1000 0.0186 ## 5 1.1017 nan 0.1000 0.0147 ## 6 1.0810 nan 0.1000 0.0096 ## 7 1.0570 nan 0.1000 0.0105 ## 8 1.0375 nan 0.1000 0.0098 ## 9 1.0188 nan 0.1000 0.0081 ## 10 1.0020 nan 0.1000 0.0072 ## 20 0.9034 nan 0.1000 0.0023 ## 40 0.8070 nan 0.1000 -0.0001 ## 60 0.7671 nan 0.1000 -0.0003 ## 80 0.7500 nan 0.1000 -0.0007 ## 100 0.7415 nan 0.1000 -0.0008 ## 120 0.7349 nan 0.1000 -0.0008 ## 140 0.7282 nan 0.1000 -0.0006 ## 160 0.7222 nan 0.1000 -0.0006 ## 180 0.7190 nan 0.1000 -0.0003 ## 200 0.7160 nan 0.1000 -0.0013 ## 220 0.7128 nan 0.1000 -0.0008 ## 240 0.7092 nan 0.1000 -0.0012 ## 250 0.7066 nan 0.1000 -0.0005 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2597 nan 0.1000 0.0362 ## 2 1.2019 nan 0.1000 0.0279 ## 3 1.1504 nan 0.1000 0.0230 ## 4 1.1084 nan 0.1000 0.0168 ## 5 1.0702 nan 0.1000 0.0184 ## 6 1.0378 nan 0.1000 0.0136 ## 7 1.0090 nan 0.1000 0.0131 ## 8 0.9848 nan 0.1000 0.0115 ## 9 0.9632 nan 0.1000 0.0078 ## 10 0.9410 nan 0.1000 0.0094 ## 20 0.8215 nan 0.1000 0.0037 ## 40 0.7478 nan 0.1000 -0.0001 ## 60 0.7184 nan 0.1000 -0.0008 ## 80 0.6985 nan 0.1000 -0.0007 ## 100 0.6799 nan 0.1000 -0.0013 ## 120 0.6660 nan 0.1000 0.0002 ## 140 0.6534 nan 0.1000 -0.0009 ## 160 0.6423 nan 0.1000 -0.0016 ## 180 0.6325 nan 0.1000 -0.0016 ## 200 0.6226 nan 0.1000 -0.0011 ## 220 0.6139 nan 0.1000 -0.0010 ## 240 0.6043 nan 0.1000 -0.0017 ## 250 0.6016 nan 0.1000 -0.0012 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2549 nan 0.1000 0.0413 ## 2 1.1843 nan 0.1000 0.0345 ## 3 1.1274 nan 0.1000 0.0245 ## 4 1.0809 nan 0.1000 0.0213 ## 5 1.0418 nan 0.1000 0.0178 ## 6 1.0056 nan 0.1000 0.0128 ## 7 0.9797 nan 0.1000 0.0122 ## 8 0.9562 nan 0.1000 0.0083 ## 9 0.9288 nan 0.1000 0.0116 ## 10 0.9070 nan 0.1000 0.0088 ## 20 0.7819 nan 0.1000 0.0013 ## 40 0.7119 nan 0.1000 -0.0008 ## 60 0.6706 nan 0.1000 -0.0013 ## 80 0.6482 nan 0.1000 -0.0018 ## 100 0.6271 nan 0.1000 -0.0027 ## 120 0.6065 nan 0.1000 -0.0013 ## 140 0.5901 nan 0.1000 -0.0021 ## 160 0.5709 nan 0.1000 -0.0009 ## 180 0.5563 nan 0.1000 -0.0005 ## 200 0.5427 nan 0.1000 -0.0025 ## 220 0.5333 nan 0.1000 -0.0012 ## 240 0.5239 nan 0.1000 -0.0015 ## 250 0.5186 nan 0.1000 -0.0018 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2503 nan 0.1000 0.0405 ## 2 1.1806 nan 0.1000 0.0343 ## 3 1.1211 nan 0.1000 0.0276 ## 4 1.0676 nan 0.1000 0.0242 ## 5 1.0226 nan 0.1000 0.0193 ## 6 0.9842 nan 0.1000 0.0164 ## 7 0.9549 nan 0.1000 0.0120 ## 8 0.9274 nan 0.1000 0.0125 ## 9 0.8994 nan 0.1000 0.0100 ## 10 0.8810 nan 0.1000 0.0068 ## 20 0.7641 nan 0.1000 0.0015 ## 40 0.6902 nan 0.1000 -0.0013 ## 60 0.6497 nan 0.1000 -0.0026 ## 80 0.6225 nan 0.1000 -0.0020 ## 100 0.5898 nan 0.1000 -0.0009 ## 120 0.5662 nan 0.1000 -0.0014 ## 140 0.5460 nan 0.1000 -0.0015 ## 160 0.5293 nan 0.1000 -0.0029 ## 180 0.5142 nan 0.1000 -0.0018 ## 200 0.5021 nan 0.1000 -0.0021 ## 220 0.4850 nan 0.1000 -0.0025 ## 240 0.4738 nan 0.1000 -0.0010 ## 250 0.4697 nan 0.1000 -0.0025 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2510 nan 0.1000 0.0400 ## 2 1.1788 nan 0.1000 0.0356 ## 3 1.1185 nan 0.1000 0.0284 ## 4 1.0652 nan 0.1000 0.0255 ## 5 1.0228 nan 0.1000 0.0209 ## 6 0.9819 nan 0.1000 0.0175 ## 7 0.9467 nan 0.1000 0.0146 ## 8 0.9149 nan 0.1000 0.0133 ## 9 0.8872 nan 0.1000 0.0105 ## 10 0.8681 nan 0.1000 0.0067 ## 20 0.7435 nan 0.1000 0.0013 ## 40 0.6620 nan 0.1000 -0.0012 ## 60 0.6096 nan 0.1000 -0.0011 ## 80 0.5775 nan 0.1000 -0.0017 ## 100 0.5491 nan 0.1000 -0.0016 ## 120 0.5248 nan 0.1000 -0.0030 ## 140 0.5082 nan 0.1000 -0.0016 ## 160 0.4873 nan 0.1000 -0.0022 ## 180 0.4700 nan 0.1000 -0.0020 ## 200 0.4543 nan 0.1000 -0.0013 ## 220 0.4388 nan 0.1000 -0.0031 ## 240 0.4269 nan 0.1000 -0.0013 ## 250 0.4237 nan 0.1000 -0.0015 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2737 nan 0.1000 0.0306 ## 2 1.2188 nan 0.1000 0.0241 ## 3 1.1752 nan 0.1000 0.0184 ## 4 1.1384 nan 0.1000 0.0168 ## 5 1.1052 nan 0.1000 0.0134 ## 6 1.0800 nan 0.1000 0.0103 ## 7 1.0601 nan 0.1000 0.0106 ## 8 1.0416 nan 0.1000 0.0097 ## 9 1.0234 nan 0.1000 0.0070 ## 10 1.0079 nan 0.1000 0.0055 ## 20 0.9090 nan 0.1000 0.0034 ## 40 0.8145 nan 0.1000 0.0004 ## 60 0.7708 nan 0.1000 0.0003 ## 80 0.7528 nan 0.1000 -0.0009 ## 100 0.7433 nan 0.1000 -0.0010 ## 120 0.7366 nan 0.1000 -0.0002 ## 140 0.7317 nan 0.1000 -0.0007 ## 160 0.7262 nan 0.1000 -0.0009 ## 180 0.7217 nan 0.1000 -0.0005 ## 200 0.7172 nan 0.1000 -0.0013 ## 220 0.7141 nan 0.1000 -0.0012 ## 240 0.7102 nan 0.1000 -0.0002 ## 250 0.7086 nan 0.1000 -0.0012 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2660 nan 0.1000 0.0361 ## 2 1.2076 nan 0.1000 0.0275 ## 3 1.1575 nan 0.1000 0.0247 ## 4 1.1197 nan 0.1000 0.0178 ## 5 1.0845 nan 0.1000 0.0173 ## 6 1.0529 nan 0.1000 0.0131 ## 7 1.0259 nan 0.1000 0.0123 ## 8 0.9969 nan 0.1000 0.0117 ## 9 0.9748 nan 0.1000 0.0082 ## 10 0.9535 nan 0.1000 0.0076 ## 20 0.8305 nan 0.1000 0.0033 ## 40 0.7475 nan 0.1000 -0.0002 ## 60 0.7207 nan 0.1000 -0.0018 ## 80 0.7037 nan 0.1000 -0.0017 ## 100 0.6882 nan 0.1000 -0.0014 ## 120 0.6802 nan 0.1000 -0.0010 ## 140 0.6709 nan 0.1000 -0.0015 ## 160 0.6613 nan 0.1000 -0.0014 ## 180 0.6523 nan 0.1000 -0.0004 ## 200 0.6449 nan 0.1000 -0.0011 ## 220 0.6367 nan 0.1000 -0.0014 ## 240 0.6286 nan 0.1000 -0.0013 ## 250 0.6233 nan 0.1000 -0.0019 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2551 nan 0.1000 0.0346 ## 2 1.1931 nan 0.1000 0.0315 ## 3 1.1316 nan 0.1000 0.0272 ## 4 1.0872 nan 0.1000 0.0204 ## 5 1.0437 nan 0.1000 0.0187 ## 6 1.0098 nan 0.1000 0.0139 ## 7 0.9818 nan 0.1000 0.0107 ## 8 0.9579 nan 0.1000 0.0111 ## 9 0.9321 nan 0.1000 0.0129 ## 10 0.9121 nan 0.1000 0.0066 ## 20 0.7838 nan 0.1000 0.0022 ## 40 0.7153 nan 0.1000 -0.0001 ## 60 0.6832 nan 0.1000 -0.0011 ## 80 0.6612 nan 0.1000 -0.0019 ## 100 0.6415 nan 0.1000 -0.0020 ## 120 0.6252 nan 0.1000 -0.0020 ## 140 0.6084 nan 0.1000 -0.0017 ## 160 0.5936 nan 0.1000 -0.0009 ## 180 0.5834 nan 0.1000 -0.0021 ## 200 0.5669 nan 0.1000 -0.0009 ## 220 0.5568 nan 0.1000 -0.0011 ## 240 0.5467 nan 0.1000 -0.0027 ## 250 0.5422 nan 0.1000 -0.0015 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2574 nan 0.1000 0.0391 ## 2 1.1858 nan 0.1000 0.0330 ## 3 1.1308 nan 0.1000 0.0237 ## 4 1.0768 nan 0.1000 0.0205 ## 5 1.0311 nan 0.1000 0.0203 ## 6 0.9934 nan 0.1000 0.0175 ## 7 0.9643 nan 0.1000 0.0121 ## 8 0.9342 nan 0.1000 0.0118 ## 9 0.9061 nan 0.1000 0.0104 ## 10 0.8869 nan 0.1000 0.0078 ## 20 0.7608 nan 0.1000 0.0000 ## 40 0.6861 nan 0.1000 -0.0012 ## 60 0.6452 nan 0.1000 -0.0007 ## 80 0.6147 nan 0.1000 -0.0020 ## 100 0.5919 nan 0.1000 -0.0015 ## 120 0.5685 nan 0.1000 -0.0015 ## 140 0.5516 nan 0.1000 -0.0007 ## 160 0.5322 nan 0.1000 -0.0024 ## 180 0.5188 nan 0.1000 -0.0013 ## 200 0.5045 nan 0.1000 -0.0011 ## 220 0.4913 nan 0.1000 -0.0012 ## 240 0.4791 nan 0.1000 -0.0021 ## 250 0.4735 nan 0.1000 -0.0011 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2526 nan 0.1000 0.0413 ## 2 1.1759 nan 0.1000 0.0342 ## 3 1.1130 nan 0.1000 0.0286 ## 4 1.0692 nan 0.1000 0.0198 ## 5 1.0244 nan 0.1000 0.0208 ## 6 0.9858 nan 0.1000 0.0160 ## 7 0.9512 nan 0.1000 0.0156 ## 8 0.9230 nan 0.1000 0.0086 ## 9 0.8945 nan 0.1000 0.0128 ## 10 0.8741 nan 0.1000 0.0071 ## 20 0.7489 nan 0.1000 0.0012 ## 40 0.6645 nan 0.1000 -0.0017 ## 60 0.6174 nan 0.1000 -0.0018 ## 80 0.5875 nan 0.1000 -0.0022 ## 100 0.5620 nan 0.1000 -0.0009 ## 120 0.5374 nan 0.1000 -0.0026 ## 140 0.5202 nan 0.1000 -0.0017 ## 160 0.5003 nan 0.1000 -0.0014 ## 180 0.4829 nan 0.1000 -0.0021 ## 200 0.4660 nan 0.1000 -0.0024 ## 220 0.4560 nan 0.1000 -0.0024 ## 240 0.4425 nan 0.1000 -0.0020 ## 250 0.4349 nan 0.1000 -0.0021 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2722 nan 0.1000 0.0322 ## 2 1.2256 nan 0.1000 0.0203 ## 3 1.1806 nan 0.1000 0.0216 ## 4 1.1444 nan 0.1000 0.0179 ## 5 1.1141 nan 0.1000 0.0147 ## 6 1.0890 nan 0.1000 0.0118 ## 7 1.0707 nan 0.1000 0.0085 ## 8 1.0493 nan 0.1000 0.0108 ## 9 1.0332 nan 0.1000 0.0069 ## 10 1.0157 nan 0.1000 0.0086 ## 20 0.9131 nan 0.1000 0.0039 ## 40 0.8200 nan 0.1000 0.0010 ## 60 0.7820 nan 0.1000 0.0004 ## 80 0.7654 nan 0.1000 -0.0014 ## 100 0.7561 nan 0.1000 -0.0009 ## 120 0.7476 nan 0.1000 -0.0011 ## 140 0.7412 nan 0.1000 -0.0003 ## 160 0.7365 nan 0.1000 -0.0004 ## 180 0.7340 nan 0.1000 -0.0005 ## 200 0.7299 nan 0.1000 -0.0007 ## 220 0.7266 nan 0.1000 -0.0008 ## 240 0.7237 nan 0.1000 -0.0008 ## 250 0.7234 nan 0.1000 -0.0005 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2651 nan 0.1000 0.0334 ## 2 1.2070 nan 0.1000 0.0269 ## 3 1.1557 nan 0.1000 0.0231 ## 4 1.1153 nan 0.1000 0.0171 ## 5 1.0815 nan 0.1000 0.0155 ## 6 1.0482 nan 0.1000 0.0152 ## 7 1.0172 nan 0.1000 0.0130 ## 8 0.9922 nan 0.1000 0.0112 ## 9 0.9735 nan 0.1000 0.0078 ## 10 0.9545 nan 0.1000 0.0083 ## 20 0.8345 nan 0.1000 0.0033 ## 40 0.7573 nan 0.1000 -0.0001 ## 60 0.7333 nan 0.1000 -0.0011 ## 80 0.7105 nan 0.1000 -0.0006 ## 100 0.6966 nan 0.1000 -0.0011 ## 120 0.6815 nan 0.1000 -0.0012 ## 140 0.6641 nan 0.1000 -0.0008 ## 160 0.6482 nan 0.1000 -0.0016 ## 180 0.6413 nan 0.1000 -0.0012 ## 200 0.6329 nan 0.1000 -0.0008 ## 220 0.6240 nan 0.1000 -0.0015 ## 240 0.6160 nan 0.1000 -0.0011 ## 250 0.6111 nan 0.1000 -0.0017 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2637 nan 0.1000 0.0352 ## 2 1.1913 nan 0.1000 0.0330 ## 3 1.1333 nan 0.1000 0.0254 ## 4 1.0844 nan 0.1000 0.0231 ## 5 1.0391 nan 0.1000 0.0195 ## 6 1.0021 nan 0.1000 0.0152 ## 7 0.9729 nan 0.1000 0.0118 ## 8 0.9513 nan 0.1000 0.0086 ## 9 0.9272 nan 0.1000 0.0096 ## 10 0.9064 nan 0.1000 0.0085 ## 20 0.7931 nan 0.1000 0.0041 ## 40 0.7237 nan 0.1000 -0.0004 ## 60 0.6981 nan 0.1000 -0.0012 ## 80 0.6718 nan 0.1000 -0.0026 ## 100 0.6529 nan 0.1000 -0.0016 ## 120 0.6396 nan 0.1000 -0.0010 ## 140 0.6233 nan 0.1000 -0.0013 ## 160 0.6101 nan 0.1000 -0.0011 ## 180 0.5932 nan 0.1000 -0.0015 ## 200 0.5794 nan 0.1000 -0.0010 ## 220 0.5645 nan 0.1000 -0.0011 ## 240 0.5498 nan 0.1000 -0.0018 ## 250 0.5448 nan 0.1000 -0.0012 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2583 nan 0.1000 0.0380 ## 2 1.1835 nan 0.1000 0.0345 ## 3 1.1231 nan 0.1000 0.0268 ## 4 1.0730 nan 0.1000 0.0218 ## 5 1.0329 nan 0.1000 0.0173 ## 6 0.9952 nan 0.1000 0.0161 ## 7 0.9630 nan 0.1000 0.0130 ## 8 0.9348 nan 0.1000 0.0126 ## 9 0.9109 nan 0.1000 0.0108 ## 10 0.8940 nan 0.1000 0.0058 ## 20 0.7714 nan 0.1000 0.0009 ## 40 0.6915 nan 0.1000 -0.0022 ## 60 0.6527 nan 0.1000 -0.0006 ## 80 0.6232 nan 0.1000 -0.0011 ## 100 0.6008 nan 0.1000 -0.0018 ## 120 0.5783 nan 0.1000 -0.0017 ## 140 0.5562 nan 0.1000 -0.0023 ## 160 0.5375 nan 0.1000 -0.0014 ## 180 0.5165 nan 0.1000 -0.0019 ## 200 0.5043 nan 0.1000 -0.0021 ## 220 0.4907 nan 0.1000 -0.0022 ## 240 0.4783 nan 0.1000 -0.0019 ## 250 0.4728 nan 0.1000 -0.0022 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2435 nan 0.1000 0.0421 ## 2 1.1748 nan 0.1000 0.0315 ## 3 1.1147 nan 0.1000 0.0294 ## 4 1.0672 nan 0.1000 0.0236 ## 5 1.0265 nan 0.1000 0.0188 ## 6 0.9873 nan 0.1000 0.0165 ## 7 0.9555 nan 0.1000 0.0151 ## 8 0.9289 nan 0.1000 0.0100 ## 9 0.9030 nan 0.1000 0.0085 ## 10 0.8836 nan 0.1000 0.0077 ## 20 0.7615 nan 0.1000 0.0016 ## 40 0.6861 nan 0.1000 -0.0016 ## 60 0.6393 nan 0.1000 -0.0017 ## 80 0.6022 nan 0.1000 -0.0021 ## 100 0.5787 nan 0.1000 -0.0023 ## 120 0.5532 nan 0.1000 -0.0024 ## 140 0.5303 nan 0.1000 -0.0017 ## 160 0.5079 nan 0.1000 -0.0015 ## 180 0.4894 nan 0.1000 -0.0017 ## 200 0.4716 nan 0.1000 -0.0019 ## 220 0.4537 nan 0.1000 -0.0017 ## 240 0.4389 nan 0.1000 -0.0007 ## 250 0.4327 nan 0.1000 -0.0013 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2786 nan 0.1000 0.0297 ## 2 1.2244 nan 0.1000 0.0249 ## 3 1.1873 nan 0.1000 0.0203 ## 4 1.1524 nan 0.1000 0.0140 ## 5 1.1216 nan 0.1000 0.0141 ## 6 1.0961 nan 0.1000 0.0125 ## 7 1.0730 nan 0.1000 0.0103 ## 8 1.0536 nan 0.1000 0.0086 ## 9 1.0376 nan 0.1000 0.0067 ## 10 1.0208 nan 0.1000 0.0083 ## 20 0.9191 nan 0.1000 0.0025 ## 40 0.8304 nan 0.1000 0.0006 ## 60 0.7967 nan 0.1000 -0.0009 ## 80 0.7809 nan 0.1000 -0.0007 ## 100 0.7724 nan 0.1000 -0.0012 ## 120 0.7661 nan 0.1000 -0.0004 ## 140 0.7613 nan 0.1000 -0.0012 ## 160 0.7558 nan 0.1000 -0.0004 ## 180 0.7511 nan 0.1000 -0.0004 ## 200 0.7460 nan 0.1000 -0.0006 ## 220 0.7421 nan 0.1000 -0.0013 ## 240 0.7388 nan 0.1000 -0.0007 ## 250 0.7365 nan 0.1000 -0.0009 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2662 nan 0.1000 0.0324 ## 2 1.2096 nan 0.1000 0.0223 ## 3 1.1565 nan 0.1000 0.0234 ## 4 1.1199 nan 0.1000 0.0184 ## 5 1.0844 nan 0.1000 0.0144 ## 6 1.0520 nan 0.1000 0.0153 ## 7 1.0254 nan 0.1000 0.0130 ## 8 1.0023 nan 0.1000 0.0091 ## 9 0.9789 nan 0.1000 0.0100 ## 10 0.9608 nan 0.1000 0.0074 ## 20 0.8450 nan 0.1000 0.0015 ## 40 0.7728 nan 0.1000 -0.0010 ## 60 0.7478 nan 0.1000 -0.0011 ## 80 0.7312 nan 0.1000 -0.0005 ## 100 0.7193 nan 0.1000 -0.0008 ## 120 0.7053 nan 0.1000 -0.0015 ## 140 0.6952 nan 0.1000 -0.0006 ## 160 0.6855 nan 0.1000 -0.0007 ## 180 0.6758 nan 0.1000 -0.0013 ## 200 0.6663 nan 0.1000 -0.0011 ## 220 0.6599 nan 0.1000 -0.0008 ## 240 0.6511 nan 0.1000 -0.0015 ## 250 0.6471 nan 0.1000 -0.0012 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2605 nan 0.1000 0.0359 ## 2 1.1990 nan 0.1000 0.0295 ## 3 1.1393 nan 0.1000 0.0250 ## 4 1.0933 nan 0.1000 0.0198 ## 5 1.0564 nan 0.1000 0.0142 ## 6 1.0189 nan 0.1000 0.0153 ## 7 0.9922 nan 0.1000 0.0114 ## 8 0.9677 nan 0.1000 0.0095 ## 9 0.9451 nan 0.1000 0.0105 ## 10 0.9247 nan 0.1000 0.0070 ## 20 0.8103 nan 0.1000 0.0028 ## 40 0.7386 nan 0.1000 -0.0008 ## 60 0.7043 nan 0.1000 -0.0029 ## 80 0.6768 nan 0.1000 -0.0006 ## 100 0.6509 nan 0.1000 -0.0012 ## 120 0.6340 nan 0.1000 -0.0015 ## 140 0.6159 nan 0.1000 -0.0020 ## 160 0.6042 nan 0.1000 -0.0015 ## 180 0.5892 nan 0.1000 -0.0009 ## 200 0.5800 nan 0.1000 -0.0015 ## 220 0.5693 nan 0.1000 -0.0015 ## 240 0.5583 nan 0.1000 -0.0014 ## 250 0.5545 nan 0.1000 -0.0009 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2535 nan 0.1000 0.0373 ## 2 1.1854 nan 0.1000 0.0315 ## 3 1.1321 nan 0.1000 0.0243 ## 4 1.0809 nan 0.1000 0.0232 ## 5 1.0425 nan 0.1000 0.0193 ## 6 1.0039 nan 0.1000 0.0155 ## 7 0.9741 nan 0.1000 0.0124 ## 8 0.9470 nan 0.1000 0.0120 ## 9 0.9198 nan 0.1000 0.0119 ## 10 0.9026 nan 0.1000 0.0068 ## 20 0.7913 nan 0.1000 0.0012 ## 40 0.7186 nan 0.1000 -0.0003 ## 60 0.6837 nan 0.1000 -0.0016 ## 80 0.6483 nan 0.1000 -0.0009 ## 100 0.6236 nan 0.1000 -0.0014 ## 120 0.5982 nan 0.1000 -0.0024 ## 140 0.5790 nan 0.1000 -0.0015 ## 160 0.5596 nan 0.1000 -0.0035 ## 180 0.5411 nan 0.1000 -0.0007 ## 200 0.5267 nan 0.1000 -0.0020 ## 220 0.5139 nan 0.1000 -0.0012 ## 240 0.4973 nan 0.1000 -0.0021 ## 250 0.4919 nan 0.1000 -0.0009 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2535 nan 0.1000 0.0388 ## 2 1.1840 nan 0.1000 0.0318 ## 3 1.1232 nan 0.1000 0.0262 ## 4 1.0698 nan 0.1000 0.0244 ## 5 1.0285 nan 0.1000 0.0185 ## 6 0.9955 nan 0.1000 0.0134 ## 7 0.9661 nan 0.1000 0.0146 ## 8 0.9362 nan 0.1000 0.0139 ## 9 0.9138 nan 0.1000 0.0090 ## 10 0.8958 nan 0.1000 0.0069 ## 20 0.7770 nan 0.1000 0.0004 ## 40 0.7015 nan 0.1000 -0.0011 ## 60 0.6610 nan 0.1000 -0.0024 ## 80 0.6300 nan 0.1000 -0.0035 ## 100 0.5995 nan 0.1000 -0.0016 ## 120 0.5673 nan 0.1000 -0.0024 ## 140 0.5459 nan 0.1000 -0.0038 ## 160 0.5245 nan 0.1000 -0.0029 ## 180 0.5053 nan 0.1000 -0.0016 ## 200 0.4902 nan 0.1000 -0.0012 ## 220 0.4748 nan 0.1000 -0.0026 ## 240 0.4577 nan 0.1000 -0.0022 ## 250 0.4513 nan 0.1000 -0.0027 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2725 nan 0.1000 0.0315 ## 2 1.2219 nan 0.1000 0.0246 ## 3 1.1809 nan 0.1000 0.0195 ## 4 1.1501 nan 0.1000 0.0136 ## 5 1.1172 nan 0.1000 0.0151 ## 6 1.0930 nan 0.1000 0.0122 ## 7 1.0669 nan 0.1000 0.0116 ## 8 1.0449 nan 0.1000 0.0093 ## 9 1.0285 nan 0.1000 0.0066 ## 10 1.0180 nan 0.1000 0.0038 ## 20 0.9148 nan 0.1000 0.0024 ## 40 0.8255 nan 0.1000 -0.0002 ## 60 0.7874 nan 0.1000 0.0005 ## 80 0.7699 nan 0.1000 -0.0002 ## 100 0.7627 nan 0.1000 -0.0013 ## 120 0.7552 nan 0.1000 -0.0003 ## 140 0.7490 nan 0.1000 -0.0009 ## 160 0.7430 nan 0.1000 -0.0010 ## 180 0.7382 nan 0.1000 -0.0008 ## 200 0.7349 nan 0.1000 -0.0005 ## 220 0.7314 nan 0.1000 -0.0013 ## 240 0.7268 nan 0.1000 -0.0007 ## 250 0.7253 nan 0.1000 -0.0006 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2623 nan 0.1000 0.0345 ## 2 1.2026 nan 0.1000 0.0294 ## 3 1.1548 nan 0.1000 0.0218 ## 4 1.1129 nan 0.1000 0.0177 ## 5 1.0812 nan 0.1000 0.0157 ## 6 1.0509 nan 0.1000 0.0147 ## 7 1.0280 nan 0.1000 0.0099 ## 8 1.0037 nan 0.1000 0.0104 ## 9 0.9810 nan 0.1000 0.0097 ## 10 0.9609 nan 0.1000 0.0089 ## 20 0.8459 nan 0.1000 0.0019 ## 40 0.7693 nan 0.1000 -0.0002 ## 60 0.7363 nan 0.1000 -0.0008 ## 80 0.7162 nan 0.1000 -0.0023 ## 100 0.7052 nan 0.1000 -0.0015 ## 120 0.6917 nan 0.1000 -0.0028 ## 140 0.6756 nan 0.1000 -0.0020 ## 160 0.6652 nan 0.1000 -0.0003 ## 180 0.6557 nan 0.1000 -0.0014 ## 200 0.6468 nan 0.1000 -0.0006 ## 220 0.6353 nan 0.1000 -0.0006 ## 240 0.6277 nan 0.1000 -0.0007 ## 250 0.6244 nan 0.1000 -0.0011 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2566 nan 0.1000 0.0386 ## 2 1.1937 nan 0.1000 0.0297 ## 3 1.1355 nan 0.1000 0.0258 ## 4 1.0909 nan 0.1000 0.0201 ## 5 1.0509 nan 0.1000 0.0179 ## 6 1.0145 nan 0.1000 0.0162 ## 7 0.9840 nan 0.1000 0.0123 ## 8 0.9551 nan 0.1000 0.0122 ## 9 0.9369 nan 0.1000 0.0088 ## 10 0.9164 nan 0.1000 0.0072 ## 20 0.8017 nan 0.1000 0.0007 ## 40 0.7255 nan 0.1000 -0.0002 ## 60 0.6940 nan 0.1000 -0.0009 ## 80 0.6684 nan 0.1000 -0.0008 ## 100 0.6504 nan 0.1000 -0.0011 ## 120 0.6325 nan 0.1000 -0.0013 ## 140 0.6140 nan 0.1000 -0.0013 ## 160 0.5974 nan 0.1000 -0.0002 ## 180 0.5833 nan 0.1000 -0.0010 ## 200 0.5666 nan 0.1000 -0.0011 ## 220 0.5550 nan 0.1000 -0.0020 ## 240 0.5451 nan 0.1000 -0.0023 ## 250 0.5372 nan 0.1000 -0.0015 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2498 nan 0.1000 0.0417 ## 2 1.1832 nan 0.1000 0.0281 ## 3 1.1271 nan 0.1000 0.0244 ## 4 1.0777 nan 0.1000 0.0218 ## 5 1.0407 nan 0.1000 0.0151 ## 6 1.0058 nan 0.1000 0.0141 ## 7 0.9753 nan 0.1000 0.0148 ## 8 0.9445 nan 0.1000 0.0133 ## 9 0.9192 nan 0.1000 0.0115 ## 10 0.9036 nan 0.1000 0.0060 ## 20 0.7800 nan 0.1000 0.0017 ## 40 0.7027 nan 0.1000 -0.0018 ## 60 0.6571 nan 0.1000 -0.0015 ## 80 0.6285 nan 0.1000 -0.0028 ## 100 0.6088 nan 0.1000 -0.0028 ## 120 0.5816 nan 0.1000 -0.0033 ## 140 0.5649 nan 0.1000 -0.0020 ## 160 0.5430 nan 0.1000 -0.0012 ## 180 0.5294 nan 0.1000 -0.0016 ## 200 0.5109 nan 0.1000 -0.0021 ## 220 0.4943 nan 0.1000 -0.0027 ## 240 0.4793 nan 0.1000 -0.0010 ## 250 0.4732 nan 0.1000 -0.0016 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2514 nan 0.1000 0.0374 ## 2 1.1825 nan 0.1000 0.0323 ## 3 1.1249 nan 0.1000 0.0244 ## 4 1.0758 nan 0.1000 0.0248 ## 5 1.0288 nan 0.1000 0.0205 ## 6 0.9950 nan 0.1000 0.0151 ## 7 0.9607 nan 0.1000 0.0120 ## 8 0.9307 nan 0.1000 0.0132 ## 9 0.9071 nan 0.1000 0.0100 ## 10 0.8828 nan 0.1000 0.0094 ## 20 0.7598 nan 0.1000 0.0013 ## 40 0.6744 nan 0.1000 -0.0017 ## 60 0.6343 nan 0.1000 -0.0018 ## 80 0.5947 nan 0.1000 -0.0013 ## 100 0.5667 nan 0.1000 -0.0019 ## 120 0.5457 nan 0.1000 -0.0029 ## 140 0.5235 nan 0.1000 -0.0024 ## 160 0.5036 nan 0.1000 -0.0019 ## 180 0.4856 nan 0.1000 -0.0030 ## 200 0.4678 nan 0.1000 -0.0020 ## 220 0.4534 nan 0.1000 -0.0023 ## 240 0.4412 nan 0.1000 -0.0018 ## 250 0.4336 nan 0.1000 -0.0019 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2803 nan 0.1000 0.0300 ## 2 1.2282 nan 0.1000 0.0252 ## 3 1.1859 nan 0.1000 0.0212 ## 4 1.1515 nan 0.1000 0.0150 ## 5 1.1195 nan 0.1000 0.0146 ## 6 1.0928 nan 0.1000 0.0127 ## 7 1.0679 nan 0.1000 0.0107 ## 8 1.0480 nan 0.1000 0.0092 ## 9 1.0320 nan 0.1000 0.0078 ## 10 1.0185 nan 0.1000 0.0060 ## 20 0.9144 nan 0.1000 0.0035 ## 40 0.8227 nan 0.1000 0.0009 ## 60 0.7776 nan 0.1000 -0.0003 ## 80 0.7595 nan 0.1000 -0.0005 ## 100 0.7498 nan 0.1000 -0.0002 ## 120 0.7441 nan 0.1000 -0.0002 ## 140 0.7392 nan 0.1000 -0.0006 ## 160 0.7347 nan 0.1000 -0.0004 ## 180 0.7323 nan 0.1000 -0.0013 ## 200 0.7259 nan 0.1000 -0.0003 ## 220 0.7226 nan 0.1000 -0.0003 ## 240 0.7204 nan 0.1000 -0.0004 ## 250 0.7180 nan 0.1000 -0.0010 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2639 nan 0.1000 0.0365 ## 2 1.2041 nan 0.1000 0.0277 ## 3 1.1555 nan 0.1000 0.0224 ## 4 1.1135 nan 0.1000 0.0190 ## 5 1.0741 nan 0.1000 0.0174 ## 6 1.0444 nan 0.1000 0.0128 ## 7 1.0172 nan 0.1000 0.0120 ## 8 0.9928 nan 0.1000 0.0114 ## 9 0.9698 nan 0.1000 0.0105 ## 10 0.9523 nan 0.1000 0.0071 ## 20 0.8290 nan 0.1000 0.0023 ## 40 0.7563 nan 0.1000 -0.0012 ## 60 0.7274 nan 0.1000 -0.0011 ## 80 0.7078 nan 0.1000 -0.0014 ## 100 0.6940 nan 0.1000 -0.0008 ## 120 0.6795 nan 0.1000 -0.0015 ## 140 0.6698 nan 0.1000 -0.0008 ## 160 0.6585 nan 0.1000 -0.0011 ## 180 0.6453 nan 0.1000 -0.0002 ## 200 0.6358 nan 0.1000 -0.0007 ## 220 0.6313 nan 0.1000 -0.0019 ## 240 0.6263 nan 0.1000 -0.0020 ## 250 0.6222 nan 0.1000 -0.0013 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2566 nan 0.1000 0.0352 ## 2 1.1939 nan 0.1000 0.0309 ## 3 1.1369 nan 0.1000 0.0252 ## 4 1.0871 nan 0.1000 0.0217 ## 5 1.0427 nan 0.1000 0.0188 ## 6 1.0083 nan 0.1000 0.0132 ## 7 0.9772 nan 0.1000 0.0138 ## 8 0.9496 nan 0.1000 0.0146 ## 9 0.9264 nan 0.1000 0.0103 ## 10 0.9027 nan 0.1000 0.0097 ## 20 0.7804 nan 0.1000 0.0026 ## 40 0.7150 nan 0.1000 -0.0014 ## 60 0.6814 nan 0.1000 -0.0023 ## 80 0.6575 nan 0.1000 -0.0021 ## 100 0.6395 nan 0.1000 -0.0021 ## 120 0.6198 nan 0.1000 -0.0014 ## 140 0.6030 nan 0.1000 -0.0016 ## 160 0.5852 nan 0.1000 -0.0013 ## 180 0.5747 nan 0.1000 -0.0017 ## 200 0.5616 nan 0.1000 -0.0014 ## 220 0.5461 nan 0.1000 -0.0019 ## 240 0.5350 nan 0.1000 -0.0025 ## 250 0.5289 nan 0.1000 -0.0008 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2508 nan 0.1000 0.0399 ## 2 1.1799 nan 0.1000 0.0327 ## 3 1.1202 nan 0.1000 0.0267 ## 4 1.0681 nan 0.1000 0.0230 ## 5 1.0267 nan 0.1000 0.0188 ## 6 0.9903 nan 0.1000 0.0144 ## 7 0.9579 nan 0.1000 0.0146 ## 8 0.9311 nan 0.1000 0.0122 ## 9 0.9093 nan 0.1000 0.0100 ## 10 0.8920 nan 0.1000 0.0071 ## 20 0.7669 nan 0.1000 0.0027 ## 40 0.6912 nan 0.1000 -0.0021 ## 60 0.6458 nan 0.1000 -0.0026 ## 80 0.6124 nan 0.1000 -0.0004 ## 100 0.5870 nan 0.1000 -0.0014 ## 120 0.5656 nan 0.1000 -0.0013 ## 140 0.5518 nan 0.1000 -0.0023 ## 160 0.5341 nan 0.1000 -0.0011 ## 180 0.5194 nan 0.1000 -0.0010 ## 200 0.5076 nan 0.1000 -0.0026 ## 220 0.4978 nan 0.1000 -0.0018 ## 240 0.4803 nan 0.1000 -0.0012 ## 250 0.4720 nan 0.1000 -0.0022 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2479 nan 0.1000 0.0415 ## 2 1.1762 nan 0.1000 0.0296 ## 3 1.1123 nan 0.1000 0.0268 ## 4 1.0657 nan 0.1000 0.0197 ## 5 1.0210 nan 0.1000 0.0199 ## 6 0.9815 nan 0.1000 0.0159 ## 7 0.9479 nan 0.1000 0.0105 ## 8 0.9197 nan 0.1000 0.0126 ## 9 0.8955 nan 0.1000 0.0088 ## 10 0.8742 nan 0.1000 0.0090 ## 20 0.7586 nan 0.1000 0.0012 ## 40 0.6843 nan 0.1000 -0.0018 ## 60 0.6355 nan 0.1000 -0.0027 ## 80 0.5939 nan 0.1000 -0.0014 ## 100 0.5648 nan 0.1000 -0.0014 ## 120 0.5434 nan 0.1000 -0.0016 ## 140 0.5223 nan 0.1000 -0.0021 ## 160 0.5026 nan 0.1000 -0.0021 ## 180 0.4889 nan 0.1000 -0.0021 ## 200 0.4758 nan 0.1000 -0.0012 ## 220 0.4520 nan 0.1000 -0.0008 ## 240 0.4417 nan 0.1000 -0.0027 ## 250 0.4358 nan 0.1000 -0.0023 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.2643 nan 0.1000 0.0332 ## 2 1.2100 nan 0.1000 0.0272 ## 3 1.1593 nan 0.1000 0.0240 ## 4 1.1180 nan 0.1000 0.0196 ## 5 1.0825 nan 0.1000 0.0170 ## 6 1.0508 nan 0.1000 0.0135 ## 7 1.0219 nan 0.1000 0.0112 ## 8 0.9984 nan 0.1000 0.0118 ## 9 0.9778 nan 0.1000 0.0072 ## 10 0.9565 nan 0.1000 0.0096 ## 20 0.8389 nan 0.1000 0.0025 ## 40 0.7624 nan 0.1000 -0.0009 ## 50 0.7453 nan 0.1000 -0.0006 oj.gbm ## Stochastic Gradient Boosting ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 772, 770, 771, 771, 772, 771, ... ## Resampling results across tuning parameters: ## ## interaction.depth n.trees ROC Sens Spec ## 1 50 0.89 0.87 0.72 ## 1 100 0.89 0.87 0.73 ## 1 150 0.89 0.87 0.73 ## 1 200 0.88 0.87 0.72 ## 1 250 0.88 0.86 0.73 ## 2 50 0.89 0.87 0.75 ## 2 100 0.88 0.86 0.73 ## 2 150 0.88 0.87 0.74 ## 2 200 0.88 0.86 0.72 ## 2 250 0.88 0.85 0.73 ## 3 50 0.88 0.87 0.72 ## 3 100 0.88 0.86 0.75 ## 3 150 0.87 0.85 0.72 ## 3 200 0.87 0.84 0.72 ## 3 250 0.87 0.83 0.72 ## 4 50 0.89 0.86 0.76 ## 4 100 0.88 0.84 0.73 ## 4 150 0.87 0.84 0.73 ## 4 200 0.87 0.85 0.72 ## 4 250 0.87 0.85 0.72 ## 5 50 0.89 0.86 0.76 ## 5 100 0.88 0.85 0.75 ## 5 150 0.87 0.84 0.75 ## 5 200 0.86 0.85 0.75 ## 5 250 0.86 0.84 0.73 ## ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## ## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10 ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were n.trees = 50, interaction.depth = ## 2, shrinkage = 0.1 and n.minobsinnode = 10. plot(oj.gbm) oj.pred &lt;- predict(oj.gbm, oj_test, type = &quot;raw&quot;) plot(oj_test$Purchase, oj.pred, main = &quot;Gradient Boosing Classification: Predicted vs. Actual&quot;, xlab = &quot;Actual&quot;, ylab = &quot;Predicted&quot;) (oj.conf &lt;- confusionMatrix(data = oj.pred, reference = oj_test$Purchase)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 117 18 ## MM 13 65 ## ## Accuracy : 0.854 ## 95% CI : (0.8, 0.899) ## No Information Rate : 0.61 ## P-Value [Acc &gt; NIR] : 0.00000000000000483 ## ## Kappa : 0.691 ## ## Mcnemar&#39;s Test P-Value : 0.472 ## ## Sensitivity : 0.900 ## Specificity : 0.783 ## Pos Pred Value : 0.867 ## Neg Pred Value : 0.833 ## Prevalence : 0.610 ## Detection Rate : 0.549 ## Detection Prevalence : 0.634 ## Balanced Accuracy : 0.842 ## ## &#39;Positive&#39; Class : CH ## oj.gbm.acc &lt;- as.numeric(oj.conf$overall[1]) rm(oj.pred) rm(oj.conf) #plot(oj.bag$, oj.bag$finalModel$y) #plot(varImp(oj.gbm), main=&quot;Variable Importance with Gradient Boosting&quot;) 10.5.0.2 Gradient Boosting Regression Example Again using the Carseats data set to predict Sales, this time I’ll use the gradient boosting method by specifying method = \"gbm\". I’ll use tuneLength = 5 and not worry about tuneGrid anymore. Caret tunes the following hyperparameters. n.trees: number of boosting iterations (increasing n.trees reduces the error on training set, but may lead to over-fitting) interaction.depth: maximum tree depth (the default six - node tree appears to do an excellent job) shrinkage: learning rate (reduces the impact of each additional fitted base-learner (tree) by reducing the size of incremental steps and thus penalizes the importance of each consecutive iteration. The intuition is that it is better to improve a model by taking many small steps than by taking fewer large steps. If one of the boosting iterations turns out to be erroneous, its negative impact can be easily corrected in subsequent steps.) n.minobsinnode: mimimum terminal node size carseats.gbm &lt;- train(Sales ~ ., data = carseats_train, method = &quot;gbm&quot;, # for bagged tree tuneLength = 5, # choose up to 5 combinations of tuning parameters metric = &quot;RMSE&quot;, # evaluate hyperparamter combinations with ROC trControl = trainControl( method = &quot;cv&quot;, # k-fold cross validation number = 10, # 10 folds savePredictions = &quot;final&quot;, # save predictions for the optimal tuning parameter1 verboseIter = FALSE, returnData = FALSE ) ) ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.6155 nan 0.1000 0.3126 ## 2 7.3677 nan 0.1000 0.2471 ## 3 7.1383 nan 0.1000 0.1559 ## 4 6.8796 nan 0.1000 0.3000 ## 5 6.6084 nan 0.1000 0.2696 ## 6 6.3846 nan 0.1000 0.1575 ## 7 6.1551 nan 0.1000 0.2016 ## 8 5.9837 nan 0.1000 0.1171 ## 9 5.7969 nan 0.1000 0.1558 ## 10 5.6503 nan 0.1000 0.1243 ## 20 4.5758 nan 0.1000 0.0472 ## 40 3.3276 nan 0.1000 0.0043 ## 60 2.6161 nan 0.1000 0.0154 ## 80 2.1215 nan 0.1000 -0.0029 ## 100 1.7822 nan 0.1000 -0.0166 ## 120 1.5354 nan 0.1000 -0.0016 ## 140 1.3313 nan 0.1000 0.0067 ## 160 1.2074 nan 0.1000 -0.0030 ## 180 1.0966 nan 0.1000 0.0005 ## 200 1.0083 nan 0.1000 -0.0019 ## 220 0.9572 nan 0.1000 -0.0008 ## 240 0.9048 nan 0.1000 -0.0052 ## 250 0.8922 nan 0.1000 -0.0051 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.4939 nan 0.1000 0.5303 ## 2 6.9609 nan 0.1000 0.4503 ## 3 6.5646 nan 0.1000 0.3312 ## 4 6.2135 nan 0.1000 0.2836 ## 5 5.9347 nan 0.1000 0.2472 ## 6 5.6654 nan 0.1000 0.2045 ## 7 5.3757 nan 0.1000 0.2134 ## 8 5.1883 nan 0.1000 0.1810 ## 9 5.0431 nan 0.1000 0.1205 ## 10 4.8440 nan 0.1000 0.0749 ## 20 3.4421 nan 0.1000 0.1291 ## 40 2.0285 nan 0.1000 0.0246 ## 60 1.4136 nan 0.1000 0.0003 ## 80 1.0839 nan 0.1000 0.0003 ## 100 0.9011 nan 0.1000 0.0052 ## 120 0.8195 nan 0.1000 -0.0075 ## 140 0.7664 nan 0.1000 -0.0036 ## 160 0.7243 nan 0.1000 -0.0010 ## 180 0.6839 nan 0.1000 -0.0028 ## 200 0.6448 nan 0.1000 -0.0048 ## 220 0.6123 nan 0.1000 -0.0035 ## 240 0.5897 nan 0.1000 -0.0038 ## 250 0.5767 nan 0.1000 -0.0050 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.3122 nan 0.1000 0.5956 ## 2 6.7737 nan 0.1000 0.4295 ## 3 6.3033 nan 0.1000 0.4215 ## 4 5.9081 nan 0.1000 0.2966 ## 5 5.5342 nan 0.1000 0.3320 ## 6 5.1666 nan 0.1000 0.2748 ## 7 4.9365 nan 0.1000 0.1688 ## 8 4.6557 nan 0.1000 0.2144 ## 9 4.4412 nan 0.1000 0.1072 ## 10 4.2075 nan 0.1000 0.1861 ## 20 2.7722 nan 0.1000 0.0107 ## 40 1.4659 nan 0.1000 0.0203 ## 60 1.0163 nan 0.1000 -0.0055 ## 80 0.8430 nan 0.1000 -0.0074 ## 100 0.7427 nan 0.1000 -0.0031 ## 120 0.6731 nan 0.1000 -0.0078 ## 140 0.6151 nan 0.1000 -0.0080 ## 160 0.5814 nan 0.1000 -0.0074 ## 180 0.5452 nan 0.1000 -0.0054 ## 200 0.5023 nan 0.1000 -0.0071 ## 220 0.4697 nan 0.1000 -0.0058 ## 240 0.4340 nan 0.1000 -0.0022 ## 250 0.4207 nan 0.1000 -0.0088 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.3508 nan 0.1000 0.7012 ## 2 6.7011 nan 0.1000 0.5533 ## 3 6.1585 nan 0.1000 0.5138 ## 4 5.7274 nan 0.1000 0.3542 ## 5 5.2438 nan 0.1000 0.3597 ## 6 4.9357 nan 0.1000 0.2545 ## 7 4.6359 nan 0.1000 0.2195 ## 8 4.3960 nan 0.1000 0.2294 ## 9 4.1786 nan 0.1000 0.1539 ## 10 3.9850 nan 0.1000 0.1414 ## 20 2.4927 nan 0.1000 0.0532 ## 40 1.2882 nan 0.1000 0.0112 ## 60 0.8551 nan 0.1000 -0.0074 ## 80 0.6752 nan 0.1000 -0.0044 ## 100 0.5795 nan 0.1000 -0.0097 ## 120 0.4983 nan 0.1000 -0.0038 ## 140 0.4440 nan 0.1000 -0.0032 ## 160 0.4044 nan 0.1000 -0.0056 ## 180 0.3666 nan 0.1000 -0.0076 ## 200 0.3337 nan 0.1000 -0.0065 ## 220 0.3066 nan 0.1000 -0.0022 ## 240 0.2736 nan 0.1000 -0.0039 ## 250 0.2608 nan 0.1000 -0.0038 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.2719 nan 0.1000 0.7029 ## 2 6.5623 nan 0.1000 0.5180 ## 3 6.0426 nan 0.1000 0.4144 ## 4 5.5613 nan 0.1000 0.4419 ## 5 5.1070 nan 0.1000 0.2878 ## 6 4.7641 nan 0.1000 0.2383 ## 7 4.4677 nan 0.1000 0.2141 ## 8 4.2399 nan 0.1000 0.0706 ## 9 3.9751 nan 0.1000 0.1927 ## 10 3.7100 nan 0.1000 0.1701 ## 20 2.2565 nan 0.1000 0.0859 ## 40 1.1000 nan 0.1000 -0.0080 ## 60 0.7464 nan 0.1000 -0.0020 ## 80 0.5790 nan 0.1000 -0.0024 ## 100 0.4941 nan 0.1000 -0.0054 ## 120 0.4290 nan 0.1000 -0.0030 ## 140 0.3723 nan 0.1000 -0.0052 ## 160 0.3330 nan 0.1000 -0.0042 ## 180 0.2951 nan 0.1000 -0.0056 ## 200 0.2627 nan 0.1000 -0.0038 ## 220 0.2336 nan 0.1000 -0.0018 ## 240 0.2089 nan 0.1000 -0.0017 ## 250 0.1964 nan 0.1000 -0.0029 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.8758 nan 0.1000 0.1611 ## 2 7.4638 nan 0.1000 0.4117 ## 3 7.1509 nan 0.1000 0.2868 ## 4 6.8550 nan 0.1000 0.2921 ## 5 6.6144 nan 0.1000 0.2482 ## 6 6.4519 nan 0.1000 0.1654 ## 7 6.3108 nan 0.1000 0.0792 ## 8 6.1631 nan 0.1000 0.1260 ## 9 6.0160 nan 0.1000 0.1119 ## 10 5.8043 nan 0.1000 0.1567 ## 20 4.6275 nan 0.1000 0.0715 ## 40 3.4435 nan 0.1000 -0.0080 ## 60 2.6905 nan 0.1000 0.0048 ## 80 2.1544 nan 0.1000 0.0127 ## 100 1.7772 nan 0.1000 -0.0062 ## 120 1.4927 nan 0.1000 -0.0058 ## 140 1.3013 nan 0.1000 0.0010 ## 160 1.1609 nan 0.1000 0.0023 ## 180 1.0670 nan 0.1000 -0.0058 ## 200 0.9890 nan 0.1000 -0.0088 ## 220 0.9407 nan 0.1000 0.0000 ## 240 0.9016 nan 0.1000 -0.0042 ## 250 0.8853 nan 0.1000 -0.0037 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.6412 nan 0.1000 0.5298 ## 2 7.0819 nan 0.1000 0.4917 ## 3 6.5521 nan 0.1000 0.3297 ## 4 6.1791 nan 0.1000 0.2744 ## 5 5.9412 nan 0.1000 0.1842 ## 6 5.6434 nan 0.1000 0.2504 ## 7 5.3703 nan 0.1000 0.2167 ## 8 5.1224 nan 0.1000 0.1739 ## 9 4.9715 nan 0.1000 0.1184 ## 10 4.7654 nan 0.1000 0.1615 ## 20 3.3795 nan 0.1000 0.0636 ## 40 2.0395 nan 0.1000 0.0080 ## 60 1.4605 nan 0.1000 -0.0029 ## 80 1.1344 nan 0.1000 -0.0025 ## 100 0.9495 nan 0.1000 -0.0087 ## 120 0.8562 nan 0.1000 -0.0037 ## 140 0.7855 nan 0.1000 -0.0043 ## 160 0.7298 nan 0.1000 -0.0057 ## 180 0.6813 nan 0.1000 -0.0010 ## 200 0.6441 nan 0.1000 -0.0027 ## 220 0.6118 nan 0.1000 -0.0040 ## 240 0.5838 nan 0.1000 -0.0071 ## 250 0.5734 nan 0.1000 -0.0007 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.4922 nan 0.1000 0.6088 ## 2 6.9029 nan 0.1000 0.5644 ## 3 6.4669 nan 0.1000 0.4526 ## 4 5.9980 nan 0.1000 0.4160 ## 5 5.5505 nan 0.1000 0.2603 ## 6 5.2643 nan 0.1000 0.2640 ## 7 5.0169 nan 0.1000 0.1944 ## 8 4.8024 nan 0.1000 0.1732 ## 9 4.5720 nan 0.1000 0.1124 ## 10 4.3508 nan 0.1000 0.1700 ## 20 2.8015 nan 0.1000 0.0839 ## 40 1.5058 nan 0.1000 0.0041 ## 60 1.0433 nan 0.1000 -0.0028 ## 80 0.8409 nan 0.1000 -0.0038 ## 100 0.7262 nan 0.1000 -0.0091 ## 120 0.6504 nan 0.1000 -0.0011 ## 140 0.5944 nan 0.1000 -0.0082 ## 160 0.5390 nan 0.1000 -0.0053 ## 180 0.5004 nan 0.1000 -0.0058 ## 200 0.4655 nan 0.1000 -0.0034 ## 220 0.4306 nan 0.1000 -0.0051 ## 240 0.4014 nan 0.1000 -0.0052 ## 250 0.3922 nan 0.1000 -0.0045 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.5384 nan 0.1000 0.6216 ## 2 6.8492 nan 0.1000 0.5668 ## 3 6.2425 nan 0.1000 0.4236 ## 4 5.7761 nan 0.1000 0.4459 ## 5 5.3374 nan 0.1000 0.3815 ## 6 5.0229 nan 0.1000 0.2852 ## 7 4.7392 nan 0.1000 0.2818 ## 8 4.4323 nan 0.1000 0.1682 ## 9 4.2010 nan 0.1000 0.2173 ## 10 3.9190 nan 0.1000 0.1539 ## 20 2.3973 nan 0.1000 0.0126 ## 40 1.2163 nan 0.1000 0.0078 ## 60 0.8634 nan 0.1000 -0.0088 ## 80 0.7010 nan 0.1000 -0.0174 ## 100 0.6125 nan 0.1000 -0.0047 ## 120 0.5409 nan 0.1000 -0.0043 ## 140 0.4880 nan 0.1000 -0.0087 ## 160 0.4416 nan 0.1000 -0.0077 ## 180 0.4022 nan 0.1000 -0.0087 ## 200 0.3700 nan 0.1000 -0.0066 ## 220 0.3326 nan 0.1000 -0.0048 ## 240 0.3010 nan 0.1000 -0.0020 ## 250 0.2897 nan 0.1000 -0.0057 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.4641 nan 0.1000 0.6228 ## 2 6.7842 nan 0.1000 0.6612 ## 3 6.2574 nan 0.1000 0.5100 ## 4 5.6693 nan 0.1000 0.5417 ## 5 5.2506 nan 0.1000 0.3664 ## 6 4.8195 nan 0.1000 0.3499 ## 7 4.4803 nan 0.1000 0.2959 ## 8 4.1807 nan 0.1000 0.1964 ## 9 3.9058 nan 0.1000 0.1460 ## 10 3.6831 nan 0.1000 0.1246 ## 20 2.1373 nan 0.1000 0.0659 ## 40 1.0923 nan 0.1000 0.0073 ## 60 0.7575 nan 0.1000 -0.0129 ## 80 0.6127 nan 0.1000 -0.0132 ## 100 0.5130 nan 0.1000 -0.0123 ## 120 0.4313 nan 0.1000 -0.0060 ## 140 0.3732 nan 0.1000 -0.0102 ## 160 0.3229 nan 0.1000 -0.0040 ## 180 0.2862 nan 0.1000 -0.0015 ## 200 0.2556 nan 0.1000 -0.0035 ## 220 0.2289 nan 0.1000 -0.0049 ## 240 0.2066 nan 0.1000 -0.0024 ## 250 0.1957 nan 0.1000 -0.0030 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 8.0133 nan 0.1000 0.2136 ## 2 7.6170 nan 0.1000 0.3813 ## 3 7.2757 nan 0.1000 0.3566 ## 4 6.9741 nan 0.1000 0.2780 ## 5 6.7490 nan 0.1000 0.2272 ## 6 6.6030 nan 0.1000 0.0955 ## 7 6.3914 nan 0.1000 0.1949 ## 8 6.2217 nan 0.1000 0.1345 ## 9 6.0156 nan 0.1000 0.1629 ## 10 5.8483 nan 0.1000 0.1217 ## 20 4.7672 nan 0.1000 0.0084 ## 40 3.5325 nan 0.1000 0.0072 ## 60 2.7373 nan 0.1000 0.0204 ## 80 2.2393 nan 0.1000 0.0177 ## 100 1.8533 nan 0.1000 -0.0056 ## 120 1.5588 nan 0.1000 -0.0006 ## 140 1.3684 nan 0.1000 -0.0059 ## 160 1.2137 nan 0.1000 -0.0029 ## 180 1.0929 nan 0.1000 0.0053 ## 200 1.0225 nan 0.1000 -0.0018 ## 220 0.9612 nan 0.1000 -0.0014 ## 240 0.9268 nan 0.1000 -0.0089 ## 250 0.9073 nan 0.1000 -0.0005 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.6769 nan 0.1000 0.4910 ## 2 7.1336 nan 0.1000 0.4624 ## 3 6.7130 nan 0.1000 0.3977 ## 4 6.3914 nan 0.1000 0.2512 ## 5 6.0813 nan 0.1000 0.3267 ## 6 5.8205 nan 0.1000 0.1912 ## 7 5.5575 nan 0.1000 0.1760 ## 8 5.3300 nan 0.1000 0.1566 ## 9 5.1406 nan 0.1000 0.0895 ## 10 4.9999 nan 0.1000 0.0993 ## 20 3.4990 nan 0.1000 0.1416 ## 40 2.1301 nan 0.1000 0.0205 ## 60 1.4553 nan 0.1000 -0.0010 ## 80 1.1386 nan 0.1000 -0.0129 ## 100 0.9532 nan 0.1000 -0.0062 ## 120 0.8461 nan 0.1000 -0.0020 ## 140 0.7941 nan 0.1000 -0.0103 ## 160 0.7479 nan 0.1000 -0.0069 ## 180 0.7076 nan 0.1000 -0.0046 ## 200 0.6764 nan 0.1000 -0.0034 ## 220 0.6375 nan 0.1000 -0.0039 ## 240 0.6118 nan 0.1000 -0.0084 ## 250 0.5967 nan 0.1000 -0.0038 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.5573 nan 0.1000 0.6889 ## 2 6.9522 nan 0.1000 0.5641 ## 3 6.4789 nan 0.1000 0.3647 ## 4 6.1178 nan 0.1000 0.1990 ## 5 5.7289 nan 0.1000 0.3137 ## 6 5.4295 nan 0.1000 0.1935 ## 7 5.0791 nan 0.1000 0.2922 ## 8 4.8212 nan 0.1000 0.2147 ## 9 4.5435 nan 0.1000 0.1970 ## 10 4.3324 nan 0.1000 0.1800 ## 20 2.8598 nan 0.1000 0.0779 ## 40 1.5947 nan 0.1000 0.0099 ## 60 1.0802 nan 0.1000 -0.0061 ## 80 0.8896 nan 0.1000 -0.0082 ## 100 0.7621 nan 0.1000 -0.0069 ## 120 0.6690 nan 0.1000 -0.0080 ## 140 0.6158 nan 0.1000 -0.0050 ## 160 0.5707 nan 0.1000 -0.0019 ## 180 0.5336 nan 0.1000 -0.0046 ## 200 0.4984 nan 0.1000 -0.0062 ## 220 0.4607 nan 0.1000 -0.0045 ## 240 0.4323 nan 0.1000 -0.0052 ## 250 0.4196 nan 0.1000 -0.0041 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.4906 nan 0.1000 0.6789 ## 2 6.9506 nan 0.1000 0.3624 ## 3 6.3953 nan 0.1000 0.4020 ## 4 5.9823 nan 0.1000 0.3891 ## 5 5.5966 nan 0.1000 0.2061 ## 6 5.1879 nan 0.1000 0.2865 ## 7 4.8319 nan 0.1000 0.2808 ## 8 4.5432 nan 0.1000 0.2153 ## 9 4.2635 nan 0.1000 0.2355 ## 10 3.9960 nan 0.1000 0.1091 ## 20 2.4108 nan 0.1000 0.1056 ## 40 1.2650 nan 0.1000 0.0002 ## 60 0.8721 nan 0.1000 0.0013 ## 80 0.6979 nan 0.1000 -0.0035 ## 100 0.5984 nan 0.1000 -0.0062 ## 120 0.5296 nan 0.1000 -0.0059 ## 140 0.4772 nan 0.1000 -0.0068 ## 160 0.4300 nan 0.1000 -0.0125 ## 180 0.3866 nan 0.1000 -0.0057 ## 200 0.3463 nan 0.1000 -0.0064 ## 220 0.3136 nan 0.1000 -0.0027 ## 240 0.2853 nan 0.1000 -0.0041 ## 250 0.2719 nan 0.1000 -0.0052 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.4823 nan 0.1000 0.6726 ## 2 6.8225 nan 0.1000 0.6054 ## 3 6.2303 nan 0.1000 0.4431 ## 4 5.7262 nan 0.1000 0.3136 ## 5 5.2917 nan 0.1000 0.3020 ## 6 4.9481 nan 0.1000 0.2905 ## 7 4.5620 nan 0.1000 0.2910 ## 8 4.3225 nan 0.1000 0.1791 ## 9 4.0699 nan 0.1000 0.2349 ## 10 3.8179 nan 0.1000 0.1849 ## 20 2.3059 nan 0.1000 0.0418 ## 40 1.1688 nan 0.1000 -0.0035 ## 60 0.7851 nan 0.1000 0.0072 ## 80 0.6170 nan 0.1000 -0.0038 ## 100 0.5177 nan 0.1000 -0.0026 ## 120 0.4381 nan 0.1000 -0.0056 ## 140 0.3801 nan 0.1000 -0.0035 ## 160 0.3340 nan 0.1000 -0.0031 ## 180 0.2933 nan 0.1000 -0.0080 ## 200 0.2558 nan 0.1000 -0.0058 ## 220 0.2289 nan 0.1000 -0.0028 ## 240 0.2021 nan 0.1000 -0.0050 ## 250 0.1882 nan 0.1000 -0.0018 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.4102 nan 0.1000 0.3839 ## 2 7.0494 nan 0.1000 0.3040 ## 3 6.8614 nan 0.1000 0.1707 ## 4 6.6267 nan 0.1000 0.2365 ## 5 6.4159 nan 0.1000 0.2192 ## 6 6.2072 nan 0.1000 0.1818 ## 7 5.9995 nan 0.1000 0.1441 ## 8 5.8238 nan 0.1000 0.1470 ## 9 5.6784 nan 0.1000 0.0881 ## 10 5.5111 nan 0.1000 0.1191 ## 20 4.4453 nan 0.1000 0.0629 ## 40 3.2797 nan 0.1000 0.0303 ## 60 2.5893 nan 0.1000 0.0080 ## 80 2.1025 nan 0.1000 -0.0054 ## 100 1.7328 nan 0.1000 0.0016 ## 120 1.5056 nan 0.1000 0.0039 ## 140 1.3304 nan 0.1000 -0.0021 ## 160 1.2081 nan 0.1000 -0.0085 ## 180 1.1043 nan 0.1000 0.0002 ## 200 1.0182 nan 0.1000 0.0011 ## 220 0.9519 nan 0.1000 -0.0138 ## 240 0.9161 nan 0.1000 -0.0036 ## 250 0.9019 nan 0.1000 -0.0080 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.3296 nan 0.1000 0.5238 ## 2 6.8236 nan 0.1000 0.4935 ## 3 6.4024 nan 0.1000 0.3814 ## 4 6.0460 nan 0.1000 0.3352 ## 5 5.7799 nan 0.1000 0.2079 ## 6 5.5073 nan 0.1000 0.2249 ## 7 5.2509 nan 0.1000 0.1626 ## 8 5.0841 nan 0.1000 0.0927 ## 9 4.8799 nan 0.1000 0.1436 ## 10 4.7477 nan 0.1000 0.0329 ## 20 3.3287 nan 0.1000 0.0705 ## 40 2.0283 nan 0.1000 0.0048 ## 60 1.3816 nan 0.1000 0.0072 ## 80 1.0728 nan 0.1000 -0.0016 ## 100 0.9094 nan 0.1000 0.0076 ## 120 0.8114 nan 0.1000 -0.0058 ## 140 0.7446 nan 0.1000 -0.0069 ## 160 0.7042 nan 0.1000 -0.0008 ## 180 0.6679 nan 0.1000 -0.0038 ## 200 0.6315 nan 0.1000 -0.0026 ## 220 0.6063 nan 0.1000 -0.0076 ## 240 0.5797 nan 0.1000 -0.0034 ## 250 0.5662 nan 0.1000 -0.0046 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.2648 nan 0.1000 0.6118 ## 2 6.7163 nan 0.1000 0.5338 ## 3 6.2474 nan 0.1000 0.4288 ## 4 5.8703 nan 0.1000 0.2640 ## 5 5.5532 nan 0.1000 0.3193 ## 6 5.2463 nan 0.1000 0.3198 ## 7 4.9416 nan 0.1000 0.1943 ## 8 4.6990 nan 0.1000 0.1722 ## 9 4.4949 nan 0.1000 0.1469 ## 10 4.2995 nan 0.1000 0.1804 ## 20 2.7721 nan 0.1000 0.0861 ## 40 1.4803 nan 0.1000 0.0123 ## 60 1.0423 nan 0.1000 -0.0095 ## 80 0.8424 nan 0.1000 0.0072 ## 100 0.7137 nan 0.1000 -0.0079 ## 120 0.6440 nan 0.1000 -0.0074 ## 140 0.5827 nan 0.1000 -0.0044 ## 160 0.5432 nan 0.1000 -0.0097 ## 180 0.5079 nan 0.1000 -0.0044 ## 200 0.4749 nan 0.1000 -0.0006 ## 220 0.4448 nan 0.1000 -0.0068 ## 240 0.4197 nan 0.1000 -0.0041 ## 250 0.4073 nan 0.1000 -0.0040 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.1301 nan 0.1000 0.6518 ## 2 6.4611 nan 0.1000 0.5540 ## 3 5.8979 nan 0.1000 0.4264 ## 4 5.4531 nan 0.1000 0.3997 ## 5 5.0649 nan 0.1000 0.2551 ## 6 4.7495 nan 0.1000 0.2099 ## 7 4.4799 nan 0.1000 0.2317 ## 8 4.1899 nan 0.1000 0.2269 ## 9 4.0084 nan 0.1000 0.0679 ## 10 3.8183 nan 0.1000 0.0965 ## 20 2.3118 nan 0.1000 0.0409 ## 40 1.2467 nan 0.1000 0.0054 ## 60 0.8880 nan 0.1000 -0.0007 ## 80 0.7226 nan 0.1000 -0.0048 ## 100 0.6209 nan 0.1000 -0.0051 ## 120 0.5444 nan 0.1000 -0.0087 ## 140 0.4935 nan 0.1000 -0.0049 ## 160 0.4445 nan 0.1000 -0.0053 ## 180 0.3989 nan 0.1000 -0.0063 ## 200 0.3638 nan 0.1000 -0.0079 ## 220 0.3337 nan 0.1000 -0.0052 ## 240 0.3055 nan 0.1000 -0.0048 ## 250 0.2926 nan 0.1000 -0.0020 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.1126 nan 0.1000 0.7800 ## 2 6.4349 nan 0.1000 0.5736 ## 3 5.8711 nan 0.1000 0.5557 ## 4 5.4355 nan 0.1000 0.3727 ## 5 5.0058 nan 0.1000 0.2490 ## 6 4.5813 nan 0.1000 0.2892 ## 7 4.2200 nan 0.1000 0.3013 ## 8 3.8990 nan 0.1000 0.2893 ## 9 3.6677 nan 0.1000 0.1964 ## 10 3.4290 nan 0.1000 0.1762 ## 20 2.1135 nan 0.1000 0.0418 ## 40 1.0761 nan 0.1000 -0.0015 ## 60 0.7465 nan 0.1000 -0.0119 ## 80 0.6002 nan 0.1000 -0.0082 ## 100 0.4915 nan 0.1000 -0.0042 ## 120 0.4217 nan 0.1000 -0.0055 ## 140 0.3558 nan 0.1000 -0.0027 ## 160 0.3103 nan 0.1000 -0.0039 ## 180 0.2649 nan 0.1000 -0.0052 ## 200 0.2323 nan 0.1000 -0.0017 ## 220 0.2076 nan 0.1000 -0.0018 ## 240 0.1858 nan 0.1000 -0.0025 ## 250 0.1740 nan 0.1000 -0.0033 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.3456 nan 0.1000 0.2909 ## 2 7.1114 nan 0.1000 0.1263 ## 3 6.7570 nan 0.1000 0.2751 ## 4 6.5482 nan 0.1000 0.1337 ## 5 6.3257 nan 0.1000 0.2345 ## 6 6.1206 nan 0.1000 0.1383 ## 7 5.9301 nan 0.1000 0.1914 ## 8 5.7797 nan 0.1000 0.1295 ## 9 5.6019 nan 0.1000 0.1135 ## 10 5.4685 nan 0.1000 0.0847 ## 20 4.4722 nan 0.1000 0.0460 ## 40 3.3338 nan 0.1000 0.0101 ## 60 2.6477 nan 0.1000 0.0048 ## 80 2.1648 nan 0.1000 0.0153 ## 100 1.7916 nan 0.1000 -0.0011 ## 120 1.5267 nan 0.1000 0.0018 ## 140 1.3281 nan 0.1000 -0.0029 ## 160 1.1995 nan 0.1000 -0.0011 ## 180 1.1018 nan 0.1000 0.0001 ## 200 1.0288 nan 0.1000 -0.0059 ## 220 0.9667 nan 0.1000 -0.0033 ## 240 0.9174 nan 0.1000 -0.0045 ## 250 0.8974 nan 0.1000 -0.0025 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.1892 nan 0.1000 0.5473 ## 2 6.7512 nan 0.1000 0.4586 ## 3 6.4426 nan 0.1000 0.2760 ## 4 6.1358 nan 0.1000 0.2140 ## 5 5.8079 nan 0.1000 0.2639 ## 6 5.5512 nan 0.1000 0.2127 ## 7 5.3613 nan 0.1000 0.1317 ## 8 5.0590 nan 0.1000 0.2354 ## 9 4.9117 nan 0.1000 0.1361 ## 10 4.7130 nan 0.1000 0.1626 ## 20 3.4020 nan 0.1000 0.0257 ## 40 2.0751 nan 0.1000 0.0224 ## 60 1.4101 nan 0.1000 -0.0005 ## 80 1.1014 nan 0.1000 0.0065 ## 100 0.9405 nan 0.1000 -0.0067 ## 120 0.8391 nan 0.1000 -0.0066 ## 140 0.7718 nan 0.1000 -0.0054 ## 160 0.7291 nan 0.1000 -0.0095 ## 180 0.6810 nan 0.1000 -0.0036 ## 200 0.6457 nan 0.1000 -0.0071 ## 220 0.6189 nan 0.1000 -0.0034 ## 240 0.5895 nan 0.1000 -0.0023 ## 250 0.5794 nan 0.1000 -0.0084 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.1359 nan 0.1000 0.6309 ## 2 6.5967 nan 0.1000 0.4591 ## 3 6.1698 nan 0.1000 0.4132 ## 4 5.7599 nan 0.1000 0.3928 ## 5 5.4519 nan 0.1000 0.3237 ## 6 5.1401 nan 0.1000 0.2872 ## 7 4.9050 nan 0.1000 0.1392 ## 8 4.6196 nan 0.1000 0.2846 ## 9 4.3738 nan 0.1000 0.1828 ## 10 4.1835 nan 0.1000 0.1700 ## 20 2.8099 nan 0.1000 0.0580 ## 40 1.6151 nan 0.1000 -0.0065 ## 60 1.1301 nan 0.1000 0.0002 ## 80 0.8944 nan 0.1000 -0.0077 ## 100 0.7517 nan 0.1000 -0.0139 ## 120 0.6730 nan 0.1000 -0.0085 ## 140 0.6057 nan 0.1000 -0.0038 ## 160 0.5547 nan 0.1000 -0.0097 ## 180 0.5094 nan 0.1000 -0.0103 ## 200 0.4766 nan 0.1000 -0.0066 ## 220 0.4450 nan 0.1000 -0.0040 ## 240 0.4151 nan 0.1000 -0.0027 ## 250 0.4013 nan 0.1000 -0.0047 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.0294 nan 0.1000 0.7373 ## 2 6.4316 nan 0.1000 0.6149 ## 3 5.9113 nan 0.1000 0.3359 ## 4 5.5546 nan 0.1000 0.3535 ## 5 5.1917 nan 0.1000 0.2802 ## 6 4.8371 nan 0.1000 0.3563 ## 7 4.5468 nan 0.1000 0.2089 ## 8 4.3248 nan 0.1000 0.1893 ## 9 4.0651 nan 0.1000 0.1052 ## 10 3.8138 nan 0.1000 0.1666 ## 20 2.3320 nan 0.1000 0.0422 ## 40 1.2332 nan 0.1000 0.0086 ## 60 0.8464 nan 0.1000 0.0058 ## 80 0.6737 nan 0.1000 -0.0128 ## 100 0.5934 nan 0.1000 -0.0097 ## 120 0.5242 nan 0.1000 -0.0050 ## 140 0.4705 nan 0.1000 -0.0050 ## 160 0.4233 nan 0.1000 -0.0078 ## 180 0.3867 nan 0.1000 -0.0076 ## 200 0.3531 nan 0.1000 -0.0040 ## 220 0.3242 nan 0.1000 -0.0039 ## 240 0.3013 nan 0.1000 -0.0036 ## 250 0.2843 nan 0.1000 -0.0029 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.0098 nan 0.1000 0.5845 ## 2 6.3343 nan 0.1000 0.4158 ## 3 5.8514 nan 0.1000 0.3832 ## 4 5.3437 nan 0.1000 0.4617 ## 5 4.9793 nan 0.1000 0.3545 ## 6 4.6111 nan 0.1000 0.3660 ## 7 4.2957 nan 0.1000 0.2518 ## 8 3.9570 nan 0.1000 0.2273 ## 9 3.7320 nan 0.1000 0.1854 ## 10 3.5140 nan 0.1000 0.1503 ## 20 2.1057 nan 0.1000 0.0764 ## 40 1.0592 nan 0.1000 0.0046 ## 60 0.7323 nan 0.1000 -0.0010 ## 80 0.5881 nan 0.1000 -0.0058 ## 100 0.4956 nan 0.1000 -0.0051 ## 120 0.4248 nan 0.1000 -0.0025 ## 140 0.3718 nan 0.1000 -0.0073 ## 160 0.3294 nan 0.1000 -0.0062 ## 180 0.2819 nan 0.1000 -0.0026 ## 200 0.2497 nan 0.1000 -0.0034 ## 220 0.2233 nan 0.1000 -0.0058 ## 240 0.1990 nan 0.1000 -0.0018 ## 250 0.1871 nan 0.1000 -0.0021 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.5868 nan 0.1000 0.4115 ## 2 7.2833 nan 0.1000 0.2331 ## 3 7.0345 nan 0.1000 0.2570 ## 4 6.8101 nan 0.1000 0.1071 ## 5 6.5893 nan 0.1000 0.1563 ## 6 6.3092 nan 0.1000 0.1664 ## 7 6.1025 nan 0.1000 0.1476 ## 8 5.9663 nan 0.1000 0.0749 ## 9 5.7474 nan 0.1000 0.1445 ## 10 5.5960 nan 0.1000 0.0876 ## 20 4.4901 nan 0.1000 0.0532 ## 40 3.2925 nan 0.1000 0.0364 ## 60 2.6190 nan 0.1000 0.0090 ## 80 2.1208 nan 0.1000 0.0132 ## 100 1.7732 nan 0.1000 -0.0007 ## 120 1.5132 nan 0.1000 0.0046 ## 140 1.3283 nan 0.1000 0.0031 ## 160 1.1925 nan 0.1000 -0.0002 ## 180 1.0847 nan 0.1000 -0.0013 ## 200 0.9981 nan 0.1000 -0.0003 ## 220 0.9475 nan 0.1000 -0.0035 ## 240 0.9021 nan 0.1000 -0.0047 ## 250 0.8843 nan 0.1000 -0.0029 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.3282 nan 0.1000 0.5172 ## 2 6.9512 nan 0.1000 0.3872 ## 3 6.5318 nan 0.1000 0.3539 ## 4 6.1978 nan 0.1000 0.2757 ## 5 5.9066 nan 0.1000 0.2460 ## 6 5.6366 nan 0.1000 0.2517 ## 7 5.3731 nan 0.1000 0.1878 ## 8 5.1982 nan 0.1000 0.1405 ## 9 5.0115 nan 0.1000 0.1659 ## 10 4.8495 nan 0.1000 0.1095 ## 20 3.3891 nan 0.1000 0.1033 ## 40 2.0749 nan 0.1000 0.0310 ## 60 1.4400 nan 0.1000 0.0118 ## 80 1.1141 nan 0.1000 0.0030 ## 100 0.9360 nan 0.1000 0.0034 ## 120 0.8234 nan 0.1000 -0.0051 ## 140 0.7648 nan 0.1000 -0.0025 ## 160 0.7108 nan 0.1000 -0.0075 ## 180 0.6698 nan 0.1000 -0.0052 ## 200 0.6366 nan 0.1000 -0.0069 ## 220 0.6020 nan 0.1000 -0.0060 ## 240 0.5770 nan 0.1000 -0.0049 ## 250 0.5640 nan 0.1000 -0.0035 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.2616 nan 0.1000 0.6773 ## 2 6.8094 nan 0.1000 0.2167 ## 3 6.3219 nan 0.1000 0.4713 ## 4 5.9148 nan 0.1000 0.3566 ## 5 5.5959 nan 0.1000 0.2337 ## 6 5.2724 nan 0.1000 0.3200 ## 7 4.9638 nan 0.1000 0.1900 ## 8 4.7283 nan 0.1000 0.1678 ## 9 4.5198 nan 0.1000 0.1260 ## 10 4.3024 nan 0.1000 0.1451 ## 20 2.7404 nan 0.1000 0.0379 ## 40 1.5392 nan 0.1000 -0.0055 ## 60 1.0399 nan 0.1000 0.0123 ## 80 0.8192 nan 0.1000 0.0004 ## 100 0.7016 nan 0.1000 -0.0083 ## 120 0.6325 nan 0.1000 -0.0080 ## 140 0.5744 nan 0.1000 -0.0063 ## 160 0.5259 nan 0.1000 -0.0054 ## 180 0.4819 nan 0.1000 -0.0111 ## 200 0.4502 nan 0.1000 -0.0055 ## 220 0.4164 nan 0.1000 -0.0031 ## 240 0.3874 nan 0.1000 -0.0056 ## 250 0.3749 nan 0.1000 -0.0012 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.4180 nan 0.1000 0.3827 ## 2 6.7805 nan 0.1000 0.4994 ## 3 6.2576 nan 0.1000 0.4550 ## 4 5.8389 nan 0.1000 0.2288 ## 5 5.4627 nan 0.1000 0.3687 ## 6 5.1119 nan 0.1000 0.3303 ## 7 4.7963 nan 0.1000 0.3248 ## 8 4.5143 nan 0.1000 0.2401 ## 9 4.2403 nan 0.1000 0.1986 ## 10 4.0630 nan 0.1000 0.1325 ## 20 2.4827 nan 0.1000 0.0652 ## 40 1.2766 nan 0.1000 0.0153 ## 60 0.8585 nan 0.1000 -0.0103 ## 80 0.6931 nan 0.1000 -0.0009 ## 100 0.5966 nan 0.1000 -0.0085 ## 120 0.5283 nan 0.1000 -0.0111 ## 140 0.4718 nan 0.1000 -0.0106 ## 160 0.4263 nan 0.1000 -0.0058 ## 180 0.3870 nan 0.1000 -0.0051 ## 200 0.3533 nan 0.1000 -0.0027 ## 220 0.3194 nan 0.1000 -0.0032 ## 240 0.2922 nan 0.1000 -0.0019 ## 250 0.2785 nan 0.1000 -0.0040 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.1462 nan 0.1000 0.6854 ## 2 6.5338 nan 0.1000 0.3961 ## 3 6.0020 nan 0.1000 0.4159 ## 4 5.5196 nan 0.1000 0.3879 ## 5 5.0940 nan 0.1000 0.3601 ## 6 4.7927 nan 0.1000 0.2963 ## 7 4.4249 nan 0.1000 0.2827 ## 8 4.1084 nan 0.1000 0.2133 ## 9 3.8253 nan 0.1000 0.1881 ## 10 3.6343 nan 0.1000 0.1233 ## 20 2.1522 nan 0.1000 0.0354 ## 40 1.0580 nan 0.1000 0.0161 ## 60 0.7191 nan 0.1000 0.0018 ## 80 0.5670 nan 0.1000 -0.0057 ## 100 0.4832 nan 0.1000 -0.0050 ## 120 0.4124 nan 0.1000 -0.0054 ## 140 0.3602 nan 0.1000 -0.0078 ## 160 0.3179 nan 0.1000 -0.0036 ## 180 0.2778 nan 0.1000 -0.0047 ## 200 0.2450 nan 0.1000 -0.0059 ## 220 0.2183 nan 0.1000 -0.0040 ## 240 0.1929 nan 0.1000 -0.0048 ## 250 0.1839 nan 0.1000 -0.0040 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.4540 nan 0.1000 0.3641 ## 2 7.1373 nan 0.1000 0.3291 ## 3 6.8416 nan 0.1000 0.2730 ## 4 6.5864 nan 0.1000 0.1830 ## 5 6.3867 nan 0.1000 0.1603 ## 6 6.1413 nan 0.1000 0.2019 ## 7 5.9610 nan 0.1000 0.1395 ## 8 5.8194 nan 0.1000 0.1027 ## 9 5.6546 nan 0.1000 0.0827 ## 10 5.4631 nan 0.1000 0.0971 ## 20 4.3922 nan 0.1000 0.0188 ## 40 3.2426 nan 0.1000 0.0345 ## 60 2.5668 nan 0.1000 0.0223 ## 80 2.0713 nan 0.1000 0.0088 ## 100 1.7438 nan 0.1000 -0.0065 ## 120 1.4921 nan 0.1000 0.0043 ## 140 1.3219 nan 0.1000 0.0085 ## 160 1.1951 nan 0.1000 0.0011 ## 180 1.1011 nan 0.1000 -0.0045 ## 200 1.0331 nan 0.1000 -0.0013 ## 220 0.9765 nan 0.1000 -0.0020 ## 240 0.9362 nan 0.1000 -0.0053 ## 250 0.9130 nan 0.1000 0.0009 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.2752 nan 0.1000 0.5651 ## 2 6.7895 nan 0.1000 0.4457 ## 3 6.4533 nan 0.1000 0.2484 ## 4 6.1292 nan 0.1000 0.3299 ## 5 5.8643 nan 0.1000 0.2757 ## 6 5.6456 nan 0.1000 0.1455 ## 7 5.3780 nan 0.1000 0.2465 ## 8 5.1363 nan 0.1000 0.1833 ## 9 4.9400 nan 0.1000 0.1011 ## 10 4.7344 nan 0.1000 0.1801 ## 20 3.2834 nan 0.1000 0.0459 ## 40 2.0454 nan 0.1000 0.0186 ## 60 1.4428 nan 0.1000 0.0008 ## 80 1.0931 nan 0.1000 0.0050 ## 100 0.9245 nan 0.1000 -0.0066 ## 120 0.8152 nan 0.1000 -0.0040 ## 140 0.7456 nan 0.1000 -0.0028 ## 160 0.6979 nan 0.1000 -0.0052 ## 180 0.6573 nan 0.1000 -0.0035 ## 200 0.6239 nan 0.1000 -0.0037 ## 220 0.5949 nan 0.1000 -0.0042 ## 240 0.5695 nan 0.1000 -0.0039 ## 250 0.5578 nan 0.1000 -0.0039 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.2004 nan 0.1000 0.5575 ## 2 6.6875 nan 0.1000 0.5080 ## 3 6.1988 nan 0.1000 0.4364 ## 4 5.7727 nan 0.1000 0.3191 ## 5 5.4082 nan 0.1000 0.3497 ## 6 5.1722 nan 0.1000 0.1589 ## 7 4.8592 nan 0.1000 0.1847 ## 8 4.6133 nan 0.1000 0.2159 ## 9 4.3824 nan 0.1000 0.2002 ## 10 4.1895 nan 0.1000 0.1182 ## 20 2.7390 nan 0.1000 0.0733 ## 40 1.5162 nan 0.1000 -0.0008 ## 60 1.0207 nan 0.1000 0.0026 ## 80 0.8292 nan 0.1000 -0.0016 ## 100 0.7057 nan 0.1000 -0.0089 ## 120 0.6346 nan 0.1000 -0.0085 ## 140 0.5675 nan 0.1000 -0.0056 ## 160 0.5176 nan 0.1000 -0.0074 ## 180 0.4752 nan 0.1000 -0.0050 ## 200 0.4333 nan 0.1000 -0.0054 ## 220 0.4032 nan 0.1000 -0.0070 ## 240 0.3743 nan 0.1000 -0.0022 ## 250 0.3642 nan 0.1000 -0.0062 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.1706 nan 0.1000 0.6334 ## 2 6.5472 nan 0.1000 0.5452 ## 3 5.9610 nan 0.1000 0.4332 ## 4 5.5223 nan 0.1000 0.3950 ## 5 5.1475 nan 0.1000 0.2716 ## 6 4.8185 nan 0.1000 0.2069 ## 7 4.5333 nan 0.1000 0.2125 ## 8 4.2886 nan 0.1000 0.2125 ## 9 4.0286 nan 0.1000 0.1507 ## 10 3.8317 nan 0.1000 0.1457 ## 20 2.3895 nan 0.1000 0.0665 ## 40 1.2552 nan 0.1000 0.0223 ## 60 0.8795 nan 0.1000 0.0039 ## 80 0.7069 nan 0.1000 -0.0107 ## 100 0.6120 nan 0.1000 -0.0067 ## 120 0.5331 nan 0.1000 -0.0065 ## 140 0.4731 nan 0.1000 -0.0091 ## 160 0.4256 nan 0.1000 -0.0038 ## 180 0.3789 nan 0.1000 -0.0053 ## 200 0.3447 nan 0.1000 -0.0045 ## 220 0.3123 nan 0.1000 -0.0067 ## 240 0.2807 nan 0.1000 -0.0018 ## 250 0.2696 nan 0.1000 -0.0060 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.1181 nan 0.1000 0.6333 ## 2 6.4637 nan 0.1000 0.5209 ## 3 5.8982 nan 0.1000 0.5202 ## 4 5.4003 nan 0.1000 0.3026 ## 5 4.9882 nan 0.1000 0.2926 ## 6 4.6393 nan 0.1000 0.2601 ## 7 4.3294 nan 0.1000 0.2207 ## 8 4.0488 nan 0.1000 0.2276 ## 9 3.7542 nan 0.1000 0.2489 ## 10 3.5442 nan 0.1000 0.1313 ## 20 2.0640 nan 0.1000 0.0561 ## 40 1.0503 nan 0.1000 0.0116 ## 60 0.7338 nan 0.1000 -0.0011 ## 80 0.5913 nan 0.1000 -0.0046 ## 100 0.4891 nan 0.1000 -0.0047 ## 120 0.4050 nan 0.1000 -0.0044 ## 140 0.3428 nan 0.1000 -0.0083 ## 160 0.2935 nan 0.1000 -0.0027 ## 180 0.2561 nan 0.1000 0.0005 ## 200 0.2280 nan 0.1000 -0.0051 ## 220 0.1969 nan 0.1000 -0.0034 ## 240 0.1764 nan 0.1000 -0.0047 ## 250 0.1670 nan 0.1000 -0.0035 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.9913 nan 0.1000 0.3761 ## 2 7.7852 nan 0.1000 0.1808 ## 3 7.4353 nan 0.1000 0.3342 ## 4 7.2002 nan 0.1000 0.1743 ## 5 6.8764 nan 0.1000 0.3014 ## 6 6.7249 nan 0.1000 0.1377 ## 7 6.5608 nan 0.1000 0.0969 ## 8 6.3857 nan 0.1000 0.1119 ## 9 6.2081 nan 0.1000 0.2099 ## 10 6.0676 nan 0.1000 0.0350 ## 20 4.8481 nan 0.1000 0.0326 ## 40 3.5311 nan 0.1000 0.0243 ## 60 2.7637 nan 0.1000 -0.0042 ## 80 2.2298 nan 0.1000 0.0035 ## 100 1.8541 nan 0.1000 0.0040 ## 120 1.5720 nan 0.1000 -0.0142 ## 140 1.3723 nan 0.1000 0.0089 ## 160 1.2296 nan 0.1000 -0.0073 ## 180 1.1206 nan 0.1000 -0.0095 ## 200 1.0353 nan 0.1000 -0.0009 ## 220 0.9629 nan 0.1000 -0.0014 ## 240 0.9290 nan 0.1000 -0.0011 ## 250 0.9156 nan 0.1000 -0.0036 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.7709 nan 0.1000 0.4607 ## 2 7.2930 nan 0.1000 0.4519 ## 3 6.7154 nan 0.1000 0.2983 ## 4 6.4052 nan 0.1000 0.2385 ## 5 6.0937 nan 0.1000 0.2545 ## 6 5.7996 nan 0.1000 0.2344 ## 7 5.6159 nan 0.1000 0.1053 ## 8 5.4356 nan 0.1000 0.1415 ## 9 5.2146 nan 0.1000 0.1220 ## 10 5.0671 nan 0.1000 0.1222 ## 20 3.5111 nan 0.1000 0.0306 ## 40 2.0860 nan 0.1000 0.0249 ## 60 1.4474 nan 0.1000 0.0088 ## 80 1.1308 nan 0.1000 -0.0062 ## 100 0.9648 nan 0.1000 0.0020 ## 120 0.8736 nan 0.1000 -0.0089 ## 140 0.8043 nan 0.1000 -0.0064 ## 160 0.7498 nan 0.1000 -0.0083 ## 180 0.7018 nan 0.1000 0.0001 ## 200 0.6598 nan 0.1000 -0.0037 ## 220 0.6329 nan 0.1000 -0.0035 ## 240 0.6049 nan 0.1000 -0.0068 ## 250 0.5905 nan 0.1000 -0.0046 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.5947 nan 0.1000 0.5713 ## 2 6.9919 nan 0.1000 0.5903 ## 3 6.5417 nan 0.1000 0.4225 ## 4 6.0968 nan 0.1000 0.3213 ## 5 5.7124 nan 0.1000 0.3371 ## 6 5.3694 nan 0.1000 0.2772 ## 7 5.0464 nan 0.1000 0.2062 ## 8 4.7692 nan 0.1000 0.2608 ## 9 4.6153 nan 0.1000 0.0811 ## 10 4.4195 nan 0.1000 0.0977 ## 20 2.9502 nan 0.1000 0.1132 ## 40 1.5511 nan 0.1000 0.0204 ## 60 1.0570 nan 0.1000 0.0142 ## 80 0.8503 nan 0.1000 -0.0047 ## 100 0.7300 nan 0.1000 -0.0177 ## 120 0.6452 nan 0.1000 -0.0077 ## 140 0.5915 nan 0.1000 -0.0082 ## 160 0.5400 nan 0.1000 -0.0058 ## 180 0.5009 nan 0.1000 -0.0040 ## 200 0.4669 nan 0.1000 -0.0059 ## 220 0.4400 nan 0.1000 -0.0040 ## 240 0.4131 nan 0.1000 -0.0079 ## 250 0.4039 nan 0.1000 -0.0059 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.6580 nan 0.1000 0.6541 ## 2 7.0177 nan 0.1000 0.5653 ## 3 6.4657 nan 0.1000 0.5278 ## 4 5.9869 nan 0.1000 0.4523 ## 5 5.5715 nan 0.1000 0.3559 ## 6 5.1874 nan 0.1000 0.2871 ## 7 4.8558 nan 0.1000 0.2317 ## 8 4.5402 nan 0.1000 0.2236 ## 9 4.3011 nan 0.1000 0.1610 ## 10 4.0022 nan 0.1000 0.2235 ## 20 2.4352 nan 0.1000 0.0364 ## 40 1.2432 nan 0.1000 0.0032 ## 60 0.8629 nan 0.1000 -0.0036 ## 80 0.7137 nan 0.1000 -0.0020 ## 100 0.6171 nan 0.1000 -0.0102 ## 120 0.5389 nan 0.1000 -0.0066 ## 140 0.4772 nan 0.1000 -0.0067 ## 160 0.4290 nan 0.1000 -0.0091 ## 180 0.3873 nan 0.1000 -0.0116 ## 200 0.3411 nan 0.1000 -0.0052 ## 220 0.3122 nan 0.1000 -0.0030 ## 240 0.2868 nan 0.1000 -0.0020 ## 250 0.2726 nan 0.1000 -0.0027 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.4799 nan 0.1000 0.8543 ## 2 6.8255 nan 0.1000 0.5990 ## 3 6.2454 nan 0.1000 0.4987 ## 4 5.7558 nan 0.1000 0.3832 ## 5 5.3790 nan 0.1000 0.3350 ## 6 5.0578 nan 0.1000 0.2406 ## 7 4.7324 nan 0.1000 0.1941 ## 8 4.3901 nan 0.1000 0.2642 ## 9 4.0528 nan 0.1000 0.2389 ## 10 3.7874 nan 0.1000 0.2426 ## 20 2.1952 nan 0.1000 0.0768 ## 40 1.1025 nan 0.1000 -0.0081 ## 60 0.7689 nan 0.1000 -0.0002 ## 80 0.6122 nan 0.1000 -0.0061 ## 100 0.5200 nan 0.1000 -0.0109 ## 120 0.4400 nan 0.1000 -0.0069 ## 140 0.3837 nan 0.1000 -0.0055 ## 160 0.3328 nan 0.1000 -0.0034 ## 180 0.2920 nan 0.1000 -0.0077 ## 200 0.2544 nan 0.1000 -0.0035 ## 220 0.2241 nan 0.1000 -0.0036 ## 240 0.1988 nan 0.1000 -0.0047 ## 250 0.1865 nan 0.1000 -0.0028 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.2748 nan 0.1000 0.3849 ## 2 6.9549 nan 0.1000 0.3086 ## 3 6.6188 nan 0.1000 0.2131 ## 4 6.3662 nan 0.1000 0.2414 ## 5 6.1655 nan 0.1000 0.1744 ## 6 5.9621 nan 0.1000 0.1800 ## 7 5.7756 nan 0.1000 0.1729 ## 8 5.6347 nan 0.1000 0.1348 ## 9 5.5429 nan 0.1000 0.0634 ## 10 5.3761 nan 0.1000 0.0928 ## 20 4.3688 nan 0.1000 0.0591 ## 40 3.2068 nan 0.1000 0.0402 ## 60 2.5475 nan 0.1000 0.0114 ## 80 2.0354 nan 0.1000 0.0023 ## 100 1.7011 nan 0.1000 -0.0072 ## 120 1.4362 nan 0.1000 -0.0015 ## 140 1.2570 nan 0.1000 -0.0034 ## 160 1.1369 nan 0.1000 0.0021 ## 180 1.0223 nan 0.1000 -0.0062 ## 200 0.9563 nan 0.1000 -0.0038 ## 220 0.8991 nan 0.1000 -0.0058 ## 240 0.8598 nan 0.1000 -0.0059 ## 250 0.8460 nan 0.1000 -0.0086 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.1432 nan 0.1000 0.5851 ## 2 6.7241 nan 0.1000 0.4090 ## 3 6.2802 nan 0.1000 0.3640 ## 4 5.9965 nan 0.1000 0.1559 ## 5 5.7218 nan 0.1000 0.2864 ## 6 5.4669 nan 0.1000 0.2326 ## 7 5.1666 nan 0.1000 0.2190 ## 8 4.9344 nan 0.1000 0.2265 ## 9 4.7538 nan 0.1000 0.1466 ## 10 4.5570 nan 0.1000 0.1311 ## 20 3.2524 nan 0.1000 0.1132 ## 40 1.9609 nan 0.1000 0.0491 ## 60 1.3733 nan 0.1000 -0.0028 ## 80 1.0764 nan 0.1000 -0.0083 ## 100 0.8912 nan 0.1000 -0.0043 ## 120 0.7917 nan 0.1000 -0.0084 ## 140 0.7298 nan 0.1000 -0.0027 ## 160 0.6894 nan 0.1000 -0.0061 ## 180 0.6543 nan 0.1000 -0.0077 ## 200 0.6175 nan 0.1000 -0.0009 ## 220 0.5885 nan 0.1000 -0.0089 ## 240 0.5540 nan 0.1000 -0.0036 ## 250 0.5386 nan 0.1000 -0.0019 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.0286 nan 0.1000 0.5874 ## 2 6.5422 nan 0.1000 0.4614 ## 3 6.0807 nan 0.1000 0.4788 ## 4 5.6220 nan 0.1000 0.3139 ## 5 5.3339 nan 0.1000 0.1849 ## 6 4.9990 nan 0.1000 0.2584 ## 7 4.7023 nan 0.1000 0.2324 ## 8 4.4521 nan 0.1000 0.2002 ## 9 4.2254 nan 0.1000 0.1721 ## 10 4.0158 nan 0.1000 0.1061 ## 20 2.5603 nan 0.1000 0.0649 ## 40 1.4633 nan 0.1000 0.0221 ## 60 1.0126 nan 0.1000 0.0040 ## 80 0.8089 nan 0.1000 -0.0011 ## 100 0.6913 nan 0.1000 -0.0016 ## 120 0.6212 nan 0.1000 -0.0039 ## 140 0.5555 nan 0.1000 0.0013 ## 160 0.5178 nan 0.1000 -0.0070 ## 180 0.4779 nan 0.1000 -0.0062 ## 200 0.4357 nan 0.1000 -0.0056 ## 220 0.4076 nan 0.1000 -0.0034 ## 240 0.3838 nan 0.1000 -0.0051 ## 250 0.3724 nan 0.1000 -0.0045 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.0052 nan 0.1000 0.7357 ## 2 6.3136 nan 0.1000 0.6391 ## 3 5.8579 nan 0.1000 0.3395 ## 4 5.4261 nan 0.1000 0.4118 ## 5 5.0380 nan 0.1000 0.2541 ## 6 4.7502 nan 0.1000 0.2364 ## 7 4.5107 nan 0.1000 0.1393 ## 8 4.2559 nan 0.1000 0.2325 ## 9 4.0139 nan 0.1000 0.1431 ## 10 3.7788 nan 0.1000 0.1690 ## 20 2.2976 nan 0.1000 0.0670 ## 40 1.1641 nan 0.1000 0.0258 ## 60 0.7945 nan 0.1000 -0.0042 ## 80 0.6385 nan 0.1000 -0.0010 ## 100 0.5468 nan 0.1000 -0.0065 ## 120 0.4711 nan 0.1000 -0.0070 ## 140 0.4186 nan 0.1000 -0.0051 ## 160 0.3792 nan 0.1000 -0.0062 ## 180 0.3426 nan 0.1000 -0.0072 ## 200 0.3149 nan 0.1000 -0.0066 ## 220 0.2897 nan 0.1000 -0.0081 ## 240 0.2642 nan 0.1000 -0.0020 ## 250 0.2526 nan 0.1000 -0.0020 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.0074 nan 0.1000 0.6779 ## 2 6.3049 nan 0.1000 0.6277 ## 3 5.7428 nan 0.1000 0.5037 ## 4 5.3843 nan 0.1000 0.2738 ## 5 4.9412 nan 0.1000 0.4013 ## 6 4.5370 nan 0.1000 0.2175 ## 7 4.2296 nan 0.1000 0.2194 ## 8 3.9306 nan 0.1000 0.2566 ## 9 3.6976 nan 0.1000 0.1642 ## 10 3.4742 nan 0.1000 0.1155 ## 20 2.0717 nan 0.1000 0.0554 ## 40 1.0961 nan 0.1000 0.0158 ## 60 0.7387 nan 0.1000 -0.0071 ## 80 0.5947 nan 0.1000 -0.0086 ## 100 0.4878 nan 0.1000 -0.0035 ## 120 0.4191 nan 0.1000 -0.0069 ## 140 0.3558 nan 0.1000 -0.0047 ## 160 0.3193 nan 0.1000 -0.0029 ## 180 0.2757 nan 0.1000 -0.0060 ## 200 0.2412 nan 0.1000 -0.0011 ## 220 0.2154 nan 0.1000 -0.0022 ## 240 0.1911 nan 0.1000 -0.0019 ## 250 0.1798 nan 0.1000 -0.0025 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.5373 nan 0.1000 0.3255 ## 2 7.2302 nan 0.1000 0.3081 ## 3 7.0287 nan 0.1000 0.0792 ## 4 6.8235 nan 0.1000 0.1648 ## 5 6.5846 nan 0.1000 0.2528 ## 6 6.3572 nan 0.1000 0.2142 ## 7 6.2170 nan 0.1000 0.0731 ## 8 6.0241 nan 0.1000 0.1693 ## 9 5.8911 nan 0.1000 0.1379 ## 10 5.7890 nan 0.1000 0.0396 ## 20 4.6672 nan 0.1000 0.0610 ## 40 3.3823 nan 0.1000 0.0241 ## 60 2.6081 nan 0.1000 0.0076 ## 80 2.1083 nan 0.1000 0.0027 ## 100 1.7823 nan 0.1000 -0.0038 ## 120 1.5329 nan 0.1000 0.0083 ## 140 1.3371 nan 0.1000 -0.0004 ## 160 1.1999 nan 0.1000 -0.0035 ## 180 1.0969 nan 0.1000 -0.0001 ## 200 1.0178 nan 0.1000 -0.0020 ## 220 0.9601 nan 0.1000 -0.0065 ## 240 0.9259 nan 0.1000 -0.0059 ## 250 0.9093 nan 0.1000 -0.0037 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.3367 nan 0.1000 0.5068 ## 2 6.8403 nan 0.1000 0.4133 ## 3 6.4575 nan 0.1000 0.3430 ## 4 6.1040 nan 0.1000 0.2802 ## 5 5.9071 nan 0.1000 0.1333 ## 6 5.6534 nan 0.1000 0.2078 ## 7 5.4050 nan 0.1000 0.2252 ## 8 5.2113 nan 0.1000 0.1294 ## 9 4.9828 nan 0.1000 0.1934 ## 10 4.8171 nan 0.1000 0.1146 ## 20 3.3732 nan 0.1000 0.0232 ## 40 2.0529 nan 0.1000 0.0254 ## 60 1.4630 nan 0.1000 0.0107 ## 80 1.1301 nan 0.1000 -0.0184 ## 100 0.9347 nan 0.1000 -0.0020 ## 120 0.8346 nan 0.1000 -0.0053 ## 140 0.7561 nan 0.1000 -0.0116 ## 160 0.7038 nan 0.1000 -0.0083 ## 180 0.6608 nan 0.1000 -0.0050 ## 200 0.6231 nan 0.1000 -0.0062 ## 220 0.5885 nan 0.1000 -0.0053 ## 240 0.5666 nan 0.1000 -0.0073 ## 250 0.5537 nan 0.1000 -0.0016 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.3914 nan 0.1000 0.5561 ## 2 6.7379 nan 0.1000 0.5924 ## 3 6.1931 nan 0.1000 0.3638 ## 4 5.7353 nan 0.1000 0.2706 ## 5 5.4272 nan 0.1000 0.2604 ## 6 5.1269 nan 0.1000 0.1462 ## 7 4.9406 nan 0.1000 0.0649 ## 8 4.7122 nan 0.1000 0.1369 ## 9 4.5181 nan 0.1000 0.1104 ## 10 4.3030 nan 0.1000 0.1313 ## 20 2.8485 nan 0.1000 0.0934 ## 40 1.6130 nan 0.1000 -0.0081 ## 60 1.1086 nan 0.1000 -0.0165 ## 80 0.8744 nan 0.1000 -0.0020 ## 100 0.7519 nan 0.1000 -0.0066 ## 120 0.6638 nan 0.1000 -0.0039 ## 140 0.5997 nan 0.1000 -0.0044 ## 160 0.5553 nan 0.1000 -0.0082 ## 180 0.5230 nan 0.1000 -0.0054 ## 200 0.4907 nan 0.1000 -0.0056 ## 220 0.4566 nan 0.1000 -0.0051 ## 240 0.4234 nan 0.1000 -0.0047 ## 250 0.4103 nan 0.1000 -0.0062 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.3959 nan 0.1000 0.5344 ## 2 6.7840 nan 0.1000 0.6032 ## 3 6.2042 nan 0.1000 0.5214 ## 4 5.7561 nan 0.1000 0.3567 ## 5 5.3542 nan 0.1000 0.3154 ## 6 5.0748 nan 0.1000 0.2697 ## 7 4.6856 nan 0.1000 0.3381 ## 8 4.4209 nan 0.1000 0.2125 ## 9 4.1834 nan 0.1000 0.1959 ## 10 3.9558 nan 0.1000 0.1757 ## 20 2.5107 nan 0.1000 0.0757 ## 40 1.3385 nan 0.1000 0.0197 ## 60 0.9193 nan 0.1000 0.0067 ## 80 0.7247 nan 0.1000 -0.0105 ## 100 0.6080 nan 0.1000 -0.0060 ## 120 0.5370 nan 0.1000 -0.0086 ## 140 0.4808 nan 0.1000 -0.0094 ## 160 0.4331 nan 0.1000 -0.0018 ## 180 0.3896 nan 0.1000 -0.0076 ## 200 0.3487 nan 0.1000 -0.0060 ## 220 0.3156 nan 0.1000 -0.0044 ## 240 0.2857 nan 0.1000 -0.0023 ## 250 0.2735 nan 0.1000 -0.0045 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.2002 nan 0.1000 0.7128 ## 2 6.5508 nan 0.1000 0.5996 ## 3 5.9803 nan 0.1000 0.5293 ## 4 5.5269 nan 0.1000 0.4154 ## 5 5.1301 nan 0.1000 0.3923 ## 6 4.7897 nan 0.1000 0.2007 ## 7 4.4224 nan 0.1000 0.2482 ## 8 4.1528 nan 0.1000 0.1840 ## 9 3.8740 nan 0.1000 0.1938 ## 10 3.6367 nan 0.1000 0.1894 ## 20 2.1725 nan 0.1000 0.0933 ## 40 1.0934 nan 0.1000 0.0063 ## 60 0.7734 nan 0.1000 -0.0003 ## 80 0.6267 nan 0.1000 -0.0113 ## 100 0.5248 nan 0.1000 -0.0115 ## 120 0.4509 nan 0.1000 -0.0086 ## 140 0.3909 nan 0.1000 -0.0042 ## 160 0.3417 nan 0.1000 -0.0095 ## 180 0.2979 nan 0.1000 -0.0010 ## 200 0.2641 nan 0.1000 -0.0051 ## 220 0.2324 nan 0.1000 -0.0048 ## 240 0.2065 nan 0.1000 -0.0029 ## 250 0.1944 nan 0.1000 -0.0033 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 7.5853 nan 0.1000 0.4622 ## 2 7.2268 nan 0.1000 0.2276 ## 3 6.9725 nan 0.1000 0.2317 ## 4 6.7255 nan 0.1000 0.1945 ## 5 6.5310 nan 0.1000 0.1967 ## 6 6.3548 nan 0.1000 0.1452 ## 7 6.1331 nan 0.1000 0.1877 ## 8 6.0205 nan 0.1000 0.0816 ## 9 5.8651 nan 0.1000 0.1151 ## 10 5.7272 nan 0.1000 0.0983 ## 20 4.5797 nan 0.1000 0.0457 ## 40 3.3805 nan 0.1000 0.0281 ## 60 2.6821 nan 0.1000 -0.0024 ## 80 2.1887 nan 0.1000 0.0083 ## 100 1.8328 nan 0.1000 0.0108 ## 120 1.5572 nan 0.1000 -0.0039 ## 140 1.3454 nan 0.1000 0.0011 ## 160 1.2090 nan 0.1000 -0.0017 ## 180 1.0998 nan 0.1000 0.0009 ## 200 1.0155 nan 0.1000 -0.0061 ## 220 0.9510 nan 0.1000 -0.0040 ## 240 0.9159 nan 0.1000 -0.0058 ## 250 0.8952 nan 0.1000 -0.0036 carseats.gbm ## Stochastic Gradient Boosting ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... ## Resampling results across tuning parameters: ## ## interaction.depth n.trees RMSE Rsquared MAE ## 1 50 1.9 0.66 1.50 ## 1 100 1.5 0.75 1.26 ## 1 150 1.4 0.79 1.11 ## 1 200 1.3 0.82 1.03 ## 1 250 1.2 0.82 0.98 ## 2 50 1.6 0.75 1.27 ## 2 100 1.3 0.81 1.06 ## 2 150 1.2 0.83 0.99 ## 2 200 1.2 0.83 0.98 ## 2 250 1.2 0.83 0.99 ## 3 50 1.4 0.78 1.16 ## 3 100 1.3 0.82 1.02 ## 3 150 1.2 0.82 0.99 ## 3 200 1.3 0.81 1.01 ## 3 250 1.3 0.81 1.02 ## 4 50 1.4 0.79 1.12 ## 4 100 1.3 0.81 1.03 ## 4 150 1.3 0.80 1.04 ## 4 200 1.3 0.80 1.06 ## 4 250 1.3 0.80 1.06 ## 5 50 1.3 0.80 1.10 ## 5 100 1.3 0.81 1.04 ## 5 150 1.3 0.81 1.04 ## 5 200 1.3 0.80 1.04 ## 5 250 1.3 0.80 1.06 ## ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## ## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were n.trees = 250, interaction.depth = ## 1, shrinkage = 0.1 and n.minobsinnode = 10. plot(carseats.gbm) carseats.pred &lt;- predict(carseats.gbm, carseats_test, type = &quot;raw&quot;) plot(carseats_test$Sales, carseats.pred, main = &quot;Gradient Boosing Regression: Predicted vs. Actual&quot;, xlab = &quot;Actual&quot;, ylab = &quot;Predicted&quot;) abline(0,1) (carseats.gbm.rmse &lt;- RMSE(pred = carseats.pred, obs = carseats_test$Sales)) ## [1] 1.4 rm(carseats.pred) #plot(varImp(carseats.gbm), main=&quot;Variable Importance with Gradient Boosting&quot;) "],
["summary.html", "10.6 Summary", " 10.6 Summary Okay, I’m going to tally up the results! For the classification division, the winner is the manual classification tree! Gradient boosting made a valiant run at it, but came up just a little short. rbind(data.frame(model = &quot;Manual Class&quot;, Acc = round(oj_model_1b_cm$overall[&quot;Accuracy&quot;], 5)), data.frame(model = &quot;Class w.tuneGrid&quot;, Acc = round(oj_model_3_cm$overall[&quot;Accuracy&quot;], 5)), data.frame(model = &quot;Bagging&quot;, Acc = round(oj.bag.acc, 5)), data.frame(model = &quot;Random Forest&quot;, Acc = round(oj.frst.acc, 5)), data.frame(model = &quot;Gradient Boosting&quot;, Acc = round(oj.gbm.acc, 5)) ) %&gt;% arrange(desc(Acc)) ## model Acc ## 1 Manual Class 0.86 ## 2 Gradient Boosting 0.85 ## 3 Class w.tuneGrid 0.85 ## 4 Bagging 0.83 ## 5 Random Forest 0.83 And now for the regression division, the winnner is… gradient boosting! rbind(data.frame(model = &quot;Manual ANOVA&quot;, RMSE = round(carseats_model_1_pruned_rmse, 5)), data.frame(model = &quot;ANOVA w.tuneGrid&quot;, RMSE = round(carseats_model_3_pruned_rmse, 5)), data.frame(model = &quot;Bagging&quot;, RMSE = round(carseats.bag.rmse, 5)), data.frame(model = &quot;Random Forest&quot;, RMSE = round(carseats.frst.rmse, 5)), data.frame(model = &quot;Gradient Boosting&quot;, RMSE = round(carseats.gbm.rmse, 5)) ) %&gt;% arrange(RMSE) ## model RMSE ## 1 Gradient Boosting 1.4 ## 2 Random Forest 1.8 ## 3 Bagging 1.9 ## 4 ANOVA w.tuneGrid 2.3 ## 5 Manual ANOVA 2.4 Here are plots of the ROC curves for all the models (one from each chapter) on the same graph. The ROCR package provides the prediction() and performance() functions which generate the data required for plotting the ROC curve, given a set of predictions and actual (true) values. The more “up and to the left” the ROC curve of a model is, the better the model. The AUC performance metric is literally the “Area Under the ROC Curve”, so the greater the area under this curve, the higher the AUC, and the better-performing the model is. library(ROCR) # List of predictions oj.class.pred &lt;- predict(oj_model_3, oj_test, type = &quot;prob&quot;)[,2] oj.bag.pred &lt;- predict(oj.bag, oj_test, type = &quot;prob&quot;)[,2] oj.frst.pred &lt;- predict(oj.frst, oj_test, type = &quot;prob&quot;)[,2] oj.gbm.pred &lt;- predict(oj.gbm, oj_test, type = &quot;prob&quot;)[,2] preds_list &lt;- list(oj.class.pred, oj.bag.pred, oj.frst.pred, oj.gbm.pred) #preds_list &lt;- list(oj.class.pred) # List of actual values (same for all) m &lt;- length(preds_list) actuals_list &lt;- rep(list(oj_test$Purchase), m) # Plot the ROC curves pred &lt;- prediction(preds_list, actuals_list) #pred &lt;- prediction(oj.class.pred[,2], oj_test$Purchase) rocs &lt;- performance(pred, &quot;tpr&quot;, &quot;fpr&quot;) plot(rocs, col = as.list(1:m), main = &quot;Test Set ROC Curves&quot;) legend(x = &quot;bottomright&quot;, legend = c(&quot;Decision Tree&quot;, &quot;Bagged Trees&quot;, &quot;Random Forest&quot;, &quot;GBM&quot;), fill = 1:m) "],
["reference-1.html", "10.7 Reference", " 10.7 Reference Penn State University, STAT 508: Applied Data Mining and Statistical Learning, “Lesson 11: Tree-based Methods”. https://newonlinecourses.science.psu.edu/stat508/lesson/11. Brownlee, Jason. “Classification And Regression Trees for Machine Learning”, Machine Learning Mastery. https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/. Brownlee, Jason. “A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning”, Machine Learning Mastery. https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/. DataCamp: Machine Learning with Tree-Based Models in R An Introduction to Statistical Learning by Gareth James, et al. SAS Documentation StatMethods: Tree-Based Models Machine Learning Plus GBM (Boosted Models) Tuning Parameters from Listen Data Harry Southworth on GitHub Gradient Boosting Classification with GBM in R in DataTechNotes Molnar, Christoph. “Interpretable machine learning. A Guide for Making Black Box Models Explainable”, 2019. https://christophm.github.io/interpretable-ml-book/. "],
["non-linear-models.html", "Chapter 11 Non-linear Models", " Chapter 11 Non-linear Models Linear methods can model nonlinear relationships by including polynomial terms, interaction effects, and variable transformations. However, it is often difficult to identify how to formulate the model. Nonlinear models may be preferable because you do not need to know the the exact form of the nonlinearity prior to model training. "],
["splines.html", "11.1 Splines", " 11.1 Splines A regression spline fits a piecewise polynomial to the range of X partitioned by knots (K knots produce K + 1 piecewise polynomials) James et al (James et al. 2013). The polynomials can be of any degree d, but are usually in the range [0, 3], most commonly 3 (a cubic spline). To avoid discontinuities in the fit, a degree-d spline is constrained to have continuity in derivatives up to degree d−1 at each knot. A cubic spline fit to a data set with K knots, performs least squares regression with an intercept and 3 + K predictors, of the form \\[y_i = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\beta_4h(X, \\xi_1) + \\beta_5h(X, \\xi_2) + \\dots + \\beta_{K+3}h(X, \\xi_K)\\] where \\(\\xi_1, \\dots, \\xi_K\\) are the knots are truncated power basis functions \\(h(X, \\xi) = (X - \\xi)^3\\) if \\(X &gt; \\xi\\), else 0. Splines can have high variance at the outer range of the predictors. A natural spline is a regression spline additionally constrained to be linear at the boundaries. How many knots should there be, and Where should the knots be placed? It is common to place knots in a uniform fashion, with equal numbers of points between each knot. The number of knots is typically chosen by trial and error using cross-validation to minimize the RSS. The number of knots is usually expressed in terms of degrees of freedom. A cubic spline will have K + 3 + 1 degrees of freedom. A natural spline has K + 3 + 1 - 5 degrees of freedom due to the constraints at the endpoints. A further constraint can be added to reduce overfitting by enforcing smoothness in the spline. Instead of minimizing the loss function \\(\\sum{(y - g(x))^2}\\) where \\(g(x)\\) is a natural spline, minimize a loss function with an additional penalty for variability: \\[L = \\sum{(y_i - g(x_i))^2 + \\lambda \\int g&#39;&#39;(t)^2dt}.\\] The function \\(g(x)\\) that minimizes the loss function is a natural cubic spline with knots at each \\(x_1, \\dots, x_n\\). This is called a smoothing spline. The larger g is, the greater the penalty on variation in the spline. In a smoothing spline, you do not optimize the number or location of the knots – there is a knot at each training observation. Instead, you optimize \\(\\lambda\\). One way to optimze \\(\\lambda\\) is cross-validation to minimize RSS. Leave-one-out cross-validation (LOOCV) can be computed efficiently for smoothing splines. References "],
["mars.html", "11.2 MARS", " 11.2 MARS Multivariate adaptive regression splines (MARS) is a non-parametric algorithm that creates a piecewise linear model to capture nonlinearities and interactions effects. The resulting model is a weighted sum of basis functions \\(B_i(X)\\): \\[\\hat{y} = \\sum_{i=1}^{k}{w_iB_i(x)}\\] The basis functions are either a constant (for the intercept), a hinge function of the form \\(\\max(0, x - x_0)\\) or \\(\\max(0, x_0 - x)\\) (a more concise representation is \\([\\pm(x - x_0)]_+\\)), or products of two or more hinge functions (for interactions). MARS automatically selects which predictors to use and what predictor values to serve as the knots of the hinge functions. MARS builds a model in two phases: the forward pass and the backward pass, similar to growing and pruning of tree models. MARS starts with a model consisting of just the intercept term equaling the mean of the response values. It then asseses every predictor to find a basis function pair consisting of opposing sides of a mirrored hinge function which produces the maximum improvement in the model error. MARS repeats the process until either it reaches a predefined limit of terms or the error improvement reaches a predefined limit. MARS generalizes the model by removing terms according to the generalized cross validation (GCV) criterion. GCV is a form of regularization: it trades off goodness-of-fit against model complexity. The earth::earth() function (documentation) performs the MARS algorithm (the term “MARS” is trademarked, so open-source implementations use “Earth” instead). The caret implementation tunes two parameters: nprune and degree. nprune is the maximum number of terms in the pruned model. degree is the maximum degree of interaction (default is 1 (no interactions)). However, there are other hyperparameters in the model that may improve performance, including minspan which regulates the number of knots in the predictors. Here is an example using the Ames housing data set (following this tutorial. library(tidyverse) library(earth) library(caret) # set up ames &lt;- AmesHousing::make_ames() set.seed(12345) idx &lt;- createDataPartition(ames$Sale_Price, p = 0.80, list = FALSE) ames_train &lt;- ames[idx, ] %&gt;% as.data.frame() ames_test &lt;- ames[-idx, ] m &lt;- train( x = subset(ames_train, select = -Sale_Price), y = ames_train$Sale_Price, method = &quot;earth&quot;, metric = &quot;RMSE&quot;, minspan = -15, trControl = trainControl(method = &quot;cv&quot;, number = 10), tuneGrid = expand.grid( degree = 1:3, nprune = seq(2, 100, length.out = 10) %&gt;% floor() ) ) The model plot shows the best tuning parameter combination. plot(m, main = &quot;MARS Parameter Tuning&quot;) m$bestTune ## nprune degree ## 25 45 3 How does this model perform against the holdout data? caret::postResample( pred = log(predict(m, newdata = ames_test)), obs = log(ames_test$Sale_Price) ) ## RMSE Rsquared MAE ## 0.165 0.855 0.093 "],
["gam.html", "11.3 GAM", " 11.3 GAM Generalized additive models (GAM) allow for non-linear relationships between each feature and the response by replacing each linear component \\(\\beta_j x_{ij}\\) with a nonlinear function \\(f_j(x_{ij})\\). The GAM model is of the form \\[y_i = \\beta_0 + \\sum{f_j(x_{ij})} + \\epsilon_i.\\] It is called an additive model because we calculate a separate \\(f_j\\) for each \\(X_j\\), and then add together all of their contributions. The advantage of GAMs is that they automatically model non-linear relationships so you do not need to manually try out many diﬀerent transformations on each variable individually. And because the model is additive, you can still examine the eﬀect of each \\(X_j\\) on \\(Y\\) individually while holding all of the other variables ﬁxed. The main limitation of GAMs is that the model is restricted to be additive, so important interactions can be missed unless you explicitly add them. "],
["support-vector-machines.html", "Chapter 12 Support Vector Machines", " Chapter 12 Support Vector Machines These notes rely on (James et al. 2013), (Hastie, Tibshirani, and Friedman 2017), and (Kuhn and Johnson 2016). I also reviewed the material in PSU’s Applied Data Mining and Statistical Learning (STAT 508), and the e1071 Support Vector Machines vignette. The Support Vector Machines (SVM) algorithm finds the optimal separating hyperplane between members of two classes using an appropriate nonlinear mapping to a sufficiently high dimension. The hyperplane is defined by the observations that lie within a margin optimized by a cost hyperparameter. These observations are called the support vectors. SVM is an extension of the support vector classifier which in turn is a generalization of the simple and intuitive maximal margin classifier. References "],
["maximal-margin-classifier.html", "12.1 Maximal Margin Classifier", " 12.1 Maximal Margin Classifier The maximal margin classifier is the optimal hyperplane defined in the (rare) case where two classes are linearly separable. Given an \\(n \\times p\\) data matrix \\(X\\) with binary response variable defined as \\(y \\in [-1, 1]\\) it may be possible to define a p-dimensional hyperplane \\(h(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 \\dots + \\beta_pX_p = x_i^T \\beta + \\beta_0 = 0\\) such that all observations of each class fall on opposite sides of the hyperplane. This “separating hyperplane” has the property that if \\(\\beta\\) is constrained to be a unit vector, \\(||\\beta|| = \\sum\\beta^2 = 1\\), then the product of the hyperplane and response variables are positive perpendicular distances from the hyperplane, the smallest of which may be termed the hyperplane margin, \\(M\\), \\[y_i (x_i^{&#39;} \\beta + \\beta_0) \\ge M.\\] The maximal margin classifier is the hyperplane with the maximum margin. That is, \\(\\max \\{M\\}\\) subject to \\(||\\beta|| = 1\\). A separating hyperplane rarely exists. In fact, even if a separating hyperplane does exist, its resulting margin is probably undesirably narrow. "],
["support-vector-classifier.html", "12.2 Support Vector Classifier", " 12.2 Support Vector Classifier The maximal margin classifier can be generalized to non-separable cases using a so-called “soft margin”. The generalization is called the support vector classifier. The soft margin allows some misclassification in the interest of greater robustness to individual observations. The support vector classifier optimizes \\[y_i (x_i^{&#39;} \\beta + \\beta_0) \\ge M(1 - \\xi_i)\\] where the \\(\\xi_i\\) are positive slack variables whose sum is bounded by some constant tuning parameter \\(\\sum{\\xi_i} \\le constant\\). The slack variable values indicate where the observation lies: \\(\\xi_i = 0\\) observations lie on the correct side of the margin; \\(\\xi_i &gt; 0\\) observation lie on the wrong side of the margin; \\(\\xi_i &gt; 1\\) observations lie on the wrong side of the hyperplane. The constant sets the tolerance for margin violation. If \\(constant = 0\\), then all observations must reside on the correct side of the margin, as in the maximal margin classifier. The \\(constant\\) controls the bias-variance trade-off. As the \\(constant\\) increases, the margin widens and allows more violations. The classifier bias increases but its variance decreases. The support vector classifier is usually defined by dropping the \\(||\\beta|| = 1\\) constraint, and defining \\(M = 1 / ||\\beta||\\). The optimization problem then becomes \\[ \\min ||\\beta|| \\hspace{2mm} s.t. \\hspace{2mm} \\begin{cases} y_i(x_i^T\\beta + \\beta_0) \\ge 1 - \\xi_i, \\hspace{2mm} \\forall i &amp; \\\\ \\xi_i \\ge 0, \\hspace{2mm} \\sum \\xi_i \\le constant. \\end{cases} \\] This is a quadratic equation with linear inequality constraints, so it is a convex optimization problem which can be solved using Lagrange multipliers. Re-express the optimization problem as \\[ \\min_{\\beta_0, \\beta} \\frac{1}{2}||\\beta||^2 = C\\sum_{i = 1}^N \\xi_i \\\\ s.t. \\xi_i \\ge 0, \\hspace{2mm} y_i(x_i^T\\beta + \\beta_0) \\ge 1 - \\xi_i, \\hspace{2mm} \\forall i \\] where the “cost” parameter \\(C\\) replaces the constant and penalizes large residuals. This optimization problem is equivalent to another optimization problem, the familiar loss + penalty formulation: \\[\\min_{\\beta_0, \\beta} \\sum_{i=1}^N{[1 - y_if(x_i)]_+} + \\frac{\\lambda}{2} ||\\beta||^2 \\] where \\(\\lambda = 1 / C\\) and \\([1 - y_if(x_i)]_+\\) is a “hinge” loss function with \\(f(x_i) = sign[Pr(Y = +1|x) - 1 / 2]\\). The parameter estimates can be written as functions of a set of unknown parameters \\((\\alpha_i)\\) and data points. The solution to the optimization problem requires only the inner products of the observations, represented as \\(\\langle x_i, x_j \\rangle\\), \\[f(x) = \\beta_0 + \\sum_{i = 1}^n {\\alpha_i \\langle x, x_i \\rangle}\\] The solution has the interesting property that only observations on or within the margin affect the hyperplane. These observations are known as support vectors. As the constant increases, the number of violating observations increase, and thus the number of support vectors increases. This property makes the algorithm robust to the extreme observations far away from the hyperplane. The parameter estimators for \\(\\alpha_i\\) are nonzero only for the support vectors in the solution—that is, if a training observation is not a support vector, then its \\(\\alpha_i\\) equals zero. The only shortcoming with the algorithm is that it presumes a linear decision boundary. "],
["support-vector-machines-1.html", "12.3 Support Vector Machines", " 12.3 Support Vector Machines Enlarging the feature space of the support vector classifier accommodates nonlinar relationships. Support vector machines do this in a specific way, using kernals. The kernal is a generalization of the inner product with form \\(K(x_i, x_i^{&#39;})\\). So the linear kernal is simply \\[K(x_i, x_i^{&#39;}) = \\langle x, x_i \\rangle\\] and the solution is \\[f(x) = \\beta_0 + \\sum_{i = 1}^n {\\alpha_i K(x_i, x_i^{&#39;})}\\] \\(K\\) can take onother form instead, such as polynomial \\[K(x, x&#39;) = (\\gamma \\langle x, x&#39; \\rangle + c_0)^d\\] or radial \\[K(x, x&#39;) = \\exp\\{-\\gamma ||x - x&#39;||^2\\}.\\] "],
["example-16.html", "12.4 Example", " 12.4 Example Here is a data set of two classes \\(y \\in [-1, 1]\\) described by two features \\(X1\\) and \\(X2\\). library(tidyverse) set.seed(1) x &lt;- matrix(rnorm (20*2), ncol=2) y &lt;- c(rep(-1, 10), rep(1, 10)) x[y==1, ] &lt;- x[y==1, ] + 1 train_data &lt;- data.frame(x, y) train_data$y &lt;- as.factor(y) A scatter plot reveals whether the classes are linearly separable. ggplot(train_data, aes(x = X1, y = X2, color = y)) + geom_point(size = 2) + labs(title = &quot;Binary response with two features&quot;) + theme(legend.position = &quot;top&quot;) No, they are not linearly separable. Now fit a support vector machine. The e1071 library implements the SVM algorithm. svm(..., kernel=\"linear\") fits a support vector classifier. Change the kernal to c(\"polynomial\", \"radial\") for SVM. Try a cost of 10. library(e1071) m &lt;- svm( y ~ ., data = train_data, kernel = &quot;linear&quot;, type = &quot;C-classification&quot;, # (default) for classification cost = 10, # default is 1 scale = FALSE # do not standardize features ) plot(m, train_data) The support vectors are plotted as “x’s”. There are seven of them. m$index ## [1] 1 2 5 7 14 16 17 The summary shows adds additional information, including the distribution of the support vector classes. summary(m) ## ## Call: ## svm(formula = y ~ ., data = train_data, kernel = &quot;linear&quot;, type = &quot;C-classification&quot;, ## cost = 10, scale = FALSE) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: linear ## cost: 10 ## ## Number of Support Vectors: 7 ## ## ( 4 3 ) ## ## ## Number of Classes: 2 ## ## Levels: ## -1 1 The seven support vectors are comprised of four in one class, three in the other. What if we lower the cost of margin violations? This will increase bias and lower variance. m &lt;- svm( y ~ ., data = train_data, kernel = &quot;linear&quot;, type = &quot;C-classification&quot;, cost = 0.1, scale = FALSE ) plot(m, train_data) There are many more support vectors now. (In case you hoped to see the linear decision boundary formulation, or at least a graphical representation of the margins, keep hoping. The model is generalized beyond two features, so it evidently does not worry too much about supporting sanitized two-feature demos.) Which cost level yields the best predictive performance on holdout data? Use cross validation to find out. SVM defaults to 10-fold CV. I’ll try seven candidate values for cost. set.seed(1) m_tune &lt;- tune( svm, y ~ ., data = train_data, kernel =&quot;linear&quot;, ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)) ) summary(m_tune) ## ## Parameter tuning of &#39;svm&#39;: ## ## - sampling method: 10-fold cross validation ## ## - best parameters: ## cost ## 0.1 ## ## - best performance: 0.05 ## ## - Detailed performance results: ## cost error dispersion ## 1 0.001 0.55 0.44 ## 2 0.010 0.55 0.44 ## 3 0.100 0.05 0.16 ## 4 1.000 0.15 0.24 ## 5 5.000 0.15 0.24 ## 6 10.000 0.15 0.24 ## 7 100.000 0.15 0.24 The lowest cross-validation error rate is 0.10 with cost = 0.1. tune() saves the best tuning parameter value. m_best &lt;- m_tune$best.model summary(m_best) ## ## Call: ## best.tune(method = svm, train.x = y ~ ., data = train_data, ranges = list(cost = c(0.001, ## 0.01, 0.1, 1, 5, 10, 100)), kernel = &quot;linear&quot;) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: linear ## cost: 0.1 ## ## Number of Support Vectors: 16 ## ## ( 8 8 ) ## ## ## Number of Classes: 2 ## ## Levels: ## -1 1 There are 16 support vectors, 8 in each class. This is a pretty wide margin. plot(m_best, train_data) What if the classes had been linearly separable? Then we could create a maximal margin classifier. train_data_2 &lt;- train_data %&gt;% mutate( X1 = X1 + ifelse(y==1, 1.0, 0), X2 = X2 + ifelse(y==1, 1.0, 0) ) ggplot(train_data_2, aes(x = X1, y = X2, color = y)) + geom_point(size = 2) + labs(title = &quot;Binary response with two features, linearly separable&quot;) Specify a huge cost = 1e5 so that no support vectors violate the margin. m2 &lt;- svm( y ~ ., data = train_data_2, kernel = &quot;linear&quot;, cost = 1e5, scale = FALSE # do not standardize features ) plot(m2, train_data_2) summary(m2) ## ## Call: ## svm(formula = y ~ ., data = train_data_2, kernel = &quot;linear&quot;, cost = 100000, ## scale = FALSE) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: linear ## cost: 100000 ## ## Number of Support Vectors: 3 ## ## ( 1 2 ) ## ## ## Number of Classes: 2 ## ## Levels: ## -1 1 This model will have very low bias, but very high variance. To fit an SVM, use a different kernel. You can use kernal = c(\"polynomial\", \"radial\", \"sigmoid\"). For a polynomial model, also specify the polynomial degree. For a radial model, include the gamma value. set.seed(1) m3_tune &lt;- tune( svm, y ~ ., data = train_data, kernel =&quot;polynomial&quot;, ranges = list( cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100), degree = c(1, 2, 3) ) ) summary(m3_tune) ## ## Parameter tuning of &#39;svm&#39;: ## ## - sampling method: 10-fold cross validation ## ## - best parameters: ## cost degree ## 1 1 ## ## - best performance: 0.1 ## ## - Detailed performance results: ## cost degree error dispersion ## 1 0.001 1 0.55 0.44 ## 2 0.010 1 0.55 0.44 ## 3 0.100 1 0.30 0.26 ## 4 1.000 1 0.10 0.21 ## 5 5.000 1 0.10 0.21 ## 6 10.000 1 0.15 0.24 ## 7 100.000 1 0.15 0.24 ## 8 0.001 2 0.70 0.42 ## 9 0.010 2 0.70 0.42 ## 10 0.100 2 0.70 0.42 ## 11 1.000 2 0.65 0.24 ## 12 5.000 2 0.50 0.33 ## 13 10.000 2 0.50 0.33 ## 14 100.000 2 0.50 0.33 ## 15 0.001 3 0.65 0.34 ## 16 0.010 3 0.65 0.34 ## 17 0.100 3 0.50 0.33 ## 18 1.000 3 0.40 0.32 ## 19 5.000 3 0.35 0.34 ## 20 10.000 3 0.35 0.34 ## 21 100.000 3 0.35 0.34 The lowest cross-validation error rate is 0.10 with cost = 1, polynomial degree 1. m3_best &lt;- m3_tune$best.model summary(m3_best) ## ## Call: ## best.tune(method = svm, train.x = y ~ ., data = train_data, ranges = list(cost = c(0.001, ## 0.01, 0.1, 1, 5, 10, 100), degree = c(1, 2, 3)), kernel = &quot;polynomial&quot;) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: polynomial ## cost: 1 ## degree: 1 ## coef.0: 0 ## ## Number of Support Vectors: 12 ## ## ( 6 6 ) ## ## ## Number of Classes: 2 ## ## Levels: ## -1 1 There are 12 support vectors, 6 in each class. This is a pretty wide margin. plot(m3_best, train_data) "],
["using-caret.html", "12.5 Using Caret", " 12.5 Using Caret The model can also be fit using caret. I’ll used LOOCV since the data set is so small. Normalize the variables to make their scale comparable. library(caret) library(kernlab) train_data_3 &lt;- train_data %&gt;% mutate(y = factor(y, labels = c(&quot;A&quot;, &quot;B&quot;))) m4 &lt;- train( y ~ ., data = train_data_3, method = &quot;svmPoly&quot;, preProcess = c(&quot;center&quot;, &quot;scale&quot;), trControl = trainControl( method = &quot;cv&quot;, number = 5, summaryFunction = twoClassSummary, # Use AUC to pick the best model classProbs=TRUE ) ) m4$bestTune ## degree scale C ## 8 1 0.1 0.5 #plot(m4) "],
["principal-components-analysis.html", "Chapter 13 Principal Components Analysis", " Chapter 13 Principal Components Analysis "],
["text-mining.html", "Chapter 14 Text Mining", " Chapter 14 Text Mining "],
["survival-analysis.html", "Chapter 15 Survival Analysis", " Chapter 15 Survival Analysis Survival analysis models time to event. Whereas linear regression outcomes are assumed to have a normal distribution, time-to-event outcomes are assumed to have a Weibull or exponential distribution. Survival analysis also deals with censoring (unknown starting event and/or ending event). These notes rely on the Survival Analysis in R DataCamp course and Applied Survival Analysis Using R by Dirk Moore (Moore 2016). Most surival analysis uses the survival package for modeling and the survminer package for visualization. library(tidyverse) library(survival) library(survminer) The examples in these notes use the following data sets. 15.0.1 Xelox Data from a Phase II clinical trial of Xeloda and exaliplatin given before surgery to advanced gastric cancer patients with para-aortic lymph node metastasis. timeWeeks is survival time; delta is the censoring indicator (1 = death, 0 = censored). data(gastricXelox, package = &quot;asaur&quot;) head(gastricXelox) ## timeWeeks delta ## 1 4 1 ## 2 8 1 ## 3 8 1 ## 4 8 1 ## 5 9 1 ## 6 11 1 15.0.2 pancreatic Data from a Phase II clinical trial of patients with locally advanced or metastatic pancreatic cancer. Column stage is a covariate indicating the diagnosis (LA = locally advance, M = metastatic); the event of interest is either progression or death. There was no censoring in this data because none of the subjects lived very long. data(pancreatic, package = &quot;asaur&quot;) head(pancreatic) ## stage onstudy progression death ## 1 M 12/16/2005 2/2/2006 10/19/2006 ## 2 M 1/6/2006 2/26/2006 4/19/2006 ## 3 LA 2/3/2006 8/2/2006 1/19/2007 ## 4 M 3/30/2006 . 5/11/2006 ## 5 LA 4/27/2006 3/11/2007 5/29/2007 ## 6 M 5/7/2006 6/25/2006 10/11/2006 15.0.3 prostateSurvival There are two outcomes of interest indicated by status, death from prostate cancer, and death from other causes, a competing risks survival analysis problem. data(&quot;prostateSurvival&quot;, package = &quot;asaur&quot;) head(prostateSurvival) ## grade stage ageGroup survTime status ## 1 mode T1c 80+ 18 0 ## 2 mode T1ab 75-79 23 0 ## 3 poor T1c 75-79 37 0 ## 4 mode T2 70-74 27 0 ## 5 mode T1c 70-74 42 0 ## 6 poor T2 75-79 38 2 15.0.4 pharmacoSmoking The primary outcome variable ttr is the time from randomization until relapse (return to smoking). Individuals who remained non-smokers for six months were censored. data(&quot;pharmacoSmoking&quot;, package = &quot;asaur&quot;) head(pharmacoSmoking) ## id ttr relapse grp age gender race employment yearsSmoking ## 1 21 182 0 patchOnly 36 Male white ft 26 ## 2 113 14 1 patchOnly 41 Male white other 27 ## 3 39 5 1 combination 25 Female white other 12 ## 4 80 16 1 combination 54 Male white ft 39 ## 5 87 0 1 combination 45 Male white other 30 ## 6 29 182 0 combination 43 Male hispanic ft 30 ## levelSmoking ageGroup2 ageGroup4 priorAttempts longestNoSmoke ## 1 heavy 21-49 35-49 0 0 ## 2 heavy 21-49 35-49 3 90 ## 3 heavy 21-49 21-34 3 21 ## 4 heavy 50+ 50-64 0 0 ## 5 heavy 21-49 35-49 0 0 ## 6 heavy 21-49 35-49 2 1825 15.0.5 hepatoCellular Overall and recurrence-free survival of patients with hepatocellular carcinoma. The survival outcomes are OS (overall survival) and RFS (recurrence-free survival), and the corresponding censoring indicators are Death and Recurrance. data(&quot;hepatoCellular&quot;, package = &quot;asaur&quot;) head(hepatoCellular) ## Number Age Gender HBsAg Cirrhosis ALT AST AFP Tumorsize Tumordifferentiation ## 1 1 57 0 1 1 1 2 2 2 1 ## 2 2 58 1 0 0 1 1 2 1 1 ## 3 3 65 1 0 0 1 1 2 2 2 ## 4 4 54 1 1 0 2 1 2 2 2 ## 5 5 71 1 1 0 2 2 2 2 2 ## 6 6 32 1 0 0 2 2 2 2 2 ## Vascularinvasion Tumormultiplicity Capsulation TNM BCLC OS Death RFS ## 1 0 1 0 2 1 83 0 13 ## 2 0 1 1 1 1 81 0 81 ## 3 1 1 1 2 2 79 0 79 ## 4 0 1 1 1 1 76 0 76 ## 5 1 2 0 2 2 7 1 3 ## 6 0 1 0 1 1 13 1 3 ## Recurrence CXCL17T CXCL17P CXCL17N CD4T CD4N CD8T CD8N CD20T CD20N CD57T ## 1 1 113.9 299.3 138.2 2.6 0 191 127 21 13 16 ## 2 0 54.1 63.5 6.2 NA NA NA NA NA NA NA ## 3 0 22.2 34.5 22.1 NA NA NA NA NA NA NA ## 4 0 8.4 16.0 11.1 NA NA NA NA NA NA NA ## 5 1 8.3 22.0 32.3 NA NA NA NA NA NA NA ## 6 1 13.7 6.4 0.0 NA NA NA NA NA NA NA ## CD57N CD15T CD15N CD68T CD68N CD4NR CD8NR CD20NR CD57NR CD15NR CD68NR CD4TR ## 1 0 73 24 47 50 0 0.59 0.061 0 0.11 0.24 0.0074 ## 2 NA NA NA NA NA NA NA NA NA NA NA NA ## 3 NA NA NA NA NA NA NA NA NA NA NA NA ## 4 NA NA NA NA NA NA NA NA NA NA NA NA ## 5 NA NA NA NA NA NA NA NA NA NA NA NA ## 6 NA NA NA NA NA NA NA NA NA NA NA NA ## CD8TR CD20TR CD57TR CD15TR CD68TR Ki67 CD34 ## 1 0.54 0.06 0.045 0.21 0.14 6 57080 ## 2 NA NA NA NA NA NA NA ## 3 NA NA NA NA NA NA NA ## 4 NA NA NA NA NA NA NA ## 5 NA NA NA NA NA NA NA ## 6 NA NA NA NA NA NA NA 15.0.6 GBSG2 GBSG2 contains time to death of 686 breast cancer patients. Column cens indicates whether or not a person in the study has died (0-censored, 1-event). data(GBSG2, package = &quot;TH.data&quot;) GBSG2 %&gt;% count(cens) ## # A tibble: 2 x 2 ## cens n ## &lt;int&gt; &lt;int&gt; ## 1 0 387 ## 2 1 299 You typically structure data in a Surv object. In the sampled data shown below, “+” indicates a censored observation. sobj &lt;- Surv(time = GBSG2$time, event = GBSG2$cens) head(sobj, n = 10) ## [1] 1814 2018 712 1807 772 448 2172+ 2161+ 471 2014+ summary(sobj) ## time status ## Min. : 8 Min. :0.00 ## 1st Qu.: 568 1st Qu.:0.00 ## Median :1084 Median :0.00 ## Mean :1124 Mean :0.44 ## 3rd Qu.:1685 3rd Qu.:1.00 ## Max. :2659 Max. :1.00 15.0.7 Unemp UnempDur contains time to re-employment of 3,343 unemployed persons. data(UnempDur, package = &quot;Ecdat&quot;) Column censor1 indicates the re-employment event in a full-time job. Column spell indicates the length of time unemployed in number of two-week intervals. UnempDur %&gt;% count(censor1) ## # A tibble: 2 x 2 ## censor1 n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 2270 ## 2 1 1073 sobj &lt;- Surv(time = UnempDur$spell, event = UnempDur$censor1) head(sobj) ## [1] 5 13 21 3 9+ 11+ summary(sobj) ## time status ## Min. : 1.0 Min. :0.00 ## 1st Qu.: 2.0 1st Qu.:0.00 ## Median : 5.0 Median :0.00 ## Mean : 6.2 Mean :0.32 ## 3rd Qu.: 9.0 3rd Qu.:1.00 ## Max. :28.0 Max. :1.00 References "],
["survival-theory.html", "15.1 Survival Theory", " 15.1 Survival Theory This section reviews the fundamentals of survival analysis, including the hazard probability density, and survival functions. You can specify the survival distribution function either as a survival function, the probability of surviving up to time \\(t\\), \\[S(t) = 1 - F(t) = pr(T &gt; t), \\hspace{3mm} 0 &lt; t &lt; \\infty\\] or as a hazard function, the instantaneous failure rate given survival up to time \\(t\\) \\[h(t) = \\lim_{\\delta \\rightarrow 0}{\\frac{pr(t &lt; T &lt; t + \\delta|T &gt; 1)}{\\delta}}.\\] The survival function is the compliment of the the cumulative risk function, \\[F(t) = Pr(T \\le t), \\hspace{3mm} 0 &lt; t &lt; \\infty\\] The probability density function, \\(f(t)\\) is the derivative of the cumulative risk function \\(F&#39;(t)\\) or the negative derivative of the survival function \\(-S&#39;(t)\\). The hazard function is the ratio of the probability density function to the survival function, \\[h(t) = \\frac{f(t)}{S(t)}\\] That is, the hazard at time \\(t\\) is the probability the event occurs in the neighborhood of time t divided by the probability that the subject is alive at time \\(t\\). The survival function is the exponent of the negative cumulative hazard function, $H(t), \\[S(t) = exp(-H(t))\\] "],
["survival-curve-estimation.html", "15.2 Survival Curve Estimation", " 15.2 Survival Curve Estimation There are parametric and non-parametric methods to estimate a survivor curve. The Kaplan-Meier estimator for the survival function is \\[\\hat{S} = \\prod_{i: t_i &lt; t}{\\frac{n_i - d_i}{n_i}}\\] where \\(n_i\\) is the number of persons under observation at time \\(i\\) and \\(d_i\\) is the number of individuals dying at time \\(i\\). The Kaplan-Meier curve falls only when a subject dies, not when a subject is censored. Calculate the Kaplan-Meier estimate with the survfit() function. Below is a Kaplan-Meier estimate fit one survival curve for all observations. Suppose you throw a party and for an hour monitor how long guests dance. Variable time is the right-censored dancing time, and obs_end indicates if you observed the person stop dancing before you stopped monitoring (1|0). dancedat &lt;- data.frame( name = c(&quot;Chris&quot;, &quot;Martin&quot;, &quot;Conny&quot;, &quot;Desi&quot;, &quot;Reni&quot;, &quot;Phil&quot;, &quot;Flo&quot;, &quot;Andrea&quot;, &quot;Isaac&quot;, &quot;Dayra&quot;, &quot;Caspar&quot;), time = c(20, 2, 14, 22, 3, 7, 4, 15, 25, 17, 12), obs_end = c(1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0) ) km &lt;- survfit(Surv(time, obs_end) ~ 1, data = dancedat) ggsurvplot( fit = km, palette = &quot;blue&quot;, linetype = 1, surv.median.line = &quot;hv&quot;, risk.table = TRUE, cumevents = TRUE, cumcensor = TRUE, tables.height = 0.1 ) km &lt;- survfit(Surv(time, cens) ~ 1, data = GBSG2) ggsurvplot(km, risk.table = TRUE, surv.median.line = &quot;hv&quot;) Several parametric distributions are available for modeling survival data. The exponential distribution is the easiest to use because it has a constant hazard \\(h(t) = \\lambda\\). The cumulative hazard is \\(H(t) = \\int_0^t \\lambda dt = \\lambda t\\) and the corresponding survival function is \\[S(t) = e^{-H(t)} = e^{-\\lambda t}.\\] The expected survival time is \\(E(T) = \\int_0^\\infty S(t)dt = \\int_0^\\infty d^{-\\lambda t} dt = 1 / \\lambda.\\). The median survival time is \\(S(t) = e^{-\\lambda t} = 0.5\\), or \\(t_{med} = \\log(2) / \\lambda\\). The Weibull distribution is more appropriate for modeling lifetimes, however. The Weibull hazard function is \\(h(t) = \\alpha \\lambda (\\lambda t)^{\\alpha - 1} = \\alpha \\lambda^\\alpha t^{\\alpha-1}\\). data.frame(t = rep(1:80, 3), alpha = c(rep(1.5, 80), rep(1, 80), rep(0.75, 80)), lambda = rep(0.03, 240)) %&gt;% mutate( f = dweibull(x = t, shape = alpha, scale = 1 / 0.03), S = pweibull(q = t, shape = alpha, scale = 1 / 0.03, lower.tail = FALSE), h = f / S # same as alpha * lambda^alpha * t^(alpha-1) ) %&gt;% ggplot(aes(x = t, y = h, color = as.factor(alpha))) + geom_line() + theme(legend.position = &quot;top&quot;) + labs(y = &quot;hazard&quot;, x = &quot;time&quot;, color = &quot;alpha&quot;, title = &quot;Weibul hazard function at varying levels of alpha&quot;, subtitle = &quot;Lambda = 0.03&quot;, caption = &quot;alpha = 1 is special case of exponential function.&quot;) The cumulative hazard function is \\(H(t) = (\\lambda t)^\\alpha\\) and the corresponding survival function is \\[S(t) = e^{-(\\lambda t)^\\alpha}.\\] The exponential distribution is a special case of the Weibull where \\(\\alpha = 1\\). The expected survival time is \\(E(t) = \\frac{\\Gamma (1 + 1 / \\alpha)}{\\lambda}\\). The median survival time is \\(t_{med} = \\frac{[\\log(2)]^{1 / \\alpha}}{\\lambda}\\) The Kaplan-Meier estimate is used mainly as a descriptive tool. The Weibull model produces a smooth survival curve instead of a step function. The Weibull model assumes a Weibull distribution. Fit a Weibull model with the survreg() function. wb &lt;- survreg(Surv(time, cens) ~ 1, data = GBSG2) # 90% of patients survive beyond time point 385 # Alternatively, 10% of patients die at time 385 predict(wb, type = &quot;quantile&quot;, p = 1 - 0.9, newdata = data.frame(1)) ## 1 ## 385 # The median survival time is 1694 predict(wb, type = &quot;quantile&quot;, p = 1 - 0.5, newdata = data.frame(1)) ## 1 ## 1694 surv &lt;- seq(.99, .01, by = -.01) t &lt;- predict(wb, type = &quot;quantile&quot;, p = 1 - surv, newdata = data.frame(1)) head(data.frame(time = t, surv = surv)) ## time surv ## 1 61 0.99 ## 2 105 0.98 ## 3 145 0.97 ## 4 183 0.96 ## 5 219 0.95 ## 6 253 0.94 surv_wb &lt;- data.frame(time = t, surv = surv, upper = NA, lower = NA, std.err = NA) ggsurvplot_df(fit = surv_wb, surv.geom = geom_line) Fit a Weibull model controlling for hormonal therapy horTh and tumor size tsize. wbmod &lt;- survreg(Surv(time, cens) ~ horTh + tsize, data = GBSG2) coef(wbmod) ## (Intercept) horThyes tsize ## 7.961 0.312 -0.012 summary(wbmod) ## ## Call: ## survreg(formula = Surv(time, cens) ~ horTh + tsize, data = GBSG2) ## Value Std. Error z p ## (Intercept) 7.96070 0.10413 76.45 &lt; 0.0000000000000002 ## horThyes 0.31176 0.09602 3.25 0.0012 ## tsize -0.01218 0.00272 -4.47 0.000007772 ## Log(scale) -0.26494 0.04952 -5.35 0.000000088 ## ## Scale= 0.767 ## ## Weibull distribution ## Loglik(model)= -2623 Loglik(intercept only)= -2637 ## Chisq= 28 on 2 degrees of freedom, p= 0.00000076 ## Number of Newton-Raphson Iterations: 5 ## n= 686 surv &lt;- seq(.99, .01, by = -.01) newdata &lt;- expand.grid( horTh = levels(GBSG2$horTh), tsize = quantile(GBSG2$tsize, probs = c(0.25, 0.50, 0.75)) ) t &lt;- predict(wbmod, type = &quot;quantile&quot;, p = 1 - surv, newdata = newdata) surv_wbmod &lt;- surv_wbmod_wide &lt;- cbind(newdata, t) %&gt;% pivot_longer(names_to = &quot;surv_id&quot;, values_to = &quot;time&quot;, cols = -c(1:2)) %&gt;% mutate(tsize = as.numeric(tsize), surv_id = as.factor(as.numeric(surv_id))) %&gt;% data.frame() surv_wbmod$surv = surv[as.numeric(surv_wbmod$surv_id)] surv_wbmod$upper = NA surv_wbmod$lower = NA surv_wbmod$std.err = NA surv_wbmod$strata = NA surv_wbmod[, c(&quot;upper&quot;, &quot;lower&quot;, &quot;std.err&quot;, &quot;strata&quot;)] &lt;- NA ggsurvplot_df(surv_wbmod, surv.geom = geom_line, linetype = &quot;horTh&quot;, color = &quot;tsize&quot;, legend.title = NULL) Interpret the coefficient as the probability of surviving falls by 0.012 per unit increase in the tumor size and increases by 0.312 if taking hormonal therapy. You can fit other models with the dist = c(\"lognormal\", \"exponential\") parameter. The Cox model (aka, proportional hazards model) is the most widely used model for survival analysis. Whereas the Weibull model is fully parametric, the Cox model is semi-parameteric. Fit a Cox proportional hazards model with coxph(). Negative values indicate a longer survival period. cxmod &lt;- coxph(Surv(time, cens) ~ horTh + tsize, data = GBSG2) coef(cxmod) ## horThyes tsize ## -0.374 0.015 newdata &lt;- expand.grid( horTh = levels(GBSG2$horTh), tsize = quantile(GBSG2$tsize, probs = c(0.25, 0.50, 0.75)) ) rownames(newdata) &lt;- letters[1:6] # Create survival curves. The rownames show up in the model cxsf &lt;- survfit(cxmod, data = GBSG2, newdata = newdata, conf.type = &quot;none&quot;) head(cxsf$surv) ## a b c d e f ## [1,] 1 1 1 1 1 1 ## [2,] 1 1 1 1 1 1 ## [3,] 1 1 1 1 1 1 ## [4,] 1 1 1 1 1 1 ## [5,] 1 1 1 1 1 1 ## [6,] 1 1 1 1 1 1 head(cxsf$time) ## [1] 8 15 16 17 18 29 # surv_summary() creates the data.frame with a nice summary from survfit() results, including columns like time (survival time) and surv (survival probability). surv_cxmod0 &lt;- surv_summary(cxsf) # get tne correspondng new_data cols surv_cxmod &lt;- cbind(surv_cxmod0, newdata[as.character(surv_cxmod0$strata), ]) ggsurvplot_df(surv_cxmod, linetype = &quot;horTh&quot;, color = &quot;tsize&quot;, legend.title = NULL, censor = FALSE) The visualization shows that patients with smaller tumors tend to survive longer and patients who receive hormonal therapy tend to survive longer. "],
["survival-curve-estimation-1.html", "15.3 Survival Curve Estimation", " 15.3 Survival Curve Estimation When "],
["proportional-hazards-model.html", "15.4 Proportional Hazards Model", " 15.4 Proportional Hazards Model "],
["survival-analysis-1.html", "Chapter 16 Survival Analysis", " Chapter 16 Survival Analysis Survival analysis models time to event. Whereas linear regression models a normal distribution of outcomes, time-to-event analysis only takes on positive values, so survival analysis uses the Weibull distribution. Another complication with surival analysis is censoring. These notes rely on the Survival Analysis in R DataCamp course and Applied Survival Analysis Using R by Dirk Moore (Moore 2016). Most surival analysis uses the survival and survminer packages. library(tidyverse) library(survival) library(survminer) The examples in these notes will use the following data sets. 16.0.1 GBSG2 GBSG2 contains time to death of 686 breast cancer patients. data(GBSG2, package = &quot;TH.data&quot;) Column cens indicate whether or not a person in the study has died (0-censored, 1-event). GBSG2 %&gt;% count(cens) ## # A tibble: 2 x 2 ## cens n ## &lt;int&gt; &lt;int&gt; ## 1 0 387 ## 2 1 299 You will typically structure your data in a Surv object. In the sampled data shown below, “+” indicates a censored observation. sobj &lt;- Surv(time = GBSG2$time, event = GBSG2$cens) head(sobj, n = 10) ## [1] 1814 2018 712 1807 772 448 2172+ 2161+ 471 2014+ summary(sobj) ## time status ## Min. : 8 Min. :0.00 ## 1st Qu.: 568 1st Qu.:0.00 ## Median :1084 Median :0.00 ## Mean :1124 Mean :0.44 ## 3rd Qu.:1685 3rd Qu.:1.00 ## Max. :2659 Max. :1.00 16.0.2 Unemp UnempDur contains time to re-employment of 3,343 unemployed persons. data(UnempDur, package = &quot;Ecdat&quot;) Column censor1 indicates the re-employment event in a full-time job. Column spell indicates the length of time unemployed in number of two-week intervals. UnempDur %&gt;% count(censor1) ## # A tibble: 2 x 2 ## censor1 n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 2270 ## 2 1 1073 sobj &lt;- Surv(time = UnempDur$spell, event = UnempDur$censor1) head(sobj) ## [1] 5 13 21 3 9+ 11+ summary(sobj) ## time status ## Min. : 1.0 Min. :0.00 ## 1st Qu.: 2.0 1st Qu.:0.00 ## Median : 5.0 Median :0.00 ## Mean : 6.2 Mean :0.32 ## 3rd Qu.: 9.0 3rd Qu.:1.00 ## Max. :28.0 Max. :1.00 References "],
["background-1.html", "16.1 Background", " 16.1 Background You can specify the survival distribution function either as a survival function of the form \\[S(t) = 1 - F(t) = pr(T &gt; t), \\hspace{3mm} 0 &lt; t &lt; \\infty\\] or as a hazard function, the instantaneous failure rate given survival up to time \\(t\\) \\[h(t) = \\lim_{\\delta \\rightarrow 0}{\\frac{pr(t &lt; T &lt; t + \\delta|T &gt; 1)}{\\delta}}.\\] The survival function is the compliment of the the cumulative distribution function. The Kaplan-Meier estimator for the survival function is \\[\\hat{S} = \\prod_{i: t_i &lt; t}{\\frac{n_i - d_i}{n_i}}\\] where \\(n_i\\) is the number of persons under observation at time \\(i\\) and \\(d_i\\) is the number of individuals dying at time \\(i\\). Calculate the Kaplan-Meier estimate with the survfit() function. library(survival) tm &lt;- c( 0, # birth 1 / 365, # first day of life 7 / 365, # seventh day of life 28 / 365, # fourth week of life 1:110 # subsequent years ) hazMale = survexp.us[, &quot;male&quot;, &quot;2004&quot;] hazFemale = survexp.us[, &quot;female&quot;, &quot;2004&quot;] #plot(tm, hazMale) "],
["appendix.html", "Appendix", " Appendix Here are miscellaneous skills, knowledge, and technologies I should know. "],
["publishing-to-bookdown.html", "Publishing to BookDown", " Publishing to BookDown The bookdown package, written by Yihui Xie, is built on top of R Markdown and the knitr package. Use it to publish a book or long manuscript where each chapter is a separate file. There are instructions for how to author a book in his bookdown book (Xie 2019). The main advantage of bookdown over R Markdown is that you can produce multi-page HTML output with numbered headers, equations, figures, etc., just like in a book. I’m using bookdown to create a compendium of all my data science notes. The first step to using bookdown is installing the **bookdown* package with install.packages(\"bookdown\"). Next, create an account at bookdown.org, and connect the account to RStudio. Follow the instructions at https://bookdown.org/home/about/. Finally, create a project in R Studio by creating a new project of type Book Project using Bookdown. After creating all of your Markdown pages, knit the book or click the Build Book button in the Build panel. References "],
["shiny-apps.html", "Shiny Apps", " Shiny Apps "],
["packages.html", "Packages", " Packages R Packages (Wickham 2015) by Hadley Wickham is a good manual on packages, but it does not include a full tutorial. The Developing R Packages Data Camp course is also helpful. I will set up my own exercise and present it here. I will create a package for my pretend organization, “MF”. The package will include the following: R Markdown template. My template will integrate code, output, and commentary in a single R Markdown. The template will produce a familiar work product containing standard content (summary, data management, exploratory analysis, methods, results, conclusions), and a standard style (colors, typeface, size, logo). Functions. Common I/O functions for database retrieval, writing to Excel. Common graphing functions for ggplot styling. I am mostly copying the logic and code from the ggthemes economist.R script. Create a package In the RStudio IDE, click File &gt; New Project. Select “New Directory”. Select “R Package”. You can also use devtools::create(\"mfstylr\"). This will create the minimum items for an R package. + R directory: R scripts with function definitions. + man directory: documentation + NAMESPACE file: information about imported functions and functions made available (managed by **roxygen2**) + DESCRIPTION file: metadata about the package Write functions in R scripts in R directory. Document with tags readable by roxygen2 package. Select XYZ &gt; Install and Restart. 16.1.1 Document Functions with roxygen Add roxygen documentation with #' characters. The first three lines are always the title, Description, and Details. They don’t need any tags, but you need to separate them with blank lines. Create Data Add an RData file to your package with use_data() Create Vignette Add a directory and template vignette with use_vignette(name, title). use_vignette(&quot;Creating-Plots-with-mfstylr&quot;, &quot;Creating Plots with mfstylr&quot;) Step 2: Create an R Markdown template I relied on this blog at free range statistics for a lot what follows. There is also good information about R Markdown and templates in Yihui Xie’s R Markdown: The Definitive Guide (Xie, Allaire, and Grolemund 2019). Use usethis::use_rmarkdown_template() to create an Rmd template. I will create a “Kaggle Report” template. In the Console (or a script), enter usethis::use_rmarkdown_template( template_name = &quot;Kaggle Report&quot;, template_dir = &quot;kaggle_report&quot;, template_description = &quot;Template for creating Kaggle reports in RMarkdown.&quot;, template_create_dir = FALSE ) Since my project directory is C:\\Users\\mpfol\\OneDrive\\Documents\\GitHub\\mfstylr, use_rmarkdown_template() creates subdirectories .\\inst\\rmarkdown\\templates\\kaggle_report\\skeleton with three files .\\inst\\rmarkdown\\templates\\kaggle_report\\template.yaml .\\inst\\rmarkdown\\templates\\kaggle_report\\skeleton\\skeleton.Rmd My kaggle report template will include a logo. Looks like there are two ways to embed an image in your document. One is a direct image loading reference !(), but I don’t think you can control the attributes this way. A second way is adding html tags. ![](logo.png) # or for more control &lt;img src=&quot;logo.png&quot; style=&quot;position:absolute;top:0px;right:0px;&quot; /&gt; References "],
["references.html", "References", " References "]
]
