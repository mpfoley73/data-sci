[
["generalized-linear-models.html", "Chapter 7 Generalized Linear Models", " Chapter 7 Generalized Linear Models These notes are primarily from PSU STAT 504 which uses Alan Agresti’s Categorical Data Analysis (Agresti 2013). I also reviewed PSU STAT 501, DataCamp’s Generalized Linar Models in R, DataCamp’s Multiple and Logistic Regression, and **Interpretable machine learning*\"** (Molnar 2020). The linear regression model, \\(E(Y|X) = X \\beta\\), structured as \\(y_i = X_i \\beta + \\epsilon_i\\) where \\(X_i \\beta = \\mu_i\\), assumes the response is a linear function of the predictors and the residuals are independent random variables normally distributed with mean zero and constant variance, \\(\\epsilon \\sim N \\left(0, \\sigma^2 \\right)\\). This implies that given some set of predictors, the response is normally distributed about its expected value, \\(y_i \\sim N \\left(\\mu_i, \\sigma^2 \\right)\\). However, there are many situations where this assumption of normality fails. Generalized linear models (GLMs) are a generalization of the linear regression model that addresses non-normal response distributions. The response given a set of predictors will not have a normal distribution if its underlying data-generating process is binomial or multinomial (proportions), Poisson (counts), or exponential (time-to-event). In these situations a regular linear regression can predict proportions outside [0, 100] or counts or times that are negative. GLMs solve this problem by modeling a function of the expected value of \\(y\\), \\(f(E(Y|X)) = X \\beta\\). There are three components to a GLM: the random component is the probability distribution of the response variable (normal, binomial, Poisson, etc.); the systematic component is the explanatory variables \\(X\\beta\\); and the link function \\(\\eta\\) specifies the link between random and systematic components, converting the response range to \\([-\\infty, +\\infty]\\). Linear regression is thus a special case of GLM where link function is the identity function, \\(f(E(Y|X)) = E(Y|X)\\). For a logistic regression, where the data generating process is binomial, the link function is \\[f(E(Y|X)) = \\ln \\left( \\frac{E(Y|X)}{1 - E(Y|X)} \\right) = \\ln \\left( \\frac{\\pi}{1 - \\pi} \\right) = logit(\\pi)\\] where \\(\\pi\\) is the event probability. (As an aside, you have probably heard of the related “probit” regression. The probit regression link function is \\(f(E(Y|X)) = \\Phi^{-1}(E(Y|X)) = \\Phi^{-1}(\\pi)\\). The difference between the logistic and probit link function is theoretical, and the practical significance is slight. You can probably safely ignore probit). For a Poisson regression, the link function is \\[f(E(Y|X)) = \\ln (E(Y|X)) = \\ln(\\lambda)\\] where \\(\\lambda\\) is the expected event rate. For an exponential regression, the link function is \\[f(E(Y|X) = -E(Y|X) = -\\lambda\\] where \\(\\lambda\\) is the expected time to event. GLM uses maximum likelihood estimation (MLE) rather than ordinary least squares (OLS) to estimate the parameters, and thus relies on large-sample approximations. In R, specify a GLM just like an linear model, but with the glm() function, specifying the distribution with the family parameter. family = \"gaussian\": linear regression family = \"binomial\": logistic regression family = binomial(link = \"probit\"): probit regression family = \"poisson\": Poisson regression References "],
["logistic-regression.html", "7.1 Logistic Regression", " 7.1 Logistic Regression Logistic regression estimates the probability of a particular level of a categorical response variable given a set of predictors. The response levels can be binary, nominal (multiple categories), or ordinal (multiple levels). The binary logistic regression model is \\[y = logit(\\pi) = \\ln \\left( \\frac{\\pi}{1 - \\pi} \\right) = X \\beta\\] where \\(\\pi\\) is the event probability. The model predicts the log odds of the response variable. The maximum likelihood estimator maximizes the likelihood function \\[L(\\beta; y, X) = \\prod_{i=1}^n \\pi_i^{y_i}(1 - \\pi_i)^{(1-y_i)} = \\prod_{i=1}^n\\frac{\\exp(y_i X_i \\beta)}{1 + \\exp(X_i \\beta)}.\\] There is no closed-form solution, so GLM estimates coefficients with interatively reweighted least squares. Here is a case study to illustrate the points. Dataset leuk contains response variable REMISS indicating whether leukemia remission occurred (1|0) and several explanatory variables. glimpse(leuk) ## Observations: 27 ## Variables: 7 ## $ REMISS &lt;dbl&gt; 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, ... ## $ CELL &lt;dbl&gt; 0.80, 0.90, 0.80, 1.00, 0.90, 1.00, 0.95, 0.95, 1.00, 0.95, ... ## $ SMEAR &lt;dbl&gt; 0.83, 0.36, 0.88, 0.87, 0.75, 0.65, 0.97, 0.87, 0.45, 0.36, ... ## $ INFIL &lt;dbl&gt; 0.66, 0.32, 0.70, 0.87, 0.68, 0.65, 0.92, 0.83, 0.45, 0.34, ... ## $ LI &lt;dbl&gt; 1.9, 1.4, 0.8, 0.7, 1.3, 0.6, 1.0, 1.9, 0.8, 0.5, 0.7, 1.2, ... ## $ BLAST &lt;dbl&gt; 1.10, 0.74, 0.18, 1.05, 0.52, 0.52, 1.23, 1.35, 0.32, 0.00, ... ## $ TEMP &lt;dbl&gt; 1.00, 0.99, 0.98, 0.99, 0.98, 0.98, 0.99, 1.02, 1.00, 1.04, ... Fit a logistic regression. m1 &lt;- glm(REMISS ~ ., family = binomial(link = logit), data = leuk) summary(m1) ## ## Call: ## glm(formula = REMISS ~ ., family = binomial(link = logit), data = leuk) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.95404 -0.66259 -0.02516 0.78184 1.57465 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 64.25808 74.96480 0.857 0.391 ## CELL 30.83006 52.13520 0.591 0.554 ## SMEAR 24.68632 61.52601 0.401 0.688 ## INFIL -24.97447 65.28088 -0.383 0.702 ## LI 4.36045 2.65798 1.641 0.101 ## BLAST -0.01153 2.26634 -0.005 0.996 ## TEMP -100.17340 77.75289 -1.288 0.198 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 34.372 on 26 degrees of freedom ## Residual deviance: 21.594 on 20 degrees of freedom ## AIC: 35.594 ## ## Number of Fisher Scoring iterations: 8 “z value” is the Wald z statistic, \\(z = \\hat{\\beta} / SE(\\hat{\\beta})\\), which if squared is the Wald chi-squared statistic, \\(z^2 = \\left(\\hat{\\beta} / SE(\\hat{\\beta}) \\right)^2\\). The p.value is the area to the right of \\(z^2\\) in the \\(\\chi_1^2\\) density curve. m1 %&gt;% tidy() %&gt;% mutate( z = estimate / std.error, p_z2 = pchisq(z^2, df = 1, lower.tail = FALSE) ) %&gt;% select(term, p.value, p_z2) ## # A tibble: 7 x 3 ## term p.value p_z2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.391 0.391 ## 2 CELL 0.554 0.554 ## 3 SMEAR 0.688 0.688 ## 4 INFIL 0.702 0.702 ## 5 LI 0.101 0.101 ## 6 BLAST 0.996 0.996 ## 7 TEMP 0.198 0.198 The “dispersion parameter” refers to overdispersion, a common issues with GLM. For a logistic regression, the response variable should be distributed \\(y_i \\sim Bin(n_i, \\pi_i)\\) with \\(\\mu_i = n_i \\pi_i\\) and \\(\\sigma^2 = \\pi (1 - \\pi)\\). Overdispersion means the data shows evidence of variance greater than \\(\\sigma^2\\). “Fisher scoring” is a method for ML estimation. Logistic regression uses an iterative procedure to fit the model, so this section indicates whether the algorithm converged. The log odds of having achieved remission when each predictor equals its mean value is \\(\\hat{y} = -2.68\\). (pred &lt;- predict(m1, newdata = data.frame(t(colMeans(leuk))))) ## 1 ## -2.684382 Log odds are not easy to interpet, but are convenient for updating prior probabilities in Bayesian analyses. Exponentiate the log odds to get the more intuitive odds. \\[\\exp (\\hat{y}) = \\exp (X \\hat{\\beta}) = \\frac{\\pi}{1 - \\pi}.\\] The odds of having achieved remission when each predictor equals its mean value is \\(\\exp(\\hat{y}) = 0.068\\). exp(pred) ## 1 ## 0.06826334 A common way to express this is with the inverse, 1 / 0.068 = 15:1. The odds of having achieved remission when each predictor equals its mean value is “15 to 1”. round(1/exp(pred), 0) ## 1 ## 15 Or, solve for \\(\\pi\\) to get the probability. \\[\\pi = \\frac{\\exp (X \\beta)}{1 + \\exp (X \\beta)}\\] The probability of having achieved remission when each predictor equals its mean value is \\(\\pi = 0.064\\). The predict() function for a logistic model returns log-odds, but can also return \\(\\pi\\) by specifying parameter type = \"response\". predict(m1, newdata = data.frame(t(colMeans(leuk))), type = &quot;response&quot;) ## 1 ## 0.06390123 Interpret the coefficient estimates using the odds ratio, the ratio of the odds before and after an increment to the predictors. The odds ratio is how much the odds would be multiplied after a \\(X_1 - X_0\\) unit increase in \\(X\\). \\[\\theta = \\frac{\\pi / (1 - \\pi) |_{X = X_1}}{\\pi / (1 - \\pi) |_{X = X_0}} = \\frac{\\exp (X_1 \\hat{\\beta})}{\\exp (X_0 \\hat{\\beta})} = \\exp ((X_1-X_0) \\hat{\\beta}) = \\exp (\\delta \\hat{\\beta})\\] Increasing LI by .01 increases the odds of remission by a factor of \\(\\exp(0.1 \\cdot 4.36) = 1.547\\) (from 15:1 to 23:1). exp(.1 * m1$coefficients) %&gt;% round(3) ## (Intercept) CELL SMEAR INFIL LI BLAST ## 617.580 21.824 11.806 0.082 1.547 0.999 ## TEMP ## 0.000 You can calculate an odds ratio with specified increments using oddsratio::or_glm(). oddsratio::or_glm(data = leuk, model = m1, incr = list( CELL = 0.01, SMEAR = 0.01, INFIL = 0.05, LI = 0.1, BLAST = 1.0, TEMP = 0.03 ) ) ## # A tibble: 6 x 5 ## predictor oddsratio `CI_low (2.5)` `CI_high (97.5)` increment ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 CELL 1.36 0.747 4.64 0.01 ## 2 SMEAR 1.28 0.537 5.15 0.01 ## 3 INFIL 0.287 0 33.1 0.05 ## 4 LI 1.55 1.04 2.99 0.1 ## 5 BLAST 0.989 0.009 91.0 1 ## 6 TEMP 0.05 0 2.02 0.03 The predicted values can also be expressed as the probabilities \\(\\pi\\). This produces the familiar signmoidal shape of the binary relationship. augment(m1, type.predict = &quot;response&quot;) %&gt;% ggplot(aes(x = LI)) + geom_point(aes(y = REMISS)) + geom_line(aes(y = .fitted)) + theme_mf() + labs(x = &quot;LI&quot;, y = &quot;Probability of REMISS&quot;, title = &quot;Binary Fitted Line Plot&quot;) Evaluate a logistic model fit with an analysis of deviance. Deviance is defined as -2 times the log-likelihood \\(-2l(\\beta)\\). The null deviance is the deviance of the null model and is analagous to SST in ANOVA. The residual deviance is the deviance of the full model and is analagous to SSE in ANOVA. logLik(glm(REMISS ~ ., data = leuk, family = &quot;binomial&quot;)) * (-2) ## &#39;log Lik.&#39; 21.59385 (df=7) anova() computes the analysis of deviance table for the model. anova(m1) ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: REMISS ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev ## NULL 26 34.372 ## CELL 1 2.5800 25 31.792 ## SMEAR 1 0.5188 24 31.273 ## INFIL 1 0.2927 23 30.980 ## LI 1 6.7818 22 24.199 ## BLAST 1 0.3271 21 23.871 ## TEMP 1 2.2775 20 21.594 The deviance of the null model (no regressors) is 34.372. The deviance of the full model is 21.594. glance() also returns the null and residual deviances. glance(m1) ## # A tibble: 1 x 7 ## null.deviance df.null logLik AIC BIC deviance df.residual ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 34.4 26 -10.8 35.6 44.7 21.6 20 Use the GainCurvePlot() function to plot the gain curve (background on gain curve at Data Science Central from the model predictions. The x-axis is the fraction of items seen when sorted by the predicted value, and the y-axis is the cumulative summed true outcome. The “wizard” curve is the gain curve when the data is sorted by the true outcome. If the model’s gain curve is close to the wizard gain curve, then the model sorted the response variable well. The grey area is the gain over a random sorting. augment(m1) %&gt;% data.frame() %&gt;% GainCurvePlot(xvar = &quot;.fitted&quot;, truthVar = &quot;REMISS&quot;, title = &quot;Logistic Model&quot;) REMISS equals 1 in 9 of the 27 responses. The wizard curve shows that after sorting the responses it encounters all 9 1s (100%) after looking at 9 of the 27 response (33%). The bottom of the grey diagonal shows that after making random predictions and sorting the predictions, it encounters only 3 1s (33%) after looking at 9 of the 27 responses (33%). It has to look at all 27 responses (100%) to encounter all 9 1s (100%). The gain curve encounters 5 1s (55%) after looking at 9 of the 27 responses (33%). It has to look at 14 responses to encounter all 9 1s (100%). Another way to evaluate the predictive model is the ROC curve. It evaluates all possible thresholds for splitting predicted probabilities into predicted classes. This is often a much more useful metric than simply ranking models by their accuracy at a set threshold, as different models might require different calibration steps (looking at a confusion matrix at each step) to find the optimal classification threshold for that model. library(caTools) ## Warning: package &#39;caTools&#39; was built under R version 3.6.2 colAUC(m1$fitted.values, m1$data$REMISS, plotROC = TRUE) ## [,1] ## 0 vs. 1 0.8950617 7.1.1 Case Study This case study is from PSU STAT 504. This study investigated the gender differences in Piaget’s water level test. 166 subjects performed the test and other tests of knowledge and spatial ability. Here are the pass/fail results of the study by sex. prop.table(table(h2o$sex, h2o$y), margin = 1) %&gt;% round(digits = 4) ## ## fail pass ## M 0.3559 0.6441 ## F 0.7009 0.2991 The pass rates are very different. The Pearson Chi-squared test of equality of two proportions calculates a Chi-Square value of 18.562, p-value &lt; 0.0001, so reject the hypothesis that the pass rates are the same. chisq.test(table(h2o$sex, h2o$y), correct = FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: table(h2o$sex, h2o$y) ## X-squared = 18.562, df = 1, p-value = 1.645e-05 Fit a logistic regression. m &lt;- glm(y ~ sex, data = h2o, family = binomial(link = logit)) anova(m) ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: y ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev ## NULL 165 226.04 ## sex 1 18.658 164 207.38 The likelihood ratio is \\(G^2 = 18.658\\), so reject the null hypothesis of no sex effect - there is statistically significant difference in pass rates. Here are the coefficient estimators. tidy(m) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.593 0.272 2.18 0.0292 ## 2 sexF -1.44 0.344 -4.20 0.0000271 The odds ratio (females vs males) is exp(tidy(m)[2, ]$estimate) ## [1] 0.2357895 meaning the odds of a female passing are 0.24 times that of a male. Or take the inverse to get the male vs female odds ratio, 1 / exp(tidy(m)[2, ]$estimate) ## [1] 4.241071 meaning the odds of a male passing are 4.24 times that of a female. oddsratio::or_glm() calculates the odds ratio from an increment in the predictor values, in this case, incrementing sex from male to female. oddsratio::or_glm(h2o, m, 1) ## # A tibble: 1 x 5 ## predictor oddsratio `CI_low (2.5)` `CI_high (97.5)` increment ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 sexF 0.236 0.118 0.458 Indicator variable What if we control for the results of the gravity test (gravity = number of correct answers on test)? Here is a logistic regression of pass ~ gravity. m &lt;- glm(y ~ gravity, data = h2o, family = binomial(link = logit)) anova(m) ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: y ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev ## NULL 165 226.04 ## gravity 1 42.176 164 183.86 The likelihood ratio is \\(G^2 = 42.2\\), so reject H0 that there is no gravity effect - there is a statistically significant difference between the gravity score and the pass rate. The odds of passing the water level task increase by 2.225 for each additional right answer on gravity. oddsratio::or_glm(h2o, m, incr = list(gravity = 1)) ## # A tibble: 1 x 5 ## predictor oddsratio `CI_low (2.5)` `CI_high (97.5)` increment ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 gravity 2.22 1.71 3 1 augment(m) %&gt;% group_by(gravity) %&gt;% summarize( observed = mean(as.numeric(y) - 1), fitted = mean(exp(.fitted) / (1 + exp(.fitted))) ) %&gt;% pivot_longer(cols = -gravity, names_to = &quot;model&quot;, values_to = &quot;rate&quot;) %&gt;% ggplot(aes(x = gravity, y = rate, color = model)) + geom_point() + scale_color_mf() + theme_mf() + labs(title = &quot;Water Test Pass Rate&quot;, color = &quot;&quot;) You can model sex and gravity with and without interaction effects. Here is the model without interaction effects. \\(G^2 = 32.3\\) so m &lt;- glm(y ~ sex + gravity, data = h2o, family = binomial(link = &quot;logit&quot;)) anova(m) ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: y ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev ## NULL 165 226.04 ## sex 1 18.658 164 207.38 ## gravity 1 32.319 163 175.06 tidy(m) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -1.92 0.574 -3.35 0.000810 ## 2 sexF -1.12 0.382 -2.93 0.00334 ## 3 gravity 0.740 0.147 5.05 0.000000443 "],
["multinomial-logistic-regression.html", "7.2 Multinomial Logistic Regression", " 7.2 Multinomial Logistic Regression The following notes rely heavily on the [PSU STAT 504 course notes](https://online.stat.psu.edu/stat504/node/171/. Multinomial logistic regression models the odds the multinomial response variable \\(Y \\sim Mult(n, \\pi)\\) is in level \\(j\\) relative to baseline category \\(j^*\\) for all pairs of categories as a function of \\(k\\) explanatory variables, \\(X = (X_1, X_2, ... X_k)\\). \\[\\log \\left( \\frac{\\pi_{ij}}{\\pi_{ij^*}} \\right) = x_i^T \\beta_j, \\hspace{5mm} j \\ne j^2\\] Interpet the \\(k^{th}\\) element of \\(\\beta_j\\) as the increase in log-odds of falling a response in category \\(j\\) relative to category \\(j^*\\) resulting from a one-unit increase in the \\(k^{th}\\) predictor term, holding the other terms constant. Multinomial model is a type of GLM. Here is an example using multinomial logistic regression. A researcher classified the stomach contents of \\(n = 219\\) alligators according to \\(r = 5\\) categories (fish, Inv., Rept, Bird, Other) as a function of covariates Lake, Sex, and Size.. gator_dat &lt;- tribble( ~profile, ~Gender, ~Size, ~Lake, ~Fish, ~Invertebrate, ~Reptile, ~Bird, ~Other, &quot;1&quot;, &quot;f&quot;, &quot;&lt;2.3&quot;, &quot;george&quot;, 3, 9, 1, 0, 1, &quot;2&quot;, &quot;m&quot;, &quot;&lt;2.3&quot;, &quot;george&quot;, 13, 10, 0, 2, 2, &quot;3&quot;, &quot;f&quot;, &quot;&gt;2.3&quot;, &quot;george&quot;, 8, 1, 0, 0, 1, &quot;4&quot;, &quot;m&quot;, &quot;&gt;2.3&quot;, &quot;george&quot;, 9, 0, 0, 1, 2, &quot;5&quot;, &quot;f&quot;, &quot;&lt;2.3&quot;, &quot;hancock&quot;, 16, 3, 2, 2, 3, &quot;6&quot;, &quot;m&quot;, &quot;&lt;2.3&quot;, &quot;hancock&quot;, 7, 1, 0, 0, 5, &quot;7&quot;, &quot;f&quot;, &quot;&gt;2.3&quot;, &quot;hancock&quot;, 3, 0, 1, 2, 3, &quot;8&quot;, &quot;m&quot;, &quot;&gt;2.3&quot;, &quot;hancock&quot;, 4, 0, 0, 1, 2, &quot;9&quot;, &quot;f&quot;, &quot;&lt;2.3&quot;, &quot;oklawaha&quot;, 3, 9, 1, 0, 2, &quot;10&quot;, &quot;m&quot;, &quot;&lt;2.3&quot;, &quot;oklawaha&quot;, 2, 2, 0, 0, 1, &quot;11&quot;, &quot;f&quot;, &quot;&gt;2.3&quot;, &quot;oklawaha&quot;, 0, 1, 0, 1, 0, &quot;12&quot;, &quot;m&quot;, &quot;&gt;2.3&quot;, &quot;oklawaha&quot;, 13, 7, 6, 0, 0, &quot;13&quot;, &quot;f&quot;, &quot;&lt;2.3&quot;, &quot;trafford&quot;, 2, 4, 1, 1, 4, &quot;14&quot;, &quot;m&quot;, &quot;&lt;2.3&quot;, &quot;trafford&quot;, 3, 7, 1, 0, 1, &quot;15&quot;, &quot;f&quot;, &quot;&gt;2.3&quot;, &quot;trafford&quot;, 0, 1, 0, 0, 0, &quot;16&quot;, &quot;m&quot;, &quot;&gt;2.3&quot;, &quot;trafford&quot;, 8, 6, 6, 3, 5 ) gator_dat &lt;- gator_dat %&gt;% mutate( Gender = as_factor(Gender), Lake = fct_relevel(Lake, &quot;hancock&quot;), Size = as_factor(Size) ) There are 4 equations to estimate: \\[\\log \\left( \\frac{\\pi_j} {\\pi_{j^*}} \\right) = \\beta X\\] where \\(\\pi_{j^*}\\) is the probability of fish, the baseline category. Run a multivariate logistic regression model with VGAM::vglm(). library(VGAM) ## Warning: package &#39;VGAM&#39; was built under R version 3.6.2 vglm() fits 4 logit models. gator_vglm &lt;- vglm( cbind(Bird,Invertebrate,Reptile,Other,Fish) ~ Lake + Size + Gender, data = gator_dat, family = multinomial ) summary(gator_vglm) ## ## Call: ## vglm(formula = cbind(Bird, Invertebrate, Reptile, Other, Fish) ~ ## Lake + Size + Gender, family = multinomial, data = gator_dat) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## log(mu[,1]/mu[,5]) -1.1985 -0.5478 -0.22421 0.3678 3.478 ## log(mu[,2]/mu[,5]) -1.3218 -0.4611 0.01054 0.3810 1.866 ## log(mu[,3]/mu[,5]) -0.7033 -0.5751 -0.35511 0.2610 2.064 ## log(mu[,4]/mu[,5]) -1.6945 -0.2893 -0.10807 1.1236 1.367 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept):1 -1.8568 0.5813 -3.195 0.001400 ** ## (Intercept):2 -1.6115 0.5508 -2.926 0.003435 ** ## (Intercept):3 -2.2866 0.6566 -3.483 0.000497 *** ## (Intercept):4 -0.6642 0.3802 -1.747 0.080639 . ## Lakegeorge:1 -0.5753 0.7952 -0.723 0.469429 ## Lakegeorge:2 1.7805 0.6232 2.857 0.004277 ** ## Lakegeorge:3 -1.1295 1.1928 -0.947 0.343687 ## Lakegeorge:4 -0.7666 0.5686 -1.348 0.177563 ## Lakeoklawaha:1 -1.1256 1.1923 -0.944 0.345132 ## Lakeoklawaha:2 2.6937 0.6693 4.025 5.70e-05 *** ## Lakeoklawaha:3 1.4008 0.8105 1.728 0.083926 . ## Lakeoklawaha:4 -0.7405 0.7421 -0.998 0.318372 ## Laketrafford:1 0.6617 0.8461 0.782 0.434145 ## Laketrafford:2 2.9363 0.6874 4.272 1.94e-05 *** ## Laketrafford:3 1.9316 0.8253 2.340 0.019263 * ## Laketrafford:4 0.7912 0.5879 1.346 0.178400 ## Size&gt;2.3:1 0.7302 0.6523 1.120 0.262918 ## Size&gt;2.3:2 -1.3363 0.4112 -3.250 0.001155 ** ## Size&gt;2.3:3 0.5570 0.6466 0.861 0.388977 ## Size&gt;2.3:4 -0.2906 0.4599 -0.632 0.527515 ## Genderm:1 -0.6064 0.6888 -0.880 0.378666 ## Genderm:2 -0.4630 0.3955 -1.171 0.241796 ## Genderm:3 -0.6276 0.6853 -0.916 0.359785 ## Genderm:4 -0.2526 0.4663 -0.542 0.588100 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Names of linear predictors: log(mu[,1]/mu[,5]), log(mu[,2]/mu[,5]), ## log(mu[,3]/mu[,5]), log(mu[,4]/mu[,5]) ## ## Residual deviance: 50.2637 on 40 degrees of freedom ## ## Log-likelihood: -73.3221 on 40 degrees of freedom ## ## Number of Fisher scoring iterations: 5 ## ## No Hauck-Donner effect found in any of the estimates ## ## ## Reference group is level 5 of the response The residual deviance is 50.2637 on 40 degrees of freedom. Residual deviance tests the current model fit versus the saturated model. The saturated model, which fits a separate multinomial distribution to each of the 16 profiles (unique combinations of lake, sex and size), has 16 × 4 = 64 parameters. The current model has an intercept, three lake coefficients, one sex coefficient and one size coefficient for each of the four logit equations, for a total of 24 parameters. Therefore, the overall fit statistics have 64 − 24 = 40 degrees of freedom. E &lt;- data.frame(fitted(gator_vglm) * rowSums(gator_dat[, 5:9])) O &lt;- gator_dat %&gt;% select(Bird, Invertebrate, Reptile, Other, Fish) + .000001 (g2 &lt;- 2 * sum(O * log(O / E))) ## [1] 50.26321 indicates the model fits okay, but not great. The Residual Deviance of 50.26 with 40 df from the table above output is reasonable, with p-value of 0.1282 and the statistics/df is close to 1 that is 1.256. "],
["ordinal-logistic-regression.html", "7.3 Ordinal Logistic Regression", " 7.3 Ordinal Logistic Regression The following notes rely heavily on this UVA web page and [PSU STAT 504 class notes](https://online.stat.psu.edu/stat504/node/171/. The ordinal logistic regression model is \\[logit[P(Y \\le j)] = \\alpha_j - \\beta X, \\hspace{5mm} j \\in [1, J-1]\\] where \\(j \\in [1, J-1]\\) are the levels of the ordinal outcome variable \\(Y\\). The proportional odds model assumes there is a common set of slope parameters \\(\\beta\\) for the predictors. The ordinal outcomes are distinguished by the \\(J-1\\) intercepts \\(\\alpha_j\\). The benchmark level is \\(J\\). “Logit” means log-odds, so \\(logit[P(Y \\le j)] = \\log[P(Y \\le j) / P(Y \\gt j)]\\). Suppose we want to model the probability a respondent holds a political ideology [“Very Liberal”, “Slightly Liberal”, “Moderate”, “Slightly Conservative”, “Very Conservative”] given their party affiliation [“Republican”, “Democrat”]. ideo &lt;- c( &quot;Very Liberal&quot;, &quot;Slightly Liberal&quot;, &quot;Moderate&quot;, &quot;Slightly Conservative&quot;, &quot;Very Conservative&quot; ) ideo_cnt_rep &lt;- c(30, 46, 148, 84, 99) ideo_cnt_dem &lt;- c(80, 81, 171, 41, 55) dat &lt;- data.frame( party = factor(rep(c(&quot;Rep&quot;, &quot;Dem&quot;), c(407, 428)), levels = c(&quot;Rep&quot;, &quot;Dem&quot;)), ideo = factor( c(rep(ideo, ideo_cnt_rep), rep(ideo, ideo_cnt_dem)), levels = ideo ) ) table(dat) ## ideo ## party Very Liberal Slightly Liberal Moderate Slightly Conservative ## Rep 30 46 148 84 ## Dem 80 81 171 41 ## ideo ## party Very Conservative ## Rep 99 ## Dem 55 Fit a proportional odds model with the MASS::polr() function (polr stands for proportional odds linear regression). pom &lt;- MASS::polr(ideo ~ party, data = dat) summary(pom) ## ## Re-fitting to get Hessian ## Call: ## MASS::polr(formula = ideo ~ party, data = dat) ## ## Coefficients: ## Value Std. Error t value ## partyDem -0.9745 0.1292 -7.545 ## ## Intercepts: ## Value Std. Error t value ## Very Liberal|Slightly Liberal -2.4690 0.1318 -18.7363 ## Slightly Liberal|Moderate -1.4745 0.1090 -13.5314 ## Moderate|Slightly Conservative 0.2371 0.0942 2.5165 ## Slightly Conservative|Very Conservative 1.0695 0.1039 10.2923 ## ## Residual Deviance: 2474.985 ## AIC: 2484.985 The log-odds a Democrat identifies as “Very Liberal” or lower is \\[logit[P(Y \\lt 1)] = -2.4690 - (-0.9745)(1) = -1.4945.\\] Solve \\(logit[P(Y \\le 1)] = \\log[P(Y \\le 1) / P(Y \\gt 1)]\\) for \\(P(Y \\le 1)\\) to get the probability a Democrat identifies as “Very Liberal” or lower as \\(P(Y \\le 1) = exp(-1.4945) / (1 + exp(-1.4945)) = 0.183\\). The log odds a Democrat identifies as “Slightly Liberal” or lower is \\[logit[P(Y \\lt 2)] = -1.4745 - (-0.9745)(1) = -0.5.\\] The corresponding probability is \\(P(Y \\le 2) = exp(-0.5) / (1 + exp(-0.5)) = 0.378\\). To get the probability a Democrat identifies as “Slightly Liberal”, just subtract the adjacent cumulative probabilities, \\(P(Y \\le 2) - P(Y \\le 1) = 0.378 = 0.183 = 0.194\\). That’s what’s happening when you use the model to predict the level probabilities. predict(pom, newdata = data.frame(party = &quot;Dem&quot;), type = &quot;probs&quot;) ## Very Liberal Slightly Liberal Moderate ## 0.1832505 0.1942837 0.3930552 ## Slightly Conservative Very Conservative ## 0.1147559 0.1146547 The baseline for the model was “Republican”, so the log-odds a Republicn identifies as “Very Liberal” or lower is \\[logit[P(Y \\lt 1)] = -2.4690 - (-0.9745)(0) = -2.4690\\] with corresponding probability \\(P(Y \\le 1) = exp(-2.4690) / (1 + exp(-2.4690)) = 0.078\\). And so on. predict(pom, newdata = data.frame(party = &quot;Rep&quot;), type = &quot;probs&quot;) ## Very Liberal Slightly Liberal Moderate ## 0.07806044 0.10819225 0.37275214 ## Slightly Conservative Very Conservative ## 0.18550357 0.25549160 Here is a manual calculation of the probabilities. Note the zeta variable instead of beta because the model fits \\(\\alpha_j - \\beta X\\) instead of \\(\\alpha_j + \\beta X\\), and so it uses a new variable \\(\\zeta = -\\beta\\). Technically, the model could be expressed \\(logit[P(Y \\le j)] = \\alpha_j + \\zeta X\\). library(stringr) # cum log odds, dems an reps dclo &lt;- c(pom$zeta - pom$coefficients) rclo &lt;- c(pom$zeta - 0) # cum probs, dems and reps dcp &lt;- exp(dclo) / (1 + exp(dclo)) rcp &lt;- exp(rclo) / (1 + exp(rclo)) # fix the names and add 1 for &quot;Very...&quot; names(dcp) &lt;- str_sub(names(dcp), start = 1, end = str_locate(names(dcp), &quot;\\\\|&quot;)[, 1] - 1) names(rcp) &lt;- str_sub(names(rcp), start = 1, end = str_locate(names(rcp), &quot;\\\\|&quot;)[, 1] - 1) dcp &lt;- c(dcp, 1) rcp &lt;- c(rcp, 1) names(dcp) &lt;- c(names(dcp)[1:4], &quot;Very Conservative&quot;) names(rcp) &lt;- c(names(rcp)[1:4], &quot;Very Conservative&quot;) # Democrat probs (dp &lt;- dcp - lag(dcp, 1, 0)) ## Very Liberal Slightly Liberal Moderate ## 0.1832505 0.1942837 0.3930552 ## Slightly Conservative Very Conservative ## 0.1147559 0.1146547 (rp &lt;- rcp - lag(rcp, 1, 0)) ## Very Liberal Slightly Liberal Moderate ## 0.07806044 0.10819225 0.37275214 ## Slightly Conservative Very Conservative ## 0.18550357 0.25549160 The “proportional odds” part of the proportional odds model is that the ratios of the \\(J - 1\\) odds-ratios are identical for the different levels of the predictors. Here we have a single predictor, party. The odds ratios for party = Dem are (dcp / (1 - dcp)) / (rcp / (1 - rcp)) ## Very Liberal Slightly Liberal Moderate ## 2.649889 2.649889 2.649889 ## Slightly Conservative Very Conservative ## 2.649889 NaN Which is to say, any level of ideology \\(j\\), the estimated odds that a Democrat’s ideology is more liberal \\((\\lt j)\\) rather than more conservative \\((\\ge j)\\) is about 2.65 times a Republicans odds. The log of these odds ratios is the coefficient estimator for party. log((dcp / (1 - dcp)) / (rcp / (1 - rcp))) ## Very Liberal Slightly Liberal Moderate ## 0.9745178 0.9745178 0.9745178 ## Slightly Conservative Very Conservative ## 0.9745178 NaN Always check the assumption of proportional odds. One way to do this is by comparing the proportional odds model with a multinomial logit model, also called an unconstrained baseline logit model. The multinomial logit model models unordered responses and fits a slope to each level of the \\(J – 1\\) responses. The proportional odds model is nested in the multinomial model, so can use a likelihood ratio test to see if the models are statistically different. mlm &lt;- nnet::multinom(ideo ~ party, data = dat) ## # weights: 15 (8 variable) ## initial value 1343.880657 ## iter 10 value 1239.866743 ## final value 1235.648615 ## converged Calculate the deviance test statistic \\(D = -2 loglik(\\beta)\\). G &lt;- -2 * (logLik(pom)[1] - logLik(mlm)[1]) pchisq(G, df = length(pom$zeta) - 1, lower.tail = FALSE) ## [1] 0.2972241 The p-value is high, so the proportional odds model fits as well as the more complex multinomial logit model. "],
["poisson-regression.html", "7.4 Poisson Regression", " 7.4 Poisson Regression Poisson models count data, like “traffic tickets per day”, or “website hits per day”. The response is an expected rate or intensity. For count data, specify the generalized model, this time with family = poisson or family = quasipoisson. Recall that the probability of achieving a count \\(y\\) when the expected rate is \\(\\lambda\\) is distributed \\[P(Y = y|\\lambda) = \\frac{e^{-\\lambda} \\lambda^y}{y!}.\\] The poisson regression model is \\[\\lambda = \\exp(X \\beta).\\] You can solve this for \\(y\\) to get \\[y = X\\beta = \\ln(\\lambda).\\] That is, the model predicts the log of the response rate. For a sample of size n, the likelihood function is \\[L(\\beta; y, X) = \\prod_{i=1}^n \\frac{e^{-\\exp({X_i\\beta})}\\exp({X_i\\beta})^{y_i}}{y_i!}.\\] The log-likelihood is \\[l(\\beta) = \\sum_{i=1}^n (y_i X_i \\beta - \\sum_{i=1}^n\\exp(X_i\\beta) - \\sum_{i=1}^n\\log(y_i!).\\] Maximizing the log-likelihood has no closed-form solution, so the coefficient estimates are found through interatively reweighted least squares. Poisson processes assume the variance of the response variable equals its mean. “Equals” means the mean and variance are of a similar order of magnitude. If that assumption does not hold, use the quasi-poisson. Use Poisson regression for large datasets. If the predicted counts are much greater than zero (&gt;30), the linear regression will work fine. Whereas RMSE is not useful for logistic models, it is a good metric in Poisson. Example Dataset fire contains response variable injuries counting the number of injuries during the month and one explanatory variable, the month mo. fire &lt;- read_csv(file = &quot;C:/Users/mpfol/OneDrive/Documents/Data Science/Data/CivilInjury_0.csv&quot;) ## Parsed with column specification: ## cols( ## ID = col_double(), ## `Injury Date` = col_datetime(format = &quot;&quot;), ## `Total Injuries` = col_double() ## ) fire &lt;- fire %&gt;% mutate(mo = as.POSIXlt(`Injury Date`)$mon + 1) %&gt;% rename(dt = `Injury Date`, injuries = `Total Injuries`) str(fire) ## Classes &#39;spec_tbl_df&#39;, &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 300 obs. of 4 variables: ## $ ID : num 1 2 3 4 5 6 7 8 9 10 ... ## $ dt : POSIXct, format: &quot;2005-01-10&quot; &quot;2005-01-11&quot; ... ## $ injuries: num 1 1 1 5 2 1 1 1 1 1 ... ## $ mo : num 1 1 1 1 1 1 2 2 2 4 ... In a situation like this where there the relationship is bivariate, start with a visualization. ggplot(fire, aes(x = mo, y = injuries)) + geom_jitter() + geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;poisson&quot;)) + labs(title = &quot;Injuries by Month&quot;) Fit a poisson regression in R using glm(formula, data, family = poisson). But first, check whether the mean and variance of injuries are the same magnitude? If not, then use family = quasipoisson. mean(fire$injuries) ## [1] 1.36 var(fire$injuries) ## [1] 1.020468 They are of the same magnitude, so fit the regression with family = poisson. m2 &lt;- glm(injuries ~ mo, family = poisson, data = fire) summary(m2) ## ## Call: ## glm(formula = injuries ~ mo, family = poisson, data = fire) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.3987 -0.3473 -0.3034 -0.2502 4.3185 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.22805 0.10482 2.176 0.0296 * ## mo 0.01215 0.01397 0.870 0.3844 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 139.87 on 299 degrees of freedom ## Residual deviance: 139.11 on 298 degrees of freedom ## AIC: 792.08 ## ## Number of Fisher Scoring iterations: 5 The predicted value \\(\\hat{y}\\) is the estimated log of the response variable, \\[\\hat{y} = X \\hat{\\beta} = \\ln (\\lambda).\\] Suppose mo is January (mo = ), then the log ofinjuries` is \\(\\hat{y} = 0.323787\\). Or, more intuitively, the expected count of injuries is \\(\\exp(0.323787) = 1.38\\) predict(m2, newdata = data.frame(mo=1)) ## 1 ## 0.2401999 predict(m2, newdata = data.frame(mo=1), type = &quot;response&quot;) ## 1 ## 1.271503 Here is a plot of the predicted counts in red. augment(m2, type.predict = &quot;response&quot;) %&gt;% ggplot(aes(x = mo, y = injuries)) + geom_point() + geom_point(aes(y = .fitted), color = &quot;red&quot;) + scale_y_continuous(limits = c(0, NA)) + labs(x = &quot;Month&quot;, y = &quot;Injuries&quot;, title = &quot;Poisson Fitted Line Plot&quot;) Evaluate a logistic model fit with an analysis of deviance. (perf &lt;- glance(m2)) ## # A tibble: 1 x 7 ## null.deviance df.null logLik AIC BIC deviance df.residual ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 140. 299 -394. 792. 799. 139. 298 (pseudoR2 &lt;- 1 - perf$deviance / perf$null.deviance) ## [1] 0.005413723 The deviance of the null model (no regressors) is 139.9. The deviance of the full model is 132.2. The psuedo-R2 is very low at .05. How about the RMSE? RMSE(pred = predict(m2, type = &quot;response&quot;), obs = fire$injuries) ## [1] 1.006791 The average prediction error is about 0.99. That’s almost as much as the variance of injuries - i.e., just predicting the mean of injuries would be almost as good! Use the GainCurvePlot() function to plot the gain curve. augment(m2, type.predict = &quot;response&quot;) %&gt;% ggplot(aes(x = injuries, y = .fitted)) + geom_point() + geom_smooth(method =&quot;lm&quot;) + labs(x = &quot;Actual&quot;, y = &quot;Predicted&quot;, title = &quot;Poisson Fitted vs Actual&quot;) augment(m2) %&gt;% data.frame() %&gt;% GainCurvePlot(xvar = &quot;.fitted&quot;, truthVar = &quot;injuries&quot;, title = &quot;Poisson Model&quot;) It seems that mo was a poor predictor of injuries. "]
]
