[
["contingency-tables.html", "Chapter 2 Contingency Tables 2.1 One-Way Tables 2.2 Two-Way Tables 2.3 K-Way Tables", " Chapter 2 Contingency Tables This section discusses the analysis of categorical response data when there are either no predictor variables, or all of the predictor variables are also categorical.1 If you have no predictor variable, you are essentially testing whether the heights of a bar chart differ significantly from each or from some hypothesized distribution. This is a one-way table and you perform a one-proportion Z-test, exact binomial test, chi-squared goodness-of-fit test, or G-test. If you have a single categorical predictor variable, you are testing whether the joint frequency counts differ from the expected frequency counts in the saturated model. This is a two-way table and you are compare the frequency counts with a proportion difference test, chi-squared independence test, G-test, or Fisher’s exact test. If you have multiple categorical predictor variables, you have a k-way contingency table and you can extend the chi-square independence test and G-test to cover this as well. All of these tests are non-model analyses, so they do not handle more complicated situations such as simultaneous effects of multiple variables, or mixtures of categorical and continuous variables. In these cases you use logistic regression or linear discriminant analysis (LDA). 2.1 One-Way Tables A one-way table is a frequency table for a single categorical variable. You usually construct a one-way table to test whether the frequency counts differ from an hypothesized distribution, or to construct a confidence interval around a proportion. Here is an example. A researcher crosses tall cut-leaf tomatoes with dwarf potato-leaf tomatoes, then classifies the (n = 1,611) offspring phenotypes. The four phenotypes should occur with relative frequencies 9:3:3:1. The observed frequencies form a one-way table. PhenotypeFrequencytall cut-leaf956tall potato-leaf258dwarf cut-leaf293dwarf potato-leaf104 You have a few options for analyzing one-way tables. If you only care about a single level (like when the response variable is binary), conduct a one-proportion Z-test or an exact binomial test. If you want to compare the frequency counts to an expected distribution, in this case 9:3:3:1, conduct an exact multinomial test (recommended when n &lt;= 1,000), Pearson’s chi-squared goodness-of-fit test, or a G-test. 2.1.1 One-Proportion Z-Test Conduct a one-proportion Z-test to evaluate the success proportion of a binary response variable, or a single level of a multinomial variable. The test is also called the normal approximation method, and the Wald method. The Z-test uses the sample proportion \\(p\\) as an estimate of the population proportion \\(\\pi\\) to evaluate an hypothesized population proportion \\(\\pi_0\\) and/or construct a \\((1−\\alpha)\\%\\) confidence interval around \\(p\\) to estimate \\(\\pi\\) within a margin of error \\(\\epsilon\\). The Z-test is intuitive to learn, but it only applies when the central limit theorem conditions hold: the sample is independently drawn, meaning random assignment (experiments) or random sampling without replacement from \\(n &lt; 10\\%\\) of the population (observational studies), there are at least \\(n\\pi \\ge 5\\) successes and \\(n(1 − \\pi) \\ge 5\\) failures, the sample size is \\(n \\ge 30\\), and the probability of success is not extreme, \\(0.2 &lt; \\pi &lt; 0.8\\). If these conditions hold, the sampling distribution of \\(\\pi\\) is normally distributed around \\(p\\) with standard error \\(se_p = \\frac{s_p}{\\sqrt{n}} = \\frac{\\sqrt{p(1−p)}}{\\sqrt{n}}\\). The measured values \\(p\\) and \\(s_p\\) approximate the population values \\(\\pi\\) and \\(\\sigma_\\pi\\). You can define a \\((1 − \\alpha)\\%\\) confidence interval as \\(p \\pm z_{\\alpha / 2}se_p\\). Test the hypothesis of \\(\\pi = \\pi_0\\) with test statistic \\(z = \\frac{p − \\pi_0}{se_{\\pi_0}}\\) where \\(se_{\\pi_0} = \\frac{s_{\\pi_0}}{\\sqrt{n}} = \\frac{\\sqrt{{\\pi_0}(1−{\\pi_0})}}{\\sqrt{n}}\\). In the phenotype example above, you might test whether tall cut-leaf tomatoes occurred in the expected proportion, \\(\\pi_0 = 9 / (9 + 3 + 3 + 1) = 0.5625\\). The sample is a random assignment experiment with \\(956 \\ge 5\\) successes and \\(1611 - 956 = 655 \\ge 5\\) failures, sample size \\(n = 1611 \\ge 30\\), and probability of success \\(0.2 &lt; 0.5625 &lt; 0.8\\), so the Z-test is valid. prop.test(956, 1611, 0.5625, &quot;two.sided&quot;, correct = FALSE) ## ## 1-sample proportions test without continuity correction ## ## data: 956 out of 1611, null probability 0.5625 ## X-squared = 6.2586, df = 1, p-value = 0.01236 ## alternative hypothesis: true p is not equal to 0.5625 ## 95 percent confidence interval: ## 0.5692397 0.6171563 ## sample estimates: ## p ## 0.5934202 The first thing you’ll notice is that prop.test() performs a chi-squared goodness-of-fit test, not a one-proportion Z-test! chisq.test(c(956, 1611-956), p = c(9/(9+3+3+1), 7/(9+3+3+1)), correct = FALSE) ## ## Chi-squared test for given probabilities ## ## data: c(956, 1611 - 956) ## X-squared = 6.2586, df = 1, p-value = 0.01236 It turns out \\(P(\\chi^2 &gt; X^2)\\) equals \\(2 \\cdot P(Z &gt; z).\\) Here is the manual calculation of the chi-squared test statistic \\(X^2\\) and resulting p-value on 1 dof. pi_0 &lt;- 9 / (9+3+3+1) p &lt;- 956 / 1611 observed &lt;- c(p, 1-p) * 1611 expected &lt;- c(pi_0, 1-pi_0) * 1611 X2 &lt;- sum((observed - expected)^2 / expected) pchisq(X2, 1, lower.tail = FALSE) ## [1] 0.01235885 And here is the manual calculation of the Z-test statistic \\(z\\) and resulting p-value. se &lt;- sqrt(pi_0*(1-pi_0)) / sqrt(1611) z &lt;- (p - pi_0) / se pnorm(z, lower.tail = FALSE) * 2 ## [1] 0.01235885 The 95% CI presented by prop.test() is also not the \\(p \\pm z_{\\alpha / 2}se_p\\) Wald interval; it is the Wilson interval! DescTools::BinomCI(956, 1611, method = &quot;wilson&quot;) ## est lwr.ci upr.ci ## [1,] 0.5934202 0.5692397 0.6171563 There are a lot of methods (see ?DescTools::BinomCI), and Wilson is the one Agresti-Coull recommends. If you want Wald, use DescTools::BinomCI() with method = \"wald\". DescTools::BinomCI(956, 1611, method = &quot;wald&quot;) ## est lwr.ci upr.ci ## [1,] 0.5934202 0.5694344 0.617406 This matches the manual calculation below. z_crit = qnorm(1 - .05/2) se &lt;- sqrt(p*(1-p)) / sqrt(1611) (CI &lt;- c(p - z_crit*se, p + z_crit*se)) ## [1] 0.5694344 0.6174060 prop.test() (and chissq.test()) reported a p-value of 0.01236, so you can reject the null hypothesis that \\(\\pi = 0.5625\\). It’s good practice to plot this out to make sure your head is on straight. Incidentally, if you have a margin of error requirement, you can back into the required sample size to achieve it. Just solve the margin of error equation \\(\\epsilon = z_{\\alpha/2}^2 = \\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}\\) for \\(n = \\frac{z_{\\alpha/2}^2 \\pi_0(1-\\pi_0)}{\\epsilon^2}.\\) 2.1.2 Exact Binomial Test The Clopper-Pearson exact binomial test is precise, but theoretically complicated in that it inverts two single-tailed binomial tests. No formulas here - I will just rely on the software. The exact binomial test also works for small sample sizes and for extreme probabilities of success. It has only two conditions: independence, and at least \\(n\\pi \\ge 5\\) successes or \\(n(1−\\pi)\\ge 5\\) failures. In the phenotype example above, I tested whether tall cut-leaf tomatoes occurred in the expected proportion, \\(\\pi_0 = 9 / (9 + 3 + 3 + 1) = 0.5625\\). The sample is a random assignment experiment with \\(956 \\ge 5\\) successes and \\(1611 - 956 = 655 \\ge 5\\) failures, so the exact binomial test is valid. binom.test(956, 1611, 0.5625, &quot;two.sided&quot;) ## ## Exact binomial test ## ## data: 956 and 1611 ## number of successes = 956, number of trials = 1611, p-value = 0.01289 ## alternative hypothesis: true probability of success is not equal to 0.5625 ## 95 percent confidence interval: ## 0.5689717 0.6175276 ## sample estimates: ## probability of success ## 0.5934202 The exact binomial test uses the “method of small p-values”. In this method, the probability of observing \\(p\\) as far or further from \\(\\pi_0\\) is the sum of all \\(P(X=p_i)\\) where \\(p_i &lt;= p\\). p &lt;- dbinom(956, 1611, 0.5625) p_i &lt;- dbinom(0:1611, 1611, 0.5625) map(p_i, function(x) if_else(x &lt;= p, x, 0)) %&gt;% unlist() %&gt;% sum() ## [1] 0.01289442 The p-value of 0.01289 is pretty close to that of the 0.01236 from the one-proportion Z-test. You should study the construction of the confidence interval on Wikipedia and explain it here. 2.1.3 Chi-Squared Goodness-of-Fit Test The chi-squared goodness-of-fit test tests whether the observed frequency counts, \\(O_j\\), of the \\(J\\) levels of a categorical variable differ from the expected frequency counts, \\(E_j\\). \\(H_0\\) is \\(O_j = E_j\\). The Pearson goodness-of-fit test statistic is \\[X^2 = \\sum \\frac{(O_j - E_j)^2}{E_j}\\] where \\(O_j = p_j n\\) and \\(E_j = \\pi_j n\\). The sampling distribution of \\(X^2\\) approaches the \\(\\chi_{J-1}^2\\) as the sample size \\(n \\rightarrow \\infty\\). The assumption that \\(X^2\\) is distributed \\(\\sim \\chi^2\\) is not quite correct, so you will sometimes see researchers subtract .5 from the differences to increase the p-value, the so-called the Yates Continuity Correction. \\[X^2 = \\sum \\frac{(O_j - E_j - 0.5)^2}{E_j}\\] \\(X^2 \\rightarrow 0\\) as the saturated model (the observed data represent the fit of the saturated model, the most complex model possible with the data) proportions approach the expected proportions, \\(p_j \\rightarrow \\pi_j\\). The chi-squared test calculates the probability of the occurrence of \\(X^2\\) at least as extreme given that it is a chi-squared random variable with degrees of freedom equal to the number of levels of the variable minus one, \\(J-1 = 3\\). In the phenotypes example with all four levels, \\(X^2 = 9.55\\) and \\(P(X^2 &gt; 9.55) = 0.0228\\), so you can reject the null hypothesis of 9:3:3:1 relative proportions at the \\(\\alpha = .05\\) level. pheno_dof &lt;- length(pheno_type) - 1 (pheno_x2 &lt;- sum((pheno_obs - pheno_exp)^2 / pheno_exp)) ## [1] 9.54652 (pheno_p &lt;- pchisq(q = pheno_x2, df = pheno_dof, lower.tail = FALSE)) ## [1] 0.02284158 In practice, use the chisq.test() function. (pheno_chisq_test &lt;- chisq.test(pheno_obs, p = pheno_pi, correct = FALSE)) ## ## Chi-squared test for given probabilities ## ## data: pheno_obs ## X-squared = 9.5465, df = 3, p-value = 0.02284 The function applies the Yates continuity correction by default, so I had to specify correct = FALSE to exclude it. In this case, setting it to TRUE has almost no effect because the sample size is large. As always, novices should plot the distribution. If you reject \\(H_0\\), inspect the residuals to learn which differences contribute most to the rejection. \\(X^2\\) is a sum of squared standardized cell differences, or “Pearson residuals”, \\[r_i = \\frac{o_j - e_j}{\\sqrt{e_j}}\\] Cells with the largest \\(|r|\\) contribute the most to the total \\(X^2\\). pheno_chisq_test$residuals^2 / pheno_chisq_test$statistic ## tall cut-leaf tall potato-leaf dwarf cut-leaf dwarf potato-leaf ## 0.28682269 0.67328098 0.02848093 0.01141540 The two “tall” cells contributed over 95% of the \\(X^2\\) test statistic, with the tall potato-leaf accounting for 67%. This aligns with what you’d expect from the bar plot. If you want to test whether the data conform to a particular distribution instead of some set of theoretical values, the test is nearly the same except for an adjustment to the d.f. Suppose you sample n = 100 families and count the number of children. The count of children is a Poisson random variable, \\(J \\sim Pois(\\lambda)\\) with maximum likelihood estimate \\(\\hat{\\lambda} = \\sum{j_i O_i} / \\sum{O_i}\\). The probabilities for each possible count are \\[f(j; \\lambda) = \\frac{e^{-\\hat{\\lambda}} \\hat{\\lambda}^j}{j!}.\\] Compare the expected values to the observed values with the chi-squared goodness of fit test, but in this case, \\(df = 6 - 1 - 1\\) because the estimated parameter \\(\\lambda\\) reduces d.f. by 1. j &lt;- c(0:5) o &lt;- c(19, 26, 29, 13, 10, 3) lambda &lt;- sum(j * o) / sum(o) f &lt;- exp(-lambda) * lambda^j / factorial(j) e &lt;- f * sum(o) (X2 &lt;- sum((o - e)^2 / e)) ## [1] 2.842215 (dof &lt;- length(j) - 1 - 1) # subtract 1 for lambda estimation ## [1] 4 pchisq(q = X2, df = dof) ## [1] 0.4154322 Be careful of this adjustment to the d.f. because chisq.test() does not take this into account, and you cannot override the d.f.! 2.1.4 G-Test The G-test is a likelihood-ratio statistical significance test increasingly used instead of chi-squared tests. The test statistic is defined \\[G^2 = 2 \\sum O_j \\log \\left[ \\frac{O_j}{E_j} \\right]\\] where the 2 multiplier asymptotically aligns with the chi-squared test formula. G is distributed \\(\\sim \\chi^2\\), with the same number of degrees of freedom as in the corresponding chi-squared test. In fact, the chi-squared test statistic is a second order Taylor expansion of the natural logarithm around 1. (pheno_g2 &lt;- 2 * sum(pheno_obs * log(pheno_obs / pheno_exp))) ## [1] 9.836806 (pchisq(q = pheno_g2, df = pheno_dof, lower.tail = FALSE)) ## [1] 0.02000552 This is pretty close to the \\(X^2 = 9.5465\\) (p = 0.023) using the chi-squared test. In practice, use the DescTools::GTest() function to conduct a G-test. DescTools::GTest(pheno_obs, p = pheno_pi) ## ## Log likelihood ratio (G-test) goodness of fit test ## ## data: pheno_obs ## G = 9.8368, X-squared df = 3, p-value = 0.02001 According to the function documentation, the G-test is not usually used for 2x2 tables. 2.2 Two-Way Tables A two-way table is a frequency table for two categorical variables. If you have a single categorical predictor variable, you are testing whether the joint frequency counts differ from the expected frequency counts in the saturated model. Here is an example. A study classified (n = 5,375) high school students by their smoking behavior and the smoking behavior of their parents. The observed frequencies form a two-way table. SmokerNon.smokerTotalBoth4001,3801,780One4161,8232,239Neither1881,1681,356Total1,0044,3715,375 SmokerNon.smokerTotalBoth0.220.781.00One0.190.811.00Neither0.140.861.00 You have a few options for analyzing two-way tables. If you only care about comparing two levels (like when the response variable is binary), conduct a proportion difference Z-test or a Fisher exact test. If you want to compare the joint frequency counts to expected frequency counts under the independence model (the model of independent explanatory variables), conduct a Pearson’s chi-squared independence test, or a G-test. 2.2.1 Proportion Difference Z-Test Conduct a proportion difference Z-test to evaluate the difference in proportions of a binary response variable segmented by a binary categorical predictor variable. The Z-test uses the difference in sample proportions \\(\\hat{d} = p_1 - p_2\\) as an estimate of the difference in population proportions \\(d = \\pi_1 - \\pi_2\\) to evaluate an hypothesized difference in population proportions \\(d_0 = \\pi_0 - \\pi_1\\) and/or construct a \\((1−\\alpha)\\%\\) confidence interval around \\(\\hat{d}\\) to estimate \\(d\\) within a margin of error \\(\\epsilon\\). The Z-test applies when the central limit theorem conditions hold: the sample is independently drawn, meaning random assignment (experiments) or random sampling without replacement from \\(n &lt; 10\\%\\) of the population (observational studies), there are at least \\(n_i p_i &gt;= 5\\) successes and \\(n_i (1 - p_i) &gt;= 5\\) failures for each group \\(i\\), the sample sizes are both \\(n_i &gt;= 30\\), and the probability of success for each group is not extreme, \\(0.2 &lt; \\pi_i &lt; 0.8\\). If these conditions hold, the sampling distribution of \\(d\\) is normally distributed around \\(\\hat{d}\\) with standard error \\(se_\\hat{d} = \\sqrt{\\frac{p_1(1 - p_1)}{n_1} + \\frac{p_2(1 − p_2)}{n_2}}\\). The measured values \\(\\hat{d}\\) and \\(se_\\hat{d}\\) approximate the population values \\(d\\) and \\(se_d\\). You can define a \\((1 − \\alpha)\\%\\) confidence interval as \\(\\hat{d} \\pm z_{\\alpha / 2}se_\\hat{d}\\). Test the hypothesis of \\(d = d_0\\) with test statistic \\(z = \\frac{\\hat{d} − d_0}{se_{d_0}}\\) where \\(se_{d_0} = \\sqrt{p^*(1 - p^*) \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}\\) and \\(p^*\\) is the overall success probability. In the smoking behavior example above, you might test whether smoking rates differ for offspring of non-smoking parents vs one or both parents smoking. The proportion table shows that the measured rates are 816/4019 = 20% when one/both parents smoke and 188/1356 = 14% when neither parent smokes. SmokerNon.smokerTotalEither8163,2034,019Neither1881,1681,356Total1,0044,3715,375 SmokerNon.smokerTotalEither0.200.801.00Neither0.140.861.00 The sample is random sampling without replacement from \\(n &lt; 10\\%\\) of the population with \\((816, 188) \\ge 5\\) successes (smokers) and \\((3203, 1168) \\ge 5\\) failures (non-smokers), sample sizes \\(n = (4019, 1356) \\ge 30\\), and probabilities of success \\(0.2 &lt; (0.20, 0.14) &lt; 0.8\\) a little too extreme, so the Z-test conditions are not quite met on the \\(p &gt; .2\\) condition. Let’s see what we find anyway. prop.test(x = smoke_o2[, 1], n = rowSums(smoke_o2), correct = FALSE) ## ## 2-sample test for equality of proportions without continuity ## correction ## ## data: smoke_o2[, 1] out of rowSums(smoke_o2) ## X-squared = 27.677, df = 1, p-value = 1.434e-07 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## 0.04218944 0.08659558 ## sample estimates: ## prop 1 prop 2 ## 0.2030356 0.1386431 As was the case with the one-proportion Z-test, prop.test() apparently performs a chi-squared test, not a proportion difference Z-test! chisq.test(smoke_o2, correct = FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: smoke_o2 ## X-squared = 27.677, df = 1, p-value = 1.434e-07 \\(P(\\chi^2 &gt; X^2)\\) equals \\(2 \\cdot P(Z &gt; z).\\) Here is the manual calculation of the chi-squared test statistic \\(X^2\\) and resulting p-value on 1 dof. observed &lt;- smoke_o2 expected &lt;- matrix(rowSums(smoke_o2)) %*% t(matrix(colSums(smoke_o2))) / sum(smoke_o2) (X2 &lt;- sum((observed - expected)^2 / expected)) ## [1] 27.67658 pchisq(X2, 1, lower.tail = FALSE) ## [1] 1.433883e-07 And here is the manual calculation of the Z-test statistic \\(z\\) and resulting p-value. n &lt;- rowSums(smoke_o2) # Sample estimates (p &lt;- smoke_o2[, 1] / n) ## Either Neither ## 0.2030356 0.1386431 d_hat &lt;- p[1] - p[2] se &lt;- sqrt(p[1]*(1-p[1])/n[1] + p[2]*(1-p[2])/n[2]) z_crit &lt;- qnorm(1-.05/2) # 95% CI (ci &lt;- d_hat + z_crit*se*c(-1, 1)) ## [1] 0.04218944 0.08659558 # p-value p_star &lt;- sum(smoke_o2[,1]) / sum(smoke_o2) se &lt;- sqrt(p_star * (1-p_star) * (1 / n[1] + 1 / n[2])) z &lt;- (d_hat - 0) / se pnorm(z, lower.tail = FALSE) * 2 ## Either ## 1.433883e-07 prop.test() (and chissq.test()) reported a p-value of 1.43e-07, so you can reject the null hypothesis that \\(d = 0\\). Here is the hypothesis test plot. 2.2.2 Fisher Exact Test Fisher’s exact test is an “exact test” in that the p-value is calculated exactly from the hypergeometric distribution rather than relying on the approximation that the test statistic distribution approaches \\(\\chi^2\\) as \\(n \\rightarrow \\infty\\). The test is applicable in situations where the row totals \\(n_{i+}\\) and the column totals \\(n_+j\\) are fixed by study design (rarely applies), and the expected values of &gt;20% of cells (at least 1 cell in a 2x2 table) have expected cell counts &gt;5, and no expected cell count is &lt;1. In the smoking behavior example, supposing you know that 1,004 study participants smoke and 4,371 do not smoke, and you know that one or both parents of 4.019 participants smoked and 1,356 did not smoke, what is the probability that 816 of the 1,004 smokers had would have one or both parents smoke? dhyper(816, 1004, 4371, 4371) ## [1] 0.03574895 smoke_o2 ## student ## parents Smoker Non-smoker ## Either 816 3203 ## Neither 188 1168 The famous example of the Fisher exact test is the “Lady tea testing” example. A lady claims she can guess whether the milk was poured into the cup before or after the tea. The experiment consists of 8 trials, 4 of which pour the milk first, and 4 which pour the milk second. The lady guesses correctly 6 of the eight cups. tea &lt;- matrix(c(3, 1, 1, 3), nrow = 2, dimnames = list(Guess = c(&quot;Milk&quot;, &quot;Tea&quot;), Truth = c(&quot;Milk&quot;, &quot;Tea&quot;))) tibble::tribble( ~` `, ~Men, ~Women, ~Total, &quot;Studying&quot;, &quot;k&quot;, &quot;K-k&quot;, &quot;K&quot;, &quot;No-Studying&quot;, &quot;n-k&quot;, &quot;(N-K)-(n-k)&quot;, &quot;N-K&quot;, &quot;Total&quot;, &quot;n&quot;, &quot;N-n&quot;, &quot;N&quot; ) %&gt;% flextable::flextable() %&gt;% flextable::autofit() %&gt;% flextable::bold(part = &quot;header&quot;) %&gt;% flextable::bold(part = &quot;body&quot;, i = 3) %&gt;% flextable::bold(part = &quot;body&quot;, j = 4) %&gt;% flextable::add_header_row(values = c(&quot;&quot;, &quot;Lady&#39;s Guess&quot;, &quot;&quot;, &quot;&quot;)) Lady's Guess MenWomenTotalStudyingkK-kKNo-Studyingn-k(N-K)-(n-k)N-KTotalnN-nN #flextable::flextable(TeaTasting %&gt;% as.data.frame() %&gt;% rownames_to_column()) The p-value from the test is computed as if the margins of the table are fixed. This leads under a null hypothesis of independence to a hypergeometric distribution of the numbers in the cells of the table ([Wikipedial(https://en.wikipedia.org/wiki/Fisher%27s_exact_test)). Fisher’s exact test is useful for small n-size samples where the chi-squared distribution assumption of the chi-squared and G-test tests fails. Fisher’s exact test is overly conservative (p values too high) for large n-sizes. tibble::tribble( ~` `, ~Men, ~Women, ~Total, &quot;Studying&quot;, &quot;k&quot;, &quot;K-k&quot;, &quot;K&quot;, &quot;No-Studying&quot;, &quot;n-k&quot;, &quot;(N-K)-(n-k)&quot;, &quot;N-K&quot;, &quot;Total&quot;, &quot;n&quot;, &quot;N-n&quot;, &quot;N&quot; ) %&gt;% flextable::flextable() %&gt;% flextable::autofit() %&gt;% flextable::bold(part = &quot;header&quot;) %&gt;% flextable::bold(part = &quot;body&quot;, i = 3) %&gt;% flextable::bold(part = &quot;body&quot;, j = 4) MenWomenTotalStudyingkK-kKNo-Studyingn-k(N-K)-(n-k)N-KTotalnN-nN The Hypergeometric density function is \\[f_X(k|N, K, n) = \\frac{{{K}\\choose{k}}{{N-K}\\choose{n-k}}}{{N}\\choose{n}}.\\] The density is the exact hypergeometric probability of observing this particular arrangement of the data, assuming the given marginal totals, on the null hypothesis that the conditional probabilities are equal. The phenotype example had 4 levels, but I can collapse it into 2 for sake of example. Suppose you crossed the two species and were only concerned with one attribute: size (dwarf or tall): pheno_obs2 &lt;- c(sum(pheno_obs[1:2]), sum(pheno_obs[3:4])) pheno_exp2 &lt;- c(sum(pheno_exp[1:2]), sum(pheno_exp[3:4])) fisher.test(c(1,11), c(9,3)) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: c(1, 11) and c(9, 3) ## p-value = 1 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.00000 39.00055 ## sample estimates: ## odds ratio ## 0 #fisher.test() These notes rely on PSU STATS 504 course notes. A two-way frequency table is a frequency table for two categorical variables. You usually construct a two-way table to test whether the frequency counts in one categorical variable differ from the other categorical variable using the chi-square independence test. If there is a significant difference (i.e., the variables are related), then describe the relationship with an analysis of the residuals, calculations of measures of association (difference in proportions, relative risk, or odds ratio), and partition tests. Here are three case studies that illustrate the concepts. The first is a simple 2x2 table. The second is a 3x2 table that extends some of the concepts. The third is a 2x4 table where one factor is ordinal. Study 1: “Vitamin C” 2x2 Table. A double blind study investigated whether vitamin C prevents common colds on a sample of n = 279 persons. This study has two categorical variables each with two levels, a 2x2 two way table. vitc_o &lt;- matrix( c(31, 17, 109, 122), ncol = 2, dimnames = list( treat = c(&quot;Placebo&quot;, &quot;VitaminC&quot;), resp = c(&quot;Cold&quot;, &quot;NoCold&quot;) ) ) vitc_o %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot; &quot;) %&gt;% janitor::adorn_totals(where = c(&quot;row&quot;, &quot;col&quot;)) ## Cold NoCold Total ## Placebo 31 109 140 ## VitaminC 17 122 139 ## Total 48 231 279 Study 3: “CHD” Ordinal Table. A study of classified n = 1329 patients by cholesterol level and whether they had been diagnosed with coronary heart disease (CHD). # tribble() is a little easier. chd_o &lt;- tribble( ~L_0_199, ~L_200_219, ~L_220_259, ~L_260p, 12, 8, 31, 41, 307, 246, 439, 245 ) %&gt;% as.matrix() rownames(chd_o) &lt;- c(&quot;CHD&quot;, &quot;No CHD&quot;) chd_o %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot; &quot;) %&gt;% janitor::adorn_totals(where = c(&quot;row&quot;, &quot;col&quot;)) ## L_0_199 L_200_219 L_220_259 L_260p Total ## CHD 12 8 31 41 92 ## No CHD 307 246 439 245 1237 ## Total 319 254 470 286 1329 2.2.3 Chi-Square Independence Test The chi-square independence test tests whether observed joint frequency counts \\(O_{ij}\\) differ from expected frequency counts \\(E_{ij}\\) under the independence model (the model of independent explanatory variables, \\(\\pi_{ij} = \\pi_{i+} \\pi_{+j}\\). \\(H_0\\) is \\(O_{ij} = E_{ij}\\). There are two possible test statistics for this test, Pearson \\(X^2\\) (and the continuity adjusted \\(X^2\\)), and deviance \\(G^2\\). As \\(n \\rightarrow \\infty\\) their sampling distributions approach \\(\\chi_{df}^2\\) with degrees of freedom (df) equal to the saturated model df \\(I \\times J - 1\\) minus the independence model df \\((I - 1) + (J - 1)\\), which you can algebraically solve for \\(df = (I - 1)(J - 1)\\). The Pearson goodness-of-fit statistic is \\[X^2 = \\sum \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\] where \\(O_{ij}\\) is the observed count, and \\(E_{ij}\\) is the product of the row and column marginal probabilities. For the Vitamin C study, \\(X^2\\) is vitc_e &lt;- sum(vitc_o) * prop.table(vitc_o, 1) * prop.table(vitc_o, 2) X2 &lt;- sum((vitc_o - vitc_e)^2 / vitc_e) print(X2) ## [1] 4.811413 and the deviance statistic is \\[G^2 = 2 \\sum_{ij} O_{ij} \\log \\left( \\frac{O_{ij}}{E_{ij}} \\right)\\] G2 &lt;- - 2 * sum(vitc_o * log(vitc_o / vitc_e)) print(G2) ## [1] 4.871697 \\(X^2\\) and \\(G^2\\) increase with the disagreement between the saturated model proportions \\(p_{ij}\\) and the independence model proportions \\(\\pi_{ij}\\). The degrees of freedom is vitc_dof &lt;- (nrow(vitc_o) - 1) * (ncol(vitc_o) - 1) print(vitc_dof) ## [1] 1 The associated p-values are pchisq(q = G2, df = vitc_dof, lower.tail = FALSE) ## [1] 0.02730064 pchisq(q = X2, df = vitc_dof, lower.tail = FALSE) ## [1] 0.02827186 The chisq.test() function applies the Yates continuity correcton by default to correct for situations with small cell counts. The Yates continuity correction subtracts 0.5 from the \\(O_{ij} - E_{ij}\\) differences. Set correct = FALSE to suppress Yates. vitc_chisq_test &lt;- chisq.test(vitc_o, correct = FALSE) print(vitc_chisq_test) ## ## Pearson&#39;s Chi-squared test ## ## data: vitc_o ## X-squared = 4.8114, df = 1, p-value = 0.02827 The Yates correction yields more conservative p-values. chisq.test(vitc_o) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: vitc_o ## X-squared = 4.1407, df = 1, p-value = 0.04186 These p-values are evidence for rejecting the independence model. Here is the chi-square test applied to the CHD data. Recall this data set is 4x2, so the degrees of freedom are \\((4-1)(2-1) = 3\\). The Yates continuity correction does not apply to data other than 2x2, so the correct = c(TRUE, FALSE) has no effect in chisq.test(). (chd_chisq_test &lt;- chisq.test(chd_o)) ## ## Pearson&#39;s Chi-squared test ## ## data: chd_o ## X-squared = 35.028, df = 3, p-value = 1.202e-07 The p-value is very low, so reject the null hypothesis of independence. This demonstrates that a relationship exists between cholesterol and CHD. Now you should describe that relationship by evaluating the (i) residuals, (ii) measures of association, and (iii) partitioning chi-square. 2.2.4 Residuals Analysis If the chi-square independence test rejects \\(H_0\\) of identical frequency distributions, the next step is to identify which cells may be driving the lack of fit. The Pearson residuals in the two-way table are \\[r_{ij} = \\frac{O_{ij} - E_{ij}}{\\sqrt{E_{ij}}}\\] where \\(X^2 = \\sum{r_{ij}}\\). The \\(r_{ij}\\) values have a normal distribution with mean 0, but with unequal variances. The standardized Pearson residual for a two-way table is \\[r_{ij} = \\frac{O_{ij} - E_{ij}}{\\sqrt{E_{ij}(1 - p_{i+})(1 - p_{+j})}}\\] and the \\(r_{ij}\\) values do have a \\(\\sim N(0, 1)\\) distribution. \\(r_{ij}^2 &gt; 4\\) is a sign of lack of fit. The chissq.test() object includes residuals that match the manual calculation. (vitc_o - vitc_e) / sqrt(vitc_e) ## resp ## treat Cold NoCold ## Placebo -1.408787 0.6421849 ## VitaminC 1.413846 -0.6444908 vitc_chisq_test$residuals ## resp ## treat Cold NoCold ## Placebo 1.408787 -0.6421849 ## VitaminC -1.413846 0.6444908 It also includes stdres that match the manual standardized calculation. (well, no it doesn’t, but I don’t know what my mistake is.) (vitc_e - vitc_o) / sqrt(vitc_e * (1 - prop.table(vitc_o, margin = 1)) * (1 - prop.table(vitc_o, margin = 2)) ) ## resp ## treat Cold NoCold ## Placebo 2.682825 -1.877887 ## VitaminC -1.877887 2.682825 vitc_chisq_test$stdres ## resp ## treat Cold NoCold ## Placebo 2.193493 -2.193493 ## VitaminC -2.193493 2.193493 Here are the squared Pearson residuals for the CHD data. The squared Pearson residuals for CHD 0-199, 200-219, and 260+ are greater than 4, and seem to be driving the lack of independence. chd_chisq_test$residuals^2 ## L_0_199 L_200_219 L_220_259 L_260p ## CHD 4.6036904 5.2229946 0.07248954 22.704433 ## No CHD 0.3423925 0.3884523 0.00539130 1.688608 2.2.5 Difference in Proportions The difference in proportions measure is the difference in the probabilities of characteristic \\(Z\\) conditioned on two groups \\(Y = 1\\) and \\(Y = 2\\): \\(\\delta = \\pi_{1|1} - \\pi_{1|2}\\). In social sciences and epidemiology \\(\\pi_{1|1}\\) and \\(\\pi_{1|2}\\) are sometimes referred to as “risk” values. The point estimate for \\(\\delta\\) is \\(r = p_{1|1} - p_{1|2}\\). Under the normal approximation method, the sampling distribution of the difference in population proportions has a normal distribution centered at \\(d\\) with variance \\(Var(\\delta)\\). The point estimate for \\(Var(\\delta)\\) is \\(Var(d)\\). \\[Var(d) = \\frac{p_{1|1} (1 - p_{1|1})}{n_{1+}} + \\frac{p_{1|2} (1 - p_{1|2})}{n_{2+}}\\] In the vitamin C acid example, \\(\\delta\\) is the difference in the row conditional frequencies. p &lt;- prop.table(vitc_o, margin = 1) d &lt;- p[2, 1] - p[1, 1] print(d) ## [1] -0.09912641 The variance is var_d &lt;- (p[2, 1])*(1 - p[2, 1]) / sum(vitc_o[2, ]) + (p[1, 1])*(1 - p[1, 1]) / sum(vitc_o[1, ]) print(var_d) ## [1] 0.002003675 The 95% CI is d + c(-1, 1) * qnorm(.975) * sqrt(var_d) ## [1] -0.18685917 -0.01139366 This is how prop.test() without the continuity correction calculates the confidence interval. (prop.test.result &lt;- prop.test(vitc_o, correct = FALSE)) ## ## 2-sample test for equality of proportions without continuity ## correction ## ## data: vitc_o ## X-squared = 4.8114, df = 1, p-value = 0.02827 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## 0.01139366 0.18685917 ## sample estimates: ## prop 1 prop 2 ## 0.2214286 0.1223022 library(mfstylr) lcl &lt;- -round(prop.test.result$conf.int[2], 3) ucl &lt;- -round(prop.test.result$conf.int[1], 3) data.frame(d_i = -300:300 / 1000) %&gt;% mutate(density = dnorm(x = d_i, mean = d, sd = sqrt(var_d))) %&gt;% mutate(rr = ifelse(d_i &lt; lcl | d_i &gt; ucl, density, 0)) %&gt;% ggplot() + geom_line(aes(x = d_i, y = density)) + geom_area(aes(x = d_i, y = rr, fill = mf_pal(12)(12)[2]), alpha = 0.8) + geom_vline(aes(xintercept = d), color = &quot;blue&quot;) + theme_mf() + labs(title = bquote(&quot;Difference in Proportions Confidence Interval&quot;), subtitle = paste0( &quot;d = &quot;, round(d, 3) ), x = &quot;d&quot;, y = &quot;Density&quot;) + theme(legend.position=&quot;none&quot;) The normal approximation method applies when the central limit theorem conditions hold: the sample is independently drawn (random sampling without replacement from \\(n &lt; 10\\%\\) of the population in observational studies, or random assignment in experiments), there are at least \\(n_i p_i &gt;= 5\\) successes and \\(n_i (1 - p_i) &gt;= 5\\) failures for each group, the sample sizes are both \\(&gt;=30\\), and the probability of success for each group is not extreme, \\((0.2, 0.8)\\). Test \\(H_0: d = \\delta_0\\) for some hypothesized population \\(\\delta\\) (usually 0) with test statistic \\[Z = \\frac{d - \\delta_0}{se_{d}}\\] where \\[se_{d} = \\sqrt{p (1 - p) \\left( \\frac{1}{n_{1+}} + \\frac{1}{n_{2+}} \\right)}\\] approximates \\(se_{\\delta_0}\\) where \\(p\\) is the pooled proportion \\[p = \\frac{n_{11} + n_{21}}{n_{1+} + n_{2+}}.\\] p_pool &lt;- (vitc_o[1, 1] + vitc_o[2, 1]) / sum(vitc_o) se_d &lt;- sqrt(p_pool * (1 - p_pool) * (1 / sum(vitc_o[1, ]) + 1 / sum(vitc_o[2, ]))) z &lt;- (d - 0) / se_d pnorm(z) * 2 ## [1] 0.02827186 lrr = qnorm(p = .05/2, mean = 0, sd = se_d, lower.tail = TRUE) urr = qnorm(p = .05/2, mean = 0, sd = se_d, lower.tail = FALSE) data.frame(d_i = -300:300 / 1000) %&gt;% mutate(density = dnorm(x = d_i, mean = 0, sd = se_d)) %&gt;% mutate(rr = ifelse(d_i &lt; lrr | d_i &gt; urr, density, 0)) %&gt;% ggplot() + geom_line(aes(x = d_i, y = density)) + geom_area(aes(x = d_i, y = rr, fill = mf_pal(12)(12)[2]), alpha = 0.8) + geom_vline(aes(xintercept = d), color = &quot;blue&quot;) + geom_vline(aes(xintercept = 0), color = &quot;black&quot;) + theme_mf() + labs(title = &quot;Hypothesis Test of Difference in Proportions&quot;, subtitle = paste0( &quot;d = &quot;, round(d, 3), &quot; (Z = &quot;, round(z, 2), &quot;, p = &quot;, round(pnorm(z) * 2, 4), &quot;).&quot; ), x = &quot;d&quot;, y = &quot;Density&quot;) + theme(legend.position=&quot;none&quot;) The null hypothesis \\(H_0: \\delta_0 = 0\\) is equivalent to saying that two variables are independent, \\(\\pi_{1|1} = \\pi_{1|2}\\), so you can also use the \\(\\chi^2\\) or \\(G^2\\) test for independence in a 2 × 2. That’s what prop.test() is doing. The square of the z-statistic is algebraically equal to \\(\\chi^2\\). The two-sided test comparing \\(Z\\) to a \\(N(0, 1)\\) is identical to comparing \\(\\chi^2\\) to a chi-square distribution with df = 1. Compare the \\(Z^2\\) to the output from prop.test(). z^2 ## [1] 4.811413 prop.test.result$statistic ## X-squared ## 4.811413 The difference in proportions is easy to interpret, but when \\(Z = 1\\) is a rare event, the individual probabilities \\(\\pi_{1|1}\\) and \\(\\pi_{1|2}\\) are both small and \\(\\delta\\) is nearly zero even when the effect is strong. In the CHD study, two of the conditional probabilities of CHD within the four cholesterol groups are similar, 0-199 (0.038) and 200-219 (.031). round(prop.table(chd_o, margin = 2), 3) ## L_0_199 L_200_219 L_220_259 L_260p ## CHD 0.038 0.031 0.066 0.143 ## No CHD 0.962 0.969 0.934 0.857 Is the difference in these proportions statistically signficant? You can test this with the difference in proportions test or a chisq test. prop.test(t(chd_o[, c(1:2)])) ## ## 2-sample test for equality of proportions with continuity correction ## ## data: t(chd_o[, c(1:2)]) ## X-squared = 0.028064, df = 1, p-value = 0.867 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## -0.02736931 0.03961229 ## sample estimates: ## prop 1 prop 2 ## 0.03761755 0.03149606 chisq.test(t(chd_o[, c(1:2)])) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: t(chd_o[, c(1:2)]) ## X-squared = 0.028064, df = 1, p-value = 0.867 You could go on to try other pairwise tests to establish which levels differ from the others. 2.2.6 Relative Risk The relative risk measure is the ratio of the probabilities of characteristic \\(Z\\) conditioned on two groups \\(Y = 1\\) and \\(Y = 2\\): \\(\\rho = \\pi_{1|1} / \\pi_{1|2}\\). In social sciences and epidemiology \\(\\rho\\) is sometimes referred to as the “relative risk”. The point estimate for \\(\\rho\\) is \\(r = p_{1|1} / p_{1|2}\\). Because \\(\\rho\\) is non-negative, a normal approximation for \\(\\log \\rho\\) has a less skewed distribution than \\(\\rho\\). The approximate variance of \\(\\log \\rho\\) is \\[Var(\\log \\rho) = \\frac{1 - \\pi_{11}/\\pi_{1+}}{n_{1+}\\pi_{11}/\\pi_{1+}} + \\frac{1 - \\pi_{21}/\\pi_{2+}}{n_{2+}\\pi_{21}/\\pi_{2+}}\\] and is estimated by \\[Var(\\log r) = \\left( \\frac{1}{n_{11}} - \\frac{1}{n_{1+}} \\right) + \\left( \\frac{1}{n_{21}} - \\frac{1}{n_{2+}} \\right)\\] In the vitamin C acid example, \\(r\\) is the ratio of the row conditional frequencies. vitc_prop &lt;- prop.table(vitc_o, margin = 1) vitc_risk &lt;- vitc_prop[2, 1] / vitc_prop[1, 1] print(vitc_risk) ## [1] 0.5523323 The variance is vitc_risk_var &lt;- 1 / vitc_o[1, 1] - 1 / sum(vitc_o[1, ]) + 1 / vitc_o[2, 1] - 1 / sum(vitc_o[2, ]) print(vitc_risk_var) ## [1] 0.07674449 The 95% CI is exp(log(vitc_risk) + c(-1, 1) * qnorm(.975) * sqrt(vitc_risk_var)) ## [1] 0.3209178 0.9506203 Thus, at 0.05 level, you can reject the independence model. People taking vitamin C are half as likely to catch a cold. In the CHD study, you could summarize the relationship between CHD and cholesterol level by a set of three relative risks using 0-199 as the baseline: 200–219 versus 0–199, 220–259 versus 0–199, and 260+ versus 0–199. (chd_prop &lt;- prop.table(chd_o, margin = 2)) ## L_0_199 L_200_219 L_220_259 L_260p ## CHD 0.03761755 0.03149606 0.06595745 0.1433566 ## No CHD 0.96238245 0.96850394 0.93404255 0.8566434 (chd_risk &lt;- chd_prop[1, ] / chd_prop[1, 1]) ## L_0_199 L_200_219 L_220_259 L_260p ## 1.0000000 0.8372703 1.7533688 3.8108974 2.2.7 Odds Ratio The odds ratio is the most commonly used measure of association. It is also a natural parameter for many of the log-linear and logistic models. The odds is the ratio of probabilities of “success” and “failure”. When conditioned on a variable, the odds ratio is \\[\\theta = \\frac{\\pi_{1|1} / \\pi_{2|1}} {\\pi_{1|2} / \\pi_{2|2}}\\] and is estimated by the sample frequencies \\[\\hat{\\theta} = \\frac{n_{11} n_{22}} {n_{12} n_{21}}\\] The log-odds ratio has a better normal approximation than the odds ratio, so define the confidence interval on the log scale. \\[Var(\\log \\hat{\\theta}) = \\frac{1}{n_{11}} + \\frac{1}{n_{12}} + \\frac{1}{n_{21}} + \\frac{1}{n_{22}}\\] For the Vitamin C example, the odds of getting a cold after taking a placebo pill are \\(0.22 / 0.78 = 0.28\\) and the odds of getting a cold after taking Vitamin C are \\(0.12 / 0.88 = 0.14\\). vitc_odds &lt;- vitc_prop[, 1] / vitc_prop[, 2] print(vitc_odds) ## Placebo VitaminC ## 0.2844037 0.1393443 The odds of getting a cold given vitamin C are \\(0.14 / 0.28 = 0.49\\) times the odds of getting cold given a placebo. vitc_theta_hat &lt;- vitc_odds[2] / vitc_odds[1] print(vitc_theta_hat) ## VitaminC ## 0.4899524 with variance (var_vitc_theta_hat &lt;- sum(1 / vitc_o)) ## [1] 0.1084526 The 95% CI is z_alpha &lt;- qnorm(p = 0.975) exp(log(vitc_theta_hat) + c(-1, 1) * z_alpha * sqrt(var_vitc_theta_hat)) ## [1] 0.2569419 0.9342709 Keep in mind the following properties of odds ratios. You can convert an odds pack to probabilities by solving \\(\\pi / (1 - \\pi)\\) for \\(\\pi = odds / (1 + odds)\\). If two variables are independent, then the conditional probabilities \\(\\pi_{1|1}\\) and \\(\\pi_{1|2}\\) will be equal and therefore the odds ratio will equal 1. If \\(\\pi_{1|1} &gt; \\pi_{1|2}\\) then the odds ratio will be \\(1 &lt; \\theta &lt; \\infty\\). If \\(\\pi_{1|1} &lt; \\pi_{1|2}\\) then the odds ratio will be \\(0 &lt; \\theta &lt; 1\\). the sample odds ratio will equal \\(0\\) or \\(\\infty\\) if any \\(n_{ij} = 0\\). If you have any empty cells add 1/2 to each cell count. 2.2.8 Partitioning Chi-Square Besides looking at the residuals or the measures of association, another way to describe the effects is to form a sequence of smaller tables by combining or collapsing rows and/or columns in a meaningful way. For the smoking study, you might ask whether a student is more likely to smoke if either parent smokes. Collapse the first two rows (1 parent smokes, both parents smoke) and run the chi-square test. smoke_clps_1 &lt;- rbind(smoke_o[1, ] + smoke_o[2, ], smoke_o[3, ]) smoke_clps_1_theta &lt;- (smoke_clps_1[1, 1] / smoke_clps_1[1, 2]) / (smoke_clps_1[2, 1] / smoke_clps_1[2, 2]) (smoke_clps_1_chisq &lt;- chisq.test(smoke_clps_1)) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: smoke_clps_1 ## X-squared = 27.254, df = 1, p-value = 1.784e-07 The estimated odds a student smokes if at least one parent smokes is 1.58 (X^2 = 27.3, p = 1.783791410^{-7}. Or you may ask, whether among students with at least one smoking parent, there is a difference between those with one smoking parent and those with two smoking parents. Answer this by running a chi-square test on the first two rows of the data table, discarding the row where neither parent smokes. smoke_clps_2_theta &lt;- (smoke_o[1, 1] / smoke_o[1, 2]) / (smoke_o[2, 1] / smoke_o[2, 2]) (smoke_clps_2_chisq &lt;- chisq.test(smoke_o[c(1:2), ])) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: smoke_o[c(1:2), ] ## X-squared = 9.045, df = 1, p-value = 0.002634 The estimated odds a student smokes if both parents smoke compared to one parent is 1.27 (X^2 = 9, p = 0.0026342. 2.2.9 Correlation When classification is ordinal, there may exist a linear trend among the levels of the characteristics. Measure the linear relationship with Pearson’s correlation coefficient, or its nonparametric alternatives, Spearman’s correlation coefficient and Kendall’s tau. In the CHD study, the four levels of cholesterol (0-199, 200-219, 220-259, and 260+) may be treated as ordinal data. You can also treat the presence of heart disease as ordinal. The Pearson correlation, \\[r = \\frac{cov(X, Y)}{s_X s_Y}\\] is #dim=dim(table) rbar &lt;- sum(margin.table(chd_o, 1) * c(1, 2)) / sum(chd_o) rdif &lt;- c(1, 2) - rbar cbar &lt;- sum(margin.table(chd_o, 2) * c(1, 2, 3, 4)) / sum(chd_o) cdif &lt;- c(1, 2, 3, 4) - cbar ssr &lt;- sum(margin.table(chd_o, 1) * (rdif^2)) ssc &lt;- sum(margin.table(chd_o, 2) * (cdif^2)) ssrc &lt;- sum(t(chd_o * rdif) * cdif) pcor &lt;- ssrc / (sqrt(ssr * ssc)) pcor ## [1] -0.1403189 M2 &lt;- (sum(chd_o) - 1) * pcor^2 M2 ## [1] 26.14753 2.3 K-Way Tables These notes rely on PSU STATS 504 course notes. A k-way table has k independent variables. Each cell in the k-way table has joint probability \\(\\pi_{ij..k}\\). You can collapse a k-way table along a dimension to create a marginal table and the resulting joint probabilities in the marginal table are referred to as marginal distributions. Alternatively, you can consider a single level of a dimension, creating a conditional distribution. Here are two case studies to illustrate the concepts. Study 1: Death Penalty. Data is collected for n = 326 murder cases along three dimensions: defendant’s color, victim’s color, and whether or not the defendant received the death penalty. deathp &lt;- array(c(19, 132, 11, 52, 0, 9, 6, 97), dim=c(2,2,2)) dimnames(deathp) &lt;- list( DeathPen = c(&quot;yes&quot;,&quot;no&quot;), Defendant=c(&quot;white&quot;,&quot;black&quot;), Victim=c(&quot;white&quot;,&quot;black&quot;) ) ftable(deathp, row.vars = c(&quot;Defendant&quot;, &quot;Victim&quot;), col.vars = &quot;DeathPen&quot;) ## DeathPen yes no ## Defendant Victim ## white white 19 132 ## black 0 9 ## black white 11 52 ## black 6 97 Study 2: Boy Scouts. Data is collected for n = 800 Boy Scouts along three dimensions: socioeconomic status, scout status, and delinquency status. scout &lt;- expand.grid( delinquent = c(&quot;no&quot;,&quot;yes&quot;), scout = c(&quot;no&quot;, &quot;yes&quot;), SES = c(&quot;low&quot;, &quot;med&quot;,&quot;high&quot;) ) scout &lt;- cbind(scout, count = c(169,42,43,11,132,20,104,14,59,2,196,8)) scout_ct &lt;- xtabs(count ~ ., scout) ftable(scout_ct, row.vars = c(&quot;SES&quot;, &quot;scout&quot;), col.vars = &quot;delinquent&quot;) ## delinquent no yes ## SES scout ## low no 169 42 ## yes 43 11 ## med no 132 20 ## yes 104 14 ## high no 59 2 ## yes 196 8 2.3.1 Odds Ratio In the DeathP study, the marginal odds ratio is 1.18, meaning the odds of death penalty for a white defendant are 1.18 times as high as they are for a black defendant. deathp_m &lt;- margin.table(deathp, margin = c(1, 2)) deathp_p &lt;- prop.table(deathp_m, margin = 2) deathp_odds &lt;- deathp_p[1, ] / deathp_p[2, ] deathp_or &lt;- deathp_odds[1] / deathp_odds[2] print(deathp_or) ## white ## 1.18106 The conditional odds ratio given the victim is white is 0.68, and 0.79 given the victim is black, meaning the odds of death penalty for a white defendant are 0.68 times as high as they are for a black defendent if the victim is white, and 0.79 times as high as they are for a black defendent if the victim is black. deathp_m &lt;- margin.table(deathp[, , 1], margin = c(2, 1)) deathp_p &lt;- prop.table(deathp_m, margin = 2) deathp_odds &lt;- deathp_p[1, ] / deathp_p[2, ] deathp_or &lt;- deathp_odds[1] / deathp_odds[2] print(deathp_or) ## yes ## 0.6804408 deathp_m &lt;- margin.table(deathp[, , 2], margin = c(2, 1)) + 0.5 deathp_p &lt;- prop.table(deathp_m, margin = 2) deathp_odds &lt;- deathp_p[1, ] / deathp_p[2, ] deathp_or &lt;- deathp_odds[1] / deathp_odds[2] print(deathp_or) ## yes ## 0.7894737 Notice above that the second margin table had a zero in one cell. To get an odds ratio in this case, the convention is to add 0.5 to all cells. Interesting that the marginal odds of a white defendent receiving the death penalty are &gt;1, but the conditional odds of a white defendent receiving the death penalty given the race of the victim are &lt;1. Simpson’s paradox is the phenomenon that a pair of variables can have marginal association and partial (conditional) associations in opposite direction. Another way to think about this is that the nature and direction of association changes due to presence or absence of a third (possibly confounding) variable. 2.3.2 Chi-Square Independence Test There are several ways to think about “independence” when dealing with a k-way table. Mutual independence. All variables are independent from each other, (A, B, C). Joint independence. Two variables are jointly independent of the third, (AB, C). Marginal independence. Two variables are independent if you ignore the third, (A, B). Conditional independence Two variables are independent given the third, (AC, BC). Homogeneous associations Conditional (partial) odds-ratios are not related on the value of the third, (AB, AC, BC). Under the assumption that the model of independence is true, once you know the marginal probability values, we have enough information to estimate all unknown cell probabilities. Because each of the marginal probability vectors must add up to one, the number of free parameters in the model is (I − 1) + (J − 1) + (K −I ). This is exactly like the two-way table 2.3.2.1 Mutual Independence The simplest model is that ALL variables are independent of one another (A, B, C). The joint probabilities are equal to the product of the marginal probabilities, \\(\\pi_{ijk} = \\pi_i \\pi_j \\pi_k\\). In terms of odds ratios, the model (A, B, C) implies that the odds ratios in the marginal tables A × B, B × C, and A × C are equal to 1. The chi-square independence test tests whether observed joint frequency counts \\(O_{ijk}\\) differ from expected frequency counts \\(E_{ijk}\\) under the independence model \\(\\pi_{ijk} = \\pi_{i+} \\pi_{+j} \\pi_{k+}\\). \\(H_0\\) is \\(O_{ijk} = E_{ijk}\\). This is essentially a one-way table chi-squre goodness-of-fit test. For the Boy Scouts study, \\(X^2\\) and its associated p-value is scout_e &lt;- array(NA, dim(scout_ct)) for (i in 1:dim(scout_ct)[1]) { for (j in 1:dim(scout_ct)[2]) { for (k in 1:dim(scout_ct)[3]) { scout_e[i,j,k] &lt;- ( margin.table(scout_ct, 3)[k] * margin.table(scout_ct, 2)[j] * margin.table(scout_ct, 1)[i]) / (sum(scout_ct))^2 } } } scout_df &lt;- (prod(dim(scout_ct)) - 1) - sum(dim(scout_ct) - 1) scout_x2 &lt;- sum((scout_ct - scout_e)^2 / scout_e) print(scout_x2) ## [1] 214.9233 pchisq(scout_x2, df = scout_df, lower.tail = FALSE) ## [1] 7.882742e-43 You can also get X^2 this way, although longer code. x &lt;- scout %&gt;% group_by(delinquent) %&gt;% summarize(n = sum(count)) %&gt;% ungroup() %&gt;% mutate(p = n / sum(n)) ## `summarise()` ungrouping output (override with `.groups` argument) p_d &lt;- x$p names(p_d) &lt;- x$delinquent x &lt;- scout %&gt;% group_by(scout) %&gt;% summarize(n = sum(count)) %&gt;% ungroup() %&gt;% mutate(p = n / sum(n)) ## `summarise()` ungrouping output (override with `.groups` argument) p_b &lt;- x$p names(p_b) &lt;- x$scout x &lt;- scout %&gt;% group_by(SES) %&gt;% summarize(n = sum(count)) %&gt;% ungroup() %&gt;% mutate(p = n / sum(n)) ## `summarise()` ungrouping output (override with `.groups` argument) p_s &lt;- x$p names(p_s) &lt;- x$SES scout_e &lt;- scout %&gt;% mutate(e = as.numeric(p_d[delinquent] * p_b[scout] * p_s[SES] * sum(count))) x2 &lt;- sum((scout_e$count - scout_e$e)^2 / scout_e$e) print(x2) ## [1] 214.9233 The deviance statistic is scout_g2 &lt;- 2 * sum(scout_ct * log(scout_ct / scout_e$e)) print(scout_g2) ## [1] 218.6622 pchisq(scout_g2, df = scout_df, lower.tail = FALSE) ## [1] 1.268665e-43 Safe to say the mutual independence model does not fit. chisq.test() does not test the complete independence model for k-way tables. Instead, conduct tests on all \\({3 \\choose 2} = 3\\) embedded 2-way tables. For the Boy Scouts study, you can conduct a series of 2-way chi-sq tests. SES*scout: (scout_ses_scout &lt;- margin.table(scout_ct, c(3, 2))) ## scout ## SES no yes ## low 211 54 ## med 152 118 ## high 61 204 chisq.test(scout_ses_scout) ## ## Pearson&#39;s Chi-squared test ## ## data: scout_ses_scout ## X-squared = 172.2, df = 2, p-value &lt; 2.2e-16 SES*delinquent (scout_ses_delinquent &lt;- margin.table(scout_ct, c(3, 1))) ## delinquent ## SES no yes ## low 212 53 ## med 236 34 ## high 255 10 chisq.test(scout_ses_delinquent) ## ## Pearson&#39;s Chi-squared test ## ## data: scout_ses_delinquent ## X-squared = 32.826, df = 2, p-value = 7.445e-08 delinquent*scout (scout_delinquent_scout &lt;- margin.table(scout_ct, c(1, 2))) ## scout ## delinquent no yes ## no 360 343 ## yes 64 33 chisq.test(scout_delinquent_scout) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: scout_delinquent_scout ## X-squared = 6.884, df = 1, p-value = 0.008697 The odds ratio for scout delinquency vs non-scout delinquency is 0.54 with 95% CI (0.347, 0.845), so reject the null hypothesis that boy scout status and delinquent status are independent of one another (OR = 1), and thus that boy scout status, delinquent status, and socioeconomic status are not mutually independent. (scout_or &lt;- vcd::oddsratio(scout_delinquent_scout, log = FALSE)) ## odds ratios for delinquent and scout ## ## [1] 0.5411808 confint(scout_or) ## 2.5 % 97.5 % ## no:yes/no:yes 0.3466943 0.8447691 2.3.2.2 Joint Independence The joint independence model is that two variables are jointly independent of a third (AB, C). The joint probabilities are equal to the product of the AB marginal probability and the C marginal probability, \\(\\pi_{ijk} = \\pi_{ij} \\pi_k\\). The degrees of freedom in the associated chi-sq test are the number of free parameters in the saturated model minus the number of free parameters in the joint independence model. \\(df = (IJK-1) - ((IJ-1) + (K-1)) = (IJ-1)(K-1)\\). From the Boy Scouts study, suppose juvenile delinquency is the response variable. Test the null hypothesis that juvenile delinquency is independent of boy scout status and socioeconomic status. scout_d_bs &lt;- ftable( scout_ct, row.vars = c(&quot;SES&quot;, &quot;scout&quot;), col.vars = &quot;delinquent&quot; ) print(scout_d_bs) ## delinquent no yes ## SES scout ## low no 169 42 ## yes 43 11 ## med no 132 20 ## yes 104 14 ## high no 59 2 ## yes 196 8 chisq.test(scout_d_bs) ## ## Pearson&#39;s Chi-squared test ## ## data: scout_d_bs ## X-squared = 32.958, df = 5, p-value = 3.837e-06 Notice how the contingency table is constructed to not indicate whether SES and scout are related. 2.3.2.3 Marginal Independence The marginal independence model is that two variables are independent while ignoring the third (A, B). The joint probabilities are equal to the product of the two marginal probabilities, \\(\\pi_{ij} = \\pi_{i} \\pi_j\\), just like a 2-way table. The degrees of freedom in the associated chi-sq test are the number of free parameters in the saturated model minus the number of free parameters in the marginal independence model. \\(df = (I - 1) + (J - 1) = (I-1)(J-1)\\). From the Boy Scouts study, suppose juvenile delinquency is the response variable. Test the null hypothesis that juvenile delinquency is independent of boy scout status and socioeconomic status. ct &lt;- xtabs(count ~ scout + SES, scout) print(ct) ## SES ## scout low med high ## no 211 152 61 ## yes 54 118 204 chisq.test(ct) ## ## Pearson&#39;s Chi-squared test ## ## data: ct ## X-squared = 172.2, df = 2, p-value &lt; 2.2e-16 2.3.2.4 Conditional Independence The conditional independence model is that two variables are independent given a third (AB, AC). The joint probabilities are equal to the product of the conditional probabilities (B|A and C|A) and the marginal probability of A, \\(\\pi_{ijk} = \\pi_{j|i}\\pi_{k|i}\\pi_{i++}\\). You can test for conditional independence (AB, AC) by individually testing the independence (B, C) for each level of A. \\(X^2\\) and \\(G^2\\) are the sum of the individual values and the degrees of freedom are \\(I(J-1)(K-1)\\). A better way to do it is with the Cochran-Mantel-Haenszel Test. The degrees of freedom in the associated chi-sq test are the number of free parameters in the saturated model minus the number of free parameters in the joint independence model. \\(df = (IJK-1) - ((IJ-1) + (K-1)) = (IJ-1)(K-1)\\). From the Boy Scouts study, the section on joint independence found that scouting and SES were jointly independent of delinquency, so it must be that either scouting is independent of delinquency, or SES is independent of delinquency, or both are independent of delinquency. The marginal test below establishes the independance of (scout, delinquent). ct &lt;- xtabs(count ~ scout + delinquent, scout) print(ct) ## delinquent ## scout no yes ## no 360 64 ## yes 343 33 chisq.test(ct, correct = FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: ct ## X-squared = 7.4652, df = 1, p-value = 0.00629 The odds ratio 0.54 is a strong negative relationship between boy scout status and delinquency - boy scouts are 46% less likely (on the odds scale) to be delinquent than non-boy scouts. ct &lt;- xtabs(count ~ scout + delinquent, scout) p &lt;- prop.table(ct, margin = 1) (or &lt;- (p[2, 2] / p[2, 1]) / (p[1, 2] / p[1, 1])) ## [1] 0.5411808 However, one may ask is scouting and delinquency conditionally independent given SES? You can answer this question with three chi-square tests, one for each level of SES. ct &lt;- xtabs(count ~ scout + delinquent, scout[scout$SES == &quot;low&quot;, ]) print(ct) ## delinquent ## scout no yes ## no 169 42 ## yes 43 11 chisq.test(ct, correct = FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: ct ## X-squared = 0.0058145, df = 1, p-value = 0.9392 ct &lt;- xtabs(count ~ scout + delinquent, scout[scout$SES == &quot;med&quot;, ]) print(ct) ## delinquent ## scout no yes ## no 132 20 ## yes 104 14 chisq.test(ct, correct = FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: ct ## X-squared = 0.10098, df = 1, p-value = 0.7507 ct &lt;- xtabs(count ~ scout + delinquent, scout[scout$SES == &quot;high&quot;, ]) print(ct) ## delinquent ## scout no yes ## no 59 2 ## yes 196 8 chisq.test(ct, correct = FALSE) ## Warning in chisq.test(ct, correct = FALSE): Chi-squared approximation may be ## incorrect ## ## Pearson&#39;s Chi-squared test ## ## data: ct ## X-squared = 0.053447, df = 1, p-value = 0.8172 You can add up the three \\(X^2\\) values and run a chi-sq test with \\(df = 3\\). p = 0.984 indicating that the conditional independence model fits extremely well. pchisq(0.0058145 + 0.10098 + 0.053447, df = 3, lower.tail = FALSE) ## [1] 0.983737 The other way of testing for independence is the Cochran-Mantel-Haenszel test. The Cochran-Mantel-Haenszel (CMH) test statistic \\[M^2 = \\frac{\\left[ \\sum_k{(n_{11k} - \\mu_{11k})}\\right]^2}{\\sum_k{Var(n_{11k})}}\\] For the Boy Scouts study, the test is mantelhaen.test(scout_ct) ## ## Mantel-Haenszel chi-squared test without continuity correction ## ## data: scout_ct ## Mantel-Haenszel X-squared = 0.0080042, df = 1, p-value = 0.9287 ## alternative hypothesis: true common odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.5970214 1.6009845 ## sample estimates: ## common odds ratio ## 0.9776615 X2= 0.008 (df = 1, p-value = 0.9287) indicating the conditional independence model is a good fit for this data, so do not reject the null hypothesis. 2.3.2.5 Homogenous Associations Whereas the conditional independence model, (AB, AC) requires the BC odds ratios at each level of A to equal 1, the homogeneous association model, (AB, AC, BC), requires the BC odds ratios at each level of A to be identical, but not necessarily equal to 1. The Breslow-Day statistic is \\[X^2 = \\sum_{ijk}{\\frac{\\left(O_{ijk} - E_{ijk}\\right)^2}{E_{ijk}}}\\] DescTools::BreslowDayTest(scout_ct) ## ## Breslow-Day test on Homogeneity of Odds Ratios ## ## data: scout_ct ## X-squared = 0.1518, df = 2, p-value = 0.9269 These notes are primarily from the PSU STATS 500 and PSU STATS 504 courses.↩︎ "]
]
