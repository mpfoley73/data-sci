[
["support-vector-machines.html", "Chapter 9 Support Vector Machines", " Chapter 9 Support Vector Machines The following notes rely on (James et al. 2013), (Hastie, Tibshirani, and Friedman 2017), and (Kuhn and Johnson 2016). The Support Vector Machine (SVM) algorithm finds the optimal hyperplane that classifies the data points. The optimal hyperplane is the one which maximizes the margin betweeen the data points and the hyperplane. Support vectors are data points close to the hyperplane and influence the position and orientation of the hyperplane. Maximize the margin with the hinge loss function: \\[c(x, y, f(x)) = \\begin{cases} 0, &amp; \\mbox{if } y \\cdot f(x)&gt;1 \\\\ 1-y \\cdot f(x), &amp; \\mbox{else} \\end{cases}\\] dfd \\[f(n) = \\begin{cases} n/2, &amp; \\mbox{if } n\\mbox{ is even} \\\\ 3n+1, &amp; \\mbox{if } n\\mbox{ is odd} \\end{cases}\\] A separable dataset is one in which the classes do not overlap, so the classes can be separated by a decision boundary. The maximal margin separator is the decision boundary that is furthest from both classes. It is located at the mean of the relevant extreme points from each class. The kernal is the type of decision boundary (linear, polynomial, etc.). library(tidyverse) ## -- Attaching packages ------------------------------------------------ tidyverse 1.2.1 -- ## v ggplot2 3.2.1 v purrr 0.3.2 ## v tibble 2.1.3 v dplyr 0.8.3 ## v tidyr 1.0.0 v stringr 1.4.0 ## v readr 1.3.1 v forcats 0.4.0 ## -- Conflicts --------------------------------------------------- tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() df &lt;- data.frame( samp = 1:25, sugar = c(10.9, 10.9, 10.6, 10, 8, 8.2, 8.6, 10.9, 10.7, 8, 7.7, 7.8, 8.4, 11.5, 11.2, 8.9, 8.7, 7.4, 10.9, 10, 11.4, 10.8, 8.5, 8.2, 10.6) ) ggplot(df, aes(x = sugar)) + geom_point(aes(y = 0)) The e1071 library implements the SVM algorithm. The following function builds a linear SVM classifier. library(e1071) svm_model &lt;- svm(samp ~ ., df, type = &quot;C-classification&quot;, kernel = &quot;linear&quot;, scale = FALSE) print(svm_model) ## ## Call: ## svm(formula = samp ~ ., data = df, type = &quot;C-classification&quot;, ## kernel = &quot;linear&quot;, scale = FALSE) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: linear ## cost: 1 ## ## Number of Support Vectors: 25 Build the weight vector from the coeficients and sv elements in the model. J w &lt;- t(svm_model$coefs) %*% svm_model$SV slope_1 &lt;- w[1]/w[2] intercept_1 &lt;- svm_model$rho/w[2] References "]
]
