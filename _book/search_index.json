[
["cluster-analysis.html", "Chapter 11 Cluster Analysis", " Chapter 11 Cluster Analysis These notes are primarily taken from studying DataCamp courses Cluster Analysis in R and Unsupervised Learning in R, AIHR, UC Business Analytics R Programming Guide, and PSU STAT-505. Cluster analysis is a data exploration (mining) tool for dividing features into clusters, distinct populations with no a priori defining characteristics. The goal is to describe those populations with the observed data. Popular uses of clustering are audience segmentation, creating personas, detecting anomalies, and pattern recognition in images. There are two common approaches to cluster analysis. Agglomerative hierarchical algorithms start by defining each data point as a cluster, then repeatedly combine the two closest clusters into a new cluster until all data points are merged into a single cluster. Non-hierarchical methods such as K-means initially randomly partitions the data into a set of K clusters, then iteratively moves observations into different clusters until there is no sensible reassignment possible. Setup I will learn by example, using the IBM HR Analytics Employee Attrition &amp; Performance data set from Kaggle to discover which factors are associated with employee turnover and whether distinct clusters of employees are more susceptible to turnover. The clusters can help personalize employee experience (AIHR). This data set includes 1,470 employee records consisting of the EmployeeNumber, a flag for Attrition during some time frame, and 32 other descriptive variables. library(tidyverse) library(plotly) # interactive graphing library(cluster) # daisy and pam library(Rtsne) # dimensionality reduction and visualization library(dendextend) # color_branches set.seed(1234) # reproducibility dat &lt;- read_csv(&quot;./input/WA_Fn-UseC_-HR-Employee-Attrition.csv&quot;) dat &lt;- dat %&gt;% mutate_if(is.character, as_factor) %&gt;% mutate( EnvironmentSatisfaction = factor(EnvironmentSatisfaction, ordered = TRUE), StockOptionLevel = factor(StockOptionLevel, ordered = TRUE), JobLevel = factor(JobLevel, ordered = TRUE), JobInvolvement = factor(JobInvolvement, ordered = TRUE) ) %&gt;% select(EmployeeNumber, Attrition, everything()) my_skim &lt;- skimr::skim_with(numeric = skimr::sfl(p25 = NULL, p50 = NULL, p75 = NULL, hist = NULL)) my_skim(dat) Table 11.1: Data summary Name dat Number of rows 1470 Number of columns 35 _______________________ Column type frequency: factor 13 numeric 22 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts Attrition 0 1 FALSE 2 No: 1233, Yes: 237 BusinessTravel 0 1 FALSE 3 Tra: 1043, Tra: 277, Non: 150 Department 0 1 FALSE 3 Res: 961, Sal: 446, Hum: 63 EducationField 0 1 FALSE 6 Lif: 606, Med: 464, Mar: 159, Tec: 132 EnvironmentSatisfaction 0 1 TRUE 4 3: 453, 4: 446, 2: 287, 1: 284 Gender 0 1 FALSE 2 Mal: 882, Fem: 588 JobInvolvement 0 1 TRUE 4 3: 868, 2: 375, 4: 144, 1: 83 JobLevel 0 1 TRUE 5 1: 543, 2: 534, 3: 218, 4: 106 JobRole 0 1 FALSE 9 Sal: 326, Res: 292, Lab: 259, Man: 145 MaritalStatus 0 1 FALSE 3 Mar: 673, Sin: 470, Div: 327 Over18 0 1 FALSE 1 Y: 1470 OverTime 0 1 FALSE 2 No: 1054, Yes: 416 StockOptionLevel 0 1 TRUE 4 0: 631, 1: 596, 2: 158, 3: 85 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p100 EmployeeNumber 0 1 1024.87 602.02 1 2068 Age 0 1 36.92 9.14 18 60 DailyRate 0 1 802.49 403.51 102 1499 DistanceFromHome 0 1 9.19 8.11 1 29 Education 0 1 2.91 1.02 1 5 EmployeeCount 0 1 1.00 0.00 1 1 HourlyRate 0 1 65.89 20.33 30 100 JobSatisfaction 0 1 2.73 1.10 1 4 MonthlyIncome 0 1 6502.93 4707.96 1009 19999 MonthlyRate 0 1 14313.10 7117.79 2094 26999 NumCompaniesWorked 0 1 2.69 2.50 0 9 PercentSalaryHike 0 1 15.21 3.66 11 25 PerformanceRating 0 1 3.15 0.36 3 4 RelationshipSatisfaction 0 1 2.71 1.08 1 4 StandardHours 0 1 80.00 0.00 80 80 TotalWorkingYears 0 1 11.28 7.78 0 40 TrainingTimesLastYear 0 1 2.80 1.29 0 6 WorkLifeBalance 0 1 2.76 0.71 1 4 YearsAtCompany 0 1 7.01 6.13 0 40 YearsInCurrentRole 0 1 4.23 3.62 0 18 YearsSinceLastPromotion 0 1 2.19 3.22 0 15 YearsWithCurrManager 0 1 4.12 3.57 0 17 You would normally start a cluster analysis with an exploration of the data to determine which variables are interesting and relevant to your goal. I’ll bypass that rigor and just use a binary correlation analysis. Binary correlation analysis converts features into binary format by binning the continuous features and one-hot encoding the binary features. correlate() calculates the correlation coefficient for each binary feature to the response variable. A Correlation Funnel is an tornado plot that lists the highest correlation features (based on absolute magnitude) at the top of the and the lowest correlation features at the bottom. Read more on the correlationfunel GitHub README. Using binary correlation, I’ll include just the variables with a correlation coefficient of at least 0.10. For our employee attrition data set, OverTime (Y|N) has the largest correlation, JobLevel = 1, MonthlyIncome &lt;= 2,695.80, etc. dat %&gt;% select(-EmployeeNumber) %&gt;% correlationfunnel::binarize(n_bins = 5, thresh_infreq = 0.01) %&gt;% correlationfunnel::correlate(Attrition__Yes) %&gt;% correlationfunnel::plot_correlation_funnel(interactive = FALSE) %&gt;% ggplotly() # Makes prettier, but drops the labels Using the cutoff of 0.1 leaves 14 features for the analysis. vars &lt;- c( &quot;EmployeeNumber&quot;, &quot;Attrition&quot;, &quot;OverTime&quot;, &quot;JobLevel&quot;, &quot;MonthlyIncome&quot;, &quot;YearsAtCompany&quot;, &quot;StockOptionLevel&quot;, &quot;YearsWithCurrManager&quot;, &quot;TotalWorkingYears&quot;, &quot;MaritalStatus&quot;, &quot;Age&quot;, &quot;YearsInCurrentRole&quot;, &quot;JobRole&quot;, &quot;EnvironmentSatisfaction&quot;, &quot;JobInvolvement&quot;, &quot;BusinessTravel&quot; ) dat_2 &lt;- dat %&gt;% select(one_of(vars)) Data Preparation The concept of distance is central to clustering. Two observations are similar if the distance between their features is relatively small. To compare feature distances, they should be on a similar scale. There are many ways to define distance (see options in ?dist), but the two most common are Euclidean, \\(d = \\sqrt{\\sum{(x_i - y_i)^2}}\\), and binary, 1 minus the proportion of shared features (Wikipedia, PSU-505). If you have a mix a feature types, use the Gower distance (Analytics Vidhya) range-normalizes the quantitative variables, one-hot encodes the nominal variables, and ranks the ordinal variables. Then it calculates distances using the Manhattan distance for quantitative and ordinal variables, and the Dice coefficient for nominal variables. Gower’s distance is computationally expensive, so you could one-hot encode the data and standardize the variables as \\((x - \\bar{x}) / sd(x)\\) so that each feature has a mean of 0 and standard deviation of 1, like this: # cannot one-hot encode ordered factors, so change to unordered x &lt;- dat_2 %&gt;% mutate_if(is.ordered, ~factor(., ordered = FALSE)) dat_2_mtrx &lt;- mltools::one_hot(data.table::as.data.table(x[, 2:16])) %&gt;% as.matrix() row.names(dat_2_mtrx) &lt;- dat_2$EmployeeNumber dat_2_mtrx &lt;- na.omit(dat_2_mtrx) dat_2_mtrx &lt;- scale(dat_2_mtrx) dat_2_dist &lt;- dist(dat_2_mtrx) But normally you would go ahead and calculate Gower’s distance using daisy(). dat_2_gwr &lt;- cluster::daisy(dat_2[, 2:16], metric = &quot;gower&quot;) As a sanity check, let’s see the most similar and dissimilar pairs of employees according to their Gower distance. Here are the most similar employees. x &lt;- as.matrix(dat_2_gwr) dat_2[which(x == min(x[x != 0]), arr.ind = TRUE)[1, ], ] %&gt;% t() %&gt;% as.data.frame() %&gt;% rownames_to_column() %&gt;% flextable::flextable() %&gt;% flextable::autofit() rownameV1V2EmployeeNumber1624 614AttritionYesYesOverTimeYesYesJobLevel11MonthlyIncome15691878YearsAtCompany00StockOptionLevel00YearsWithCurrManager00TotalWorkingYears00MaritalStatusSingleSingleAge1818YearsInCurrentRole00JobRoleSales RepresentativeSales RepresentativeEnvironmentSatisfaction22JobInvolvement33BusinessTravelTravel_FrequentlyTravel_Frequently They are identical except for MonthlyIncome. Here are the most dissimilar employees. dat_2[which(x == max(x), arr.ind = TRUE)[1, ], ] %&gt;% t() %&gt;% as.data.frame() %&gt;% rownames_to_column() %&gt;% flextable::flextable() %&gt;% flextable::autofit() rownameV1V2EmployeeNumber1094 825AttritionNoYesOverTimeNoYesJobLevel15MonthlyIncome 462119246YearsAtCompany 331StockOptionLevel30YearsWithCurrManager28TotalWorkingYears 340MaritalStatusMarriedSingleAge2758YearsInCurrentRole 215JobRoleLaboratory TechnicianResearch DirectorEnvironmentSatisfaction14JobInvolvement13BusinessTravelNon-TravelTravel_Rarely These two employees have nothing in common. With the data preparation complete, we can finally perform our cluster analysis. I’ll try K-means and HCA. "],
["k-means.html", "11.1 K-Means", " 11.1 K-Means The k-means clustering algorithm randomly assigns all observations to one of \\(k\\) clusters. K-means then iteratively calculates the cluster centroids and reassigns the observations to their nearest centroid. The centroid is the mean of the points in the cluster (Hence the name “k-means”). The iterations continue until either the centroids stabilize or the iterations reach a set maximum, iter.max (typically 50). The result is k clusters with the minimum total intra-cluster variation. The centroid of cluster \\(c_i \\in C\\) is the mean of the cluster observations \\(S_i\\): \\(c_i = \\frac{1}{|S_i|} \\sum_{x_i \\in S_i}{x_i}\\). The nearest centroid is the minimum squared euclidean distance, \\(\\underset{c_i \\in C}{\\operatorname{arg min}} dist(c_i, x)^2\\). A more robust version of k-means is partitioning around medoids (pam), which minimizes the sum of dissimilarities instead of a sum of squared euclidean distances. That’s what I’ll use. The algorithm will converge to a result, but the result may only be a local optimum. Other random starting centroids may yield a different local optimum. Common practice is to run the k-means algorithm nstart times and select the lowest within-cluster sum of squared distances among the cluster members. A typical number of runs is nstart = 20. Choosing K What is the best number of clusters? You may have a preference in advance, but more likely you will use a scree plot or use the silhouette method. The scree plot is a plot of the total within-cluster sum of squared distances as a function of k. The sum of squares always decreases as k increases, but at a declining rate. The optimal k is at the “elbow” in the curve - the point at which the curve flattens. kmeans() returns an object of class kmeans, a list in which one of the components is the model sum of squares tot.withinss. In the scree plot below, the elbow may be k = 5. wss &lt;- map_dbl(2:10, ~ kmeans(dat_2_gwr, centers = .x)$tot.withinss) wss_tbl &lt;- tibble(k = 2:10, wss) ggplot(wss_tbl, aes(x = k, y = wss)) + geom_point(size = 2) + geom_line() + scale_x_continuous(breaks = 2:10) + labs(title = &quot;Scree Plot&quot;) The silhouette method calculates the within-cluster distance \\(C(i)\\) for each observation, and its distance to the nearest cluster \\(N(i)\\). The silhouette width is \\(S = 1 - C(i) / N(i)\\) for \\(C(i) &lt; N(i)\\) and \\(S = N(i) / C(i) - 1\\) for \\(C(i) &gt; N(i)\\). A value close to 1 means the observation is well-matched to its current cluster; A value near 0 means the observation is on the border between the two clusters; and a value near -1 means the observation is better-matched to the other cluster. The optimal number of clusters is the number that maximizes the total silhouette width. cluster::pam() returns a list in which one of the components is the average width silinfo$avg.width. In the silhouette plot below, the maximum width is at k = 6. sil &lt;- map_dbl(2:10, ~ pam(dat_2_gwr, k = .x)$silinfo$avg.width) sil_tbl &lt;- tibble(k = 2:10, sil) ggplot(sil_tbl, aes(x = k, y = sil)) + geom_point(size = 2) + geom_line() + scale_x_continuous(breaks = 2:10) + labs(title = &quot;Silhouette Plot&quot;) Summarize Results Run pam() again and attach the results to the original table for visualization and summary statistics. mdl &lt;- pam(dat_2_gwr, k = 6) dat_3 &lt;- dat_2 %&gt;% mutate(cluster = as.factor(mdl$clustering)) Here are the six medoids from our data set. dat_2[mdl$medoids, ] %&gt;% t() %&gt;% as.data.frame() %&gt;% rownames_to_column() %&gt;% flextable::flextable() %&gt;% flextable::autofit() rownameV1V2V3V4V5V6EmployeeNumber1171 35 65 221 7471408AttritionNoNoYesNoNoNoOverTimeNoNoYesNoNoNoJobLevel221124MonthlyIncome 5155 6825 3441 2713 530416799YearsAtCompany 6 9 2 5 820StockOptionLevel010111YearsWithCurrManager522279TotalWorkingYears 910 2 51021MaritalStatusSingleMarriedSingleMarriedDivorcedMarriedAge424228283042YearsInCurrentRole472277JobRoleSales ExecutiveSales ExecutiveLaboratory TechnicianResearch ScientistSales ExecutiveManagerEnvironmentSatisfaction233333JobInvolvement333333BusinessTravelTravel_RarelyTravel_RarelyTravel_RarelyTravel_RarelyTravel_RarelyTravel_Rarely We’re most concerned about attrition. Do high-attrition employees fall into a particular cluster? Yes! 79.7% of cluster 3 left the company - that’s 59.5% of all turnover in the company. dat_3_smry &lt;- dat_3 %&gt;% count(cluster, Attrition) %&gt;% group_by(cluster) %&gt;% mutate(cluster_n = sum(n), turnover_rate = scales::percent(n / sum(n), accuracy = 0.1)) %&gt;% ungroup() %&gt;% filter(Attrition == &quot;Yes&quot;) %&gt;% mutate(pct_of_turnover = scales::percent(n / sum(n), accuracy = 0.1)) %&gt;% select(cluster, cluster_n, turnover_n = n, turnover_rate, pct_of_turnover) dat_3_smry %&gt;% flextable::flextable() %&gt;% flextable::autofit() clustercluster_nturnover_nturnover_ratepct_of_turnover1268238.6%9.7%2280248.6%10.1%317714179.7%59.5%4364298.0%12.2%520284.0%3.4%6179126.7%5.1% You can get some sense of the quality of clustering by constructing the Barnes-Hut t-Distributed Stochastic Neighbor Embedding (t-SNE). dat_4 &lt;- dat_3 %&gt;% left_join(dat_3_smry, by = &quot;cluster&quot;) %&gt;% rename(Cluster = cluster) %&gt;% mutate( MonthlyIncome = MonthlyIncome %&gt;% scales::dollar(), description = str_glue(&quot;Turnover = {Attrition} MaritalDesc = {MaritalStatus} Age = {Age} Job Role = {JobRole} Job Level {JobLevel} Overtime = {OverTime} Current Role Tenure = {YearsInCurrentRole} Professional Tenure = {TotalWorkingYears} Monthly Income = {MonthlyIncome} Cluster: {Cluster} Cluster Size: {cluster_n} Cluster Turnover Rate: {turnover_rate} Cluster Turnover Count: {turnover_n} &quot;)) tsne_obj &lt;- Rtsne(dat_2_gwr, is_distance = TRUE) tsne_tbl &lt;- tsne_obj$Y %&gt;% as_tibble() %&gt;% setNames(c(&quot;X&quot;, &quot;Y&quot;)) %&gt;% bind_cols(dat_4) %&gt;% mutate(Cluster = as_factor(Cluster)) g &lt;- tsne_tbl %&gt;% ggplot(aes(x = X, y = Y, colour = Cluster, text = description)) + geom_point() ggplotly(g) Another common approach is to take summary statistics and draw your own conclusions. You might start by asking which attributes differ among the clusters. The box plots below show the distribution of the numeric variables. All of the numeric variable distributions appear to vary among the clusters. my_boxplot &lt;- function(y_var){ dat_3 %&gt;% ggplot(aes(x = cluster, y = !!sym(y_var))) + geom_boxplot() + geom_jitter(aes(color = Attrition), alpha = 0.2, height = 0.10) + theme_minimal() + theme(legend.position = &quot;none&quot;) + labs(x = &quot;&quot;, y = &quot;&quot;, title = y_var) } vars_numeric &lt;- dat_3 %&gt;% select(-EmployeeNumber) %&gt;% select_if(is.numeric) %&gt;% colnames() g &lt;- map(vars_numeric, my_boxplot) gridExtra::marrangeGrob(g, nrow=1, ncol = 2) You can perform an analysis of variance to confirm. The table below collects the ANOVA results for each of the numeric variables. The results indicate that there are significant differences among clusters at the .01 level for all of the numeric variables. km_aov &lt;- vars_numeric %&gt;% map(~ aov(rlang::eval_tidy(expr(!!sym(.x) ~ cluster)), data = dat_3)) km_aov %&gt;% map(anova) %&gt;% map(~ data.frame(F = .x$`F value`[[1]], p = .x$`Pr(&gt;F)`[[1]])) %&gt;% bind_rows() %&gt;% bind_cols(Attribute = vars_numeric) %&gt;% select(Attribute, everything()) %&gt;% flextable::flextable() %&gt;% flextable::autofit() %&gt;% flextable::colformat_num(j = 2, digits = 2) %&gt;% flextable::colformat_num(j = 3, digits = 4) AttributeFpMonthlyIncome718.880.0000YearsAtCompany134.580.0000YearsWithCurrManager65.530.0000TotalWorkingYears342.880.0000Age98.760.0000YearsInCurrentRole69.530.0000 We’re particularly interested in cluster 3, the high attrition cluster, and what sets it apart from the others. Right away, we can see that clusters 3 and 4 similar in almost all of theses attributes. They tend to have the smaller incomes, company and job tenure, years with their current manager, total working experience, and age. I.e., they tend to be lower on the career ladder. To drill into cluster differences to determine which clusters differ from others, use the Tukey HSD post hoc test with Bonferroni method applied to control the experiment-wise error rate. That is, only reject the null hypothesis of equal means among clusters if the p-value is less than \\(\\alpha / p\\), or \\(.05 / 6 = 0.0083\\). The significantly different cluster combinations are shown in bold. Clusters 3 and 4 differ from the others on all six measures. However, there are no significant differences between c3 and c4 (highlighted green) for the numeric variables. km_hsd &lt;- map(km_aov, TukeyHSD) map(km_hsd, ~ .x$cluster %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% filter(str_detect(rowname, &quot;-&quot;))) %&gt;% map2(vars_numeric, bind_cols) %&gt;% bind_rows() %&gt;% select(predictor = `...6`, everything()) %&gt;% mutate(cluster_a = str_sub(rowname, start = 1, end = 1), cluster = paste0(&quot;c&quot;, str_sub(rowname, start = 3, end = 3))) %&gt;% pivot_wider(c(predictor, cluster_a, cluster, p.adj), names_from = cluster_a, values_from = p.adj, names_prefix = &quot;c&quot;) %&gt;% flextable::flextable() %&gt;% flextable::colformat_num(j = c(3:7), digits = 4) %&gt;% flextable::bold(i = ~ c2 &lt; .05 / length(vars_numeric), j = ~ c2, bold = TRUE) %&gt;% flextable::bold(i = ~ c3 &lt; .05 / length(vars_numeric), j = ~ c3, bold = TRUE) %&gt;% flextable::bold(i = ~ c4 &lt; .05 / length(vars_numeric), j = ~ c4, bold = TRUE) %&gt;% flextable::bold(i = ~ c5 &lt; .05 / length(vars_numeric), j = ~ c5, bold = TRUE) %&gt;% flextable::bold(i = ~ c6 &lt; .05 / length(vars_numeric), j = ~ c6, bold = TRUE) %&gt;% flextable::bg(i = ~ cluster == &quot;c3&quot;, j = ~ c4, bg = &quot;#B6E2D3&quot;) %&gt;% flextable::autofit() predictorclusterc2c3c4c5c6MonthlyIncomec10.65870.00000.00000.97200.0000MonthlyIncomec20.00000.00000.98990.0000MonthlyIncomec30.39750.00000.0000MonthlyIncomec40.00000.0000MonthlyIncomec50.0000YearsAtCompanyc10.99990.00000.00000.99220.0000YearsAtCompanyc20.00000.00000.99890.0000YearsAtCompanyc30.99350.00000.0000YearsAtCompanyc40.00000.0000YearsAtCompanyc50.0000YearsWithCurrManagerc11.00000.00000.00000.97630.0000YearsWithCurrManagerc20.00000.00000.95950.0000YearsWithCurrManagerc30.99650.00000.0000YearsWithCurrManagerc40.00000.0000YearsWithCurrManagerc50.0000TotalWorkingYearsc10.98400.00000.00000.99800.0000TotalWorkingYearsc20.00000.00000.89340.0000TotalWorkingYearsc30.99980.00000.0000TotalWorkingYearsc40.00000.0000TotalWorkingYearsc50.0000Agec10.99800.00000.00000.99890.0000Agec20.00000.00000.96910.0000Agec30.05320.00000.0000Agec40.00000.0000Agec50.0000YearsInCurrentRolec10.10130.00000.00000.64710.0000YearsInCurrentRolec20.00000.00000.95730.0000YearsInCurrentRolec30.98540.00000.0000YearsInCurrentRolec40.00000.0000YearsInCurrentRolec50.0000 So at this point we have kind of an incomplete picture. We know the high-attrition employees are low on the career ladder, but cluster 4 is also low on the career ladder and they are not high-attrition. How about the factor variables? The tile plots below show that clusters 3 and 4 are lab technicians and research scientists. They are both at the lowest job level. But three factors distinguish these clusters from each other: cluster 3 is far more likely to work overtime, have no stock options, and be single. my_tileplot &lt;- function(y_var){ dat_3 %&gt;% count(cluster, !!sym(y_var)) %&gt;% ungroup() %&gt;% group_by(cluster) %&gt;% mutate(pct = n / sum(n)) %&gt;% ggplot(aes(y = !!sym(y_var), x = cluster, fill = pct)) + geom_tile() + scale_fill_gradient(low = &quot;#E9EAEC&quot;, high = &quot;#FAD02C&quot;) + geom_text(aes(label = scales::percent(pct, accuracy = 1.0)), size = 3) + theme_minimal() + theme(legend.position = &quot;none&quot;) } vars_factor &lt;- dat_3 %&gt;% select(-cluster) %&gt;% select_if(is.factor) %&gt;% colnames() g &lt;- map(vars_factor, my_tileplot) gridExtra::marrangeGrob(g, nrow=1, ncol = 2) You can perform a chi-squared independence test to confirm. The table below collects the Chis-Sq test results for each of the factor variables. The results indicate that there are significant differences among clusters at the .05 level for all of the factor variables except EnvironmentSatisfaction and JobInvolvement. km_chisq &lt;- vars_factor %&gt;% map(~ janitor::tabyl(dat_3, cluster, !!sym(.x))) %&gt;% map(janitor::chisq.test) km_chisq %&gt;% map(~ data.frame(ChiSq = .x$statistic[[1]], df = .x$parameter[[1]], p = .x$p.value[[1]])) %&gt;% bind_rows() %&gt;% bind_cols(Attribute = vars_factor) %&gt;% select(Attribute, everything()) %&gt;% flextable::flextable() %&gt;% flextable::autofit() %&gt;% flextable::colformat_num(j = ~ ChiSq, digits = 1) %&gt;% flextable::colformat_num(j = ~ p, digits = 4) AttributeChiSqdfpAttrition603.250.0000OverTime199.950.0000JobLevel1,855.6200.0000StockOptionLevel731.2150.0000MaritalStatus1,853.4100.0000JobRole1,754.0400.0000EnvironmentSatisfaction20.0150.1712JobInvolvement22.6150.0938BusinessTravel19.7100.0324 We’re particularly interested in cluster 3, the high attrition cluster, and what sets it apart from the others, and cluster 4 in particular. We think it is that cluster 3 is far more likely to work overtime, have no stock options, and be single, but let’s perform a residuals analysis on the the chi-sq test to check. The residuals with absolute value &gt;2 are driving the differences among the clusters. km_chisq %&gt;% map(~ data.frame(.x$residuals)) %&gt;% map(data.frame) %&gt;% map(t) %&gt;% map(data.frame) %&gt;% map(rownames_to_column, var = &quot;Predictor&quot;) %&gt;% map(~ filter(.x, Predictor != &quot;cluster&quot;)) %&gt;% map(~ select(.x, Predictor, c1 = X1, c2 = X2, c3 = X3, c4 = X4, c5 = X5, c6 = X6)) %&gt;% map(~ mutate_at(.x, vars(2:7), as.numeric)) %&gt;% map(flextable::flextable) %&gt;% map(~ flextable::colformat_num(.x, j = ~ c1 + c2 + c3 + c4 + c5 + c6, digits = 1)) %&gt;% map(~ flextable::bold(.x, i = ~ abs(c1) &gt; 2, j = ~ c1, bold = TRUE)) %&gt;% map(~ flextable::bold(.x, i = ~ abs(c2) &gt; 2, j = ~ c2, bold = TRUE)) %&gt;% map(~ flextable::bold(.x, i = ~ abs(c3) &gt; 2, j = ~ c3, bold = TRUE)) %&gt;% map(~ flextable::bold(.x, i = ~ abs(c4) &gt; 2, j = ~ c4, bold = TRUE)) %&gt;% map(~ flextable::bold(.x, i = ~ abs(c5) &gt; 2, j = ~ c5, bold = TRUE)) %&gt;% map(~ flextable::bold(.x, i = ~ abs(c6) &gt; 2, j = ~ c6, bold = TRUE)) %&gt;% # map(~ flextable::bg(i = ~ cluster == &quot;c3&quot;, j = ~ c4, bg = &quot;#B6E2D3&quot;) %&gt;% map2(vars_factor, ~ flextable::set_caption(.x, caption = .y)) ## [[1]] ## a flextable object. ## col_keys: `Predictor`, `c1`, `c2`, `c3`, `c4`, `c5`, `c6` ## header has 1 row(s) ## body has 2 row(s) ## original dataset sample: ## Predictor c1 c2 c3 c4 c5 c6 ## 1 Yes -3.074284 -3.146800 21.052736 -3.875086 -4.304940 -3.138300 ## 2 No 1.347835 1.379627 -9.229989 1.698924 1.887382 1.375901 ## ## [[2]] ## a flextable object. ## col_keys: `Predictor`, `c1`, `c2`, `c3`, `c4`, `c5`, `c6` ## header has 1 row(s) ## body has 2 row(s) ## original dataset sample: ## Predictor c1 c2 c3 c4 c5 c6 ## 1 Yes -4.000828 -0.5884457 10.725697 -3.449431 -0.15403618 0.04836361 ## 2 No 2.513485 0.3696858 -6.738324 2.167075 0.09677186 -0.03038401 ## ## [[3]] ## a flextable object. ## col_keys: `Predictor`, `c1`, `c2`, `c3`, `c4`, `c5`, `c6` ## header has 1 row(s) ## body has 5 row(s) ## original dataset sample: ## Predictor c1 c2 c3 c4 c5 c6 ## 1 X1 -4.622859 -9.678341 9.599235 15.569991 -5.512378 -8.1314460 ## 2 X2 4.828776 9.745394 -5.150270 -7.324817 5.208898 -8.0637760 ## 3 X3 2.578522 4.729466 -3.561905 -7.211066 4.210211 0.2822892 ## 4 X4 -2.121266 -3.825733 -3.012750 -5.123243 -2.768472 20.6230820 ## 5 X5 -2.418986 -3.625308 -2.535454 -4.133487 -3.079226 19.1807890 ## ## [[4]] ## a flextable object. ## col_keys: `Predictor`, `c1`, `c2`, `c3`, `c4`, `c5`, `c6` ## header has 1 row(s) ## body has 4 row(s) ## original dataset sample: ## Predictor c1 c2 c3 c4 c5 c6 ## 1 X0 14.261198 -6.03754600 7.6891380 -4.3398430 -8.667412 -2.1488560 ## 2 X1 -10.423939 4.83128800 -6.1104130 3.8210350 3.989103 3.1019730 ## 3 X2 -5.367070 2.71691560 -2.9860991 0.4598298 6.071044 -0.9665264 ## 4 X3 -3.936572 -0.04733811 -0.6985228 1.0794749 4.775144 -1.0413857 ## ## [[5]] ## a flextable object. ## col_keys: `Predictor`, `c1`, `c2`, `c3`, `c4`, `c5`, `c6` ## header has 1 row(s) ## body has 3 row(s) ## original dataset sample: ## Predictor c1 c2 c3 c4 c5 c6 ## 1 Single 19.587144 -9.461702 9.492289 -6.338612 -8.036481 -3.9961330 ## 2 Married -10.986571 13.408220 -6.224744 6.611745 -9.616666 3.6508300 ## 3 Divorced -7.721161 -7.892130 -2.450022 -1.886047 23.430922 -0.4466382 ## ## [[6]] ## a flextable object. ## col_keys: `Predictor`, `c1`, `c2`, `c3`, `c4`, `c5`, `c6` ## header has 1 row(s) ## body has 9 row(s) ## original dataset sample: ## Predictor c1 c2 c3 c4 c5 ## 1 Sales.Executive 4.224222 10.393935 -3.551837 -8.984643 3.616083 ## 2 Research.Scientist -3.047504 -7.055555 1.153688 15.017279 -3.808570 ## 3 Laboratory.Technician -0.468456 -3.037306 7.308603 2.730491 -1.272337 ## 4 Manufacturing.Director 2.249256 1.975299 -2.742469 -3.822520 3.601185 ## 5 Healthcare.Representative 1.865554 4.213539 -2.964428 -4.993130 3.299386 ## c6 ## 1 -5.6656560 ## 2 -5.9629240 ## 3 -5.4378120 ## 4 -0.8701804 ## 5 -0.9894197 ## ## [[7]] ## a flextable object. ## col_keys: `Predictor`, `c1`, `c2`, `c3`, `c4`, `c5`, `c6` ## header has 1 row(s) ## body has 4 row(s) ## original dataset sample: ## Predictor c1 c2 c3 c4 c5 c6 ## 1 X1 1.4207444 -1.1006522 0.47951650 -0.39635550 -0.4843633 0.2410757 ## 2 X2 1.8906688 1.2623375 -1.28554920 -0.71964310 -0.7067058 -0.8369268 ## 3 X3 -2.0453569 0.7228204 0.06162142 0.83358388 0.2219346 0.1129351 ## 4 X4 -0.5890421 -0.8627985 0.58649848 0.05346915 0.7297500 0.3651772 ## ## [[8]] ## a flextable object. ## col_keys: `Predictor`, `c1`, `c2`, `c3`, `c4`, `c5`, `c6` ## header has 1 row(s) ## body has 4 row(s) ## original dataset sample: ## Predictor c1 c2 c3 c4 c5 c6 ## 1 X1 -0.03392631 -0.7065995 1.8998844 -1.44533350 1.3604659 -0.3481475 ## 2 X2 0.56028040 0.4225771 0.5724950 -0.40027460 -1.6062743 0.4937853 ## 3 X3 0.21879629 -0.2592379 -0.3437552 -0.06366268 0.2494020 0.2241808 ## 4 X4 -1.41557010 0.4909903 -1.5222857 1.89954330 0.9469245 -1.0829262 ## ## [[9]] ## a flextable object. ## col_keys: `Predictor`, `c1`, `c2`, `c3`, `c4`, `c5`, `c6` ## header has 1 row(s) ## body has 3 row(s) ## original dataset sample: ## Predictor c1 c2 c3 c4 c5 ## 1 Travel_Rarely -1.0263102 -0.33108864 -0.05226557 0.6056579 -0.02704771 ## 2 Travel_Frequently 1.3367306 0.03277861 1.84355843 -0.9165098 -0.82079013 ## 3 Non.Travel 0.8897836 0.82850980 -2.36743050 -0.3516054 1.18671160 ## c6 ## 1 0.8869173 ## 2 -1.3309690 ## 3 -0.5300458 Cluster 3 are significantly more likely to work overtime while cluster 4 (and 1) are significantly less likely. Cluster 3 (and 1) are significantly more likely to have no stock options and be single. "],
["hca.html", "11.2 HCA", " 11.2 HCA Hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which builds a hierarchy of clusters (usually presented in a dendrogram). The HCA process is: Calculate the distance between each observation with dist() or daisy(). We did that above when we created dat_2_gwr. Cluster the two closest observations into a cluster with hclust(). Then calculate the cluster’s distance to the remaining observations. If the shortest distance is between two observations, define a second cluster, otherwise adds the observation as a new level to the cluster. The process repeats until all observations belong to a single cluster. The “distance” to a cluster can be defined as: complete: distance to the furthest member of the cluster, single: distance to the closest member of the cluster, average: average distance to all members of the cluster, or centroid: distance between the centroids of each cluster. Complete and average distances tend to produce more balanced trees and are most common. Pruning an unbalanced tree can result in most observations assigned to one cluster and only a few observations assigned to other clusters. This is useful for identifying outliers. mdl_hc &lt;- hclust(dat_2_gwr, method = &quot;complete&quot;) Evaluate the hclust tree with a dendogram, principal component analysis (PCA), and/or summary statistics. The vertical lines in a dendogram indicate the distance between nodes and their associated cluster. Choose the number of clusters to keep by identifying a cut point that creates a reasonable number of clusters with a substantial number of observations per cluster (I know, “reasonable” and “substantial” are squishy terms). Below, cutting at height 0.65 to create 7 clusters seems good. # Inspect the tree to choose a size. plot(color_branches(as.dendrogram(mdl_hc), k = 7)) abline(h = .65, col = &quot;red&quot;) “Cut” the hierarchical tree into the desired number of clusters (k) or height h with cutree(hclust, k = NULL, h = NULL). cutree() returns a vector of cluster memberships. Attach this vector back to the original dataframe for visualization and summary statistics. dat_2_clstr_hca &lt;- dat_2 %&gt;% mutate(cluster = cutree(mdl_hc, k = 7)) Calculate summary statistics and draw conclusions. Useful summary statistics are typically membership count, and feature averages (or proportions). dat_2_clstr_hca %&gt;% group_by(cluster) %&gt;% summarise_if(is.numeric, funs(mean(.))) ## # A tibble: 7 x 8 ## cluster EmployeeNumber MonthlyIncome YearsAtCompany YearsWithCurrMa~ ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1051. 8056. 8.21 4.23 ## 2 2 1029. 5146. 5.98 3.85 ## 3 3 974. 4450. 5.29 3.51 ## 4 4 1088. 3961. 4.60 2.64 ## 5 5 980. 16534. 14.2 6.80 ## 6 6 1036. 12952. 14.9 7.64 ## 7 7 993. 13733. 12.8 6.53 ## # ... with 3 more variables: TotalWorkingYears &lt;dbl&gt;, Age &lt;dbl&gt;, ## # YearsInCurrentRole &lt;dbl&gt; 11.2.0.1 K-Means vs HCA Hierarchical clustering has some advantages over k-means. It can use any distance method - not just euclidean. The results are stable - k-means can produce different results each time. While they can both be evaluated with the silhouette and elbow plots, hierachical clustering can also be evaluated with a dendogram. But hierarchical clusters has one significant drawback: it is computationally complex compared to k-means. For this last reason, k-means is more common. "]
]
