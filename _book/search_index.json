[
["probability.html", "Chapter 1 Probability ", " Chapter 1 Probability "],
["principles.html", "1.1 Principles", " 1.1 Principles Here are three rules that come up all the time. \\(Pr(A \\cup B) = Pr(A)+Pr(B) - Pr(AB)\\). This rule generalizes to \\(Pr(A \\cup B \\cup C)=Pr(A)+Pr(B)+Pr(C)-Pr(AB)-Pr(AC)-Pr(BC)+Pr(ABC)\\). \\(Pr(A|B) = \\frac{P(AB)}{P(B)}\\) If A and B are independent, \\(Pr(A \\cap B) = Pr(A)Pr(B)\\), and \\(Pr(A|B)=Pr(A)\\). Uniform distributions on finite sample spaces often reduce to counting the elements of A and the sample space S, a process called combinatorics. Here are three important combinatorial rules. Multiplication Rule. \\(|S|=|S_1 |⋯|S_k|\\). How many outcomes are possible from a sequence of 4 coin flips and 2 rolls of a die? \\(|S|=|S_1| \\cdot |S_2| \\dots |S_6| = 2 \\cdot 2 \\cdot 2 \\cdot 2 \\cdot 6 \\cdot 6 = 288\\). How many subsets are possible from a set of n=10 elements? In each subset, each element is either included or not, so there are \\(2^n = 1024\\) subsets. How many subsets are possible from a set of n=10 elements taken k at a time with replacement? Each experiment has \\(n\\) possible outcomes and is repeated \\(k\\) times, so there are \\(n^k\\) subsets. Permutations. The number of ordered arrangements (permutations) of a set of \\(|S|=n\\) items taken \\(k\\) at a time without replacement has \\(n(n-1) \\dots (n-k+1)\\) subsets because each draw is one of k experiments with decreasing number of possible outcomes. \\[_nP_k = \\frac{n!}{(n-k)!}\\] Notice that if \\(k=0\\) then there is 1 permutation; if \\(k=1\\) then there are \\(n\\) permutations; if \\(k=n\\) then there are \\(n!\\) permutations. How many ways can you distribute 4 jackets among 4 people? \\(_nP_k = \\frac{4!}{(4-4)!} = 4! = 24\\) How many ways can you distribute 4 jackets among 2 people? \\(_nP_k = \\frac{4!}{(4-2)!} = 12\\) Subsets. The number of unordered arrangements (combinations) of a set of \\(|S|=n\\) items taken \\(k\\) at a time without replacement has \\[_nC_k = {n \\choose k} = \\frac{n!}{k!(n-k)!}\\] combinations and is called the binomial coefficient. The binomial coefficient is the number of different subsets. Notice that if k=0 then there is 1 subset; if k=1 then there are n subsets; if k=n then there is 1 subset. The connection with the permutation rule is that there are \\(n!/(n-k)!\\) permutations and each permutation has \\(k!\\) permutations. How many subsets of 7 people can be taken from a set of 12 persons? \\(_{12}C_7 = {12 \\choose 7} = \\frac{12!}{7!(12-7)!} = 792\\) If you are dealt five cards, what is the probability of getting a “full-house” hand containing three kings and two aces (KKKAA)? \\[P(F) = \\frac{{4 \\choose 3} {4 \\choose 2}}{{52 \\choose 5}}\\] Distinguishable permutations. The number of unordered arrangements (distinguishable permutations) of a set of \\(|S|=n\\) items in which \\(n_1\\) are of one type, \\(n_2\\) are of another type, etc., is \\[{n \\choose {n_1, n_2, \\dots, n_k}} = \\frac{n!}{n_{1}! n_{2}! \\dots n_{k}!}\\] How many ordered arrangements are there of the letters in the word PHILIPPINES? There are n=11 objects. \\(|P|=n_1=3\\); \\(|H|=n_2=1\\); \\(|I|=n_3=3\\); \\(|L|=n_4=1\\); \\(|N|=n_5=1\\); \\(|E|=n_6=1\\); \\(|S|=n_7=1\\). \\[{n \\choose {n_1, n_2, \\dots, n_k}} = \\frac{11!}{3! 1! 3! 1! 1! 1! 1!} = 1,108,800\\] How many ways can a research pool of 15 subjects be divided into three equally sized test groups? \\[{n \\choose {n_1, n_2, \\dots, n_k}} = \\frac{15!}{5! 5! 5!} = 756,756\\] "],
["discrete-distributions.html", "1.2 Discrete Distributions", " 1.2 Discrete Distributions These notes rely heavily on PSU STATS 504 course notes. The most important discrete distributions are the Binomial, Poisson, and Multinomial. Sometimes useful are the related Bernoulli, negative binomial, geometric, and hypergeometric distributions. A discrete random variable \\(X\\) is described by its probability mass function \\(f(x) = P(X = x)\\). The set of \\(x\\) values for which \\(f(x) &gt; 0\\) is called the support. If the distribution depends on unknown parameter(s) \\(\\theta\\) we write it as \\(f(x; \\theta)\\) (frequentist) or \\(f(x | \\theta)\\) (Bayesian). 1.2.1 Bernoulli If \\(X\\) is the result of a trial with two outcomes of probability \\(P(X = 1) = \\pi\\) and \\(P(X = 0) = 1 - \\pi\\), then \\(X\\) is a random variable with a Bernoulli distribution \\[f(x) = \\pi^x (1 - \\pi)^{1 - x}, \\hspace{1cm} x \\in (0, 1)\\] with \\(E(X) = \\pi\\) and \\(V(X) = \\pi(1 - \\pi)\\). 1.2.2 Binomial If \\(X\\) is the count of successful events in \\(n\\) identical and independent Bernoulli trials of success probability \\(\\pi\\), then \\(X\\) is a random variable with a binomial distribution \\(X \\sim Bin(n,\\pi)\\) \\[f(x;\\pi) = \\frac{n!}{x!(n-x)!} \\pi^x (1-\\pi)^{n-x} \\hspace{1cm} x \\in (0, 1, ..., n), \\hspace{2mm} \\pi \\in [0, 1]\\] with \\(E(X)=n\\pi\\) and \\(V(X) = n\\pi(1-\\pi)\\). The Bernoulli distribution is a special case of the binomial with \\(n = 1\\). As n increases for fixed \\(\\pi\\), the binomial distribution approaches normal distribution \\(N(n\\pi, n\\pi(1−\\pi))\\). The binomial distribution assumes independent trials - if sampling without replacement from a finite population, then the hypergeometric distribution is appropriate. Example What is the probability 2 out of 10 coin flips are heads if the probability of heads is 0.3? Function dbinom() calculates the binomial probability. dbinom(x = 2, size = 10, prob = 0.3) ## [1] 0.2334744 A simulation of n = 10,000 random samples of size 10 gives a similar result. rbinom() generates a random sample of numbers from the binomial distribution. data.frame(cnt = rbinom(n = 10000, size = 10, prob = 0.3)) %&gt;% count(cnt) %&gt;% ungroup() %&gt;% mutate(pct = n / sum(n), X_eq_x = cnt == 2) %&gt;% ggplot(aes(x = as.factor(cnt), y = n, fill = X_eq_x, label = pct)) + geom_col(alpha = 0.8) + scale_fill_mf() + geom_label(aes(label = round(pct, 2)), size = 3, alpha = .6) + theme_mf() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Binomial Distribution&quot;, subtitle = paste0( &quot;P(X=2) successes in 10 trials when p = 0.3 is &quot;, round(dbinom(2, 10, 0.3), 4), &quot;.&quot; ), x = &quot;Successes&quot;, y = &quot;Count&quot;, caption = &quot;Simulation from n = 10,000 binomial random samples.&quot;) Example What is the probability of &lt;=2 heads in 10 coin flips where probability of heads is 0.3? The cumulative probability is the sum of the first three bars in the simulation above. Function pbinom() calculates the cumulative binomial probability. pbinom(q = 2, size = 10, prob = 0.3, lower.tail = TRUE) ## [1] 0.3827828 Example What is the expected number of heads in 25 coin flips if the probability of heads is 0.3? The expected value, \\(\\mu = np\\), is 7.5. Here’s an empirical test from 10,000 samples. mean(rbinom(n = 10000, size = 25, prob = .3)) ## [1] 7.526 The variance, \\(\\sigma^2 = np (1 - p)\\), is 5.25. Here’s an empirical test. var(rbinom(n = 10000, size = 25, prob = .3)) ## [1] 5.217501 Example Suppose X and Y are independent random variables distributed \\(X \\sim Bin(10, .6)\\) and \\(Y \\sim Bin(10, .7)\\). What is the probability that either variable is &lt;=4? Let \\(P(A) = P(X&lt;=4)\\) and \\(P(B) = P(Y&lt;=4)\\). Then \\(P(A|B) = P(A) + P(B) - P(AB)\\), and because the events are independent, \\(P(AB) = P(A)P(B)\\). p_a &lt;- pbinom(q = 4, size = 10, prob = 0.6, lower.tail = TRUE) p_b &lt;- pbinom(q = 4, size = 10, prob = 0.7, lower.tail = TRUE) p_a + p_b - (p_a * p_b) ## [1] 0.2057164 Here’s an empirical test. df &lt;- data.frame( x = rbinom(10000, 10, 0.6), y = rbinom(10000, 10, 0.7) ) mean(if_else(df$x &lt;= 4 | df$y &lt;= 4, 1, 0)) ## [1] 0.1972 1.2.3 Poission If \\(X\\) is the number of successes in \\(n\\) (many) trials when the probability of success \\(\\lambda / n\\) is small, then \\(X\\) is a random variable with a Poisson distribution \\(X \\sim Poisson(\\lambda)\\) \\[f(x;\\lambda) = \\frac{e^{-\\lambda} \\lambda^x}{x!} \\hspace{1cm} x \\in (0, 1, ...), \\hspace{2mm} \\lambda &gt; 0\\] with \\(E(X)=\\lambda\\) and \\(V(X) = \\lambda\\). \\(Poison(\\lambda) \\rightarrow Bin(n, \\pi)\\) when \\(n\\pi = \\lambda\\) and \\(n \\rightarrow \\infty\\) and \\(\\pi \\rightarrow 0\\). Because the Poisson is limit of the \\(Bin(n, \\pi)\\), it is useful as an approximation to the binomial when \\(n\\) is large (\\(n&gt;=20\\)) and \\(\\pi\\) small (\\(p&lt;=0.05\\)). When the observed variance is greater than \\(\\lambda\\) (overdispersion), the Negative Binomial distribution can be used instead of Poisson. Example What is the probability of making 2 to 4 sales in a week if the average sales rate is 3 per week? Function dpois() calculates the binomial probability. # Using cumulative probability ppois(q = 4, lambda = 3, lower.tail = TRUE) - ppois(q = 1, lambda = 3, lower.tail = TRUE) ## [1] 0.616115 # Using exact probability dpois(x = 2, lambda = 3) + dpois(x = 3, lambda = 3) + dpois(x = 4, lambda = 4) ## [1] 0.6434504 Example Suppose a baseball player has a p=.300 batting average. What is the probability of X&lt;=150 hits in n=500 at bats? X=150? X&gt;150? ppois(q = 150, lambda = .300 * 500, lower.tail = TRUE) ## [1] 0.52 dpois(x = 150, lambda = .300 * 500) ## [1] 0.033 ppois(q = 150, lambda = .300 * 500, lower.tail = FALSE) ## [1] 0.48 The Poisson distribution approximates the binomial distribution with \\(\\lambda=np\\) if \\(n&gt;=20\\) and \\(p&lt;=0.05\\). Example What is the distribution of successes from a sample of n = 50 when the probability of success is p = .03? options(scipen = 999, digits = 2) # sig digits n = 0:10 df &lt;- data.frame(events = 0:10, Poisson = dpois(x = n, lambda = .03 * 50), Binomial = dbinom(x = n, size = 50, p = .03)) df_tidy &lt;- gather(df, key = &quot;Distribution&quot;, value = &quot;density&quot;, -c(events)) ggplot(df_tidy, aes(x = factor(events), y = density, fill = Distribution)) + geom_col(position = &quot;dodge&quot;) + theme_mf() + scale_fill_mf() + labs(title = &quot;Poisson(15) and Binomial(50, .03)&quot;, subtitle = &quot;Poisson approximates binomial when n &gt;= 20 and p &lt;= .05.&quot;, x = &quot;Events (x)&quot;, y = &quot;Density&quot;, fill = &quot;&quot;) Example Suppose the probability that a drug produces a certain side effect is p = = 0.1% and n = 1,000 patients in a clinical trial receive the drug. What is the probability 0 people experience the side effect? The expected value is np, 1. The probability of measuring 0 when the expected value is 1 is dpois(x = 0, lambda = 1000 * .001) = 0.37. 1.2.4 Multinomial If \\(X = (X_1, X_2, \\cdots, X_k)\\) are the counts of successful events in \\(n\\) identical and independent trials of success probabilities \\(\\pi = (\\pi_1, \\pi_2, \\cdots, \\pi_k)\\), then \\(X\\) is a random variable with a multinomial distribution \\(X \\sim Mult(n,\\pi)\\) \\[f(x;\\pi) = \\frac{n!}{x_{1}! x_{2}! \\cdots x_{k}!} \\pi^{x_1} \\pi^{x_2} \\cdots \\pi^{x_k} \\hspace{1cm} x \\in \\{0, 1, ..., n \\}, \\hspace{2mm} \\pi \\in [0, 1]\\] with \\(E(X)=n\\pi = (n \\pi_1 + n \\pi_2 + \\cdots + n \\pi_k)\\) and covariance matrix \\[V(X) = \\begin{bmatrix}n\\pi_{1}(1-\\pi_{1}) &amp; -n\\pi_{1}\\pi_{2} &amp; \\cdots &amp; -n\\pi_{1}\\pi_{k}\\\\ -n\\pi_{1}\\pi_{2} &amp; n\\pi_{2}(1-\\pi_{2}) &amp; \\cdots &amp; -n\\pi_{2}\\pi_{k}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ -n\\pi_{1}\\pi_{k} &amp; -n\\pi_{2}\\pi_{k} &amp; \\cdots &amp; n\\pi_{k}(1-\\pi_{k}) \\end{bmatrix}\\] The individual components of a multinomial random vector are binomial and have a binomial distribution, \\(X_i = Bin(n, \\pi_i)\\). Example Suppose a city population is 20% black, 15% Hispanic, and 65% other. From a random sample of \\(n = 12\\) persons, what is the probability of 4 black and 8 other? \\[f(x;\\pi) = \\frac{12!}{4! 0! 8!} (0.20)^4 (0.15)^0 (0.65)^8 = 0.0252\\] Function dmultinom() calculates the multinomial probability. dmultinom(x = c(4, 0, 8), prob = c(0.20, 0.15, 0.65)) ## [1] 0.025 To calculate the probability of &lt;= 1 black, combine Hispanic and other, then sum the probability of black = 1 and black = 2. \\[f(x;\\pi) = \\frac{12!}{0! 12!} (0.20)^0 (0.80)^{12} + \\frac{12!}{1! 11!} (0.20)^1 (0.80)^{11} = 0.2748\\] dmultinom(x = c(0, 12), prob = c(0.20, 0.80)) + dmultinom(x = c(1, 11), prob = c(0.20, 0.80)) ## [1] 0.27 1.2.5 Negative-Binomial If \\(X\\) is the count of failure events ocurring prior to reaching \\(r\\) successful events in a sequence of Bernouli trias of success probability \\(p\\), then \\(X\\) is a random variable with a negative-binomial distribution \\(X \\sim NB(r, p)\\). The probability of \\(X = x\\) failures prior to \\(r\\) successes is \\[f(x;r, p) = {{x + r - 1} \\choose {r - 1}} p^r (1-p)^{x}.\\] with \\(E(X) = r (1 - p) / p\\) and \\(V(X) = r (1-p) / p^2\\). When the data has overdispersion, model the data with the negative-binomial distribution instead of Poission. Examples An oil company has a \\(p = 0.20\\) chance of striking oil when drilling a well. What is the probability the company drills \\(x + r = 7\\) wells to strike oil \\(r = 3\\) times? Note that the question is formulated as counting total events, \\(x + r = 7\\), so translate it to total failed events, \\(x = 4\\). \\[f(x;r, p) = {{4 + 3 - 1} \\choose {3 - 1}} (0.20)^3 (1 - 0.20)^4 = 0.049.\\] Function dnbinom() calculates the negative-binomial probability. Parameter x equals the number of failures, \\(x - r\\). dnbinom(x = 4, size = 3, prob = 0.2) ## [1] 0.049 The expected number of failures prior to 3 successes is \\(E(X) = 3 (1 - 0.20) / 0.20 = 12\\) with variance \\(V(X) = 3 (1 - 0.20) / 0.20^2 = 60\\). Confirm this with a simulation from n = 10,000 random samples using rnbinom(). my_dat &lt;- rnbinom(n = 10000, size = 3, prob = 0.20) mean(my_dat) ## [1] 12 var(my_dat) ## [1] 59 1.2.6 Geometric If \\(X\\) is the count of independent Bernoulli trials of success probability \\(p\\) required to achieve the first successful trial, then \\(X\\) is a random variable with a geometric distribution \\(X \\sim G(p)\\). The probability of \\(X=x\\) trials is \\[f(x; p) = p(1-p)^{x-1}.\\] with \\(E(X)=\\frac{{n}}{{p}}\\) and \\(V(X) = \\frac{(1-p)}{p^2}\\). The probability of \\(X&lt;=n\\) trials is \\[F(X=n) = 1 - (1-p)^n.\\] Example What is the probability a marketer encounters x = 3 people on the street who did not attend a sporting event before the first success if the population probability is p = 0.20? \\[f(4; 0.20) = 0.20(1-0.20)^{4-1} = 0.102.\\] Function pgeom() calculates the geometric distribution probability. Parameter x is the number of failures, not the number of trials. dgeom(x = 3, prob = 0.20) ## [1] 0.1 data.frame(cnt = rgeom(n = 10000, prob = 0.20)) %&gt;% count(cnt) %&gt;% top_n(n = 15, wt = n) %&gt;% ungroup() %&gt;% mutate(pct = round(n / sum(n), 3), X_eq_x = cnt == 3) %&gt;% ggplot(aes(x = as.factor(cnt), y = n, fill = X_eq_x, label = pct)) + geom_col(alpha = 0.8) + scale_fill_mf() + geom_text(size = 3) + theme_mf() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Distribution of trials prior to first success&quot;, subtitle = paste(&quot;P(X = 3) | X ~ G(.2) = &quot;, round(dgeom(2, .2), 3)), x = &quot;Unsuccessful trials&quot;, y = &quot;Count&quot;, caption = &quot;simulation of n = 10,000 samples from geometric dist.&quot;) What is the probability the marketer fails to find someone who attended a game in x &lt;= 5 trials before finding someone who attended a game on the sixth trial when the population probability is p = 0.20? p = 0.20 n = 5 # exact pgeom(q = n, prob = p, lower.tail = TRUE) ## [1] 0.74 # simulated mean(rgeom(n = 10000, prob = p) &lt;= n) ## [1] 0.74 What is the probability the marketer fails to find someone who attended a game on x &gt;= 5 trials before finding someone who attended a game on the next trial? p = 0.20 n = 5 # exact pgeom(q = n, prob = p, lower.tail = FALSE) ## [1] 0.26 # simulated mean(rgeom(n = 10000, prob = p) &gt; n) ## [1] 0.27 The expected number of trials to achieve the first success is 1 / 0.20 = 5, V(X) = (1 - 0.20) / 0.20^2 = 20? p = 0.20 # mean # exact 1 / p ## [1] 5 # simulated mean(rgeom(n = 10000, prob = p)) + 1 ## [1] 5 # Variance # exact (1 - p) / p^2 ## [1] 20 # simulated var(rgeom(n = 100000, prob = p)) ## [1] 20 1.2.7 Hypergeometric If \\(X\\) is the count of successful events in a sample sof size \\(k\\) without replacement from a population containing \\(M\\) successes and \\(N\\) non-successes, then \\(X\\) is a random variable with a hypergeometric distribution \\[f(x|m,n,k) = \\frac{{{m}\\choose{x}}{{n}\\choose{k-x}}}{{m+n}\\choose{k}}.\\] with \\(E(X)=k\\frac{m}{m+n}\\) and \\(V(X) = k\\frac{m}{m+n}\\cdot\\frac{m+n-k}{m+n}\\cdot\\frac{n}{m+n-1}\\). phyper returns the cumulative probability (percentile) p at the specified value (quantile) q. qhyper returns the value (quantile) q at the specified cumulative probability (percentile) p. Example What is the probability of selecting \\(X = 14\\) red marbles from a sample of \\(k = 20\\) taken from an urn containing \\(m = 70\\) red marbles and \\(n = 30\\) green marbles? Function dhyper() calculates the hypergeometric probability. x = 14 m = 70 n = 30 k = 20 dhyper(x = x, m = m, n = n, k = k) ## [1] 0.21 The expected value is 14 and variance is 3.39. The hypergeometric random variable is similar to the binomial random variable except that it applies to situations of sampling without replacement from a small population. As the population size increases, sampling without replacement converges to sampling with replacement, and the hypergeometric distribution converges to the binomial. What if the total population size is 250? 500? 1000? 1.2.8 Gamma "],
["continuous-distributions.html", "1.3 Continuous Distributions", " 1.3 Continuous Distributions 1.3.1 Normal Random variable \\(X\\) is distributed \\(X \\sim N(\\mu, \\sigma^2)\\) if \\[f(X)=\\frac{{1}}{{\\sigma \\sqrt{{2\\pi}}}}e^{-.5(\\frac{{x-\\mu}}{{\\sigma}})^2}\\]. Example IQ scores are distributed \\(X \\sim N(100, 16^2\\). What is the probability a randomly selected person’s IQ is &lt;90? my_mean = 100 my_sd = 16 my_x = 90 # exact pnorm(q = my_x, mean = my_mean, sd = my_sd, lower.tail = TRUE) ## [1] 0.27 # simulated mean(rnorm(n = 10000, mean = my_mean, sd = my_sd) &lt;= my_x) ## [1] 0.26 library(dplyr) library(ggplot2) data.frame(x = 0:1500 / 10, prob = pnorm(q = 0:1500 / 10, mean = my_mean, sd = my_sd, lower.tail = TRUE)) %&gt;% mutate(cdf = ifelse(x &gt; 0 &amp; x &lt;= my_x, prob, 0)) %&gt;% ggplot() + geom_line(aes(x = x, y = prob)) + geom_area(aes(x = x, y = cdf), alpha = 0.3) + labs(title = bquote(&#39;X~N(&#39;~mu==.(my_mean)~&#39;,&#39;~sigma^{2}==.(my_sd)^{2}~&#39;)&#39;), subtitle = bquote(&#39;P(X&lt;=&#39;~.(my_x)~&#39;) when mean is&#39;~.(my_mean)~&#39; and variance is&#39;~.(my_sd)^{2}~&#39;.&#39;), x = &quot;x&quot;, y = &quot;Probability&quot;) 1.3.2 Example IQ scores are distributed \\(X \\sim N(100, 16^2\\). What is the probability a randomly selected person’s IQ is &gt;140? my_mean = 100 my_sd = 16 my_x = 140 # exact pnorm(q = my_x, mean = my_mean, sd = my_sd, lower.tail = FALSE) ## [1] 0.0062 # simulated mean(rnorm(n = 10000, mean = my_mean, sd = my_sd) &gt; my_x) ## [1] 0.0067 library(dplyr) library(ggplot2) data.frame(x = 0:1500 / 10, prob = pnorm(q = 0:1500 / 10, mean = my_mean, sd = my_sd, lower.tail = TRUE)) %&gt;% mutate(cdf = ifelse(x &gt; my_x &amp; x &lt; 1000, prob, 0)) %&gt;% ggplot() + geom_line(aes(x = x, y = prob)) + geom_area(aes(x = x, y = cdf), alpha = 0.3) + labs(title = bquote(&#39;X~N(&#39;~mu==.(my_mean)~&#39;,&#39;~sigma^{2}==.(my_sd)^{2}~&#39;)&#39;), subtitle = bquote(&#39;P(X&lt;=&#39;~.(my_x)~&#39;) when mean is&#39;~.(my_mean)~&#39; and variance is&#39;~.(my_sd)^{2}~&#39;.&#39;), x = &quot;x&quot;, y = &quot;Probability&quot;) 1.3.3 Example IQ scores are distributed \\(X \\sim N(100, 16^2\\). What is the probability a randomly selected person’s IQ is between 92 and 114? my_mean = 100 my_sd = 16 my_x_l = 92 my_x_h = 114 # exact pnorm(q = my_x_h, mean = my_mean, sd = my_sd, lower.tail = TRUE) - pnorm(q = my_x_l, mean = my_mean, sd = my_sd, lower.tail = TRUE) ## [1] 0.5 library(dplyr) library(ggplot2) data.frame(x = 0:1500 / 10, prob = pnorm(q = 0:1500 / 10, mean = my_mean, sd = my_sd, lower.tail = TRUE)) %&gt;% mutate(cdf = ifelse(x &gt; my_x_l &amp; x &lt;= my_x_h, prob, 0)) %&gt;% ggplot() + geom_line(aes(x = x, y = prob)) + geom_area(aes(x = x, y = cdf), alpha = 0.3) + labs(title = bquote(&#39;X~N(&#39;~mu==.(my_mean)~&#39;,&#39;~sigma^{2}==.(my_sd)^{2}~&#39;)&#39;), subtitle = bquote(&#39;P(X&lt;=&#39;~.(my_x)~&#39;) when mean is&#39;~.(my_mean)~&#39; and variance is&#39;~.(my_sd)^{2}~&#39;.&#39;), x = &quot;x&quot;, y = &quot;Probability&quot;) 1.3.4 Example Class scores are distributed \\(X \\sim N(70, 10^2\\). If the instructor wants to give A’s to &gt;=85th percentile and B’s to 75th-85th percentile, what are the cutoffs? my_mean = 70 my_sd = 10 my_pct_l = .75 my_pct_h = .85 qnorm(p = my_pct_l, mean = my_mean, sd = my_sd, lower.tail = TRUE) ## [1] 77 qnorm(p = my_pct_h, mean = my_mean, sd = my_sd, lower.tail = TRUE) ## [1] 80 library(dplyr) library(ggplot2) data.frame(x = 0:1000 / 10, prob = pnorm(q = 0:1000 / 10, mean = my_mean, sd = my_sd, lower.tail = TRUE)) %&gt;% mutate(cdf = ifelse(prob &gt; my_pct_l &amp; prob &lt;= my_pct_h, prob, 0)) %&gt;% ggplot() + geom_line(aes(x = x, y = prob)) + geom_area(aes(x = x, y = cdf), alpha = 0.3) + labs(title = bquote(&#39;X~N(&#39;~mu==.(my_mean)~&#39;,&#39;~sigma^{2}==.(my_sd)^{2}~&#39;)&#39;), subtitle = bquote(&#39;P(X&lt;=x) = [&#39;~.(my_pct_l)~&#39;,&#39;~.(my_pct_h)~&#39;] when mean is&#39;~.(my_mean)~&#39; and variance is&#39;~.(my_sd)^{2}~&#39;.&#39;), x = &quot;x&quot;, y = &quot;Probability&quot;) 1.3.5 Normal Approximation to Binomial The CLT implies that certain distributions can be approximated by the normal distribution. The binomial distribution \\(X \\sim B(n,p)\\) is approximately normal with mean \\(\\mu = n p\\) and variance \\(\\sigma^2=np(1-p)\\). The approximation is useful when the expected number of successes and failures is at least 5: \\(np&gt;=5\\) and \\(n(1-p)&gt;=5\\). 1.3.6 Example A measure requires p&gt;=50% popular to pass. A sample of n=1,000 yields x=460 approvals. What is the probability that the overall population approves, P(X)&gt;0.5? my_x = 460 my_p = 0.50 my_n = 1000 my_mean = my_p * my_n my_sd = round(sqrt(my_n * my_p * (1 - my_p)), 1) # Exact binomial pbinom(q = my_x, size = my_n, prob = my_p, lower.tail = TRUE) ## [1] 0.0062 # Normal approximation pnorm(q = my_x, mean = my_p * my_n, sd = sqrt(my_n * my_p * (1 - my_p)), lower.tail = TRUE) ## [1] 0.0057 library(dplyr) library(ggplot2) library(tidyr) data.frame(x = 400:600, Normal = pnorm(q = 400:600, mean = my_p * my_n, sd = sqrt(my_n * my_p * (1 - my_p)), lower.tail = TRUE), Binomial = pbinom(q = 400:600, size = my_n, prob = my_p, lower.tail = TRUE)) %&gt;% gather(key = &quot;Distribution&quot;, value = &quot;cdf&quot;, c(-x)) %&gt;% ggplot(aes(x = x, y = cdf, color = Distribution)) + geom_line() + labs(title = bquote(&#39;X~B(n=&#39;~.(my_n)~&#39;, p=&#39;~.(my_p)~&#39;), &#39;~&#39;X~N(&#39;~mu==.(my_mean)~&#39;,&#39;~sigma^{2}==.(my_sd)^{2}~&#39;)&#39;), subtitle = &quot;Normal approximation to the binomial&quot;, x = &quot;x&quot;, y = &quot;Probability&quot;) The Poisson distribution \\(x~P(\\lambda)\\) is approximately normal with mean \\(\\mu = \\lambda\\) and variance \\(\\sigma^2 = \\lambda\\), for large values of \\(\\lambda\\). 1.3.7 Example The annual number of earthquakes registering at least 2.5 on the Richter Scale and having an epicenter within 40 miles of downtown Memphis follows a Poisson distribution with mean \\(\\lambda=6.5\\). What is the probability that at least \\(x&gt;=9\\)* such earthquakes will strike next year?* my_x = 9 my_lambda = 6.5 my_sd = round(sqrt(my_lambda), 2) # Exact Poisson ppois(q = my_x - 1, lambda = my_lambda, lower.tail = FALSE) ## [1] 0.21 # Normal approximation pnorm(q = my_x - 0.5, mean = my_lambda, sd = my_sd, lower.tail = FALSE) ## [1] 0.22 library(dplyr) library(ggplot2) library(tidyr) data.frame(x = 0:200 / 10, Normal = pnorm(q = 0:200 / 10, mean = my_lambda, sd = my_sd, lower.tail = TRUE), Poisson = ppois(q = 0:200 / 10, lambda = my_lambda, lower.tail = TRUE)) %&gt;% gather(key = &quot;Distribution&quot;, value = &quot;cdf&quot;, c(-x)) %&gt;% ggplot(aes(x = x, y = cdf, color = Distribution)) + geom_line() + labs(title = bquote(&#39;X~P(&#39;~lambda~&#39;=&#39;~.(my_lambda)~&#39;), &#39;~&#39;X~N(&#39;~mu==.(my_lambda)~&#39;,&#39;~sigma^{2}==.(my_lambda)~&#39;)&#39;), subtitle = &quot;Normal approximation to the Poisson&quot;, x = &quot;x&quot;, y = &quot;Probability&quot;) 1.3.8 From Sample to Population Suppose a person’s blood pressure typically measures 160?20 mm. If one takes n=5 blood pressure readings, what is the probability the average will be &lt;=150? my_mu = 160 my_sigma = 20 my_n = 5 my_x = 150 my_se = round(my_sigma / sqrt(my_n), 1) pnorm(q = my_x, mean = my_mu, sd = my_sigma / sqrt(my_n), lower.tail = TRUE) ## [1] 0.13 library(dplyr) library(ggplot2) data.frame(x = 1000:2000 / 10, prob = pnorm(q = 1000:2000 / 10, mean = my_mu, sd = my_sigma / sqrt(my_n), lower.tail = TRUE)) %&gt;% mutate(cdf = ifelse(x &gt; 0 &amp; x &lt;= my_x, prob, 0)) %&gt;% ggplot() + geom_line(aes(x = x, y = prob)) + geom_area(aes(x = x, y = cdf), alpha = 0.3) + labs(title = bquote(&#39;X~N(&#39;~mu==.(my_mu)~&#39;,&#39;~sigma^{2}==.(my_se)^{2}~&#39;)&#39;), subtitle = bquote(&#39;P(X&lt;=&#39;~.(my_x)~&#39;) when mean is&#39;~.(my_mu)~&#39; and variance is&#39;~sigma~&#39;/sqrt(n)&#39;~.(my_se)^{2}~&#39;.&#39;), x = &quot;x&quot;, y = &quot;Probability&quot;) knitr::include_app(&quot;https://mpfoley73.shinyapps.io/shiny_dist/&quot;, height = &quot;600px&quot;) "],
["join-distributions.html", "1.4 Join Distributions", " 1.4 Join Distributions "],
["likelihood.html", "1.5 Likelihood", " 1.5 Likelihood The likelihood function is the likelihood of a parameter \\(\\theta\\) given an observed value of the random variable \\(X\\). The likelihood function is identical to the probability distribution function, except that it reverses which variable is considered fixed. E.g., the binomial probability distribution expresses the probability that \\(X = x\\) given the success probability \\(\\theta = \\pi\\). \\[f(x|\\pi) = \\frac{n!}{x!(n-x)!} \\pi^x (1-\\pi)^{n-x}.\\] The corresponding likelihood function expresses the probability that \\(\\pi = p\\) given the observed value \\(x\\). \\[L(p|x) = \\frac{n!}{x!(n-x)!} p^x (1-p)^{n-x}.\\] You usually want to know the value of \\(\\theta\\) at the maximum of the likelihood function. When taking derivatives, any multiplicative constant is irrevelant and can be discarded. So for the binomial distribution, the likelihood function for \\(\\pi\\) may instead be expressed as \\[L(p|x) \\propto p^x (1-p)^{n-x}\\] Calculating the maximum is usually simplified using the log-likelihood, \\(l(\\theta|x) = \\log L(\\theta|x)\\). For the binomial distribution, \\(l(p|x) = x \\log p + (n - x) \\log (1 - p)\\). Frequently you derive loglikelihood from a sample. The overall likelihood is the product of the individual likelihoods, and the overall loglikelihood is the log of the overall likelihood. \\[l(\\theta|x) = \\log \\prod_{i=1}^n f(x_i|\\theta)\\] Here are plots of the binomial log-likelihood of \\(pi\\) for several values of \\(X\\) from a sample of size \\(n = 5\\). As the total sample size \\(n\\) grows, the loglikelihood function becomes more sharply peaked around its maximum, and becomes nearly quadratic (i.e. a parabola, if there is a single parameter). Here is the same plot with \\(n = 500\\). The value of \\(\\theta\\) that maximizes \\(l\\) (and \\(L\\)) is the maximum-likelihood estimator (MLE) of \\(\\theta\\), \\(\\hat{\\theta}\\). E.g., suppose you have an experiment of \\(n = 5\\) Bernoulli trials \\(\\left(X \\sim Bin(5, \\pi) \\right)\\) with and \\(X = 3\\) successful events. A plot of \\(L(p|x) = p^3(1 - p)^2\\) shows the MLE is at \\(p = 0.6\\). This approach is called maximum-likelihood estimation. MLE usually involves setting the derivatives to zero and solving for \\(theta\\). "]
]
