[
["index.html", "My Data Science Notes Intro", " My Data Science Notes Michael Foley 2020-07-26 Intro These notes are pulled from various classes, tutorials, books, etc. and are intended for my own consumption. If you are finding this on the internet, I hope it is useful to you, but you should know that I am just a student and there’s a good chance whatever you’re reading here is mistaken. "],
["probability.html", "Chapter 1 Probability ", " Chapter 1 Probability "],
["principles.html", "1.1 Principles", " 1.1 Principles Here are three rules that come up all the time. \\(Pr(A \\cup B) = Pr(A)+Pr(B) - Pr(AB)\\). This rule generalizes to \\(Pr(A \\cup B \\cup C)=Pr(A)+Pr(B)+Pr(C)-Pr(AB)-Pr(AC)-Pr(BC)+Pr(ABC)\\). \\(Pr(A|B) = \\frac{P(AB)}{P(B)}\\) If A and B are independent, \\(Pr(A \\cap B) = Pr(A)Pr(B)\\), and \\(Pr(A|B)=Pr(A)\\). Uniform distributions on finite sample spaces often reduce to counting the elements of A and the sample space S, a process called combinatorics. Here are three important combinatorial rules. Multiplication Rule. \\(|S|=|S_1 |⋯|S_k|\\). How many outcomes are possible from a sequence of 4 coin flips and 2 rolls of a die? \\(|S|=|S_1| \\cdot |S_2| \\dots |S_6| = 2 \\cdot 2 \\cdot 2 \\cdot 2 \\cdot 6 \\cdot 6 = 288\\). How many subsets are possible from a set of n=10 elements? In each subset, each element is either included or not, so there are \\(2^n = 1024\\) subsets. How many subsets are possible from a set of n=10 elements taken k at a time with replacement? Each experiment has \\(n\\) possible outcomes and is repeated \\(k\\) times, so there are \\(n^k\\) subsets. Permutations. The number of ordered arrangements (permutations) of a set of \\(|S|=n\\) items taken \\(k\\) at a time without replacement has \\(n(n-1) \\dots (n-k+1)\\) subsets because each draw is one of k experiments with decreasing number of possible outcomes. \\[_nP_k = \\frac{n!}{(n-k)!}\\] Notice that if \\(k=0\\) then there is 1 permutation; if \\(k=1\\) then there are \\(n\\) permutations; if \\(k=n\\) then there are \\(n!\\) permutations. How many ways can you distribute 4 jackets among 4 people? \\(_nP_k = \\frac{4!}{(4-4)!} = 4! = 24\\) How many ways can you distribute 4 jackets among 2 people? \\(_nP_k = \\frac{4!}{(4-2)!} = 12\\) Subsets. The number of unordered arrangements (combinations) of a set of \\(|S|=n\\) items taken \\(k\\) at a time without replacement has \\[_nC_k = {n \\choose k} = \\frac{n!}{k!(n-k)!}\\] combinations and is called the binomial coefficient. The binomial coefficient is the number of different subsets. Notice that if k=0 then there is 1 subset; if k=1 then there are n subsets; if k=n then there is 1 subset. The connection with the permutation rule is that there are \\(n!/(n-k)!\\) permutations and each permutation has \\(k!\\) permutations. How many subsets of 7 people can be taken from a set of 12 persons? \\(_{12}C_7 = {12 \\choose 7} = \\frac{12!}{7!(12-7)!} = 792\\) If you are dealt five cards, what is the probability of getting a “full-house” hand containing three kings and two aces (KKKAA)? \\[P(F) = \\frac{{4 \\choose 3} {4 \\choose 2}}{{52 \\choose 5}}\\] Distinguishable permutations. The number of unordered arrangements (distinguishable permutations) of a set of \\(|S|=n\\) items in which \\(n_1\\) are of one type, \\(n_2\\) are of another type, etc., is \\[{n \\choose {n_1, n_2, \\dots, n_k}} = \\frac{n!}{n_{1}! n_{2}! \\dots n_{k}!}\\] How many ordered arrangements are there of the letters in the word PHILIPPINES? There are n=11 objects. \\(|P|=n_1=3\\); \\(|H|=n_2=1\\); \\(|I|=n_3=3\\); \\(|L|=n_4=1\\); \\(|N|=n_5=1\\); \\(|E|=n_6=1\\); \\(|S|=n_7=1\\). \\[{n \\choose {n_1, n_2, \\dots, n_k}} = \\frac{11!}{3! 1! 3! 1! 1! 1! 1!} = 1,108,800\\] How many ways can a research pool of 15 subjects be divided into three equally sized test groups? \\[{n \\choose {n_1, n_2, \\dots, n_k}} = \\frac{15!}{5! 5! 5!} = 756,756\\] "],
["disc-dist.html", "1.2 Discrete Distributions", " 1.2 Discrete Distributions These notes rely heavily on PSU STATS 504 course notes. The most important discrete distributions are the Binomial, Poisson, and Multinomial. Sometimes useful are the related Bernoulli, negative binomial, geometric, and hypergeometric distributions. A discrete random variable \\(X\\) is described by its probability mass function \\(f(x) = P(X = x)\\). The set of \\(x\\) values for which \\(f(x) &gt; 0\\) is called the support. If the distribution depends on unknown parameter(s) \\(\\theta\\) we write it as \\(f(x; \\theta)\\) (frequentist) or \\(f(x | \\theta)\\) (Bayesian). 1.2.1 Bernoulli If \\(X\\) is the result of a trial with two outcomes of probability \\(P(X = 1) = \\pi\\) and \\(P(X = 0) = 1 - \\pi\\), then \\(X\\) is a random variable with a Bernoulli distribution \\[f(x) = \\pi^x (1 - \\pi)^{1 - x}, \\hspace{1cm} x \\in (0, 1)\\] with \\(E(X) = \\pi\\) and \\(Var(X) = \\pi(1 - \\pi)\\). 1.2.2 Binomial If \\(X\\) is the count of successful events in \\(n\\) identical and independent Bernoulli trials of success probability \\(\\pi\\), then \\(X\\) is a random variable with a binomial distribution \\(X \\sim Bin(n,\\pi)\\) \\[f(x;n, \\pi) = \\frac{n!}{x!(n-x)!} \\pi^x (1-\\pi)^{n-x} \\hspace{1cm} x \\in (0, 1, ..., n), \\hspace{2mm} \\pi \\in [0, 1]\\] with \\(E(X)=n\\pi\\) and \\(Var(X) = n\\pi(1-\\pi)\\). Binomial sampling is used to model counts of one level of a categorical variable over a fixed sample size. Here is a simple analysis of data from a Binomial process. Data set dat contains frequencies of high-risk drinkers vs non-high-risk drinkers in a college survey. ## ## No Yes ## 685 630 The MLE of \\(\\pi\\) from the Binomial distribution is the sample mean. x &lt;- sum(dat$high_risk == &quot;Yes&quot;) n &lt;- nrow(dat) p &lt;- x / n print(p) ## [1] 0.4790875 Here is the binomial distribution \\(f(x; \\pi), \\hspace{5mm} x \\in [550, 700]\\). events &lt;- round(seq(from = 550, to = 700, length = 20), 0) density &lt;- dbinom(x = events, prob = p, size = n) prob &lt;- pbinom(q = events, prob = p, size = n, lower.tail = TRUE) df &lt;- data.frame(events, density, prob) ggplot(df, aes(x = factor(events))) + # geom_col(aes(y = density)) + geom_col(aes(y = density), fill = mf_pal()(1), alpha = 0.8) + geom_text( aes(label = round(density, 3), y = density + 0.001), position = position_dodge(0.9), size = 3, vjust = 0 ) + geom_line( data = df, aes(x = as.numeric(factor(events)), y = prob/40), color = mf_pal()(1), size = 1) + scale_y_continuous(sec.axis = sec_axis(~.*40, name = &quot;Cum Prob&quot;)) + theme_mf() + labs(title = &quot;PMF and CDF of Binomial Distribution&quot;, subtitle = &quot;Bin(1315, 0.479).&quot;, x = &quot;Events (x)&quot;, y = &quot;Density&quot;) There are several ways to calculate a confidence interval for \\(\\pi\\). One method is the normal approximation (Wald) interval. \\[\\pi = p \\pm z_{\\alpha /2} \\sqrt{\\frac{p (1 - p)}{n}}\\] alpha &lt;- .05 z &lt;- qnorm(1 - alpha / 2) se &lt;- sqrt(p * (1 - p) / n) p + c(-z*se, z*se) ## [1] 0.4520868 0.5060882 This method is easy to understand and calculate by hand, but its accuracy suffers when \\(np&lt;5\\) or \\(n(1-p)&lt;5\\) and it does not work at all when \\(p = 0\\) or \\(p = 1\\). Option two is the Wilson method. \\[\\frac{p + \\frac{z^2}{2n}}{1 + \\frac{z^2}{n}} \\pm \\frac{z}{1 + \\frac{z^2}{n}} \\sqrt{\\frac{p(1 - p)}{n} + \\frac{z^2}{4n^2}}\\] est &lt;- (p + (z^2)/(2*n)) / (1 + (z^2) / n) pm &lt;- z / (1 + (z^2)/n) * sqrt(p*(1-p)/n + (z^2) / (4*(n^2))) est + c(-pm, pm) ## [1] 0.4521869 0.5061098 This is what prop.test() does when you set correct = FALSE. prop.test(x = x, n = n, correct = FALSE) ## ## 1-sample proportions test without continuity correction ## ## data: x out of n, null probability 0.5 ## X-squared = 2.3004, df = 1, p-value = 0.1293 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.4521869 0.5061098 ## sample estimates: ## p ## 0.4790875 There is a second version of the Wilson interval that applies a “continuity correction” that aligns the “minimum coverage probability”, rather than the “average probability”, with the nominal value. I’ll need to learn what’s inside those quotations at some point. prop.test(x = x, n = n) ## ## 1-sample proportions test with continuity correction ## ## data: x out of n, null probability 0.5 ## X-squared = 2.2175, df = 1, p-value = 0.1365 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.4518087 0.5064898 ## sample estimates: ## p ## 0.4790875 Finally, there is the Clopper-Pearson exact confidence interval. Clopper-Pearson inverts two single-tailed binomial tests at the desired alpha. This is a non-trivial calculation, so there is no easy formula to crank through. Just use the binom.test() function and pray no one asks for an explanation. binom.test(x = x, n = n) ## ## Exact binomial test ## ## data: x and n ## number of successes = 630, number of trials = 1315, p-value = 0.1364 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.4517790 0.5064896 ## sample estimates: ## probability of success ## 0.4790875 The expected probability of no one being a high-risk drinker is \\(f(0;0.479) = \\frac{1315!}{0!(1315-0)!} 0.479^0 (1-0.479)^{1315-0} = 0\\). dbinom(x = 0, size = n, p = p) ## [1] 0 The expected probability of half the population being a high-risk drinker, \\(f(658, 0.479)\\), is impossible to write out, and slow to calculate. pbinom(q = .5*n, size = n, prob = p, lower.tail = FALSE) ## [1] 0.06455096 As n increases for fixed \\(\\pi\\), the binomial distribution approaches normal distribution \\(N(n\\pi, n\\pi(1−\\pi))\\). The normal distribution is a good approximation when \\(n\\) is large. pnorm(q = 0.5, mean = p, sd = se, lower.tail = FALSE) ## [1] 0.06450357 Here are some more examples using smaller sample sizes. The probability 2 out of 10 coin flips are heads if the probability of heads is 0.3: dbinom(x = 2, size = 10, prob = 0.3) ## [1] 0.2334744 Here is a simulation from n = 10,000 random samples of size 10. rbinom() generates a random sample of numbers from the binomial distribution. data.frame(cnt = rbinom(n = 10000, size = 10, prob = 0.3)) %&gt;% count(cnt) %&gt;% ungroup() %&gt;% mutate(pct = n / sum(n), X_eq_x = cnt == 2) %&gt;% ggplot(aes(x = as.factor(cnt), y = n, fill = X_eq_x, label = pct)) + geom_col(alpha = 0.8) + scale_fill_mf() + geom_label(aes(label = round(pct, 2)), size = 3, alpha = .6) + theme_mf() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Binomial Distribution&quot;, subtitle = paste0( &quot;P(X=2) successes in 10 trials when p = 0.3 is &quot;, round(dbinom(2, 10, 0.3), 4), &quot;.&quot; ), x = &quot;Successes&quot;, y = &quot;Count&quot;, caption = &quot;Simulation from n = 10,000 binomial random samples.&quot;) What is the probability of &lt;=2 heads in 10 coin flips where probability of heads is 0.3? The cumulative probability is the sum of the first three bars in the simulation above. Function pbinom() calculates the cumulative binomial probability. pbinom(q = 2, size = 10, prob = 0.3, lower.tail = TRUE) ## [1] 0.3827828 What is the expected number of heads in 25 coin flips if the probability of heads is 0.3? The expected value, \\(\\mu = np\\), is 7.5. Here’s an empirical test from 10,000 samples. mean(rbinom(n = 10000, size = 25, prob = .3)) ## [1] 7.5191 The variance, \\(\\sigma^2 = np (1 - p)\\), is 5.25. Here’s an empirical test. var(rbinom(n = 10000, size = 25, prob = .3)) ## [1] 5.248123 Suppose X and Y are independent random variables distributed \\(X \\sim Bin(10, .6)\\) and \\(Y \\sim Bin(10, .7)\\). What is the probability that either variable is &lt;=4? Let \\(P(A) = P(X&lt;=4)\\) and \\(P(B) = P(Y&lt;=4)\\). Then \\(P(A|B) = P(A) + P(B) - P(AB)\\), and because the events are independent, \\(P(AB) = P(A)P(B)\\). p_a &lt;- pbinom(q = 4, size = 10, prob = 0.6, lower.tail = TRUE) p_b &lt;- pbinom(q = 4, size = 10, prob = 0.7, lower.tail = TRUE) p_a + p_b - (p_a * p_b) ## [1] 0.2057164 Here’s an empirical test. df &lt;- data.frame( x = rbinom(10000, 10, 0.6), y = rbinom(10000, 10, 0.7) ) mean(if_else(df$x &lt;= 4 | df$y &lt;= 4, 1, 0)) ## [1] 0.2116 A couple other points to remember: The Bernoulli distribution is a special case of the binomial with \\(n = 1\\). The binomial distribution assumes independent trials. If you sample without replacement from a finite population, use the hypergeometric distribution. 1.2.3 Poission If \\(X\\) is the number of successes in \\(n\\) (many) trials when the probability of success \\(\\lambda / n\\) is small, then \\(X\\) is a random variable with a Poisson distribution \\(X \\sim Poisson(\\lambda)\\) \\[f(x;\\lambda) = \\frac{e^{-\\lambda} \\lambda^x}{x!} \\hspace{1cm} x \\in (0, 1, ...), \\hspace{2mm} \\lambda &gt; 0\\] with \\(E(X)=\\lambda\\) and \\(Var(X) = \\lambda\\). The Poisson likelihood function is \\[L(\\lambda; x) = \\prod_{i=1}^N f(x_i; \\lambda) = \\prod_{i=1}^N \\frac{e^{-\\lambda} \\lambda^x_i}{x_i !} = \\frac{e^{-n \\lambda} \\lambda^{\\sum x_i}}{\\prod x_i}.\\] The Poisson loglikelihood function is \\[l(\\lambda; x) = \\sum_{i=1}^N x_i \\log \\lambda - n \\lambda.\\] One can show that the loglikelihood function is maximized at \\[\\hat{\\lambda} = \\sum_{i=1}^N x_i / n.\\] Thus, for a Poisson sample, the MLE for \\(\\lambda\\) is just the sample mean. Poisson sampling is used to model counts of events that occur randomly over a fixed period of time. Here is a simple analysis of data from a Poisson process. Data set dat contains frequencies of goal counts during the first round matches of the 2002 World Cup. ## goals freq ## 1 0 23 ## 2 1 37 ## 3 2 20 ## 4 3 11 ## 5 4 2 ## 6 5 1 ## 7 6 0 ## 8 7 0 ## 9 8 1 The MLE of \\(\\lambda\\) from the Poisson distribution is the sample mean. lambda &lt;- weighted.mean(dat$goals, dat$freq) print(lambda) ## [1] 1.378947 The 0.95 CI is \\(\\lambda \\pm z_{.05/2} \\sqrt{\\lambda / n}\\) n &lt;- sum(dat$freq) z &lt;- qnorm(0.975) se &lt;- sqrt(lambda / n) paste0(&quot;[&quot;, round(lambda - z*se, 2), &quot;, &quot;, round(lambda + z*se, 2),&quot;]&quot;) ## [1] &quot;[1.14, 1.62]&quot; The expected probability of scoring 2 goals in a match is \\(\\frac{e^{-1.38} 1.38^2}{2!} = 0.239\\). dpois(x = 2, lambda = lambda) ## [1] 0.2394397 events &lt;- 0:10 density &lt;- dpois(x = events, lambda = 3) prob &lt;- ppois(q = events, lambda = 3, lower.tail = TRUE) df &lt;- data.frame(events, density, prob) ggplot(df, aes(x = factor(events), y = density)) + geom_col() + geom_text( aes(label = round(density, 3), y = density + 0.01), position = position_dodge(0.9), size = 3, vjust = 0 ) + geom_line( data = df, aes(x = events, y = prob/4), size = 1) + scale_y_continuous(sec.axis = sec_axis(~.*4, name = &quot;Cum Prob&quot;)) + theme_mf() + scale_fill_mf() + labs(title = &quot;PMF and CDF of Poisson Distribution&quot;, subtitle = &quot;Poisson(3).&quot;, x = &quot;Events (x)&quot;, y = &quot;Density&quot;) The expected probability of scoring 2 to 4 goals in a match is sum(dpois(x = c(2:4), lambda = lambda)) ## [1] 0.3874391 Or, using the cumulative probability distribution, ppois(q = 4, lambda = lambda) - ppois(q = 1, lambda = lambda) ## [1] 0.3874391 How well does the Poisson distribution fit the 2002 World Cup data? dat %&gt;% mutate(pred = n * dpois(x = goals, lambda = lambda)) %&gt;% rename(obs = freq) %&gt;% pivot_longer(cols = -goals) %&gt;% ggplot(aes(x = goals, y = value, color = name)) + geom_point() + theme_mf() + scale_color_mf() + geom_smooth(se = FALSE) + labs( title = &quot;Poisson Dist: Observed vs Expected&quot;, color = &quot;&quot;, y = &quot;frequencey&quot; ) It fits the data pretty good! \\(Poison(\\lambda) \\rightarrow Bin(n, \\pi)\\) when \\(n\\pi = \\lambda\\) and \\(n \\rightarrow \\infty\\) and \\(\\pi \\rightarrow 0\\). Because the Poisson is limit of the \\(Bin(n, \\pi)\\), it is useful as an approximation to the binomial when \\(n\\) is large (\\(n&gt;=20\\)) and \\(\\pi\\) small (\\(p&lt;=0.05\\)). For example, suppose a baseball player has a p=.03 chance of hitting a homerun. What is the probability of X&gt;=20 homeruns in 500 at-bats? This is a binomial process because the sample size is fixed. pbinom(q = 20, size = 500, prob = 0.03, lower.tail = FALSE) ## [1] 0.07979678 But \\(n\\) is large and \\(\\pi\\) is small, so the Poission distribution will work well too. ppois(q = 20, lambda = 0.03 * 500, lower.tail = FALSE) ## [1] 0.08297091 What is the distribution of successes from a sample of n = 50 when the probability of success is p = .03? n = 500 p = 0.03 x = 0:30 data.frame( events = x, Poisson = dpois(x = x, lambda = p * n), Binomial = dbinom(x = x, size = n, p = p) ) %&gt;% pivot_longer(cols = -events) %&gt;% ggplot(aes(x = events, y = value, color = name)) + geom_point() + theme_mf() + scale_color_mf() + labs(title = &quot;Poisson(15) vs. Bin(500, .03)&quot;, subtitle = &quot;Poisson approximation to binomial.&quot;, x = &quot;Events&quot;, y = &quot;Density&quot;, color = &quot;&quot;) When the observed variance is greater than \\(\\lambda\\) (overdispersion), the Negative Binomial distribution can be used instead of Poisson. Suppose the probability that a drug produces a certain side effect is p = = 0.1% and n = 1,000 patients in a clinical trial receive the drug. What is the probability 0 people experience the side effect? The expected value is np, 1. The probability of measuring 0 when the expected value is 1 is dpois(x = 0, lambda = 1000 * .001) = 0.3678794. 1.2.4 Multinomial If \\(X = (X_1, X_2, \\cdots, X_k)\\) are the counts of successful events in \\(n\\) identical and independent trials of success probabilities \\(\\pi = (\\pi_1, \\pi_2, \\cdots, \\pi_k)\\), then \\(X\\) is a random variable with a multinomial distribution \\(X \\sim Mult(n,\\pi)\\) \\[f(x; n, \\pi) = \\frac{n!}{x_{1}! x_{2}! \\cdots x_{k}!} \\pi^{x_1} \\pi^{x_2} \\cdots \\pi^{x_k} \\hspace{1cm} x \\in \\{0, 1, ..., n \\}, \\hspace{2mm} \\pi \\in [0, 1]\\] with expected values vector \\(E(X_j) = n\\pi_j\\) and covariance matrix \\[Var(X) = \\begin{bmatrix}n\\pi_{1}(1-\\pi_{1}) &amp; -n\\pi_{1}\\pi_{2} &amp; \\cdots &amp; -n\\pi_{1}\\pi_{k}\\\\ -n\\pi_{1}\\pi_{2} &amp; n\\pi_{2}(1-\\pi_{2}) &amp; \\cdots &amp; -n\\pi_{2}\\pi_{k}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ -n\\pi_{1}\\pi_{k} &amp; -n\\pi_{2}\\pi_{k} &amp; \\cdots &amp; n\\pi_{k}(1-\\pi_{k}) \\end{bmatrix}\\] so \\(Var(X_j) = n \\pi_j (1 - \\pi_j)\\) and \\(cov(X_j, X_k) = -n \\pi_j \\pi_k\\). The individual components of a multinomial random vector are binomial and have a binomial distribution, \\(X_i = Bin(n, \\pi_i)\\). Binomial is a special case of multinomial for k = 2. Suppose a city population is 20% black, 15% Hispanic, and 65% other. From a random sample of \\(n = 12\\) persons, what is the probability of 4 black and 8 other? \\[f(x;\\pi) = \\frac{12!}{4! 0! 8!} (0.20)^4 (0.15)^0 (0.65)^8 = 0.0252\\] Function dmultinom() calculates the multinomial probability. dmultinom(x = c(4, 0, 8), prob = c(0.20, 0.15, 0.65)) ## [1] 0.025 To calculate the probability of &lt;= 1 black, combine Hispanic and other, then sum the probability of black = 1 and black = 2. \\[f(x;\\pi) = \\frac{12!}{0! 12!} (0.20)^0 (0.80)^{12} + \\frac{12!}{1! 11!} (0.20)^1 (0.80)^{11} = 0.2748\\] dmultinom(x = c(0, 12), prob = c(0.20, 0.80)) + dmultinom(x = c(1, 11), prob = c(0.20, 0.80)) ## [1] 0.27 1.2.5 Negative-Binomial If \\(X\\) is the count of failure events ocurring prior to reaching \\(r\\) successful events in a sequence of Bernouli trias of success probability \\(p\\), then \\(X\\) is a random variable with a negative-binomial distribution \\(X \\sim NB(r, p)\\). The probability of \\(X = x\\) failures prior to \\(r\\) successes is \\[f(x;r, p) = {{x + r - 1} \\choose {r - 1}} p^r (1-p)^{x}.\\] with \\(E(X) = r (1 - p) / p\\) and \\(Var(X) = r (1-p) / p^2\\). When the data has overdispersion, model the data with the negative-binomial distribution instead of Poission. Examples An oil company has a \\(p = 0.20\\) chance of striking oil when drilling a well. What is the probability the company drills \\(x + r = 7\\) wells to strike oil \\(r = 3\\) times? Note that the question is formulated as counting total events, \\(x + r = 7\\), so translate it to total failed events, \\(x = 4\\). \\[f(x;r, p) = {{4 + 3 - 1} \\choose {3 - 1}} (0.20)^3 (1 - 0.20)^4 = 0.049.\\] Function dnbinom() calculates the negative-binomial probability. Parameter x equals the number of failures, \\(x - r\\). dnbinom(x = 4, size = 3, prob = 0.2) ## [1] 0.049 The expected number of failures prior to 3 successes is \\(E(X) = 3 (1 - 0.20) / 0.20 = 12\\) with variance \\(Var(X) = 3 (1 - 0.20) / 0.20^2 = 60\\). Confirm this with a simulation from n = 10,000 random samples using rnbinom(). my_dat &lt;- rnbinom(n = 10000, size = 3, prob = 0.20) mean(my_dat) ## [1] 12 var(my_dat) ## [1] 60 1.2.6 Geometric If \\(X\\) is the count of Bernoulli trials of success probability \\(p\\) required to achieve the first successful event, then \\(X\\) is a random variable with a geometric distribution \\(X \\sim G(p)\\). The probability of \\(X = x\\) trials is \\[f(x; p) = p(1-p)^{x-1}.\\] with \\(E(X)=\\frac{{n}}{{p}}\\) and \\(Var(X) = \\frac{(1-p)}{p^2}\\). The probability of \\(X&lt;=n\\) trials is \\[F(X=n) = 1 - (1-p)^n.\\] Examples What is the probability a marketer encounters x = 3 people on the street who did not attend a sporting event before the first success if the population probability is p = 0.20? \\[f(4; 0.20) = 0.20(1-0.20)^{4-1} = 0.102.\\] Function dgeom() calculates the geometric distribution probability. Parameter x is the number of failures, not the number of trials. dgeom(x = 3, prob = 0.20) ## [1] 0.1 data.frame(cnt = rgeom(n = 10000, prob = 0.20)) %&gt;% count(cnt) %&gt;% top_n(n = 15, wt = n) %&gt;% ungroup() %&gt;% mutate(pct = round(n / sum(n), 3), X_eq_x = cnt == 3) %&gt;% ggplot(aes(x = as.factor(cnt), y = n, fill = X_eq_x, label = pct)) + geom_col(alpha = 0.8) + scale_fill_mf() + geom_text(size = 3) + theme_mf() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Distribution of trials prior to first success&quot;, subtitle = paste(&quot;P(X = 3) | X ~ G(.2) = &quot;, round(dgeom(2, .2), 3)), x = &quot;Unsuccessful trials&quot;, y = &quot;Count&quot;, caption = &quot;simulation of n = 10,000 samples from geometric dist.&quot;) What is the probability the marketer fails to find someone who attended a game in x &lt;= 5 trials before finding someone who attended a game on the sixth trial when the population probability is p = 0.20? p = 0.20 n = 5 # exact pgeom(q = n, prob = p, lower.tail = TRUE) ## [1] 0.74 # simulated mean(rgeom(n = 10000, prob = p) &lt;= n) ## [1] 0.74 What is the probability the marketer fails to find someone who attended a game on x &gt;= 5 trials before finding someone who attended a game on the next trial? p = 0.20 n = 5 # exact pgeom(q = n, prob = p, lower.tail = FALSE) ## [1] 0.26 # simulated mean(rgeom(n = 10000, prob = p) &gt; n) ## [1] 0.26 The expected number of trials to achieve the first success is 1 / 0.20 = 5, Var(X) = (1 - 0.20) / 0.20^2 = 20? p = 0.20 # mean # exact 1 / p ## [1] 5 # simulated mean(rgeom(n = 10000, prob = p)) + 1 ## [1] 5 # Variance # exact (1 - p) / p^2 ## [1] 20 # simulated var(rgeom(n = 100000, prob = p)) ## [1] 20 1.2.7 Hypergeometric If \\(X\\) is the count of successful events in a sample of size \\(n\\) without replacement from a population of size \\(N\\) containing \\(K\\) successes and \\(N-K\\) non-successes, then \\(X\\) is a random variable with a hypergeometric distribution \\[f(x|N,K,n) = \\frac{{{K}\\choose{k}}{{N-K}\\choose{n-k}}}{{N}\\choose{n}}.\\] with \\(E(X) = n\\frac{K}{N}\\) and \\(Var(X) = n \\frac{K}{N} \\cdot \\frac{N-n}{N} \\cdot \\frac{N-K}{N-1}\\). The formula follows from the frequency table of the possible outcomes. Sampled Not Sampled Total success k K-k K non-success n-k (N-K)-(n-k) N-K Total n N-n N If \\(X\\) is the count of successful events in a sample of size \\(k\\) without replacement from a population containing \\(M\\) successes and \\(N\\) non-successes, then \\(X\\) is a random variable with a hypergeometric distribution \\[f(x|m,n,k) = \\frac{{{m}\\choose{x}}{{n}\\choose{k-x}}}{{m+n}\\choose{k}}.\\] with \\(E(X)=k\\frac{m}{m+n}\\) and \\(Var(X) = k\\frac{m}{m+n}\\cdot\\frac{m+n-k}{m+n}\\cdot\\frac{n}{m+n-1}\\). phyper returns the cumulative probability (percentile) p at the specified value (quantile) q. qhyper returns the value (quantile) q at the specified cumulative probability (percentile) p. Example What is the probability of selecting \\(X = 14\\) red marbles from a sample of \\(k = 20\\) taken from an urn containing \\(m = 70\\) red marbles and \\(n = 30\\) green marbles? Function dhyper() calculates the hypergeometric probability. x = 14 m = 70 n = 30 k = 20 dhyper(x = x, m = m, n = n, k = k) ## [1] 0.21 The expected value is 14 and variance is 3.39. The hypergeometric random variable is similar to the binomial random variable except that it applies to situations of sampling without replacement from a small population. As the population size increases, sampling without replacement converges to sampling with replacement, and the hypergeometric distribution converges to the binomial. What if the total population size is 250? 500? 1000? 1.2.8 Gamma If \\(X\\) is the interval until the \\(\\alpha^{th}\\) successful event when the average interval is \\(\\theta\\), then \\(X\\) is a random variable with a gamma distribution \\(X \\sim \\Gamma(\\alpha, \\theta)\\). The probability of an interval of \\(X = x\\) is \\[f(x; \\alpha, \\theta) = \\frac{1}{\\Gamma(\\alpha)\\theta^\\alpha}x^{\\alpha-1}e^{-x/\\theta}.\\] where \\(\\Gamma(\\alpha) = (1 - \\alpha)!\\) with \\(E(X) = \\alpha \\theta\\) and \\(Var(X) = \\alpha \\theta^2\\). Examples On average, someone sends a money order once per 15 minutes (\\(\\theta = .25\\)). What is the probability someone sends \\(\\alpha = 10\\) money orders in less than \\(x = 3\\) hours?* theta = 0.25 alpha = 10 pgamma(q = 3, shape = alpha, scale = 0.25) ## [1] 0.76 data.frame(x = 0:1000 / 100, prob = pgamma(q = 0:1000 / 100, shape = alpha, scale = theta, lower.tail = TRUE)) %&gt;% mutate(Interval = ifelse(x &gt;= 0 &amp; x &lt;= 3, &quot;0 to 3&quot;, &quot;other&quot;)) %&gt;% ggplot(aes(x = x, y = prob, fill = Interval)) + geom_area(alpha = 0.9) + theme_mf() + scale_fill_mf() + labs(title = &quot;X ~ Gam(alpha = 10, theta = .25)&quot;, subtitle = &quot;Probability of 10 events in X hours when the mean time to an event is .25 hours.&quot;, x = &quot;Interval (x)&quot;, y = &quot;pgamma&quot;) "],
["cont-dist.html", "1.3 Continuous Distributions", " 1.3 Continuous Distributions 1.3.1 Normal Random variable \\(X\\) is distributed \\(X \\sim N(\\mu, \\sigma^2)\\) if \\[f(X)=\\frac{{1}}{{\\sigma \\sqrt{{2\\pi}}}}e^{-.5(\\frac{{x-\\mu}}{{\\sigma}})^2}\\]. Example IQ scores are distributed \\(X \\sim N(100, 16^2\\). What is the probability a randomly selected person’s IQ is &lt;90? my_mean = 100 my_sd = 16 my_x = 90 # exact pnorm(q = my_x, mean = my_mean, sd = my_sd, lower.tail = TRUE) ## [1] 0.27 # simulated mean(rnorm(n = 10000, mean = my_mean, sd = my_sd) &lt;= my_x) ## [1] 0.26 library(dplyr) library(ggplot2) data.frame(x = 0:1500 / 10, prob = pnorm(q = 0:1500 / 10, mean = my_mean, sd = my_sd, lower.tail = TRUE)) %&gt;% mutate(cdf = ifelse(x &gt; 0 &amp; x &lt;= my_x, prob, 0)) %&gt;% ggplot() + geom_line(aes(x = x, y = prob)) + geom_area(aes(x = x, y = cdf), alpha = 0.3) + labs(title = bquote(&#39;X~N(&#39;~mu==.(my_mean)~&#39;,&#39;~sigma^{2}==.(my_sd)^{2}~&#39;)&#39;), subtitle = bquote(&#39;P(X&lt;=&#39;~.(my_x)~&#39;) when mean is&#39;~.(my_mean)~&#39; and variance is&#39;~.(my_sd)^{2}~&#39;.&#39;), x = &quot;x&quot;, y = &quot;Probability&quot;) 1.3.1.1 Example IQ scores are distributed \\(X \\sim N(100, 16^2\\). What is the probability a randomly selected person’s IQ is &gt;140? my_mean = 100 my_sd = 16 my_x = 140 # exact pnorm(q = my_x, mean = my_mean, sd = my_sd, lower.tail = FALSE) ## [1] 0.0062 # simulated mean(rnorm(n = 10000, mean = my_mean, sd = my_sd) &gt; my_x) ## [1] 0.0073 library(dplyr) library(ggplot2) data.frame(x = 0:1500 / 10, prob = pnorm(q = 0:1500 / 10, mean = my_mean, sd = my_sd, lower.tail = TRUE)) %&gt;% mutate(cdf = ifelse(x &gt; my_x &amp; x &lt; 1000, prob, 0)) %&gt;% ggplot() + geom_line(aes(x = x, y = prob)) + geom_area(aes(x = x, y = cdf), alpha = 0.3) + labs(title = bquote(&#39;X~N(&#39;~mu==.(my_mean)~&#39;,&#39;~sigma^{2}==.(my_sd)^{2}~&#39;)&#39;), subtitle = bquote(&#39;P(X&lt;=&#39;~.(my_x)~&#39;) when mean is&#39;~.(my_mean)~&#39; and variance is&#39;~.(my_sd)^{2}~&#39;.&#39;), x = &quot;x&quot;, y = &quot;Probability&quot;) 1.3.1.2 Example IQ scores are distributed \\(X \\sim N(100, 16^2\\). What is the probability a randomly selected person’s IQ is between 92 and 114? my_mean = 100 my_sd = 16 my_x_l = 92 my_x_h = 114 # exact pnorm(q = my_x_h, mean = my_mean, sd = my_sd, lower.tail = TRUE) - pnorm(q = my_x_l, mean = my_mean, sd = my_sd, lower.tail = TRUE) ## [1] 0.5 library(dplyr) library(ggplot2) data.frame(x = 0:1500 / 10, prob = pnorm(q = 0:1500 / 10, mean = my_mean, sd = my_sd, lower.tail = TRUE)) %&gt;% mutate(cdf = ifelse(x &gt; my_x_l &amp; x &lt;= my_x_h, prob, 0)) %&gt;% ggplot() + geom_line(aes(x = x, y = prob)) + geom_area(aes(x = x, y = cdf), alpha = 0.3) + labs(title = bquote(&#39;X~N(&#39;~mu==.(my_mean)~&#39;,&#39;~sigma^{2}==.(my_sd)^{2}~&#39;)&#39;), subtitle = bquote(&#39;P(X&lt;=&#39;~.(my_x)~&#39;) when mean is&#39;~.(my_mean)~&#39; and variance is&#39;~.(my_sd)^{2}~&#39;.&#39;), x = &quot;x&quot;, y = &quot;Probability&quot;) 1.3.1.3 Example Class scores are distributed \\(X \\sim N(70, 10^2\\). If the instructor wants to give A’s to &gt;=85th percentile and B’s to 75th-85th percentile, what are the cutoffs? my_mean = 70 my_sd = 10 my_pct_l = .75 my_pct_h = .85 qnorm(p = my_pct_l, mean = my_mean, sd = my_sd, lower.tail = TRUE) ## [1] 77 qnorm(p = my_pct_h, mean = my_mean, sd = my_sd, lower.tail = TRUE) ## [1] 80 library(dplyr) library(ggplot2) data.frame(x = 0:1000 / 10, prob = pnorm(q = 0:1000 / 10, mean = my_mean, sd = my_sd, lower.tail = TRUE)) %&gt;% mutate(cdf = ifelse(prob &gt; my_pct_l &amp; prob &lt;= my_pct_h, prob, 0)) %&gt;% ggplot() + geom_line(aes(x = x, y = prob)) + geom_area(aes(x = x, y = cdf), alpha = 0.3) + labs(title = bquote(&#39;X~N(&#39;~mu==.(my_mean)~&#39;,&#39;~sigma^{2}==.(my_sd)^{2}~&#39;)&#39;), subtitle = bquote(&#39;P(X&lt;=x) = [&#39;~.(my_pct_l)~&#39;,&#39;~.(my_pct_h)~&#39;] when mean is&#39;~.(my_mean)~&#39; and variance is&#39;~.(my_sd)^{2}~&#39;.&#39;), x = &quot;x&quot;, y = &quot;Probability&quot;) 1.3.1.4 Normal Approximation to Binomial The CLT implies that certain distributions can be approximated by the normal distribution. The binomial distribution \\(X \\sim B(n,p)\\) is approximately normal with mean \\(\\mu = n p\\) and variance \\(\\sigma^2=np(1-p)\\). The approximation is useful when the expected number of successes and failures is at least 5: \\(np&gt;=5\\) and \\(n(1-p)&gt;=5\\). 1.3.1.5 Example A measure requires p&gt;=50% popular to pass. A sample of n=1,000 yields x=460 approvals. What is the probability that the overall population approves, P(X)&gt;0.5? my_x = 460 my_p = 0.50 my_n = 1000 my_mean = my_p * my_n my_sd = round(sqrt(my_n * my_p * (1 - my_p)), 1) # Exact binomial pbinom(q = my_x, size = my_n, prob = my_p, lower.tail = TRUE) ## [1] 0.0062 # Normal approximation pnorm(q = my_x, mean = my_p * my_n, sd = sqrt(my_n * my_p * (1 - my_p)), lower.tail = TRUE) ## [1] 0.0057 library(dplyr) library(ggplot2) library(tidyr) data.frame(x = 400:600, Normal = pnorm(q = 400:600, mean = my_p * my_n, sd = sqrt(my_n * my_p * (1 - my_p)), lower.tail = TRUE), Binomial = pbinom(q = 400:600, size = my_n, prob = my_p, lower.tail = TRUE)) %&gt;% gather(key = &quot;Distribution&quot;, value = &quot;cdf&quot;, c(-x)) %&gt;% ggplot(aes(x = x, y = cdf, color = Distribution)) + geom_line() + labs(title = bquote(&#39;X~B(n=&#39;~.(my_n)~&#39;, p=&#39;~.(my_p)~&#39;), &#39;~&#39;X~N(&#39;~mu==.(my_mean)~&#39;,&#39;~sigma^{2}==.(my_sd)^{2}~&#39;)&#39;), subtitle = &quot;Normal approximation to the binomial&quot;, x = &quot;x&quot;, y = &quot;Probability&quot;) The Poisson distribution \\(x~P(\\lambda)\\) is approximately normal with mean \\(\\mu = \\lambda\\) and variance \\(\\sigma^2 = \\lambda\\), for large values of \\(\\lambda\\). 1.3.1.6 Example The annual number of earthquakes registering at least 2.5 on the Richter Scale and having an epicenter within 40 miles of downtown Memphis follows a Poisson distribution with mean \\(\\lambda=6.5\\). What is the probability that at least \\(x&gt;=9\\)* such earthquakes will strike next year?* my_x = 9 my_lambda = 6.5 my_sd = round(sqrt(my_lambda), 2) # Exact Poisson ppois(q = my_x - 1, lambda = my_lambda, lower.tail = FALSE) ## [1] 0.21 # Normal approximation pnorm(q = my_x - 0.5, mean = my_lambda, sd = my_sd, lower.tail = FALSE) ## [1] 0.22 library(dplyr) library(ggplot2) library(tidyr) data.frame(x = 0:200 / 10, Normal = pnorm(q = 0:200 / 10, mean = my_lambda, sd = my_sd, lower.tail = TRUE), Poisson = ppois(q = 0:200 / 10, lambda = my_lambda, lower.tail = TRUE)) %&gt;% gather(key = &quot;Distribution&quot;, value = &quot;cdf&quot;, c(-x)) %&gt;% ggplot(aes(x = x, y = cdf, color = Distribution)) + geom_line() + labs(title = bquote(&#39;X~P(&#39;~lambda~&#39;=&#39;~.(my_lambda)~&#39;), &#39;~&#39;X~N(&#39;~mu==.(my_lambda)~&#39;,&#39;~sigma^{2}==.(my_lambda)~&#39;)&#39;), subtitle = &quot;Normal approximation to the Poisson&quot;, x = &quot;x&quot;, y = &quot;Probability&quot;) 1.3.1.7 From Sample to Population Suppose a person’s blood pressure typically measures 160?20 mm. If one takes n=5 blood pressure readings, what is the probability the average will be &lt;=150? my_mu = 160 my_sigma = 20 my_n = 5 my_x = 150 my_se = round(my_sigma / sqrt(my_n), 1) pnorm(q = my_x, mean = my_mu, sd = my_sigma / sqrt(my_n), lower.tail = TRUE) ## [1] 0.13 library(dplyr) library(ggplot2) data.frame(x = 1000:2000 / 10, prob = pnorm(q = 1000:2000 / 10, mean = my_mu, sd = my_sigma / sqrt(my_n), lower.tail = TRUE)) %&gt;% mutate(cdf = ifelse(x &gt; 0 &amp; x &lt;= my_x, prob, 0)) %&gt;% ggplot() + geom_line(aes(x = x, y = prob)) + geom_area(aes(x = x, y = cdf), alpha = 0.3) + labs(title = bquote(&#39;X~N(&#39;~mu==.(my_mu)~&#39;,&#39;~sigma^{2}==.(my_se)^{2}~&#39;)&#39;), subtitle = bquote(&#39;P(X&lt;=&#39;~.(my_x)~&#39;) when mean is&#39;~.(my_mu)~&#39; and variance is&#39;~sigma~&#39;/sqrt(n)&#39;~.(my_se)^{2}~&#39;.&#39;), x = &quot;x&quot;, y = &quot;Probability&quot;) knitr::include_app(&quot;https://mpfoley73.shinyapps.io/shiny_dist/&quot;, height = &quot;600px&quot;) "],
["join-distributions.html", "1.4 Join Distributions", " 1.4 Join Distributions "],
["likelihood.html", "1.5 Likelihood", " 1.5 Likelihood The likelihood function is the likelihood of a parameter \\(\\theta\\) given an observed value of the random variable \\(X\\). The likelihood function is identical to the probability distribution function, except that it reverses which variable is considered fixed. E.g., the binomial probability distribution expresses the probability that \\(X = x\\) given the success probability \\(\\theta = \\pi\\). \\[f(x|\\pi) = \\frac{n!}{x!(n-x)!} \\pi^x (1-\\pi)^{n-x}.\\] The corresponding likelihood function expresses the probability that \\(\\pi = p\\) given the observed value \\(x\\). \\[L(p|x) = \\frac{n!}{x!(n-x)!} p^x (1-p)^{n-x}.\\] You usually want to know the value of \\(\\theta\\) at the maximum of the likelihood function. When taking derivatives, any multiplicative constant is irrevelant and can be discarded. So for the binomial distribution, the likelihood function for \\(\\pi\\) may instead be expressed as \\[L(p|x) \\propto p^x (1-p)^{n-x}\\] Calculating the maximum is usually simplified using the log-likelihood, \\(l(\\theta|x) = \\log L(\\theta|x)\\). For the binomial distribution, \\(l(p|x) = x \\log p + (n - x) \\log (1 - p)\\). Frequently you derive loglikelihood from a sample. The overall likelihood is the product of the individual likelihoods, and the overall loglikelihood is the log of the overall likelihood. \\[l(\\theta|x) = \\log \\prod_{i=1}^n f(x_i|\\theta)\\] Here are plots of the binomial log-likelihood of \\(pi\\) for several values of \\(X\\) from a sample of size \\(n = 5\\). As the total sample size \\(n\\) grows, the loglikelihood function becomes more sharply peaked around its maximum, and becomes nearly quadratic (i.e. a parabola, if there is a single parameter). Here is the same plot with \\(n = 500\\). The value of \\(\\theta\\) that maximizes \\(l\\) (and \\(L\\)) is the maximum-likelihood estimator (MLE) of \\(\\theta\\), \\(\\hat{\\theta}\\). E.g., suppose you have an experiment of \\(n = 5\\) Bernoulli trials \\(\\left(X \\sim Bin(5, \\pi) \\right)\\) with and \\(X = 3\\) successful events. A plot of \\(L(p|x) = p^3(1 - p)^2\\) shows the MLE is at \\(p = 0.6\\). This approach is called maximum-likelihood estimation. MLE usually involves setting the derivatives to zero and solving for \\(theta\\). "],
["statistical-tests.html", "Chapter 2 Statistical Tests", " Chapter 2 Statistical Tests This section describes interactions or associations between two or three categorical variables mostly via single summary statistics and with significance testing. This non-model based analysis does not handle more complicated situations such as simultaneous effects of multiple variables, or mixtures of categorical and continuous variables. "],
["chi-square-test.html", "2.1 Chi-Square Test", " 2.1 Chi-Square Test These notes rely on PSU STAT 500, Wikipedia, and Disha M. The chi-square test compares observed categorical variable frequency counts \\(O\\) with their expected values \\(E\\). The test statistic \\(X^2 = \\sum (O - E)^2 / E\\) is distributed \\(\\chi^2\\). \\(H_0: O = E\\) and \\(H_a\\) is at least one pair of frequency counts differ. The chi-square test relies on the central limit theorem, so it is valid for independent, normally distributed samples, typically affirmed with at least 5 successes and failures in each cell. There a small variations in the chi-square for its various applications. The chi-square goodness-of-fit test tests whether observed frequency counts \\(O_j\\) of the \\(j \\in (0, 1, \\cdots k)\\) levels of a single categorical variable differ from expected frequency counts \\(E_j\\). \\(H_0\\) is \\(O_j = E_j\\). The chi-square independence test tests whether observed joint frequency counts \\(O_{ij}\\) of the \\(i \\in (0, 1, \\cdots I)\\) levels of categorical variable \\(Y\\) and the \\(j \\in (0, 1, \\cdots J)\\) levels of categorical variable \\(Z\\) differ from expected frequency counts \\(E_{ij}\\) under the independence model where \\(\\pi_{ij} = \\pi_{i+} \\pi_{+j}\\), the joint densities. \\(H_0\\) is \\(O_{ij} = E_{ij}\\). The chi-square homogeneity test tests whether frequency counts of the \\(R\\) levels of a categorical variable are distributed identically across \\(C\\) different populations. "],
["one-way-tables.html", "2.2 One-Way Tables", " 2.2 One-Way Tables These notes rely on PSU STATS 504 course notes. A one-way table is a frequency table for a single categorical variable. You usually construct a one-way table to test whether the frequency counts differ from a hypothesized distribution using the chi-square goodness-of-fit test. You may also simply want to construct a confidence interval around a proportion. Here is an example. A researcher crosses tall cut-leaf tomatoes with dwarf potato-leaf tomatoes, then classifies the (\\(n = 1,611\\)) offspring phenotypes. o &lt;- c(926, 288, 293, 104) cell_names &lt;- c(&quot;tall cut-leaf&quot;, &quot;tall potato-leaf&quot;, &quot;dwarf cut-leaf&quot;, &quot;dwarf potato-leaf&quot;) names(o) &lt;- cell_names print(o) ## tall cut-leaf tall potato-leaf dwarf cut-leaf dwarf potato-leaf ## 926 288 293 104 The four phenotypes are expected to occur with relative frequencies 9:3:3:1. pi &lt;- c(9, 3, 3, 1) / (9 + 3 + 3 + 1) print(pi) ## [1] 0.562 0.188 0.188 0.062 e &lt;- sum(o) * pi names(e) &lt;- cell_names print(e) ## tall cut-leaf tall potato-leaf dwarf cut-leaf dwarf potato-leaf ## 906 302 302 101 data.frame(O = o, E = e) %&gt;% rownames_to_column(var = &quot;i&quot;) %&gt;% pivot_longer(cols = -i, values_to = &quot;freq&quot;) %&gt;% group_by(name) %&gt;% mutate(pct = freq / sum(freq)) %&gt;% ungroup() %&gt;% ggplot(aes(x = i, y = freq, fill = name, label = paste0(round(freq, 0), &quot;\\n&quot;, scales::percent(pct, accuracy = 0.1))) ) + geom_col(position = position_dodge()) + geom_text(position = position_dodge(width = 0.9), size = 2.8) + theme_mf() + scale_fill_mf() + labs(title = &quot;Observed vs Expected&quot;, fill = &quot;&quot;) Do the observed phenotype counts conform to the expected proportions? This is a goodness-of-fit question because you are comparing frequencies from a single categorical variable to a set of hypothesized frequencies. 2.2.1 Chi-Square Goodness-of-Fit Test The chi-square goodness-of-fit test tests whether observed frequency counts \\(O_j\\) of the \\(J\\) levels of a categorical variable differ from expected frequency counts \\(E_j\\) in a sample. \\(H_0\\) is \\(O_j = E_j\\). There are two possible test statistics for this test, Pearson \\(X^2\\) and deviance \\(G^2\\). The sampling distributions of \\(X^2\\) and \\(G^2\\) approach the \\(\\chi_{J-1}^2\\) as the sample size \\(n \\rightarrow \\infty\\). It’s a good idea to calculate both test statistics. The Pearson goodness-of-fit statistic is \\[X^2 = \\sum \\frac{(O_j - E_j)^2}{E_j}\\] where \\(O_j = p_j n\\) and \\(E_j = \\pi_j n\\). There is a variation of the \\(X^2\\) statistic that corrects for small cell counts by subtracting 0.5 from each cell, the Yates Continuity Correction. \\[X^2 = \\sum \\frac{(O_j - E_j - 0.5)^2}{E_j}\\] The deviance statistic, aka likelihood-ratio chi-square test statistic, is \\[G^2 = 2 \\sum O_j \\log \\left[ \\frac{O_j}{E_j} \\right]\\] If the saturated model (the observed data represent the fit of the saturated model, the most complex model possible with the data) proportions \\(p_j\\) equal equal the expected proportions \\(\\pi_j\\), \\(X^2\\) and \\(G^2\\) will equal zero. Large values indicate the data do not agree well with the proposed model. You can perform a chi-square test of significance with the \\(G^2\\) and \\(X^2\\) test statistics with \\(dof\\) degrees of freedom (d.f.). The chi-square test is reliable when at least 80% of \\(E_j &gt;= 5\\). Calculate \\(X^2\\) as x2 &lt;- sum((o - e)^2 / e) = 1.47 and the \\(G^2\\) as g2 &lt;- 2 * sum(o * log(o / e)) = 1.48. The degrees of freedom are length(o) - 1 = 3. The chi-sq test p-values are nearly identical. pchisq(q = x2, df = dof, lower.tail = FALSE) ## [1] 0.69 pchisq(q = g2, df = dof, lower.tail = FALSE) ## [1] 0.69 chisq.test() performs the chi-square test of the Pearson test statistic. chisq.test(o, p = pi) ## ## Chi-squared test for given probabilities ## ## data: o ## X-squared = 1, df = 3, p-value = 0.7 The p-values based on the \\(\\chi^2\\) distribution with 3 d.f. are about 0.69, so the test fails to reject the null hypothesis that the observed frequencies are consistent with the theory. The plot of the chi-squared distribution shows \\(X^2\\) well outside the \\(\\alpha = 0.05\\) range of rejection. alpha &lt;- 0.05 dof &lt;- length(e) - 1 lrr = -Inf p_val &lt;- pchisq(q = x2, df = length(o) - 1, lower.tail = FALSE) urr = qchisq(p = alpha, df = dof, lower.tail = FALSE) data.frame(chi2 = seq(from = 0, to = 20, by = .1)) %&gt;% mutate(density = dchisq(x = chi2, df = dof)) %&gt;% mutate(rr = ifelse(chi2 &lt; lrr | chi2 &gt; urr, density, 0)) %&gt;% ggplot() + geom_line(aes(x = chi2, y = density), color = mf_pal(12)(12)[12], size = 0.8) + geom_area(aes(x = chi2, y = rr), fill = mf_pal(12)(12)[2], alpha = 0.8) + geom_vline(aes(xintercept = x2), color = mf_pal(12)(12)[11], size = 0.8) + labs(title = bquote(&quot;Chi-Square Goodness-of-Fit Test&quot;), subtitle = paste0(&quot;X^2=&quot;, round(x2,2), &quot;, &quot;, &quot;Critical value=&quot;, round(urr,2), &quot;, &quot;, &quot;p-value=&quot;, round(p_val,3), &quot;.&quot; ), x = &quot;chisq&quot;, y = &quot;Density&quot;) + theme(legend.position=&quot;none&quot;) + theme_mf() If you reject \\(H_0\\), you can inspect the residuals to learn which differences may have lead to rejecting the rejection. \\(X^2\\) and \\(G^2\\) are sums of squared cell comparisons, or “residuals”. The expected value of a \\(\\chi^2\\) random variable is its d.f., \\(k - 1\\), so the average residual size is \\((k - 1) / k\\). The typical residual should be within 2 \\(\\sqrt{(k - 1) / k}\\). e2_res &lt;- sqrt((o - e)^2 / e) g2_res &lt;- sign(o - e) * sqrt(abs(2 * o * log(o / e))) data.frame(e2_res) %&gt;% rownames_to_column() %&gt;% # pivot_longer(cols = e2_res:g2_res) %&gt;% ggplot(aes(x = rowname, y = e2_res)) + geom_point(size = 3, color = mf_pal(12)(12)[2], alpha = 0.8) + theme_mf() + labs(title = &quot;X^2 Residuals by Cell&quot;, color = &quot;&quot;, x = &quot;&quot;, y = &quot;&quot;) If you want to test whether the data conform to a particular distribution instead of some set of theoretical values, the test is nearly the same except for an adjustment to the d.f. Your first step is the estimate the distribution’s parameter(s). Then you perform the goodness of fit test, but with degrees of freedom reduced for each estimated parameter. For example, suppose you sample \\(n = 100\\) families and count the number of children. The count of children should be a Poisson random variable, \\(J \\sim Pois(\\lambda)\\). dat &lt;- data.frame(j = 0:5, o = c(19, 26, 29, 13, 10, 3)) The ML estimate for \\(\\lambda\\) is \\[\\hat{\\lambda} = \\frac{j_0 O_0 + j_1 O_1, + \\cdots j_k O_k}{O}\\] lambda_hat &lt;- sum(dat$j * dat$o) / sum(dat$o) print(lambda_hat) ## [1] 1.8 The probabilities for each possible count are \\[f(j; \\lambda) = \\frac{e^{-\\hat{\\lambda}} \\hat{\\lambda}^j}{j!}.\\] f &lt;- exp(-lambda_hat) * lambda_hat^dat$j / factorial(dat$j) E &lt;- f * sum(dat$o) dat &lt;- cbind(dat, e = E) dat %&gt;% rename(pois = e) %&gt;% pivot_longer(cols = -j, values_to = &quot;freq&quot;) %&gt;% group_by(name) %&gt;% mutate(pct = freq / sum(freq)) %&gt;% ungroup() %&gt;% ggplot(aes(x = fct_inseq(as.factor(j)), y = freq, fill = name, label = paste0(round(freq, 0), &quot;\\n&quot;, scales::percent(pct, accuracy = 0.1))) ) + geom_col(position = position_dodge()) + geom_text(position = position_dodge(width = 0.9), size = 2.8) + theme_mf() + scale_fill_mf() + labs(title = &quot;Observed vs Expected&quot;, fill = &quot;&quot;, x = &quot;children in family&quot;) Compare the expected values to the observed values with the \\(\\chi^2\\) goodness of fit test. In this case, \\(df = 6 - 1 - 1\\) because the estimated paramater \\(\\lambda\\) reduces d.f. by 1. (X2 &lt;- sum((dat$o - dat$e)^2 / dat$e)) ## [1] 2.8 (dof &lt;- nrow(dat) - 1 - 1) ## [1] 4 pchisq(q = X2, df = dof) ## [1] 0.42 Be careful of this adjustment to the d.f. because chisq.test() does not take this into account, and you cannot override the d.f.. chisq.test(dat$o, p = dat$e / sum(dat$e)) ## Warning in chisq.test(dat$o, p = dat$e/sum(dat$e)): Chi-squared approximation ## may be incorrect ## ## Chi-squared test for given probabilities ## ## data: dat$o ## X-squared = 3, df = 5, p-value = 0.7 2.2.2 Proportion Test A special case of the one-way table is the \\(2 \\times 1\\) table for a binomial random variable. When you calculate a single proportion \\(p\\), you can compare it to a hypothesized \\(\\pi_0\\), or create a confidence interval around the estimate. Suppose a company claims to resolve at least 70% of maintenance requests within 24 hours. In a random sample of \\(n = 50\\) repair requests, the company resolves \\(O_1 = 33\\) (\\(p_1 = 66\\%)\\) within 24 hours. At a 5% level of significance, is the maintenance company’s claim valid? o &lt;- c(33, 17) n &lt;- sum(o) cell_names &lt;- c(&quot;resolved&quot;, &quot;not resolved&quot;) names(o) &lt;- cell_names print(o) ## resolved not resolved ## 33 17 The null hypothesis is that the maintenance company resolves \\(\\pi_0 = 0.70\\) of requests within 24 hours, \\(H_0: \\pi = \\pi_0\\) with alternative hypothesis \\(H_a: \\pi &lt; \\pi_0\\). This is a left-tailed test with an \\(\\alpha = 0.05\\) level of significance. pi_0 &lt;- 0.70 alpha &lt;- 0.05 The sample is independently drawn without replacement from &lt;10% of the population (by assumption) and there were &gt;=5 successes, so you can use the Clopper-Pearson exact binomial test. Clopper-Pearson inverts two single-tailed binomial tests at the desired alpha. binom.test(x = o, p = pi_0, alternative = &quot;less&quot;, conf.level = 1 - alpha) ## ## Exact binomial test ## ## data: o ## number of successes = 33, number of trials = 50, p-value = 0.3 ## alternative hypothesis: true probability of success is less than 0.7 ## 95 percent confidence interval: ## 0.00 0.77 ## sample estimates: ## probability of success ## 0.66 There is insufficient evidence (p = 0.3161) to reject \\(H_0\\) that true probability of success is less than 0.7. x &lt;- c(0:50) p_x &lt;- dbinom(x = x, size = n, prob = pi_0) observed &lt;- factor(if_else(x == o[1], 1, 0)) data.frame(x, p_x, observed) %&gt;% ggplot(aes(x = x, y = p_x, fill = observed)) + geom_col() + theme_mf() + scale_fill_mf() + labs(title = &quot;Exact Binomial&quot;) There were &gt;=5 failures, &gt;=30 observations, and the measured probability of success was within (.2,.80), so you can also use the Wald normal approximation method where \\(\\pi = p \\pm z_{\\alpha/2} SE\\) and \\(Z = (p - \\pi_0) / SE\\) where \\(SE = \\sqrt{\\pi_0 (1 - \\pi_0) / n}\\). p &lt;- o[1] / sum(o) se &lt;- sqrt(pi_0 * (1 - pi_0) / sum(o)) z &lt;- (p - pi_0) / se pnorm(q = p, mean = pi_0, sd = se, lower.tail = TRUE) ## resolved ## 0.27 Again, there is insufficient evidence (p = 0.2685) to reject \\(H_0\\) that true probability of success is less than 0.7. The 95% CI around the measured p = 0.66 is z_alpha &lt;- qnorm(0.95, mean = p, sd = se, lower.tail = FALSE) c(0, p + z_alpha * se) ## resolved ## 0.0 0.7 "],
["two-way-tables.html", "2.3 Two-Way Tables", " 2.3 Two-Way Tables These notes rely on PSU STATS 504 course notes. A two-way frequency table is a frequency table for two categorical variables. You usually construct a two-way table to test whether the frequency counts in one categorical variable differ from the other categorical variable using the chi-square independence test. If there is a significant difference (i.e., the variables are related), then describe the relationship with an analysis of the residuals, calculations of measures of association (difference in proportions, relative risk, or odds ratio), and partition tests. Here are three case studies that illustrate the concepts. The first is a simple 2x2 table. The second is a 3x2 table that extends some of the concepts. The third is a 2x4 table where one factor is ordinal. Study 1: “Vitamin C” 2x2 Table. A double blind study investigated whether vitamin C prevents common colds on a sample of n = 279 persons. This study has two categorical variables each with two levels, a 2x2 two way table. vitc_o &lt;- matrix( c(31, 17, 109, 122), ncol = 2, dimnames = list( treat = c(&quot;Placebo&quot;, &quot;VitaminC&quot;), resp = c(&quot;Cold&quot;, &quot;NoCold&quot;) ) ) vitc_o %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot; &quot;) %&gt;% janitor::adorn_totals(where = c(&quot;row&quot;, &quot;col&quot;)) ## Cold NoCold Total ## Placebo 31 109 140 ## VitaminC 17 122 139 ## Total 48 231 279 Study 2: “Smoking” 3x2 Table. An analysis classifies n = 5375 high school students by their smoking behavior and the smoking behavior of their parents. smoke_o &lt;- matrix( c(400, 416, 188, 1380, 1823, 1168), ncol = 2, dimnames = list( parents = c(&quot;Both&quot;, &quot;One&quot;, &quot;Neither&quot;), student = c(&quot;Smoker&quot;, &quot;Non-smoker&quot;)) ) smoke_o %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot; &quot;) %&gt;% janitor::adorn_totals(where = c(&quot;row&quot;, &quot;col&quot;)) ## Smoker Non.smoker Total ## Both 400 1380 1780 ## One 416 1823 2239 ## Neither 188 1168 1356 ## Total 1004 4371 5375 Study 3: “CHD” Ordinal Table. A study of classified n = 1329 patients by cholesterol level and whether they had been diagnosed with coronary heart disease (CHD). # tribble() is a little easier. chd_o &lt;- tribble( ~L_0_199, ~L_200_219, ~L_220_259, ~L_260p, 12, 8, 31, 41, 307, 246, 439, 245 ) %&gt;% as.matrix() rownames(chd_o) &lt;- c(&quot;CHD&quot;, &quot;No CHD&quot;) chd_o %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot; &quot;) %&gt;% janitor::adorn_totals(where = c(&quot;row&quot;, &quot;col&quot;)) ## L_0_199 L_200_219 L_220_259 L_260p Total ## CHD 12 8 31 41 92 ## No CHD 307 246 439 245 1237 ## Total 319 254 470 286 1329 2.3.1 Chi-Square Independence Test The chi-square independence test tests whether observed joint frequency counts \\(O_{ij}\\) differ from expected frequency counts \\(E_{ij}\\) under the independence model (the model of independent explanatory variables, \\(\\pi_{ij} = \\pi_{i+} \\pi_{+j}\\). \\(H_0\\) is \\(O_{ij} = E_{ij}\\). There are two possible test statistics for this test, Pearson \\(X^2\\) (and the continuity adjusted \\(X^2\\)), and deviance \\(G^2\\). As \\(n \\rightarrow \\infty\\) their sampling distributions approach \\(\\chi_{df}^2\\) with degrees of freedom (df) equal to the saturated model df \\(I \\times J - 1\\) minus the independence model df \\((I - 1) + (J - 1)\\), which you can algebraically solve for \\(df = (I - 1)(J - 1)\\). The Pearson goodness-of-fit statistic is \\[X^2 = \\sum \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\] where \\(O_{ij}\\) is the observed count, and \\(E_{ij}\\) is the product of the row and column marginal probabilities. For the Vitamin C study, \\(X^2\\) is vitc_e &lt;- sum(vitc_o) * prop.table(vitc_o, 1) * prop.table(vitc_o, 2) X2 &lt;- sum((vitc_o - vitc_e)^2 / vitc_e) print(X2) ## [1] 4.8 and the deviance statistic is \\[G^2 = 2 \\sum_{ij} O_{ij} \\log \\left( \\frac{O_{ij}}{E_{ij}} \\right)\\] G2 &lt;- - 2 * sum(vitc_o * log(vitc_o / vitc_e)) print(G2) ## [1] 4.9 \\(X^2\\) and \\(G^2\\) increase with the disagreement between the saturated model proportions \\(p_{ij}\\) and the independence model proportions \\(\\pi_{ij}\\). The degrees of freedom is vitc_dof &lt;- (nrow(vitc_o) - 1) * (ncol(vitc_o) - 1) print(vitc_dof) ## [1] 1 The associated p-values are pchisq(q = G2, df = vitc_dof, lower.tail = FALSE) ## [1] 0.027 pchisq(q = X2, df = vitc_dof, lower.tail = FALSE) ## [1] 0.028 The chisq.test() function applies the Yates continuity correcton by default to correct for situations with small cell counts. The Yates continuity correction subtracts 0.5 from the \\(O_{ij} - E_{ij}\\) differences. Set correct = FALSE to suppress Yates. vitc_chisq_test &lt;- chisq.test(vitc_o, correct = FALSE) print(vitc_chisq_test) ## ## Pearson&#39;s Chi-squared test ## ## data: vitc_o ## X-squared = 5, df = 1, p-value = 0.03 The Yates correction yields more conservative p-values. chisq.test(vitc_o) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: vitc_o ## X-squared = 4, df = 1, p-value = 0.04 These p-values are evidence for rejecting the independence model. Here is the chi-square test applied to the CHD data. Recall this data set is 4x2, so the degrees of freedom are \\((4-1)(2-1) = 3\\). The Yates continuity correction does not apply to data other than 2x2, so the correct = c(TRUE, FALSE) has no effect in chisq.test(). (chd_chisq_test &lt;- chisq.test(chd_o)) ## ## Pearson&#39;s Chi-squared test ## ## data: chd_o ## X-squared = 35, df = 3, p-value = 0.0000001 The p-value is very low, so reject the null hypothesis of independence. This demonstrates that a relationship exists between cholesterol and CHD. Now you should describe that relationship by evaluating the (i) residuals, (ii) measures of association, and (iii) partitioning chi-square. 2.3.2 Residuals Analysis If the chi-squared independence test rejects \\(H_0\\) of identical frequency distributions, the next step is to identify which cells may be driving the lack of fit. The Pearson residuals in the two-way table are \\[r_{ij} = \\frac{O_{ij} - E_{ij}}{\\sqrt{E_{ij}}}\\] where \\(X^2 = \\sum{r_{ij}}\\). The \\(r_{ij}\\) values have a normal distribution with mean 0, but with unequal variances. The standardized Pearson residual for a two-way table is \\[r_{ij} = \\frac{O_{ij} - E_{ij}}{\\sqrt{E_{ij}(1 - p_{i+})(1 - p_{+j})}}\\] and the \\(r_{ij}\\) values do have a \\(\\sim N(0, 1)\\) distribution. \\(r_{ij}^2 &gt; 4\\) is a sign of lack of fit. The chissq.test() object includes residuals that match the manual calculation. (vitc_o - vitc_e) / sqrt(vitc_e) ## resp ## treat Cold NoCold ## Placebo -1.4 0.64 ## VitaminC 1.4 -0.64 vitc_chisq_test$residuals ## resp ## treat Cold NoCold ## Placebo 1.4 -0.64 ## VitaminC -1.4 0.64 It also includes stdres that match the manual standardized calculation. (well, no it doesn’t, but I don’t know what my mistake is.) (vitc_e - vitc_o) / sqrt(vitc_e * (1 - prop.table(vitc_o, margin = 1)) * (1 - prop.table(vitc_o, margin = 2)) ) ## resp ## treat Cold NoCold ## Placebo 2.7 -1.9 ## VitaminC -1.9 2.7 vitc_chisq_test$stdres ## resp ## treat Cold NoCold ## Placebo 2.2 -2.2 ## VitaminC -2.2 2.2 Here are the squared Pearson residuals for the CHD data. The squared Pearson residuals for CHD 0-199, 200-219, and 260+ are greater than 4, and seem to be driving the lack of independence. chd_chisq_test$residuals^2 ## L_0_199 L_200_219 L_220_259 L_260p ## CHD 4.60 5.22 0.0725 22.7 ## No CHD 0.34 0.39 0.0054 1.7 2.3.3 Difference in Proportions The difference in proportions measure is the difference in the probabilities of characteristic \\(Z\\) conditioned on two groups \\(Y = 1\\) and \\(Y = 2\\): \\(\\delta = \\pi_{1|1} - \\pi_{1|2}\\). In social sciences and epidemiology \\(\\pi_{1|1}\\) and \\(\\pi_{1|2}\\) are sometimes referred to as “risk” values. The point estimate for \\(\\delta\\) is \\(r = p_{1|1} - p_{1|2}\\). Under the normal approximation method, the sampling distribution of the difference in population proportions has a normal distribution centered at \\(d\\) with variance \\(Var(\\delta)\\). The point estimate for \\(Var(\\delta)\\) is \\(Var(d)\\). \\[Var(d) = \\frac{p_{1|1} (1 - p_{1|1})}{n_{1+}} + \\frac{p_{1|2} (1 - p_{1|2})}{n_{2+}}\\] In the vitamin C acid example, \\(\\delta\\) is the difference in the row conditional frequencies. p &lt;- prop.table(vitc_o, margin = 1) d &lt;- p[2, 1] - p[1, 1] print(d) ## [1] -0.099 The variance is var_d &lt;- (p[2, 1])*(1 - p[2, 1]) / sum(vitc_o[2, ]) + (p[1, 1])*(1 - p[1, 1]) / sum(vitc_o[1, ]) print(var_d) ## [1] 0.002 The 95% CI is d + c(-1, 1) * qnorm(.975) * sqrt(var_d) ## [1] -0.187 -0.011 This is how prop.test() without the continuity correction calculates the confidence interval. (prop.test.result &lt;- prop.test(vitc_o, correct = FALSE)) ## ## 2-sample test for equality of proportions without continuity ## correction ## ## data: vitc_o ## X-squared = 5, df = 1, p-value = 0.03 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## 0.011 0.187 ## sample estimates: ## prop 1 prop 2 ## 0.22 0.12 lcl &lt;- -round(prop.test.result$conf.int[2], 3) ucl &lt;- -round(prop.test.result$conf.int[1], 3) data.frame(d_i = -300:300 / 1000) %&gt;% mutate(density = dnorm(x = d_i, mean = d, sd = sqrt(var_d))) %&gt;% mutate(rr = ifelse(d_i &lt; lcl | d_i &gt; ucl, density, 0)) %&gt;% ggplot() + geom_line(aes(x = d_i, y = density)) + geom_area(aes(x = d_i, y = rr, fill = mf_pal(12)(12)[2]), alpha = 0.8) + geom_vline(aes(xintercept = d), color = &quot;blue&quot;) + theme_mf() + labs(title = bquote(&quot;Difference in Proportions Confidence Interval&quot;), subtitle = paste0( &quot;d = &quot;, round(d, 3) ), x = &quot;d&quot;, y = &quot;Density&quot;) + theme(legend.position=&quot;none&quot;) The normal approximation method applies when the central limit theorem conditions hold: the sample is independently drawn (random sampling without replacement from \\(n &lt; 10\\%\\) of the population in observational studies, or random assignment in experiments), there are at least \\(n_i p_i &gt;= 5\\) successes and \\(n_i (1 - p_i) &gt;= 5\\) failures for each group, the sample sizes are both \\(&gt;=30\\), and the probability of success for each group is not extreme, \\((0.2, 0.8)\\). Test \\(H_0: d = \\delta_0\\) for some hypothesized population \\(\\delta\\) (usually 0) with test statistic \\[Z = \\frac{d - \\delta_0}{se_{d}}\\] where \\[se_{d} = \\sqrt{p (1 - p) \\left( \\frac{1}{n_{1+}} + \\frac{1}{n_{2+}} \\right)}\\] approximates \\(se_{\\delta_0}\\) where \\(p\\) is the pooled proportion \\[p = \\frac{n_{11} + n_{21}}{n_{1+} + n_{2+}}.\\] p_pool &lt;- (vitc_o[1, 1] + vitc_o[2, 1]) / sum(vitc_o) se_d &lt;- sqrt(p_pool * (1 - p_pool) * (1 / sum(vitc_o[1, ]) + 1 / sum(vitc_o[2, ]))) z &lt;- (d - 0) / se_d pnorm(z) * 2 ## [1] 0.028 lrr = qnorm(p = .05/2, mean = 0, sd = se_d, lower.tail = TRUE) urr = qnorm(p = .05/2, mean = 0, sd = se_d, lower.tail = FALSE) data.frame(d_i = -300:300 / 1000) %&gt;% mutate(density = dnorm(x = d_i, mean = 0, sd = se_d)) %&gt;% mutate(rr = ifelse(d_i &lt; lrr | d_i &gt; urr, density, 0)) %&gt;% ggplot() + geom_line(aes(x = d_i, y = density)) + geom_area(aes(x = d_i, y = rr, fill = mf_pal(12)(12)[2]), alpha = 0.8) + geom_vline(aes(xintercept = d), color = &quot;blue&quot;) + geom_vline(aes(xintercept = 0), color = &quot;black&quot;) + theme_mf() + labs(title = &quot;Hypothesis Test of Difference in Proportions&quot;, subtitle = paste0( &quot;d = &quot;, round(d, 3), &quot; (Z = &quot;, round(z, 2), &quot;, p = &quot;, round(pnorm(z) * 2, 4), &quot;).&quot; ), x = &quot;d&quot;, y = &quot;Density&quot;) + theme(legend.position=&quot;none&quot;) The null hypothesis \\(H_0: \\delta_0 = 0\\) is equivalent to saying that two variables are independent, \\(\\pi_{1|1} = \\pi_{1|2}\\), so you can also use the \\(\\chi^2\\) or \\(G^2\\) test for independence in a 2 × 2. That’s what prop.test() is doing. The square of the z-statistic is algebraically equal to \\(\\chi^2\\). The two-sided test comparing \\(Z\\) to a \\(N(0, 1)\\) is identical to comparing \\(\\chi^2\\) to a chi-square distribution with df = 1. Compare the \\(Z^2\\) to the output from prop.test(). z^2 ## [1] 4.8 prop.test.result$statistic ## X-squared ## 4.8 The difference in proportions is easy to interpret, but when \\(Z = 1\\) is a rare event, the individual probabilities \\(\\pi_{1|1}\\) and \\(\\pi_{1|2}\\) are both small and \\(\\delta\\) is nearly zero even when the effect is strong. In the CHD study, two of the conditional probabilities of CHD within the four cholesterol groups are similar, 0-199 (0.038) and 200-219 (.031). round(prop.table(chd_o, margin = 2), 3) ## L_0_199 L_200_219 L_220_259 L_260p ## CHD 0.038 0.031 0.066 0.14 ## No CHD 0.962 0.969 0.934 0.86 Is the difference in these proportions statistically signficant? You can test this with the difference in proportions test or a chisq test. prop.test(t(chd_o[, c(1:2)])) ## ## 2-sample test for equality of proportions with continuity correction ## ## data: t(chd_o[, c(1:2)]) ## X-squared = 0.03, df = 1, p-value = 0.9 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## -0.027 0.040 ## sample estimates: ## prop 1 prop 2 ## 0.038 0.031 chisq.test(t(chd_o[, c(1:2)])) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: t(chd_o[, c(1:2)]) ## X-squared = 0.03, df = 1, p-value = 0.9 You could go on to try other pairwise tests to establish which levels differ from the others. 2.3.4 Relative Risk The relative risk measure is the ratio of the probabilities of characteristic \\(Z\\) conditioned on two groups \\(Y = 1\\) and \\(Y = 2\\): \\(\\rho = \\pi_{1|1} / \\pi_{1|2}\\). In social sciences and epidemiology \\(\\rho\\) is sometimes referred to as the “relative risk”. The point estimate for \\(\\rho\\) is \\(r = p_{1|1} / p_{1|2}\\). Because \\(\\rho\\) is non-negative, a normal approximation for \\(\\log \\rho\\) has a less skewed distribution than \\(\\rho\\). The approximate variance of \\(\\log \\rho\\) is \\[Var(\\log \\rho) = \\frac{1 - \\pi_{11}/\\pi_{1+}}{n_{1+}\\pi_{11}/\\pi_{1+}} + \\frac{1 - \\pi_{21}/\\pi_{2+}}{n_{2+}\\pi_{21}/\\pi_{2+}}\\] and is estimated by \\[Var(\\log r) = \\left( \\frac{1}{n_{11}} - \\frac{1}{n_{1+}} \\right) + \\left( \\frac{1}{n_{21}} - \\frac{1}{n_{2+}} \\right)\\] In the vitamin C acid example, \\(r\\) is the ratio of the row conditional frequencies. vitc_prop &lt;- prop.table(vitc_o, margin = 1) vitc_risk &lt;- vitc_prop[2, 1] / vitc_prop[1, 1] print(vitc_risk) ## [1] 0.55 The variance is vitc_risk_var &lt;- 1 / vitc_o[1, 1] - 1 / sum(vitc_o[1, ]) + 1 / vitc_o[2, 1] - 1 / sum(vitc_o[2, ]) print(vitc_risk_var) ## [1] 0.077 The 95% CI is exp(log(vitc_risk) + c(-1, 1) * qnorm(.975) * sqrt(vitc_risk_var)) ## [1] 0.32 0.95 Thus, at 0.05 level, you can reject the independence model. People taking vitamin C are half as likely to catch a cold. In the CHD study, you could summarize the relationship between CHD and cholesterol level by a set of three relative risks using 0-199 as the baseline: 200–219 versus 0–199, 220–259 versus 0–199, and 260+ versus 0–199. (chd_prop &lt;- prop.table(chd_o, margin = 2)) ## L_0_199 L_200_219 L_220_259 L_260p ## CHD 0.038 0.031 0.066 0.14 ## No CHD 0.962 0.969 0.934 0.86 (chd_risk &lt;- chd_prop[1, ] / chd_prop[1, 1]) ## L_0_199 L_200_219 L_220_259 L_260p ## 1.00 0.84 1.75 3.81 2.3.5 Odds Ratio The odds ratio is the most commonly used measure of association. It is also a natural parameter for many of the log-linear and logistic models. The odds is the ratio of probabilities of “success” and “failure”. When conditioned on a variable, the odds ratio is \\[\\theta = \\frac{\\pi_{1|1} / \\pi_{2|1}} {\\pi_{1|2} / \\pi_{2|2}}\\] and is estimated by the sample frequencies \\[\\hat{\\theta} = \\frac{n_{11} n_{22}} {n_{12} n_{21}}\\] The log-odds ratio has a better normal approximation than the odds ratio, so define the confidence interval on the log scale. \\[Var(\\log \\hat{\\theta}) = \\frac{1}{n_{11}} + \\frac{1}{n_{12}} + \\frac{1}{n_{21}} + \\frac{1}{n_{22}}\\] For the Vitamin C example, the odds of getting a cold after taking a placebo pill are \\(0.22 / 0.78 = 0.28\\) and the odds of getting a cold after taking Vitamin C are \\(0.12 / 0.88 = 0.14\\). vitc_odds &lt;- vitc_prop[, 1] / vitc_prop[, 2] print(vitc_odds) ## Placebo VitaminC ## 0.28 0.14 The odds of getting a cold given vitamin C are \\(0.14 / 0.28 = 0.49\\) times the odds of getting cold given a placebo. vitc_theta_hat &lt;- vitc_odds[2] / vitc_odds[1] print(vitc_theta_hat) ## VitaminC ## 0.49 with variance (var_vitc_theta_hat &lt;- sum(1 / vitc_o)) ## [1] 0.11 The 95% CI is z_alpha &lt;- qnorm(p = 0.975) exp(log(vitc_theta_hat) + c(-1, 1) * z_alpha * sqrt(var_vitc_theta_hat)) ## [1] 0.26 0.93 Keep in mind the following properties of odds ratios. You can convert an odds pack to probabilities by solving \\(\\pi / (1 - \\pi)\\) for \\(\\pi = odds / (1 + odds)\\). If two variables are independent, then the conditional probabilities \\(\\pi_{1|1}\\) and \\(\\pi_{1|2}\\) will be equal and therefore the odds ratio will equal 1. If \\(\\pi_{1|1} &gt; \\pi_{1|2}\\) then the odds ratio will be \\(1 &lt; \\theta &lt; \\infty\\). If \\(\\pi_{1|1} &lt; \\pi_{1|2}\\) then the odds ratio will be \\(0 &lt; \\theta &lt; 1\\). the sample odds ratio will equal \\(0\\) or \\(\\infty\\) if any \\(n_{ij} = 0\\). If you have any empty cells add 1/2 to each cell count. 2.3.6 Partitioning Chi-Square Besides looking at the residuals or the measures of association, another way to describe the effects is to form a sequence of smaller tables by combining or collapsing rows and/or columns in a meaningful way. For the smoking study, you might ask whether a student is more likely to smoke if either parent smokes. Collapse the first two rows (1 parent smokes, both parents smoke) and run the chi-squared test. smoke_clps_1 &lt;- rbind(smoke_o[1, ] + smoke_o[2, ], smoke_o[3, ]) smoke_clps_1_theta &lt;- (smoke_clps_1[1, 1] / smoke_clps_1[1, 2]) / (smoke_clps_1[2, 1] / smoke_clps_1[2, 2]) (smoke_clps_1_chisq &lt;- chisq.test(smoke_clps_1)) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: smoke_clps_1 ## X-squared = 27, df = 1, p-value = 0.0000002 The estimated odds a student smokes if at least one parent smokes is 1.58 (X^2 = 27.3, p = 0. Or you may ask, whether among students with at least one smoking parent, there is a difference between those with one smoking parent and those with two smoking parents. Answer this by running a chi-square test on the first two rows of the data table, discarding the row where neither parent smokes. smoke_clps_2_theta &lt;- (smoke_o[1, 1] / smoke_o[1, 2]) / (smoke_o[2, 1] / smoke_o[2, 2]) (smoke_clps_2_chisq &lt;- chisq.test(smoke_o[c(1:2), ])) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: smoke_o[c(1:2), ] ## X-squared = 9, df = 1, p-value = 0.003 The estimated odds a student smokes if both parents smoke compared to one parent is 1.27 (X^2 = 9, p = 0. 2.3.7 Correlation When classification is ordinal, there may exist a linear trend among the levels of the characteristics. Measure the linear relationship with Pearson’s correlation coefficient, or its nonparametric alternatives, Spearman’s correlation coefficient and Kendall’s tau. In the CHD study, the four levels of cholesterol (0-199, 200-219, 220-259, and 260+) may be treated as ordinal data. You can also treat the presence of heart disease as ordinal. The Pearson correlation, \\[r = \\frac{cov(X, Y)}{s_X s_Y}\\] is #dim=dim(table) rbar &lt;- sum(margin.table(chd_o, 1) * c(1, 2)) / sum(chd_o) rdif &lt;- c(1, 2) - rbar cbar &lt;- sum(margin.table(chd_o, 2) * c(1, 2, 3, 4)) / sum(chd_o) cdif &lt;- c(1, 2, 3, 4) - cbar ssr &lt;- sum(margin.table(chd_o, 1) * (rdif^2)) ssc &lt;- sum(margin.table(chd_o, 2) * (cdif^2)) ssrc &lt;- sum(t(chd_o * rdif) * cdif) pcor &lt;- ssrc / (sqrt(ssr * ssc)) pcor ## [1] -0.14 M2 &lt;- (sum(chd_o) - 1) * pcor^2 M2 ## [1] 26 "],
["k-way-tables.html", "2.4 K-Way Tables", " 2.4 K-Way Tables These notes rely on PSU STATS 504 course notes. A k-way table has k independent variables. Each cell in the k-way table has joint probability \\(\\pi_{ij..k}\\). You can collapse a k-way table along a dimension to create a marginal table and the resulting joint probabilities in the marginal table are referred to as marginal distributions. Alternatively, you can consider a single level of a dimension, creating a conditional distribution. Here are two case studies to illustrate the concepts. Study 1: Death Penalty. Data is collected for n = 326 murder cases along three dimensions: defendant’s color, victim’s color, and whether or not the defendant received the death penalty. deathp &lt;- array(c(19, 132, 11, 52, 0, 9, 6, 97), dim=c(2,2,2)) dimnames(deathp) &lt;- list( DeathPen = c(&quot;yes&quot;,&quot;no&quot;), Defendant=c(&quot;white&quot;,&quot;black&quot;), Victim=c(&quot;white&quot;,&quot;black&quot;) ) ftable(deathp, row.vars = c(&quot;Defendant&quot;, &quot;Victim&quot;), col.vars = &quot;DeathPen&quot;) ## DeathPen yes no ## Defendant Victim ## white white 19 132 ## black 0 9 ## black white 11 52 ## black 6 97 Study 2: Boy Scouts. Data is collected for n = 800 Boy Scouts along three dimensions: socioeconomic status, scout status, and delinquency status. scout &lt;- expand.grid( delinquent = c(&quot;no&quot;,&quot;yes&quot;), scout = c(&quot;no&quot;, &quot;yes&quot;), SES = c(&quot;low&quot;, &quot;med&quot;,&quot;high&quot;) ) scout &lt;- cbind(scout, count = c(169,42,43,11,132,20,104,14,59,2,196,8)) scout_ct &lt;- xtabs(count ~ ., scout) ftable(scout_ct, row.vars = c(&quot;SES&quot;, &quot;scout&quot;), col.vars = &quot;delinquent&quot;) ## delinquent no yes ## SES scout ## low no 169 42 ## yes 43 11 ## med no 132 20 ## yes 104 14 ## high no 59 2 ## yes 196 8 2.4.1 Odds Ratio In the DeathP study, the marginal odds ratio is 1.18, meaning the odds of death penalty for a white defendant are 1.18 times as high as they are for a black defendant. deathp_m &lt;- margin.table(deathp, margin = c(1, 2)) deathp_p &lt;- prop.table(deathp_m, margin = 2) deathp_odds &lt;- deathp_p[1, ] / deathp_p[2, ] deathp_or &lt;- deathp_odds[1] / deathp_odds[2] print(deathp_or) ## white ## 1.2 The conditional odds ratio given the victim is white is 0.68, and 0.79 given the victim is black, meaning the odds of death penalty for a white defendant are 0.68 times as high as they are for a black defendent if the victim is white, and 0.79 times as high as they are for a black defendent if the victim is black. deathp_m &lt;- margin.table(deathp[, , 1], margin = c(2, 1)) deathp_p &lt;- prop.table(deathp_m, margin = 2) deathp_odds &lt;- deathp_p[1, ] / deathp_p[2, ] deathp_or &lt;- deathp_odds[1] / deathp_odds[2] print(deathp_or) ## yes ## 0.68 deathp_m &lt;- margin.table(deathp[, , 2], margin = c(2, 1)) + 0.5 deathp_p &lt;- prop.table(deathp_m, margin = 2) deathp_odds &lt;- deathp_p[1, ] / deathp_p[2, ] deathp_or &lt;- deathp_odds[1] / deathp_odds[2] print(deathp_or) ## yes ## 0.79 Notice above that the second margin table had a zero in one cell. To get an odds ratio in this case, the convention is to add 0.5 to all cells. Interesting that the marginal odds of a white defendent receiving the death penalty are &gt;1, but the conditional odds of a white defendent receiving the death penalty given the race of the victim are &lt;1. Simpson’s paradox is the phenomenon that a pair of variables can have marginal association and partial (conditional) associations in opposite direction. Another way to think about this is that the nature and direction of association changes due to presence or absence of a third (possibly confounding) variable. 2.4.2 Chi-Square Independence Test There are several ways to think about “independence” when dealing with a k-way table. Mutual independence. All variables are independent from each other, (A, B, C). Joint independence. Two variables are jointly independent of the third, (AB, C). Marginal independence. Two variables are independent if you ignore the third, (A, B). Conditional independence Two variables are independent given the third, (AC, BC). Homogeneous associations Conditional (partial) odds-ratios are not related on the value of the third, (AB, AC, BC). Under the assumption that the model of independence is true, once you know the marginal probability values, we have enough information to estimate all unknown cell probabilities. Because each of the marginal probability vectors must add up to one, the number of free parameters in the model is (I − 1) + (J − 1) + (K −I ). This is exactly like the two-way table 2.4.2.1 Mutual Independence The simplest model is that ALL variables are independent of one another (A, B, C). The joint probabilities are equal to the product of the marginal probabilities, \\(\\pi_{ijk} = \\pi_i \\pi_j \\pi_k\\). In terms of odds ratios, the model (A, B, C) implies that the odds ratios in the marginal tables A × B, B × C, and A × C are equal to 1. The chi-square independence test tests whether observed joint frequency counts \\(O_{ijk}\\) differ from expected frequency counts \\(E_{ijk}\\) under the independence model \\(\\pi_{ijk} = \\pi_{i+} \\pi_{+j} \\pi_{k+}\\). \\(H_0\\) is \\(O_{ijk} = E_{ijk}\\). This is essentially a one-way table chi-squre goodness-of-fit test. For the Boy Scouts study, \\(X^2\\) and its associated p-value is scout_e &lt;- array(NA, dim(scout_ct)) for (i in 1:dim(scout_ct)[1]) { for (j in 1:dim(scout_ct)[2]) { for (k in 1:dim(scout_ct)[3]) { scout_e[i,j,k] &lt;- ( margin.table(scout_ct, 3)[k] * margin.table(scout_ct, 2)[j] * margin.table(scout_ct, 1)[i]) / (sum(scout_ct))^2 } } } scout_df &lt;- (prod(dim(scout_ct)) - 1) - sum(dim(scout_ct) - 1) scout_x2 &lt;- sum((scout_ct - scout_e)^2 / scout_e) print(scout_x2) ## [1] 215 pchisq(scout_x2, df = scout_df, lower.tail = FALSE) ## [1] 0.00000000000000000000000000000000000000000079 You can also get X^2 this way, although longer code. x &lt;- scout %&gt;% group_by(delinquent) %&gt;% summarize(n = sum(count)) %&gt;% ungroup() %&gt;% mutate(p = n / sum(n)) ## `summarise()` ungrouping output (override with `.groups` argument) p_d &lt;- x$p names(p_d) &lt;- x$delinquent x &lt;- scout %&gt;% group_by(scout) %&gt;% summarize(n = sum(count)) %&gt;% ungroup() %&gt;% mutate(p = n / sum(n)) ## `summarise()` ungrouping output (override with `.groups` argument) p_b &lt;- x$p names(p_b) &lt;- x$scout x &lt;- scout %&gt;% group_by(SES) %&gt;% summarize(n = sum(count)) %&gt;% ungroup() %&gt;% mutate(p = n / sum(n)) ## `summarise()` ungrouping output (override with `.groups` argument) p_s &lt;- x$p names(p_s) &lt;- x$SES scout_e &lt;- scout %&gt;% mutate(e = as.numeric(p_d[delinquent] * p_b[scout] * p_s[SES] * sum(count))) x2 &lt;- sum((scout_e$count - scout_e$e)^2 / scout_e$e) print(x2) ## [1] 215 The deviance statistic is scout_g2 &lt;- 2 * sum(scout_ct * log(scout_ct / scout_e$e)) print(scout_g2) ## [1] 219 pchisq(scout_g2, df = scout_df, lower.tail = FALSE) ## [1] 0.00000000000000000000000000000000000000000013 Safe to say the mutual independence model does not fit. chisq.test() does not test the complete independence model for k-way tables. Instead, conduct tests on all \\({3 \\choose 2} = 3\\) embedded 2-way tables. For the Boy Scouts study, you can conduct a series of 2-way chi-sq tests. SES*scout: (scout_ses_scout &lt;- margin.table(scout_ct, c(3, 2))) ## scout ## SES no yes ## low 211 54 ## med 152 118 ## high 61 204 chisq.test(scout_ses_scout) ## ## Pearson&#39;s Chi-squared test ## ## data: scout_ses_scout ## X-squared = 172, df = 2, p-value &lt;0.0000000000000002 SES*delinquent (scout_ses_delinquent &lt;- margin.table(scout_ct, c(3, 1))) ## delinquent ## SES no yes ## low 212 53 ## med 236 34 ## high 255 10 chisq.test(scout_ses_delinquent) ## ## Pearson&#39;s Chi-squared test ## ## data: scout_ses_delinquent ## X-squared = 33, df = 2, p-value = 0.00000007 delinquent*scout (scout_delinquent_scout &lt;- margin.table(scout_ct, c(1, 2))) ## scout ## delinquent no yes ## no 360 343 ## yes 64 33 chisq.test(scout_delinquent_scout) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: scout_delinquent_scout ## X-squared = 7, df = 1, p-value = 0.009 The odds ratio for scout delinquency vs non-scout delinquency is 0.54 with 95% CI (0.347, 0.845), so reject the null hypothesis that boy scout status and delinquent status are independent of one another (OR = 1), and thus that boy scout status, delinquent status, and socioeconomic status are not mutually independent. (scout_or &lt;- vcd::oddsratio(scout_delinquent_scout, log = FALSE)) ## odds ratios for delinquent and scout ## ## [1] 0.54 confint(scout_or) ## 2.5 % 97.5 % ## no:yes/no:yes 0.35 0.84 2.4.2.2 Joint Independence The joint independence model is that two variables are jointly independent of a third (AB, C). The joint probabilities are equal to the product of the AB marginal probability and the C marginal probability, \\(\\pi_{ijk} = \\pi_{ij} \\pi_k\\). The degrees of freedom in the associated chi-sq test are the number of free parameters in the saturated model minus the number of free parameters in the joint independence model. \\(df = (IJK-1) - ((IJ-1) + (K-1)) = (IJ-1)(K-1)\\). From the Boy Scouts study, suppose juvenile delinquency is the response variable. Test the null hypothesis that juvenile delinquency is independent of boy scout status and socioeconomic status. scout_d_bs &lt;- ftable( scout_ct, row.vars = c(&quot;SES&quot;, &quot;scout&quot;), col.vars = &quot;delinquent&quot; ) print(scout_d_bs) ## delinquent no yes ## SES scout ## low no 169 42 ## yes 43 11 ## med no 132 20 ## yes 104 14 ## high no 59 2 ## yes 196 8 chisq.test(scout_d_bs) ## ## Pearson&#39;s Chi-squared test ## ## data: scout_d_bs ## X-squared = 33, df = 5, p-value = 0.000004 Notice how the contingency table is constructed to not indicate whether SES and scout are related. 2.4.2.3 Marginal Independence The marginal independence model is that two variables are independent while ignoring the third (A, B). The joint probabilities are equal to the product of the two marginal probabilities, \\(\\pi_{ij} = \\pi_{i} \\pi_j\\), just like a 2-way table. The degrees of freedom in the associated chi-sq test are the number of free parameters in the saturated model minus the number of free parameters in the marginal independence model. \\(df = (I - 1) + (J - 1) = (I-1)(J-1)\\). From the Boy Scouts study, suppose juvenile delinquency is the response variable. Test the null hypothesis that juvenile delinquency is independent of boy scout status and socioeconomic status. ct &lt;- xtabs(count ~ scout + SES, scout) print(ct) ## SES ## scout low med high ## no 211 152 61 ## yes 54 118 204 chisq.test(ct) ## ## Pearson&#39;s Chi-squared test ## ## data: ct ## X-squared = 172, df = 2, p-value &lt;0.0000000000000002 2.4.2.4 Conditional Independence The conditional independence model is that two variables are independent given a third (AB, AC). The joint probabilities are equal to the product of the conditional probabilities (B|A and C|A) and the marginal probability of A, \\(\\pi_{ijk} = \\pi_{j|i}\\pi_{k|i}\\pi_{i++}\\). You can test for conditional independence (AB, AC) by individually testing the independence (B, C) for each level of A. \\(X^2\\) and \\(G^2\\) are the sum of the individual values and the degrees of freedom are \\(I(J-1)(K-1)\\). A better way to do it is with the Cochran-Mantel-Haenszel Test. The degrees of freedom in the associated chi-sq test are the number of free parameters in the saturated model minus the number of free parameters in the joint independence model. \\(df = (IJK-1) - ((IJ-1) + (K-1)) = (IJ-1)(K-1)\\). From the Boy Scouts study, the section on joint independence found that scouting and SES were jointly independent of delinquency, so it must be that either scouting is independent of delinquency, or SES is independent of delinquency, or both are independent of delinquency. The marginal test below establishes the independance of (scout, delinquent). ct &lt;- xtabs(count ~ scout + delinquent, scout) print(ct) ## delinquent ## scout no yes ## no 360 64 ## yes 343 33 chisq.test(ct, correct = FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: ct ## X-squared = 7, df = 1, p-value = 0.006 The odds ratio 0.54 is a strong negative relationship between boy scout status and delinquency - boy scouts are 46% less likely (on the odds scale) to be delinquent than non-boy scouts. ct &lt;- xtabs(count ~ scout + delinquent, scout) p &lt;- prop.table(ct, margin = 1) (or &lt;- (p[2, 2] / p[2, 1]) / (p[1, 2] / p[1, 1])) ## [1] 0.54 However, one may ask is scouting and delinquency conditionally independent given SES? You can answer this question with three chi-square tests, one for each level of SES. ct &lt;- xtabs(count ~ scout + delinquent, scout[scout$SES == &quot;low&quot;, ]) print(ct) ## delinquent ## scout no yes ## no 169 42 ## yes 43 11 chisq.test(ct, correct = FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: ct ## X-squared = 0.006, df = 1, p-value = 0.9 ct &lt;- xtabs(count ~ scout + delinquent, scout[scout$SES == &quot;med&quot;, ]) print(ct) ## delinquent ## scout no yes ## no 132 20 ## yes 104 14 chisq.test(ct, correct = FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: ct ## X-squared = 0.1, df = 1, p-value = 0.8 ct &lt;- xtabs(count ~ scout + delinquent, scout[scout$SES == &quot;high&quot;, ]) print(ct) ## delinquent ## scout no yes ## no 59 2 ## yes 196 8 chisq.test(ct, correct = FALSE) ## Warning in chisq.test(ct, correct = FALSE): Chi-squared approximation may be ## incorrect ## ## Pearson&#39;s Chi-squared test ## ## data: ct ## X-squared = 0.05, df = 1, p-value = 0.8 You can add up the three \\(X^2\\) values and run a chi-sq test with \\(df = 3\\). p = 0.984 indicating that the conditional independence model fits extremely well. pchisq(0.0058145 + 0.10098 + 0.053447, df = 3, lower.tail = FALSE) ## [1] 0.98 The other way of testing for independence is the Cochran-Mantel-Haenszel test. The Cochran-Mantel-Haenszel (CMH) test statistic \\[M^2 = \\frac{\\left[ \\sum_k{(n_{11k} - \\mu_{11k})}\\right]^2}{\\sum_k{Var(n_{11k})}}\\] For the Boy Scouts study, the test is mantelhaen.test(scout_ct) ## ## Mantel-Haenszel chi-squared test without continuity correction ## ## data: scout_ct ## Mantel-Haenszel X-squared = 0.008, df = 1, p-value = 0.9 ## alternative hypothesis: true common odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.6 1.6 ## sample estimates: ## common odds ratio ## 0.98 X2= 0.008 (df = 1, p-value = 0.9287) indicating the conditional independence model is a good fit for this data, so do not reject the null hypothesis. 2.4.2.5 Homogenous Associations Whereas the conditional independence model, (AB, AC) requires the BC odds ratios at each level of A to equal 1, the homogeneous association model, (AB, AC, BC), requires the BC odds ratios at each level of A to be identical, but not necessarily equal to 1. The Breslow-Day statistic is \\[X^2 = \\sum_{ijk}{\\frac{\\left(O_{ijk} - E_{ijk}\\right)^2}{E_{ijk}}}\\] DescTools::BreslowDayTest(scout_ct) ## ## Breslow-Day test on Homogeneity of Odds Ratios ## ## data: scout_ct ## X-squared = 0.2, df = 2, p-value = 0.9 "],
["continuous-analysis.html", "2.5 Continuous Variable Analysis", " 2.5 Continuous Variable Analysis 2.5.1 Correlation Correlation measures the strength and direction of association between two variables. There are three common correlation tests: the Pearson product moment (Pearson’s r), Spearman’s rank-order (Spearman’s rho), and Kendall’s tau (Kendall’s tau). Use the Pearson’s r if both variables are quantitative (interval or ratio), normally distributed, and the relationship is linear with homoscedastic residuals. The Spearman’s rho and Kendal’s tao correlations are non-parametric measures, so they are valid for both quantitative and ordinal variables and do not carry the normality and homoscedasticity conditions. However, non-parametric tests have less statistical power than parametric tests, so only use these correlations if Pearson does not apply. 2.5.1.1 Pearson’s r Pearson’s \\(r\\) \\[r = \\frac{\\sum{(X_i - \\bar{X})(Y_i - \\bar{Y})}}{\\sqrt{\\sum{(X_i - \\bar{X})^2 \\sum{(Y_i - \\bar{Y})^2}}}} = \\frac{cov(X,Y)}{s_X s_Y}\\] estimates the population correlation \\(\\rho\\). Pearson’s \\(r\\) ranges from \\(-1\\) (perfect negative linear relationship) to \\(+1\\) (perfect positive linear relationship, and \\(r = 0\\) when there is no linear relationship. A correlation in the range \\((.1, .3)\\) is condidered small, \\((.3, .5)\\) medium, and \\((.5, 1.0)\\) large. Pearson’s \\(r\\) only applies if the variables are interval or ratio, normally distributed, linearly related, there are minimal outliers, and the residuals are homoscedastic. Test \\(H_0: \\rho = 0\\) with test statistic \\[T = r \\sqrt{\\frac{n-2}{1-r^2}}.\\] The nascard data set consists of \\(n = 898\\) races from 1975 - 2003. nascard &lt;- read.fwf( file = url(&quot;http://jse.amstat.org/datasets/nascard.dat.txt&quot;), widths = c(5, 6, 4, 4, 4, 5, 9, 4, 11, 30), col.names = c(&#39;series_race&#39;, &#39;year&#39;, &#39;race_year&#39;, &#39;finish_pos&#39;, &#39;start_pos&#39;, &#39;laps_comp&#39;, &#39;winnings&#39;, &#39;num_cars&#39;,&#39;car_make&#39;, &#39;driver&#39;) ) nascard_sr1 &lt;- nascard[nascard$series_race == 1,] glimpse(nascard_sr1) ## Rows: 35 ## Columns: 10 ## $ series_race &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... ## $ year &lt;dbl&gt; 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1... ## $ race_year &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... ## $ finish_pos &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, ... ## $ start_pos &lt;dbl&gt; 1, 2, 25, 27, 22, 35, 3, 21, 33, 10, 17, 24, 6, 28, 18,... ## $ laps_comp &lt;dbl&gt; 191, 191, 184, 183, 178, 175, 172, 168, 167, 166, 166, ... ## $ winnings &lt;dbl&gt; 12035, 8135, 6535, 5035, 3835, 2885, 3185, 2485, 2335, ... ## $ num_cars &lt;dbl&gt; 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35,... ## $ car_make &lt;chr&gt; &quot;Matador &quot;, &quot;Mercury &quot;, &quot;Chevrolet &quot;, &quot;Dodge ... ## $ driver &lt;chr&gt; &quot;BobbyAllison &quot;, &quot;DavidPearson ... In race 1 of 1975 \\((n = 35)\\), what was the correlation between the driver’s finishing position and prize? Explore the relationship with a scatterplot. As expected, there is a negative relationship between finish position and winnings, but it is non-linear. However, 1/winnings may be linearly related to finish position. p1 &lt;- ggplot(data = nascard_sr1, aes(x = finish_pos, y = winnings)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_mf() + labs(title = &quot;Pearson&#39;s Rho&quot;, subtitle = &quot;&quot;) p2 &lt;- ggplot(data = nascard_sr1, aes(x = finish_pos, y = 1/winnings)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_mf() + labs(title = &quot;Pearson&#39;s Rho&quot;, subtitle = &quot;y = 1 / winnings&quot;) gridExtra::grid.arrange(p1, p2, nrow = 1) (*I tried several variable tranformations and chose the one producing the highest \\(R^2\\) that also had a normal distribution in each variable.) #summary(lm(winnings ~ finish_pos, data = nascard_sr1)) # r2 = .5474 #summary(lm(log(winnings) ~ finish_pos, data = nascard_sr1)) # r2 = .9015 nascard_lm &lt;- lm(1/winnings ~ finish_pos, data = nascard_sr1) # r2 = .9509 #summary(lm(winnings ~ log(finish_pos), data = nascard_sr1)) # r2 = .883 #summary(lm(log(winnings) ~ log(finish_pos), data = nascard_sr1)) # r2 = .9766 #summary(lm(winnings ~ log(finish_pos), data = nascard_sr1)) # r2 = .883 coef(nascard_lm) ## (Intercept) finish_pos ## -0.000018 0.000045 Finish position and prize are ratio variables, so the Pearson’s r applies. Check whether each variable is normally distributed. p1 &lt;- ggplot(nascard_sr1, aes(sample = finish_pos)) + stat_qq() + stat_qq_line() + theme_mf() + labs(title = &quot;Q-Q Plots&quot;, subtitle = &quot;Finish Position&quot;) p2 &lt;- ggplot(nascard_sr1, aes(sample = winnings)) + stat_qq() + stat_qq_line() + theme_mf() + labs(title = &quot;&quot;, subtitle = &quot;Winnings&quot;) gridExtra::grid.arrange(p1, p2, nrow = 1) The normal distribution plots look good. You can also use the Anderson-Darling statistical test. nortest::ad.test(nascard_sr1$finish_pos) ## ## Anderson-Darling normality test ## ## data: nascard_sr1$finish_pos ## A = 0.4, p-value = 0.4 nortest::ad.test(1/nascard_sr1$winnings) ## ## Anderson-Darling normality test ## ## data: 1/nascard_sr1$winnings ## A = 0.3, p-value = 0.5 Both fail to reject the normality null hypothesis. Pearson’s \\(r\\) is x &lt;- nascard_sr1$finish_pos y &lt;- 1/nascard_sr1$winnings (r = sum((x - mean(x)) * (y - mean(y))) / sqrt(sum((x - mean(x))^2) * sum((y - mean(y))^2))) ## [1] 0.98 Test \\(H_0: \\rho = 0\\) with test statistic \\(T\\). n &lt;- nrow(nascard_sr1) (t = r * sqrt((n - 2) / (1 - r^2))) ## [1] 25 pt(q = t, df = n - 2, lower.tail = FALSE) * 2 ## [1] 0.000000000000000000000035 \\(P(T&gt;.9752) &lt; .0001\\), so reject \\(H_0\\) that the correlation is zero. cor.test() performs these calculations. Specify method = \"pearson\". cor.test( x = nascard_sr1$finish_pos, y = 1/nascard_sr1$winnings, alternative = &quot;two.sided&quot;, method = &quot;pearson&quot; ) ## ## Pearson&#39;s product-moment correlation ## ## data: nascard_sr1$finish_pos and 1/nascard_sr1$winnings ## t = 25, df = 33, p-value &lt;0.0000000000000002 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.95 0.99 ## sample estimates: ## cor ## 0.98 2.5.1.2 Spearman’s Rho Spearman’s \\(\\rho\\) is the Pearson’s r applied to the sample variable ranks. Let \\((X_i, Y_i)\\) be the ranks of the \\(n\\) sample pairs with mean ranks \\(\\bar{X} = \\bar{Y} = (n+1)/2\\). Spearman’s rho is \\[\\hat{\\rho} = \\frac{\\sum{(X_i - \\bar{X})(Y_i - \\bar{Y})}}{\\sqrt{\\sum{(X_i - \\bar{X})^2 \\sum{(Y_i - \\bar{Y})^2}}}}\\] Spearman’s rho is a non-parametric test, so there is no associated confidence interval. From the nascard study, what was the correlation between the driver’s starting position start_pos and finishing position finish_pos? From the scatterplot, there does not appear to be much of a relationship. nascard_sr1 %&gt;% ggplot(aes(x = start_pos, y = finish_pos)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_mf() + labs(title = &quot;Spearman&#39;s Rho&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Both star_pos and finish_pos are ordinal variables, so use Spearman’s rho instead of Pearson. Normally, you replace the variable values with their ranks, but in this case, the values are their ranks. After that, Spearman’s \\(\\rho\\) is the same as Pearson’s r. x &lt;- rank(nascard_sr1$start_pos) y &lt;- rank(nascard_sr1$finish_pos) (rho = sum((x - mean(x)) * (y - mean(y))) / sqrt(sum((x - mean(x))^2) * sum((y - mean(y))^2))) ## [1] -0.038 Test \\(H_0: \\rho = 0\\) with test statistic \\(T\\). n &lt;- nrow(nascard_sr1) (t = rho * sqrt((n - 2) / (1 - rho^2))) ## [1] -0.22 pt(q = abs(t), df = n - 2, lower.tail = FALSE) * 2 ## [1] 0.83 \\(P(T&gt;.2206) = .8268\\), so do not reject \\(H_0\\) that the correlation is zero. cor.test() performs these calculations. Specify method = \"spearman\". cor.test( x = nascard_sr1$start_pos, y = nascard_sr1$finish_pos, alternative = &quot;two.sided&quot;, method = &quot;spearman&quot; ) ## ## Spearman&#39;s rank correlation rho ## ## data: nascard_sr1$start_pos and nascard_sr1$finish_pos ## S = 7414, p-value = 0.8 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## -0.038 2.5.1.3 Kendall’s Tau The Kendall Rank correlation coefficient (Kendall’s tau) measures the relationship between ranked variables. (see also Statistics How To). Let \\((X_i, Y_i)\\) be the ranks of the \\(n\\) sample pairs. For each \\(X_i\\), count \\(Y &gt; X_i\\). The total count is \\(k\\). Kendall’s tau is \\[\\hat{\\tau} = \\frac{4k}{n(n-1)} -1\\] If \\(X\\) and \\(Y\\) have the same rank orders, \\(\\hat{\\tau} = 1\\), and if they have opposite rank orders \\(\\hat{\\tau} = -1\\). Test \\(H_0: \\tau = 0\\) with test statistic \\[Z = \\hat{\\tau} \\sqrt{\\frac{9n(n - 1)}{2(2n + 5)}}.\\] Kendall’s tau is a non-parametric test, so there is no associated confidence interval. From the nascard study, what was the correlation between the driver’s starting position start_pos and finishing position finish_pos? Spearman’s rho was \\(\\hat{\\rho} = -0.038\\). For Kendall’s tau, for each observation, count the number of other observations where both start_pos and finish_pos is larger. The sum is \\(k = 293\\). Then calculate Kendall’s \\(\\tau\\) is \\(\\hat{\\tau} = \\frac{4 * 293}{35(35-1)} -1 = -.0151\\). n &lt;- nrow(nascard_sr1) k &lt;- 0 for (i in 1:n) { for(j in i:n) { k = k + if_else( nascard_sr1[j, ]$start_pos &gt; nascard_sr1[i, ]$start_pos &amp; nascard_sr1[j, ]$finish_pos &gt; nascard_sr1[i, ]$finish_pos, 1, 0) } } (k) ## [1] 293 (tau = 4.0*k / (n * (n - 1)) - 1) ## [1] -0.015 Test \\(H_0: \\tau = 0\\) with test statistic \\(T\\). n &lt;- nrow(nascard_sr1) (t = tau * sqrt(9 * n * (n-1) / (2 * (2*n + 5)))) ## [1] -0.13 pt(q = abs(t), df = n - 2, lower.tail = FALSE) * 2 ## [1] 0.9 \\(P(|T| &gt; 0.128) = .8991\\), so do not reject \\(H_0\\) that the correlation is zero. cor.test() performs these calculations. Specify method = \"kendall\". cor.test( x = nascard_sr1$start_pos, y = nascard_sr1$finish_pos, alternative = &quot;two.sided&quot;, method = &quot;kendall&quot; ) ## ## Kendall&#39;s rank correlation tau ## ## data: nascard_sr1$start_pos and nascard_sr1$finish_pos ## T = 293, p-value = 0.9 ## alternative hypothesis: true tau is not equal to 0 ## sample estimates: ## tau ## -0.015 Here is a fun way to tie Pearson, Spearman, and Kendal together: Calculate Spearman’s rho and Kendall’s tau for all 898 races in the nascard data set and calculate the Pearson correlation of their values! nascard_smry &lt;- nest(nascard, -c(series_race, year)) %&gt;% mutate( spearman = map(data, ~ cor.test(.$start_pos, .$finish_pos, alternative = &quot;two.sided&quot;, method = &quot;spearman&quot;)), spearman_est = unlist(map(spearman, ~ .$estimate)), spearman_p = unlist(map(spearman, ~ .$p.value)), kendall = map(data, ~ cor.test(.$start_pos, .$finish_pos, alternative = &quot;two.sided&quot;, method = &quot;kendall&quot;)), kendall_est = unlist(map(kendall, ~ .$estimate)), kendall_p = unlist(map(kendall, ~ .$p.value)) ) nascard_smry2 &lt;- nascard_smry %&gt;% group_by(year) %&gt;% summarize( spearman_r = mean(spearman_est), kendall_tau = mean(kendall_est) ) %&gt;% pivot_longer(cols = c(spearman_r, kendall_tau), names_to = &quot;method&quot;, values_to = &quot;estimate&quot;) ggplot(nascard_smry2, aes(x = year, color = method)) + geom_line(aes(y = estimate)) + theme_mf() + scale_color_mf() + labs(title = &quot;Average Rank Correlations vs Year&quot;, color = &quot;&quot;) In terms of significance, the two tests usually return the same results. table( ifelse(nascard_smry$spearman_p &lt; .05, &quot;sig&quot;, &quot;insig&quot;), ifelse(nascard_smry$kendall_p &lt; .05, &quot;sig&quot;, &quot;insig&quot;) ) ## ## insig sig ## insig 344 18 ## sig 11 525 Calculate the Pearson coefficient of the relationship between Spearman’s rho and Kendall’s tau. ggplot(data = nascard_smry, aes(x = kendall_est, y = spearman_est)) + geom_point() + geom_smooth(method = lm, se = FALSE) + theme_mf() + labs(title = &quot;Pearson&#39;s r of Spearman&#39;s rho vs Kendall&#39;s tau&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; The scatterplot with fitted line has r-squared equal to the Pearson coefficient squared. cor.test( x = nascard_smry$spearman_est, y = nascard_smry$kendall_est, alternative = &quot;two.sided&quot;, method = &quot;pearson&quot; )$estimate^2 ## cor ## 0.98 summary(lm(kendall_est ~ spearman_est, data = nascard_smry))$r.squared ## [1] 0.98 "],
["experiment-design.html", "Chapter 3 Experiment Design", " Chapter 3 Experiment Design These notes are structured from the PSU STAT 503 course. "],
["single-factor.html", "3.1 Single Factor", " 3.1 Single Factor "],
["blocking.html", "3.2 Blocking", " 3.2 Blocking "],
["nested.html", "3.3 Nested", " 3.3 Nested "],
["split-plot.html", "3.4 Split Plot", " 3.4 Split Plot "],
["part-2-supervised-machine-learning.html", "PART 2: Supervised Machine Learning", " PART 2: Supervised Machine Learning Machine learning (ML) develops algorithms to identify patterns in data (unsupervised ML) or make predictions and inferences (supervised ML). Supervised ML trains the machine to learn from prior examples to predict either a categorical outcome (classification) or a numeric outcome (regression), or to infer the relationships between the outcome and its explanatory variables. Two early forms of supervised ML are linear regression (OLS) and generalized linear models (GLM) (Poisson and logistic regression). These methods have been improved with advanced linear methods, including stepwise selection, regularization (ridge, lasso, elastic net), principal components regression, and partial least squares. With greater computing capacity, non-linear models are now in use, including polynomial regression, step functions, splines, and generalized additive models (GAM). Decision trees (bagging, random forests, and, boosting) are additional options for regression and classification, and support vector machines is an additional option for classification. "],
["ordinary-least-squares.html", "Chapter 4 Ordinary Least Squares", " Chapter 4 Ordinary Least Squares library(caret) library(Metrics) ## Warning: package &#39;Metrics&#39; was built under R version 4.0.2 library(tidyverse) library(corrplot) ## Warning: package &#39;corrplot&#39; was built under R version 4.0.2 library(gridExtra) library(car) # for avPlots ## Warning: package &#39;car&#39; was built under R version 4.0.2 library(AppliedPredictiveModeling) library(e1071) # for skewness() library(purrr) # for map() library(broom) # for augment() These notes cover linear regression. "],
["linear-regression-model.html", "4.1 Linear Regression Model", " 4.1 Linear Regression Model The population regression model \\(E(Y) = X \\beta\\) summarizes the trend between the predictors and the mean responses. The individual responses vary about the population regression, \\(y_i = X_i \\beta + \\epsilon_i\\) with assumed mean structure \\(y_i \\sim N(\\mu_i, \\sigma^2)\\) and assumed constant variance \\(\\sigma^2\\). Equivalently, the model presumes a linear relationship between \\(y\\) and \\(X\\) with residuals \\(\\epsilon\\) that are independent normal random variables with mean zero and constant variance \\(\\sigma^2\\). Estimate the population regression model coefficients as \\(\\hat{y} = X \\hat{\\beta}\\), and the population variance as \\(\\hat{\\sigma}^2\\). The most common method of estimating the \\(\\beta\\) coefficients and \\(\\sigma\\) is ordinary least squares (OLS). OLS minimizes the sum of squared residuals from a random sample. The individual predicted values vary about the actual value, \\(e_i = y_i - \\hat{y}_i\\), where \\(\\hat{y}_i = X_i \\hat{\\beta}\\). The OLS model is the best linear unbiased estimator (BLUE) if the residuals are independent random variables normally distributed with mean zero and constant variance \\(\\sigma^2\\). Recall these conditions with the LINE pneumonic: Linear, Independent, Normal, and Equal. Linearity. The explanatory variables are each linearly related to the response variable: \\(E(\\epsilon | X_j) = 0\\). Independence. The residuals are unrelated to each other. Independence is violated when repeated measurements are taken, or when there is a temporal component in the model. Normality. The residuals are normally distributed: \\(\\epsilon|X \\sim N(0, \\sigma^2I)\\). Equal Variances. The variance of the residuals is constant (homoscedasticity): \\(E(\\epsilon \\epsilon&#39; | X) = \\sigma^2I\\) Additionally, you should make sure you model has “little” or no multicollinearity among the variables. "],
["parameter-estimation.html", "4.2 Parameter Estimation", " 4.2 Parameter Estimation There are two model parameters to estimate: \\(\\hat{\\beta}\\) estimates the coefficient vector \\(\\beta\\), and \\(\\hat{\\sigma}\\) estimates the variance of the residuals along the regression line. Derive the coefficient estimators by minimizing the sum of squared residuals \\(SSE = (y - X \\hat{\\beta})&#39; (y - X \\hat{\\beta})\\). The result is \\[\\hat{\\beta} = (X&#39;X)^{-1}X&#39;y.\\] The residual standard error (RSE) estimates the sample deviation around the population regression line. (Think of each value of \\(X\\) along the regression line as a subpopulation with mean \\(y_i\\) and variance \\(\\sigma^2\\). This variance is assumed to be the same for all \\(X\\).) \\[\\hat{\\sigma} = \\sqrt{(n-k-1)^{-1} e&#39;e}.\\] The standard error for the coefficient estimators is the square root of the error variance divided by \\((X&#39;X)\\). \\[SE(\\hat{\\beta}) = \\sqrt{\\hat{\\sigma}^2 (X&#39;X)^{-1}}.\\] 4.2.0.1 Example Dataset mtcars contains response variable fuel consumption mpg and 10 aspects of automobile design and performance for 32 automobiles. What is the relationship between the response variable and its predictors? d &lt;- mtcars %&gt;% mutate(vs = factor(vs, labels = c(&quot;V&quot;, &quot;S&quot;)), am = factor(am, labels = c(&quot;automatic&quot;, &quot;manual&quot;)), cyl = ordered(cyl), gear = ordered(gear), carb = ordered(carb)) glimpse(d) ## Rows: 32 ## Columns: 11 ## $ mpg &lt;dbl&gt; 21, 21, 23, 21, 19, 18, 14, 24, 23, 19, 18, 16, 17, 15, 10, 10... ## $ cyl &lt;ord&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4,... ## $ disp &lt;dbl&gt; 160, 160, 108, 258, 360, 225, 360, 147, 141, 168, 168, 276, 27... ## $ hp &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, ... ## $ drat &lt;dbl&gt; 3.9, 3.9, 3.8, 3.1, 3.1, 2.8, 3.2, 3.7, 3.9, 3.9, 3.9, 3.1, 3.... ## $ wt &lt;dbl&gt; 2.6, 2.9, 2.3, 3.2, 3.4, 3.5, 3.6, 3.2, 3.1, 3.4, 3.4, 4.1, 3.... ## $ qsec &lt;dbl&gt; 16, 17, 19, 19, 17, 20, 16, 20, 23, 18, 19, 17, 18, 18, 18, 18... ## $ vs &lt;fct&gt; V, V, S, S, V, S, V, S, S, S, S, V, V, V, V, V, V, S, S, S, S,... ## $ am &lt;fct&gt; manual, manual, manual, automatic, automatic, automatic, autom... ## $ gear &lt;ord&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3,... ## $ carb &lt;ord&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1,... The data consists of 32 observations. A scatterplot matrix of the numeric variables shows the strongest individual association with mpg is from wt (corr = -0.87) followed by disp (corr = -0.85) and hp (corr = -0.78), drat is moderately correlated with mpg (corr = 0.68), and qsec is weakly correlated with mpg (corr = 0.42). corrplot(cor(subset(d, select = c(mpg, disp, hp, drat, wt, qsec))), type = &quot;upper&quot;, method = &quot;number&quot;) Many of the Predictor variables are strongly correlated with each other. Boxplots of the categorical variables shows differences in levels, although ordinal variables gear and and carb do not have a monotonic relationshiop with mpg. p_list &lt;- list() for(i in c(&quot;cyl&quot;, &quot;vs&quot;, &quot;am&quot;, &quot;gear&quot;, &quot;carb&quot;)) { p &lt;- ggplot(d, aes_string(x = i, y = &quot;mpg&quot;)) + geom_boxplot() p_list &lt;- c(p_list, list(p)) } do.call(&quot;grid.arrange&quot;, c(p_list, ncol = 2)) I’ll drop the gear and carb predictors, and fit a population model to the remaining predictors. m &lt;- lm(mpg ~ ., data = d[,1:9]) summary(m) ## ## Call: ## lm(formula = mpg ~ ., data = d[, 1:9]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.998 -1.355 -0.311 1.199 4.110 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 19.54098 14.14642 1.38 0.181 ## cyl.L 0.34256 2.76483 0.12 0.903 ## cyl.Q 1.38843 1.11210 1.25 0.225 ## disp 0.00669 0.01351 0.49 0.626 ## hp -0.02914 0.01718 -1.70 0.104 ## drat 0.58806 1.50311 0.39 0.699 ## wt -3.15525 1.42023 -2.22 0.037 * ## qsec 0.52324 0.69013 0.76 0.456 ## vsS 1.23780 2.10606 0.59 0.563 ## ammanual 3.00091 1.85340 1.62 0.120 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.5 on 22 degrees of freedom ## Multiple R-squared: 0.877, Adjusted R-squared: 0.826 ## F-statistic: 17.4 on 9 and 22 DF, p-value: 0.0000000481 The summary() function shows \\(\\hat{\\beta}\\) as Estimate, \\(SE({\\hat{\\beta}})\\) as Std. Error, and \\(\\hat{\\sigma}\\) as Residual standard error. You can verify this by manually peforming these calculations using matrix algebra (see matrix algebra in r notes at R for Dummies). Here are the coefficient estimators, \\(\\hat{\\beta} = (X&#39;X)^{-1}X&#39;y\\). X &lt;- model.matrix(m) y &lt;- d$mpg beta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% y round(beta_hat, 5) ## [,1] ## (Intercept) 19.5410 ## cyl.L 0.3426 ## cyl.Q 1.3884 ## disp 0.0067 ## hp -0.0291 ## drat 0.5881 ## wt -3.1553 ## qsec 0.5232 ## vsS 1.2378 ## ammanual 3.0009 Here is the residual standard error, \\(\\hat{\\sigma} = \\sqrt{(n-k-1)^{-1} \\hat{e}&#39;\\hat{e}}\\). n &lt;- nrow(X) k &lt;- ncol(X) - 1 # exclude the intercept term y_hat &lt;- X %*% beta_hat sse &lt;- sum((y - y_hat)^2) rse &lt;- sqrt(sse / (n - k - 1)) cat(&quot;Residual standard error: &quot;, round(rse, 3), &quot; on &quot;, (n - k - 1), &quot; degrees of freedom.&quot;) ## Residual standard error: 2.5 on 22 degrees of freedom. Use the residual standard errors to derive the standard errors of the coefficients, \\(SE(\\hat{\\beta}) = \\sqrt{\\hat{\\sigma}^2 (X&#39;X)^{-1}}\\). se_beta_hat &lt;- sqrt(diag(rse^2 * solve(t(X) %*% X))) matrix(round(se_beta_hat, 5), dimnames = list(names(se_beta_hat), &quot;Std. Error&quot;)) ## Std. Error ## (Intercept) 14.146 ## cyl.L 2.765 ## cyl.Q 1.112 ## disp 0.014 ## hp 0.017 ## drat 1.503 ## wt 1.420 ## qsec 0.690 ## vsS 2.106 ## ammanual 1.853 "],
["model-assumptions.html", "4.3 Model Assumptions", " 4.3 Model Assumptions The linear regression model assumes the relationship between the predictors and the response is linear with the residuals that are independent random variables normally distributed with mean zero and constant variance. Additionally, you will want to check for multicollinearity in the predictors because it can produce unreliable coefficient estimates and predicted values. Use a residuals vs fits plot \\(\\left( e \\sim \\hat{Y} \\right)\\) to detect non-linearity and unequal error variances, including outliers. The polynomial trend line should show that the residuals vary around \\(e = 0\\) in a straight line (linearity). The variance should be of constant width (especially no fan shape at the low or high ends). Use a residuals normal probability plot to compares the theoretical percentiles of the normal distribution versus the observed sample percentiles. It should be approximately linear. A scale-location plot \\(\\sqrt{e / sd(e)} \\sim \\hat{y}\\) checks the homogeneity of variance of the residuals (homoscedasticity). The square root of the absolute value of the residuals should be spread equally along a horizontal line. A residuals vs leverage plot identifies influential observations. A plot of the standardized residuals vs the leverage should fall within the 95% probability band. par(mfrow = c(2, 2)) plot(m, labels.id = NULL) 4.3.1 Linearity The explanatory variables should each be linearly related to the response variable: \\(E(\\epsilon | X_j) = 0\\). A good way to test this condition is with a residuals vs fitted values plot. A curved pattern in the residuals indicates a curvature in the relationship between the response and the predictor that is not explained by our model. A linear model does not adequately describe the relationship between the predictor and the response. Test for linearity four ways: Residuals vs fits plot \\((e \\sim \\hat{Y})\\) should bounce randomly around 0. Observed vs fits plot \\((Y \\sim \\hat{Y})\\) should be symmetric along the 45-degree line. Each \\((Y \\sim X_j )\\) plot should have correlation \\(\\rho \\sim 1\\). Each \\((e \\sim X_j)\\) plot should exhibit no pattern. If the linearity condition fails, change the functional form of the model with non-linear transformations of the explanatory variables. A common way to do this is with Box-Cox transformations. \\[w_t = \\begin{cases} \\begin{array}{l} log(y_t) \\quad \\quad \\lambda = 0 \\\\ (y_t^\\lambda - 1) / \\lambda \\quad \\text{otherwise} \\end{array} \\end{cases}\\] \\(\\lambda\\) can take any value, but values near the following yield familiar transformations. \\(\\lambda = 1\\) yields no substantive transformation. \\(\\lambda = 0.5\\) is a square root plus linear transformation. \\(\\lambda = 0.333\\) is a cube root plus linear transformation. \\(\\lambda = 0\\) is a natural log transformation. \\(\\lambda = -1\\) is an inverse transformation. A common source of non-linearity in a model is skewed response or independent variables (see discussion here). mtcars has some skewed variables. tmp &lt;- map(mtcars, skewness) %&gt;% unlist() %&gt;% as.data.frame() %&gt;% rownames_to_column() colnames(tmp) &lt;- c(&quot;IV&quot;, &quot;skew&quot;) ggplot(tmp, aes(x = order(IV, skew), y = skew)) + geom_col() df &lt;- mtcars m &lt;- lm(mpg ~ hp, data = df) par(mfrow = c(2, 2)) plot(m) plot(m$model$mpg, m$fitted.values) abline(0, 1) cor(df$mpg, df$hp) ## [1] -0.78 postResample(pred = m$fitted.values, obs = m$model$mpg) ## RMSE Rsquared MAE ## 3.7 0.6 2.9 bc &lt;- BoxCoxTrans(mtcars$hp) df$hp_bc &lt;- predict(bc, mtcars$hp) m_bc &lt;- lm(mpg ~ hp_bc, data = df) plot(m_bc) plot(m_bc$model$mpg, m_bc$fitted.values) abline(0, 1) cor(df$mpg, df$hp_bc) ## [1] -0.85 postResample(pred = m_bc$fitted.values, obs = m_bc$model$mpg) ## RMSE Rsquared MAE ## 3.14 0.72 2.41 # Which vars are skewed? map(mtcars, skewness) ## $mpg ## [1] 0.61 ## ## $cyl ## [1] -0.17 ## ## $disp ## [1] 0.38 ## ## $hp ## [1] 0.73 ## ## $drat ## [1] 0.27 ## ## $wt ## [1] 0.42 ## ## $qsec ## [1] 0.37 ## ## $vs ## [1] 0.24 ## ## $am ## [1] 0.36 ## ## $gear ## [1] 0.53 ## ## $carb ## [1] 1.1 # Benchmark model: mpg ~ hp d0 &lt;- mtcars m0 &lt;- lm(mpg ~ hp, data = d0) d0 &lt;- augment(m0, d0) d0.cor &lt;- round(cor(d0$mpg, d0$hp), 2) # Benchmark diagnostics p0a &lt;- ggplot(d0, aes(x = hp, y = mpg)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;Response vs IV&quot;, subtitle = paste0(&quot;Correlation ~ 1? (rho = &quot;, d0.cor, &quot;)&quot;)) p0b &lt;- ggplot(d0, aes(x = .fitted, y = .resid)) + geom_point() + geom_abline(intercept = 0, slope = 0) + labs(title = &quot;Residuals vs Fits&quot;, subtitle = &quot;Random scatter?&quot;) p0c &lt;- ggplot(d0, aes(x = .fitted, y = mpg)) + geom_point() + geom_abline(intercept = 0, slope = 1) + expand_limits(x = c(0, 35), y = c(0, 35)) + labs(title = &quot;Observed vs Fits&quot;, subtitle = &quot;Symmetric along 45 degree line?&quot;) p0d &lt;- ggplot(d0, aes(x = hp, y = .resid)) + geom_point() + geom_abline(intercept = 0, slope = 0) + labs(title = &quot;Residuals vs IV&quot;, subtitle = &quot;Random scatter?&quot;) grid.arrange(p0a, p0b, p0c, p0d, nrow = 2) ## `geom_smooth()` using formula &#39;y ~ x&#39; # Benchmark performance postResample(pred = m0$fitted.values, obs = m0$model$mpg) ## RMSE Rsquared MAE ## 3.7 0.6 2.9 # Box-Cox transform hp d1 &lt;- mtcars bc &lt;- BoxCoxTrans(d1$hp) d1$hp_bc &lt;- predict(bc, d1$hp) m1 &lt;- lm(mpg ~ hp_bc, data = d1) d1 &lt;- augment(m1, d1) d1.cor &lt;- round(cor(d1$mpg, d1$hp_bc), 2) p1a &lt;- ggplot(d1, aes(x = hp_bc, y = mpg)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;Response vs IV&quot;, subtitle = paste0(&quot;Correlation ~ 1? (rho = &quot;, d1.cor, &quot;)&quot;)) p1b &lt;- ggplot(d1, aes(x = .fitted, y = .resid)) + geom_point() + geom_abline(intercept = 0, slope = 0) + labs(title = &quot;Residuals vs Fits&quot;, subtitle = &quot;Random scatter?&quot;) p1c &lt;- ggplot(d1, aes(x = .fitted, y = mpg)) + geom_point() + geom_abline(intercept = 0, slope = 1) + expand_limits(x = c(0, 35), y = c(0, 35)) + labs(title = &quot;Observed vs Fits&quot;, subtitle = &quot;Symmetric along 45 degree line?&quot;) p1d &lt;- ggplot(d1, aes(x = hp, y = .resid)) + geom_point() + geom_abline(intercept = 0, slope = 0) + labs(title = &quot;Residuals vs IV&quot;, subtitle = &quot;Random scatter?&quot;) grid.arrange(p1a, p1b, p1c, p1d, nrow = 2) ## `geom_smooth()` using formula &#39;y ~ x&#39; postResample(pred = m1$fitted.values, obs = m1$model$mpg) ## RMSE Rsquared MAE ## 3.14 0.72 2.41 4.3.2 Multicollinearity The multicollinearity condition is violated when two or more of the predictors in a regression model are correlated. Muticollinearity can occur for structural reasons, as when one variable is a transformation of another variable, or for data reasons, as occurs in observational studies. Multicollinearity is a problem because it inflates the variances of the estimated coefficients, resulting in larger confidence intervals. When predictor variables are correlated, the precision of the estimated regression coefficients decreases with each added correlated predictor variable. The usual interpretation of a slope coefficient as the change in the mean response for each additional unit increase in the predictor when all the other predictors are held constant breaks down because changing one predictor necessarily changes the others. A residuals vs fits plot \\((\\epsilon \\sim \\hat{Y})\\) should have correlation \\(\\rho \\sim 0\\). A correlation matrix is helpful for picking out the correlation strengths. A good rule of thumb is correlation coefficients should be less than 0.80. However, this test may not work when a variable is correlated with a function of other variables. A model with multicollinearity may have a significant F-test with insignificant individual slope estimator t-tests. Another way to detect multicollinearity is by calculating variance inflation factors. The predictor variance \\(Var(\\hat{\\beta_k})\\) increases by a factor \\[VIF_k = \\frac{1}{1 - R_k^2}\\] where \\(R_k^2\\) is the \\(R^2\\) of a regression of the \\(k^{th}\\) predictor on the remaining predictors. A \\(VIF_k\\) of \\(1\\) indicates no inflation (no corellation). A \\(VIF_k &gt;= 4\\) warrants investigation. A \\(VIF_k &gt;= 10\\) requires correction. 4.3.2.1 Example Does the model mpg ~ . exhibit multicollinearity? The correlation matrix above (and presented again below) has several correlated covariates. disp is strongly correlated with wt (r = 0.89) and hp (r = 0.79). m &lt;- lm(mpg ~ ., data = mtcars) corrplot(cor(subset(d, select = c(mpg, disp, hp, drat, wt, qsec))), type = &quot;upper&quot;, method = &quot;number&quot;) Calculate the VIFs. round(vif(m), 2) ## cyl disp hp drat wt qsec vs am gear carb ## 15.4 21.6 9.8 3.4 15.2 7.5 5.0 4.6 5.4 7.9 There are two predictors with VIFs greater than 10, cyl (GVIF = 21.36) and disp (GVIF = 13.76). One way to address multicollinearity is removing one or more of the violating predictors from the regression model. Try removing cyl. vif(m &lt;-lm(mpg ~ . - cyl, data = d[,1:9])) ## disp hp drat wt qsec vs am ## 9.9 5.4 2.8 7.6 6.0 4.2 3.5 summary(m) ## ## Call: ## lm(formula = mpg ~ . - cyl, data = d[, 1:9]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.407 -1.469 -0.282 1.142 4.537 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.4980 12.4804 1.00 0.3266 ## disp 0.0137 0.0114 1.21 0.2382 ## hp -0.0228 0.0153 -1.50 0.1478 ## drat 0.9553 1.4074 0.68 0.5038 ## wt -3.9497 1.2626 -3.13 0.0046 ** ## qsec 0.8715 0.6133 1.42 0.1682 ## vsS 0.5902 1.8330 0.32 0.7503 ## ammanual 3.0240 1.6684 1.81 0.0824 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.5 on 24 degrees of freedom ## Multiple R-squared: 0.867, Adjusted R-squared: 0.829 ## F-statistic: 22.4 on 7 and 24 DF, p-value: 0.00000000453 Removing cyl reduced the VIFs of the other variables below 10. disp is still right up there (VIF = 9.87), so it may be worth dropping it from the model too. The model summary still shows that there is only one significant (at .05 level a significance) variable (wt, p = .00457). What if I drop disp too? vif(m &lt;- lm(mpg ~ . - cyl - disp, data = d[,1:9])) ## hp drat wt qsec vs am ## 5.1 2.7 5.1 5.8 4.1 3.3 summary(m) ## ## Call: ## lm(formula = mpg ~ . - cyl - disp, data = d[, 1:9]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.369 -1.721 -0.253 1.099 4.603 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 16.1410 12.2232 1.32 0.1986 ## hp -0.0180 0.0149 -1.21 0.2380 ## drat 0.6205 1.3926 0.45 0.6597 ## wt -3.0751 1.0446 -2.94 0.0069 ** ## qsec 0.7347 0.6084 1.21 0.2385 ## vsS 0.2045 1.8217 0.11 0.9115 ## ammanual 2.5653 1.6397 1.56 0.1303 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.5 on 25 degrees of freedom ## Multiple R-squared: 0.859, Adjusted R-squared: 0.825 ## F-statistic: 25.4 on 6 and 25 DF, p-value: 0.00000000169 The model is not improved, so keep disp. m &lt;-lm(mpg ~ . - cyl, data = d[,1:9]) If the multicollinearity occurs because you are using a polynomial regression model, center the predictor variables (subtract their means). 4.3.2.2 Example Data set exerimmun (exerimun.txt) contains observations of immunoglobin in blood (a measure of immunity) and maximal oxygen uptake (a measure of exercise level) for \\(n = 30\\) individuals. igg = amount of immunoglobin in blood (mg) oxygent = maximal oxygen uptake (ml/kg) How does exercise affect the immune system? #exerimmun &lt;- read_tsv(file = &quot;./Data/exerimmun.txt&quot;) exerimmun &lt;- tribble( ~igg, ~oxygen, 881, 34.6, 1290, 45, 2147, 62.3, 1909, 58.9, 1282, 42.5, 1530, 44.3, 2067, 67.9, 1982, 58.5, 1019, 35.6, 1651, 49.6, 752, 33, 1687, 52, 1782, 61.4, 1529, 50.2, 969, 34.1, 1660, 52.5, 2121, 69.9, 1382, 38.8, 1714, 50.6, 1959, 69.4, 1158, 37.4, 965, 35.1, 1456, 43, 1273, 44.1, 1418, 49.8, 1743, 54.4, 1997, 68.5, 2177, 69.5, 1965, 63, 1264, 43.2 ) head(exerimmun) ## # A tibble: 6 x 2 ## igg oxygen ## &lt;dbl&gt; &lt;dbl&gt; ## 1 881 34.6 ## 2 1290 45 ## 3 2147 62.3 ## 4 1909 58.9 ## 5 1282 42.5 ## 6 1530 44.3 The scatterplot oxygen ~ igg shows some curvature. Formulate a quadratic polynomial regression function, \\(igg_i = \\beta_0 + \\beta_1 oxygen_i + \\beta_2 oxygen_i^2 + \\epsilon_i\\) where the error terms are assumed to be independent, and normally distributed with equal variance. ggplot(exerimmun, aes(y = igg, x = oxygen)) + geom_point() + geom_smooth(method = lm, formula = y ~ poly(x, 2), se = FALSE) + labs(title = &quot;Immunoglobin in Blood&quot;) The formulated regression fits the data well (\\(adj R^2 = .933\\)), but the terms oxygen and oxygen^2 are strongly correlated. m_blood &lt;- lm(igg ~ poly(oxygen, 2), data = exerimmun) summary(m_blood) ## ## Call: ## lm(formula = igg ~ poly(oxygen, 2), data = exerimmun) ## ## Residuals: ## Min 1Q Median 3Q Max ## -185.37 -82.13 1.05 66.01 227.38 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1557.6 19.4 80.16 &lt;0.0000000000000002 *** ## poly(oxygen, 2)1 2114.7 106.4 19.87 &lt;0.0000000000000002 *** ## poly(oxygen, 2)2 -360.8 106.4 -3.39 0.0022 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 106 on 27 degrees of freedom ## Multiple R-squared: 0.938, Adjusted R-squared: 0.933 ## F-statistic: 203 on 2 and 27 DF, p-value: &lt;0.0000000000000002 cor(exerimmun$oxygen, exerimmun$oxygen^2) ## [1] 0.99 Remove the structural multicollinearity by centering the predictors. You can scale the predictors with scale(), but be careful to scale new data when predicting new observations with predict(newdata=)! Whenever possible, perform the transformation right in the model. m_blood &lt;- lm(igg ~ I(oxygen - mean(exerimmun$oxygen)) + I((oxygen - mean(exerimmun$oxygen))^2), data = exerimmun) summary(m_blood) ## ## Call: ## lm(formula = igg ~ I(oxygen - mean(exerimmun$oxygen)) + I((oxygen - ## mean(exerimmun$oxygen))^2), data = exerimmun) ## ## Residuals: ## Min 1Q Median 3Q Max ## -185.37 -82.13 1.05 66.01 227.38 ## ## Coefficients: ## Estimate Std. Error t value ## (Intercept) 1632.196 29.349 55.61 ## I(oxygen - mean(exerimmun$oxygen)) 34.000 1.689 20.13 ## I((oxygen - mean(exerimmun$oxygen))^2) -0.536 0.158 -3.39 ## Pr(&gt;|t|) ## (Intercept) &lt;0.0000000000000002 *** ## I(oxygen - mean(exerimmun$oxygen)) &lt;0.0000000000000002 *** ## I((oxygen - mean(exerimmun$oxygen))^2) 0.0022 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 106 on 27 degrees of freedom ## Multiple R-squared: 0.938, Adjusted R-squared: 0.933 ## F-statistic: 203 on 2 and 27 DF, p-value: &lt;0.0000000000000002 The estimated intercept coefficient \\(\\hat{\\beta}_0 = 1632\\) means a person whose maximal oxygen uptake is \\(50.64\\) ml/kg (the mean value) is predicted to have \\(1632\\) mg of immunoglobin in his blood. The estimated coefficient \\(\\hat{\\beta}_1 = 34.0\\) means a person whose maximal oxygen uptake is near \\(50.64\\) ml/kg is predicted to increase by 34.0 mg for every 1 ml/kg increase in maximal oxygen uptake. By performing all transformations in the model, it is straightforward to perform predictions. Here is the predicted value of immunoglobin when maximal oxygen uptake = 90.00 ml/kg. predict(m_blood, newdata = data.frame(oxygen = 90), interval = &quot;prediction&quot;) ## fit lwr upr ## 1 2140 1640 2640 4.3.3 Normality A normal probability plot or a normal quantile plot should have values near the line with no bow-shaped deviations. A histogram should be normally distributed. A residuals vs fits plot \\((\\epsilon \\sim \\hat{Y})\\) should be randomly scattered around 0. Sometimes the normality check fails when linearity assumption does not hold, so check for linearity first. Parameter estimation is not sensitive to this condition, but prediction intervals are. 4.3.4 Equal Variances The residuals should be the same size at both low and high values of the response variable. A residuals vs fits plot \\((\\epsilon \\sim \\hat{Y})\\) should have random scatter in a band of constant width around 0, and no fan shape at the low and high ends. All tests and intervals are sensitive to this condition. "],
["prediction.html", "4.4 Prediction", " 4.4 Prediction The standard error in the expected value of \\(\\hat{y}\\) at some new set of predictors \\(X_n\\) is \\[SE(\\mu_\\hat{y}) = \\sqrt{\\hat{\\sigma}^2 (X_n (X&#39;X)^{-1} X_n&#39;)}.\\] The standard error increases the further \\(X_n\\) is from \\(\\bar{X}\\). If \\(X_n = \\bar{X}\\), the equation reduces to \\(SE(\\mu_\\hat{y}) = \\sigma / \\sqrt{n}\\). If \\(n\\) is large, or the predictor values are spread out, \\(SE(\\mu_\\hat{y})\\) will be relatively small. The \\((1 - \\alpha)\\%\\) confidence interval is \\(\\hat{y} \\pm t_{\\alpha / 2} SE(\\mu_\\hat{y})\\). The standard error in the predicted value of \\(\\hat{y}\\) at some \\(X_{new}\\) is \\[SE(\\hat{y}) = SE(\\mu_\\hat{y})^2 + \\sqrt{\\hat{\\sigma}^2}.\\] Notice the standard error for a predicted value is always greater than the standard error of the expected value. The \\((1 - \\alpha)\\%\\) prediction interval is \\(\\hat{y} \\pm t_{\\alpha / 2} SE(\\hat{y})\\). 4.4.0.1 Example What is the expected value of mpg if the predictor values equal their mean values? R performs this calucation with the predict() function with parameter interval = confidence. m &lt;-lm(mpg ~ ., data = d[,1:9]) X_new &lt;- data.frame(Const = 1, cyl = factor(round(mean(as.numeric(as.character(d$cyl))),0), levels = levels(d$cyl)), disp = mean(d$disp), hp = mean(d$hp), drat = mean(d$drat), wt = mean(d$wt), qsec = mean(d$qsec), vs = factor(&quot;S&quot;, levels = levels(d$vs)), am = factor(&quot;manual&quot;, levels = levels(d$am))) predict.lm(object = m, newdata = X_new, interval = &quot;confidence&quot;) ## fit lwr upr ## 1 21 17 25 You can verify this by manually calculating \\(SE(\\mu_\\hat{y}) = \\sqrt{\\hat{\\sigma}^2 (X_{new} (X&#39;X)^{-1} X_{new}&#39;)}\\) using matrix algebra. X2 &lt;- lapply(data.frame(model.matrix(m)), mean) %&gt;% unlist() %&gt;% t() X2[2] &lt;- contr.poly(3)[2,1] # cyl linear X2[3] &lt;- contr.poly(3)[2,2] # cyl quadratic X2[9] &lt;- 1 X2[10] &lt;- 1 y_exp &lt;- sum(m$coefficients * as.numeric(X2)) se_y_exp &lt;- as.numeric(sqrt(rse^2 * X2 %*% solve(t(X) %*% X) %*% t(X2))) t_crit &lt;- qt(p = .05 / 2, df = n - k - 1, lower.tail = FALSE) me &lt;- t_crit * se_y_exp cat(&quot;fit: &quot;, round(y_exp, 6), &quot;, 95% CI: (&quot;, round(y_exp - me, 6), &quot;, &quot;, round(y_exp + me, 6), &quot;)&quot;) ## fit: 21 , 95% CI: ( 17 , 25 ) 4.4.0.2 Example What is the predicted value of mpg if the predictor values equal their mean values? R performs this calucation with the predict() with parameter interval = prediction. predict.lm(object = m, newdata = X_new, interval = &quot;prediction&quot;) ## fit lwr upr ## 1 21 15 28 se_y_hat &lt;- sqrt(rse^2 + se_y_exp^2) me &lt;- t_crit * se_y_hat cat(&quot;fit: &quot;, round(y_exp, 6), &quot;, 95% CI: (&quot;, round(y_exp - me, 6), &quot;, &quot;, round(y_exp + me, 6), &quot;)&quot;) ## fit: 21 , 95% CI: ( 15 , 28 ) "],
["inference.html", "4.5 Inference", " 4.5 Inference Draw conclusions about the significance of the coefficient estimates with the t-test and/or F-test. 4.5.1 t-Test By assumption, the residuals are normally distributed, so the Z-test statistic could evaluate the parameter estimators, \\[Z = \\frac{\\hat{\\beta} - \\beta_0}{\\sqrt{\\sigma^2 (X&#39;X)^{-1}}}\\] where \\(\\beta_0\\) is the null-hypothesized value, usually 0. \\(\\sigma\\) is unknown, but \\(\\frac{\\hat{\\sigma}^2 (n - k)}{\\sigma^2} \\sim \\chi^2\\). The ratio of the normal distribution divided by the adjusted chi-square \\(\\sqrt{\\chi^2 / (n - k)}\\) is t-distributed, \\[t = \\frac{\\hat{\\beta} - \\beta_0}{\\sqrt{\\hat{\\sigma}^2 (X&#39;X)^{-1}}} = \\frac{\\hat{\\beta} - \\beta_0}{SE(\\hat{\\beta})}\\] The \\((1 - \\alpha)\\) confidence intervals are \\(CI = \\hat{\\beta} \\pm t_{\\alpha / 2, df} SE(\\hat{\\beta})\\) with p-value equaling the probability of measuring a \\(t\\) of that extreme, \\(p = P(t &gt; |t|)\\). For a one-tail test, divide the reported p-value by two. The \\(SE(\\hat{\\beta})\\) decreases with 1) a better fitting regression line (smaller \\(\\hat{\\sigma}^2\\)), 2) greater variation in the predictor (larger \\(X&#39;X\\)), and 3) larger sample size (larger n). 4.5.1.1 Example Define a 95% confidence interval around the slope parameters. The summary() output shows the t values and probabilities in the t value and Pr(&gt;|t|) columns. You can verify this manually using matrix algebra for \\(t = \\frac{(\\hat{\\beta} - \\beta_1)}{SE(\\hat{\\beta})}\\) with \\(\\beta_1 = 0\\). The \\((1 - \\alpha)\\) confidence interval is \\(CI = \\hat{\\beta} \\pm t_{\\alpha / 2, df} SE(\\hat{\\beta})\\). The table below gathers the parameter estimators and t-test results. t &lt;- beta_hat / se_beta_hat p_value &lt;- pt(q = abs(t), df = n - k - 1, lower.tail = FALSE) * 2 t_crit &lt;- qt(p = .05 / 2, df = n - k - 1, lower.tail = FALSE) lcl = beta_hat - t_crit * se_beta_hat ucl = beta_hat + t_crit * se_beta_hat data.frame(beta = round(beta_hat, 4), se = round(se_beta_hat, 4), t = round(t, 4), p = round(p_value, 4), lcl = round(lcl,4), ucl = round(ucl, 4)) ## beta se t p lcl ucl ## (Intercept) 19.5410 14.146 1.38 0.181 -9.797 48.8789 ## cyl.L 0.3426 2.765 0.12 0.902 -5.391 6.0765 ## cyl.Q 1.3884 1.112 1.25 0.225 -0.918 3.6948 ## disp 0.0067 0.013 0.49 0.625 -0.021 0.0347 ## hp -0.0291 0.017 -1.70 0.104 -0.065 0.0065 ## drat 0.5881 1.503 0.39 0.699 -2.529 3.7053 ## wt -3.1552 1.420 -2.22 0.037 -6.101 -0.2099 ## qsec 0.5232 0.690 0.76 0.456 -0.908 1.9545 ## vsS 1.2378 2.106 0.59 0.563 -3.130 5.6055 ## ammanual 3.0009 1.853 1.62 0.120 -0.843 6.8446 4.5.2 F-Test The F-test for the model is a test of the null hypothesis that none of the independent variables linearly predict the dependent variable, that is, the model parameters are jointly zero: \\(H_0 : \\beta_1 = \\ldots = \\beta_k = 0\\). The regression mean sum of squares \\(MSR = \\frac{(\\hat{y} - \\bar{y})&#39;(\\hat{y} - \\bar{y})}{k-1}\\) and the error mean sum of squares \\(MSE = \\frac{\\hat{\\epsilon}&#39;\\hat{\\epsilon}}{n-k}\\) are each chi-square variables. Their ratio has an F distribution with \\(k - 1\\) numerator degrees of freedom and \\(n - k\\) denominator degrees of freedom. The F statistic can also be expressed in terms of the coefficient of correlation \\(R^2 = \\frac{MSR}{MST}\\). \\[F(k - 1, n - k) = \\frac{MSR}{MSE} = \\frac{R^2}{1 - R^2} \\frac{n-k}{k-1}\\] MSE is \\(\\sigma^2\\). If \\(H_0\\) is true, that is, there is no relationship between the predictors and the response, then \\(MSR\\) is also equal to \\(\\sigma^2\\), so \\(F = 1\\). As \\(R^2 \\rightarrow 1\\), \\(F \\rightarrow \\infty\\), and as \\(R^2 \\rightarrow 0\\), \\(F \\rightarrow 0\\). F increases with \\(n\\) and decreases with \\(k\\). 4.5.2.1 Example What is the probability that all parameters are jointly equal to zero? The F-statistic is presented at the bottom of the summary() function. You can verify this manually. ssr &lt;- sum((m$fitted.values - mean(d$mpg))^2) sse &lt;- sum(m$residuals^2) sst &lt;- sum((m$mpg - mean(d$mpg))^2) msr &lt;- ssr / k mse &lt;- sse / (n - k - 1) f = msr / mse p_value &lt;- pf(q = f, df1 = k, df2 = n - k - 1, lower.tail = FALSE) cat(&quot;F-statistic: &quot;, round(f, 4), &quot; on 3 and 65 DF, p-value: &quot;, p_value) ## F-statistic: 17 on 3 and 65 DF, p-value: 0.000000048 There is sufficient evidence \\((F = 17.35, P &lt; .0001)\\) to reject \\(H_0\\) that the parameter estimators are jointly equal to zero. The aov function calculates the sequential sum of squares. The regression sum of squares SSR for mpg ~ cyl is 824.8. Adding disp to the model increases SSR by 57.6. Adding hp to the model increases SSR by 18.5. It would seem that hp does not improve the model. summary(aov(m)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## cyl 2 825 412 65.26 0.00000000056 *** ## disp 1 58 58 9.12 0.0063 ** ## hp 1 19 19 2.93 0.1011 ## drat 1 12 12 1.89 0.1836 ## wt 1 56 56 8.83 0.0070 ** ## qsec 1 2 2 0.24 0.6282 ## vs 1 0 0 0.05 0.8289 ## am 1 17 17 2.62 0.1197 ## Residuals 22 139 6 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Order matters. Had we started with disp, then added hp we would find both estimators were significant. summary(aov(lm(mpg ~ disp + hp + drat + wt + qsec + vs + am + cyl, data = d))) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## disp 1 809 809 128.00 0.00000000012 *** ## hp 1 34 34 5.33 0.031 * ## drat 1 30 30 4.77 0.040 * ## wt 1 71 71 11.16 0.003 ** ## qsec 1 13 13 2.01 0.170 ## vs 1 0 0 0.04 0.852 ## am 1 20 20 3.24 0.086 . ## cyl 2 10 5 0.82 0.451 ## Residuals 22 139 6 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],
["interpretation.html", "4.6 Interpretation", " 4.6 Interpretation A plot of the standardized coefficients shows the relative importance of each variable. The distance the coefficients are from zero shows how much a change in a standard deviation of the regressor changes the mean of the predicted value. The CI shows the precision. The plot shows not only which variables are significant, but also which are important. d_sc &lt;- d %&gt;% mutate_at(c(&quot;mpg&quot;, &quot;disp&quot;, &quot;hp&quot;, &quot;drat&quot;, &quot;wt&quot;, &quot;qsec&quot;), scale) m_sc &lt;- lm(mpg ~ ., d_sc[,1:9]) lm_summary &lt;- summary(m_sc)$coefficients df &lt;- data.frame(Features = rownames(lm_summary), Estimate = lm_summary[,&#39;Estimate&#39;], std_error = lm_summary[,&#39;Std. Error&#39;]) df$lower = df$Estimate - qt(.05/2, m_sc$df.residual) * df$std_error df$upper = df$Estimate + qt(.05/2, m_sc$df.residual) * df$std_error df &lt;- df[df$Features != &quot;(Intercept)&quot;,] ggplot(df) + geom_vline(xintercept = 0, linetype = 4) + geom_point(aes(x = Estimate, y = Features)) + geom_segment(aes(y = Features, yend = Features, x=lower, xend=upper), arrow = arrow(angle=90, ends=&#39;both&#39;, length = unit(0.1, &#39;cm&#39;))) + scale_x_continuous(&quot;Standardized Weight&quot;) + labs(title = &quot;Model Feature Importance&quot;) The added variable plot shows the bivariate relationship between \\(Y\\) and \\(X_i\\) after accounting for the other variables. For example, the partial regression plots of y ~ x1 + x2 + x3 would plot the residuals of y ~ x2 + x3 vs x1, and so on. library(car) avPlots(m) "],
["model-validation.html", "4.7 Model Validation", " 4.7 Model Validation Evaluate predictive accuracy by training the model on a training data set and testing on a test data set. 4.7.1 Accuracy Metrics The most common measures of model fit are R-squared, RMSE, RSE, MAE, Adjusted R-squared, AIC, AICc, BIC, and Mallow’s Cp. 4.7.1.1 R-Squared The coefficient of determination (R-squared) is the percent of total variation in the response variable that is explained by the regression line. \\[R^2 = 1 - \\frac{SSE}{SST}\\] where \\(SSE = \\sum_{i=1}^n{(y_i - \\hat{y}_i)^2}\\) is the sum squared differences between the predicted and observed value, \\(SST = \\sum_{i = 1}^n{(y_i - \\bar{y})^2}\\) is the sum of squared differences between the observed and overall mean value, and \\(RSS = \\sum_{i=1}^n{(\\hat{y}_i - \\bar{y})^2}\\) is the sum of squared differences between the predicted and overall mean “no-relationship line” value. At the extremes, \\(R^2 = 1\\) means all data points fall perfectly on the regression line - the predictors account for all variation in the response; \\(R^2 = 0\\) means the regression line is horizontal at \\(\\bar{y}\\) - the predictors account for none of the variation in the response. In the simple case of a single predictor variable, \\(R^2\\) equals the squared correlation between \\(x\\) and \\(y\\), \\(r = Cor(x,y)\\). ssr &lt;- sum((m$fitted.values - mean(d$mpg))^2) sse &lt;- sum(m$residuals^2) sst &lt;- sum((d$mpg - mean(d$mpg))^2) (r2 &lt;- ssr / sst) ## [1] 0.88 (r2 &lt;- 1 - sse / sst) ## [1] 0.88 summary(m)$r.squared ## [1] 0.88 \\(R^2\\) is also equal to the correlation between the fitted value and observed values, \\(R^2 = Cor(Y, \\hat{Y})^2\\). cor(m$fitted.values, d$mpg)^2 ## [1] 0.88 R-squared is proportional to the the variance in the response, SST. Given a constant percentage error in predictions, a test set with relatively low variation in the reponse will have a lower R-squared. Conversely, test sets with large variation, e.g., housing data with home sale ranging from $60K to $2M may have a large R-squared despite average prediction errors of &gt;$10K. A close variant of R-squared is the non-parametric Spearman’s rank correlation. This statistic is the correlation of the ranks of the response and the predicted values. It is used when the model goal is ranking. 4.7.1.2 RMSE The root mean squared error (RMSE) is the average prediction error (square root of mean squared error). \\[RMSE = \\sqrt{\\frac{\\sum_{i=1}^n{(y_i - \\hat{y}_i)^2}}{n}}\\] sqrt(mean((d$mpg - m$fitted.values)^2)) ## [1] 2.1 The rmse() function from the Metrics package, and the postResample() function in caret calculate RMSE. rmse(actual = d$mpg, predicted = m$fitted.values) ## [1] 2.1 postResample(pred = m$fitted.values, obs = d$mpg)[1] ## RMSE ## 2.1 The mean squared error of a model with theoretical residual of mean zero and constant variance \\(\\sigma^2\\) can be decomposed into the model’s bias and the model’s variance: \\[E[MSE] = \\sigma^2 + Bias^2 + Var.\\] A model that predicts the response closely will have low bias, but be relatively sensitive to the training data and thus have high variance. A model that predicts the response conservatively (e.g., a simple mean) will have large bias, but be relatively insensitive to nuances in the training data. Here is an example of a simulated sine wave. A model predicting the mean value at the upper and lower levels has low variance, but high bias, and a model of an actual sine wave has low bias and high variance. This is referred to as the variance-bias trade-off. 4.7.1.3 RSE The residual standard error (RSE, or model sigma \\(\\hat{\\sigma}\\)) is an estimate of the standard deviation of \\(\\epsilon\\). It is roughly the average amount the response deviates from the true regression line. \\[\\sigma = \\sqrt{\\frac{\\sum_{i=1}^n{(y_i - \\hat{y}_i)^2}}{n-k-1}}\\] sqrt(sum((d$mpg - m$fitted.values)^2) / (n - k - 1)) ## [1] 2.5 # sd is sqrt(sse / (n-1)), sigma = sqrt(sse / (n - k - 1)) sd(m$residuals) * sqrt((n - 1) / (n - k - 1)) ## [1] 2.5 summary(m)$sigma ## [1] 2.5 sigma(m) ## [1] 2.5 4.7.1.4 MAE The mean absolute error (MAE) is the average absolute prediction arror. It is less sensitive to outliers. \\[MAE = \\frac{\\sum_{i=1}^n{|y_i - \\hat{y}_i|}}{n}\\] sum(abs(d$mpg - m$fitted.values)) / n ## [1] 1.7 The postResample() function in caret conveniently calculates all three. postResample(pred = m$fitted.values, obs = d$mpg) ## RMSE Rsquared MAE ## 2.08 0.88 1.70 defaultSummary(data = data.frame(obs = d$mpg, pred = m$fitted.values), model = m) ## RMSE Rsquared MAE ## 2.08 0.88 1.70 apply(as.matrix(m$fitted.values), 2, postResample, obs = d$mpg) ## [,1] ## RMSE 2.08 ## Rsquared 0.88 ## MAE 1.70 These metrics are good for evaluating a model, but less useful for comparing models. The problem is that they tend to improve with additional variables added to the model, even if the improvement is not significant. The following metrics aid model comparison by penalizing added variables. 4.7.1.5 Adjusted R-squared The adjusted R-squared (\\(\\bar{R}^2\\)) penalizes the R-squared metric for increasing number of predictors. \\[\\bar{R}^2 = 1 - \\frac{SSE}{SST} \\cdot \\frac{n-1}{n-k-1}\\] (adj_r2 &lt;- 1 - sse/sst * (n - 1) / (n - k - 1)) ## [1] 0.83 summary(m)$adj.r.squared ## [1] 0.83 4.7.1.6 AIC Akaike’s Information Criteria (AIC) is a penalization metric. The lower the AIC, the better the model. AIC(m) ## [1] 160 4.7.1.7 AICc AICc corrects AIC for small sample sizes. AIC(m) + (2 * k * (k + 1)) / (n - k - 1) ## [1] 168 4.7.1.8 BIC The Basiean information criteria (BIC) is like AIC, but with a stronger penalty for additional variables. BIC(m) ## [1] 176 4.7.1.9 Mallows Cp Mallows Cp is a variant of AIC. 4.7.1.9.1 Example Compare the full model to a model without cyl. The glance() function from the broom package calculates many validation metrics. Here are the validation stats for the full model and then the reduced model. library(broom) glance(m) %&gt;% select(adj.r.squared, sigma, AIC, BIC, p.value) ## # A tibble: 1 x 5 ## adj.r.squared sigma AIC BIC p.value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.826 2.51 160. 176. 0.0000000481 glance(lm(mpg ~ . - cyl, d[, 1:9])) %&gt;% select(adj.r.squared, sigma, AIC, BIC, p.value) ## # A tibble: 1 x 5 ## adj.r.squared sigma AIC BIC p.value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.829 2.50 158. 171. 0.00000000453 The ajusted R2 increased and AIC and BIC decreased, meaning the full model is less efficient at explaining the variability in the response value. The residual standard error sigma is smaller for the reduced model. Finally, the F statistic p-value is smaller for the reduced model, meaning the reduced model is statistically more significant. Note that these regression metrics are all internal measures, that is they have been computed on the training dataset, not the test dataset. 4.7.2 Cross-Validation Cross-validation is a set of methods for measuring the performance of a predictive model on a test dataset. The main measures of prediction performance are R2, RMSE and MAE. 4.7.2.1 Validation Set To perform validation set cross validation, randomly split the data into a training data set and a test data set. Fit models to the training data set, then predict values with the validation set. The model that produces the best prediction performance is the preferred model. The caret package provides useful methods for cross-validation. 4.7.2.1.1 Example library(caret) set.seed(123) train_idx &lt;- createDataPartition(y = d$mpg, p = 0.80, list = FALSE) d.train &lt;- d[train_idx, ] d.test &lt;- d[-train_idx, ] Build the model using d.train, make predictions, then calculate the R2, RMSE, and MAE. Use the train() function from the caret package. Use method = \"none\" to simply fit the model to the entire data set. set.seed(123) m1 &lt;- train(mpg ~ ., data = d.train[, 1:9], method = &quot;lm&quot;, trControl = trainControl(method = &quot;none&quot;)) print(m1) ## Linear Regression ## ## 28 samples ## 8 predictor ## ## No pre-processing ## Resampling: None postResample(pred = predict(m1, newdata = d.test), obs = d.test$mpg) ## RMSE Rsquared MAE ## 3.10 0.96 2.45 The validation set method is only useful when you have a large data set to partition. A second disadvantage is that building a model on a fraction of the data leaves out information. The test error will vary with which observations are included in the training set. 4.7.2.2 LOOCV Leave one out cross validation (LOOCV) works by successively modeling with training sets leaving out one data point, then averaging the prediction errors. set.seed(123) m2 &lt;- train(mpg ~ ., data = d.train[, 1:9], method = &quot;lm&quot;, trControl = trainControl(method = &quot;LOOCV&quot;)) print(m2) ## Linear Regression ## ## 28 samples ## 8 predictor ## ## No pre-processing ## Resampling: Leave-One-Out Cross-Validation ## Summary of sample sizes: 27, 27, 27, 27, 27, 27, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 2.8 0.76 2.3 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE postResample(pred = predict(m2, newdata = d.test), obs = d.test$mpg) ## RMSE Rsquared MAE ## 3.10 0.96 2.45 This method isn’t perfect either. It repeats as many times as there are data points, so the execution time may be long. LOOCV is also sensitive to outliers. 4.7.2.3 K-fold Cross-Validation K-fold cross-validation splits the dataset into k folds (subsets), then uses k-1 of the folds for a training set and the remaining fold for a test set, then repeats for all permutations of k taken k-1 at a time. E.g., 3-fold cross-validation will partition the data into sets A, B, and C, then create train/test splits of [AB, C], [AC, B], and [BC, A]. K-fold cross-validation is less computationally expensive than LOOCV, and often yields more accurate test error rate estimates. What is the right value of k? The lower is k the more biased the estimates; the higher is k the larger the estimate variability. At the extremes k = 2 is the validation set method, and k = n is the LOOCV method. In practice, one typically performs k-fold cross-validation using k = 5 or k = 10 because these values have been empirically shown to balence bias and variance. set.seed(123) m3 &lt;- train(mpg ~ ., data = d.train[, 1:9], method = &quot;lm&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 5)) print(m3) ## Linear Regression ## ## 28 samples ## 8 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 22, 22, 23, 22, 23 ## Resampling results: ## ## RMSE Rsquared MAE ## 3 0.85 2.6 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE postResample(pred = predict(m3, newdata = d.test), obs = d.test$mpg) ## RMSE Rsquared MAE ## 3.10 0.96 2.45 4.7.2.4 Repeated K-fold CV You can also perform k-fold cross-validation multiple times and average the results. Specify method = \"repeatedcv\" and repeats = 3 in the trainControl object for three repeats. set.seed(123) m4 &lt;- train(mpg ~ ., data = d.train[, 1:9], method = &quot;lm&quot;, trControl = trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 3)) print(m4) ## Linear Regression ## ## 28 samples ## 8 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold, repeated 3 times) ## Summary of sample sizes: 22, 22, 23, 22, 23, 23, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 3.1 0.81 2.7 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE postResample(pred = predict(m4, newdata = d.test), obs = d.test$mpg) ## RMSE Rsquared MAE ## 3.10 0.96 2.45 4.7.2.5 Bootstrapping Bootstrapping randomly selects a sample of n observations with replacement from the original dataset to evaluate the model. The procedure is repeated many times. Specify method = \"boot\" and number = 100 to perform 100 bootstrap samples. set.seed(123) m5 &lt;- train(mpg ~ ., data = d.train[, 1:9], method = &quot;lm&quot;, trControl = trainControl(method = &quot;boot&quot;, number = 100)) ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit ## may be misleading ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit ## may be misleading ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit ## may be misleading print(m5) ## Linear Regression ## ## 28 samples ## 8 predictor ## ## No pre-processing ## Resampling: Bootstrapped (100 reps) ## Summary of sample sizes: 28, 28, 28, 28, 28, 28, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 3.9 0.64 3.2 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE postResample(pred = predict(m5, newdata = d.test), obs = d.test$mpg) ## RMSE Rsquared MAE ## 3.10 0.96 2.45 4.7.3 Gain Curve For supervised learning purposes, a visual way to evaluate a regression model is with the gain curve. This visualization compares a predictive model score to an actual outcome (either binary (0/1) or continuous). The gain curve plot measures how well the model score sorts the data compared to the true outcome value. The x-axis is the fraction of items seen when sorted by score, and the y-axis is the cumulative summed true outcome when sorted by score. For comparison, GainCurvePlot also plots the “wizard curve”: the gain curve when the data is sorted according to its true outcome. A relative Gini score close to 1 means the model sorts responses well. library(WVPlots) d$fitted &lt;- m$fitted.values GainCurvePlot(d, xvar = &quot;fitted&quot;, truthVar = &quot;mpg&quot;, title = &quot;Model Gain Curve&quot;) "],
["ols-reference.html", "4.8 OLS Reference", " 4.8 OLS Reference Penn State University, STAT 501, Lesson 12: Multicollinearity &amp; Other Regression Pitfalls. https://newonlinecourses.science.psu.edu/stat501/lesson/12. STHDA. Bootstrap Resampling Essentials in R. http://www.sthda.com/english/articles/38-regression-model-validation/156-bootstrap-resampling-essentials-in-r/ Molnar, Christoph. “Interpretable machine learning. A Guide for Making Black Box Models Explainable”, 2019. https://christophm.github.io/interpretable-ml-book/. "],
["generalized-linear-models.html", "Chapter 5 Generalized Linear Models", " Chapter 5 Generalized Linear Models These notes are primarily from PSU STAT 504 which uses Alan Agresti’s Categorical Data Analysis (Agresti 2013). I also reviewed PSU STAT 501, DataCamp’s Generalized Linar Models in R, DataCamp’s Multiple and Logistic Regression, and **Interpretable machine learning*\"** (Molnar 2020). The linear regression model, \\(E(Y|X) = X \\beta\\), structured as \\(y_i = X_i \\beta + \\epsilon_i\\) where \\(X_i \\beta = \\mu_i\\), assumes the response is a linear function of the predictors and the residuals are independent random variables normally distributed with mean zero and constant variance, \\(\\epsilon \\sim N \\left(0, \\sigma^2 \\right)\\). This implies that given some set of predictors, the response is normally distributed about its expected value, \\(y_i \\sim N \\left(\\mu_i, \\sigma^2 \\right)\\). However, there are many situations where this assumption of normality fails. Generalized linear models (GLMs) are a generalization of the linear regression model that addresses non-normal response distributions. The response given a set of predictors will not have a normal distribution if its underlying data-generating process is binomial or multinomial (proportions), Poisson (counts), or exponential (time-to-event). In these situations a regular linear regression can predict proportions outside [0, 100] or counts or times that are negative. GLMs solve this problem by modeling a function of the expected value of \\(y\\), \\(f(E(Y|X)) = X \\beta\\). There are three components to a GLM: the random component is the probability distribution of the response variable (normal, binomial, Poisson, etc.); the systematic component is the explanatory variables \\(X\\beta\\); and the link function \\(\\eta\\) specifies the link between random and systematic components, converting the response range to \\([-\\infty, +\\infty]\\). Linear regression is thus a special case of GLM where link function is the identity function, \\(f(E(Y|X)) = E(Y|X)\\). For a logistic regression, where the data generating process is binomial, the link function is \\[f(E(Y|X)) = \\ln \\left( \\frac{E(Y|X)}{1 - E(Y|X)} \\right) = \\ln \\left( \\frac{\\pi}{1 - \\pi} \\right) = logit(\\pi)\\] where \\(\\pi\\) is the event probability. (As an aside, you have probably heard of the related “probit” regression. The probit regression link function is \\(f(E(Y|X)) = \\Phi^{-1}(E(Y|X)) = \\Phi^{-1}(\\pi)\\). The difference between the logistic and probit link function is theoretical, and the practical significance is slight. You can probably safely ignore probit). For a Poisson regression, the link function is \\[f(E(Y|X)) = \\ln (E(Y|X)) = \\ln(\\lambda)\\] where \\(\\lambda\\) is the expected event rate. For an exponential regression, the link function is \\[f(E(Y|X) = -E(Y|X) = -\\lambda\\] where \\(\\lambda\\) is the expected time to event. GLM uses maximum likelihood estimation (MLE) rather than ordinary least squares (OLS) to estimate the parameters, and thus relies on large-sample approximations. In R, specify a GLM just like an linear model, but with the glm() function, specifying the distribution with the family parameter. family = \"gaussian\": linear regression family = \"binomial\": logistic regression family = binomial(link = \"probit\"): probit regression family = \"poisson\": Poisson regression References "],
["logistic-regression.html", "5.1 Logistic Regression", " 5.1 Logistic Regression Logistic regression estimates the probability of a particular level of a categorical response variable given a set of predictors. The response levels can be binary, nominal (multiple categories), or ordinal (multiple levels). The binary logistic regression model is \\[y = logit(\\pi) = \\ln \\left( \\frac{\\pi}{1 - \\pi} \\right) = X \\beta\\] where \\(\\pi\\) is the event probability. The model predicts the log odds of the response variable. The maximum likelihood estimator maximizes the likelihood function \\[L(\\beta; y, X) = \\prod_{i=1}^n \\pi_i^{y_i}(1 - \\pi_i)^{(1-y_i)} = \\prod_{i=1}^n\\frac{\\exp(y_i X_i \\beta)}{1 + \\exp(X_i \\beta)}.\\] There is no closed-form solution, so GLM estimates coefficients with interatively reweighted least squares. Here is a case study to illustrate the points. Dataset donner contains observations of 45 members of the Donner party with response variable (surv) an explanatory variables age and sex. glimpse(donner) ## Rows: 45 ## Columns: 3 ## $ age &lt;dbl&gt; 23, 40, 40, 30, 28, 40, 45, 62, 65, 45, 25, 28, 28, 23, 22, 23... ## $ sex &lt;fct&gt; M, F, M, M, M, M, F, M, M, F, F, M, M, M, F, F, M, F, F, M, F,... ## $ surv &lt;fct&gt; Died, Lived, Lived, Died, Died, Died, Died, Died, Died, Died, ... donner %&gt;% mutate(surv = as.numeric(surv)-1) %&gt;% ggplot(aes(x = age, y = surv, color = sex)) + geom_jitter() + geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;binomial&quot;), se = FALSE) + theme_mf() + labs(title = &quot;Donner Party Survivorship&quot;, color = &quot;&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Fit a logistic regression \\(SURV = SEX + AGE + SEX : AGE\\). m &lt;- glm(surv ~ sex*age, data = donner, family = binomial(link = logit)) summary(m) ## ## Call: ## glm(formula = surv ~ sex * age, family = binomial(link = logit), ## data = donner) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.228 -0.939 -0.555 0.779 1.700 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 7.2464 3.2052 2.26 0.024 * ## sexM -6.9280 3.3989 -2.04 0.042 * ## age -0.1941 0.0874 -2.22 0.026 * ## sexM:age 0.1616 0.0943 1.71 0.086 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 61.827 on 44 degrees of freedom ## Residual deviance: 47.346 on 41 degrees of freedom ## AIC: 55.35 ## ## Number of Fisher Scoring iterations: 5 The “z value” in the Coefficients table is the Wald z statistic, \\(z = \\hat{\\beta} / SE(\\hat{\\beta})\\), which if squared is the Wald chi-squared statistic, \\(z^2\\). The p.value is the area to the right of \\(z^2\\) in the \\(\\chi_1^2\\) density curve: m %&gt;% tidy() %&gt;% mutate( z = estimate / std.error, p_z2 = pchisq(z^2, df = 1, lower.tail = FALSE) ) %&gt;% select(term, estimate, z, p_z2) ## # A tibble: 4 x 4 ## term estimate z p_z2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 7.25 2.26 0.0238 ## 2 sexM -6.93 -2.04 0.0415 ## 3 age -0.194 -2.22 0.0264 ## 4 sexM:age 0.162 1.71 0.0865 Below the Coefficients table, the “dispersion parameter” refers to overdispersion, a common issue with GLM. For a logistic regression, the response variable should be distributed \\(y_i \\sim Bin(n_i, \\pi_i)\\) with \\(\\mu_i = n_i \\pi_i\\) and \\(\\sigma^2 = \\pi (1 - \\pi)\\). Overdispersion means the data shows evidence of variance greater than \\(\\sigma^2\\). “Fisher scoring” is a method for ML estimation. Logistic regression uses an iterative procedure to fit the model, so this section indicates whether the algorithm converged. The null deviance is the likelihood ratio \\(G^2 = 61.827\\) of the intercept-only model. The residual deviance is the likelihood ratio \\(G^2 = 47.346\\) after including all model covariates. \\(G^2\\) is large, so reject the null hypothesis of no age and sex effects. The ANOVA table shows the change in deviance from adding each variable successively to the model. anova(m) ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: surv ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev ## NULL 44 61.8 ## sex 1 4.54 43 57.3 ## age 1 6.03 42 51.3 ## sex:age 1 3.91 41 47.3 glance(m) ## # A tibble: 1 x 7 ## null.deviance df.null logLik AIC BIC deviance df.residual ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 61.8 44 -23.7 55.3 62.6 47.3 41 Plug in values to interpret the model. The log odds of a 24 year-old female surviving is \\(\\hat{y} = 2.59\\). The log odds of a 24 year-old male surviving is \\(\\hat{y} = -0.46\\). coef(m)[&quot;(Intercept)&quot;] + coef(m)[&quot;sexM&quot;]*0 + coef(m)[&quot;age&quot;]*24 + coef(m)[&quot;sexM:age&quot;]*0*24 coef(m)[&quot;(Intercept)&quot;] + coef(m)[&quot;sexM&quot;]*1 + coef(m)[&quot;age&quot;]*24 + coef(m)[&quot;sexM:age&quot;]*1*24 # Or use predict() (lo_f &lt;- predict(m, newdata = data.frame(sex = &quot;F&quot;, age = 24))) (lo_m &lt;- predict(m, newdata = data.frame(sex = &quot;M&quot;, age = 24))) Log odds are not easy to interpet. Exponentiate the log odds to get the odds. \\[odds(\\hat{y}) = \\exp (\\hat{y}) = \\frac{\\pi}{1 - \\pi}.\\] The odds of a 24 year-old female surviving is \\(\\exp(\\hat{y}) = 13.31\\). The odds of a 24 year-old male surviving is \\(\\exp(\\hat{y}) = 0.63\\). exp(lo_f) exp(lo_m) Solve for \\(\\pi\\) to get the probability. \\[\\pi = \\frac{\\exp (\\hat{y})}{1 + \\exp (\\hat{y})}\\] The probability of a 24 year-old female surviving is \\(\\pi = 0.93\\). The probability of a female of average age surviving is \\(\\pi = 0.39\\). The predict() function for a logistic model returns log-odds, but can also return \\(\\pi\\) by specifying parameter type = \"response\". exp(lo_f) / (1 + exp(lo_f)) exp(lo_m) / (1 + exp(lo_m)) # Or use predict(..., type = &quot;response&quot;) (p_f &lt;- predict(m, newdata = data.frame(sex = &quot;F&quot;, age =24), type = &quot;response&quot;)) (p_m &lt;- predict(m, newdata = data.frame(sex = &quot;M&quot;, age =24), type = &quot;response&quot;)) Interpret the coefficient estimates using the odds ratio, the ratio of the odds before and after an increment to the predictors. The odds ratio is how much the odds would be multiplied after a \\(\\delta = X_1 - X_0\\) unit increase in \\(X\\). \\[\\theta = \\frac{\\pi / (1 - \\pi) |_{X = X_1}}{\\pi / (1 - \\pi) |_{X = X_0}} = \\frac{\\exp (X_1 \\hat{\\beta})}{\\exp (X_0 \\hat{\\beta})} = \\exp ((X_1-X_0) \\hat{\\beta}) = \\exp (\\delta \\hat{\\beta})\\] The odds of a female surviving are multiplied by a factor of \\(\\exp(1 \\cdot (-0.19)) = 0.824\\) per additional year of age (or the odds fall by \\(1 - 0.824 = 17.6\\%\\)). The odds of a male surviving are multiplied by a factor of \\(\\exp(1 \\cdot (-0.161-0.19)) = 0.968\\) per additional year of age. exp(1 * (coef(m)[&quot;age&quot;] + 0*coef(m)[&quot;sexM:age&quot;])) # female exp(1 * (coef(m)[&quot;age&quot;] + 1*coef(m)[&quot;sexM:age&quot;])) # male oddsratio::or_glm() calculates the odds ratio from an increment in the predictor values. oddsratio::or_glm(donner, m, incr = list(age = 1)) ## predictor oddsratio ci_low (2.5) ci_high (97.5) increment ## 1 sexM 0.001 0.00 0.24 Indicator variable ## 2 age 0.824 0.65 0.94 1 ## 3 sexM:age 1.175 1.00 1.50 Indicator variable The predicted values can also be expressed as the probabilities \\(\\pi\\). This produces the familiar signmoidal shape of the binary relationship. augment(m, type.predict = &quot;response&quot;) %&gt;% ggplot(aes(x = age)) + geom_point(aes(y = surv)) + geom_line(aes(y = .fitted+1)) + theme_mf() + labs(x = &quot;AGE&quot;, y = &quot;Probability of SURVIVE&quot;, title = &quot;Binary Fitted Line Plot&quot;) Evaluate a logistic regression using a Gain curve or ROC curve. In the gain curve, the x-axis is the fraction of items seen when sorted by the predicted value, and the y-axis is the cumulative summed true outcome. The “wizard” curve is the gain curve when the data is sorted by the true outcome. If the model’s gain curve is close to the wizard curve, then the model predicted the response variable well. The grey area is the “gain” over a random prediction. 20 of the 45 members of the Donner party survived. The gain curve encountered 10 survivors (50%) within the first 12 observations (27%). It encountered all 20 survivors on the 37th observation. The bottom of the grey area is the outcome of a random model. Only half the survivors would be observed within 50% of the observations. The top of the grey area is the outcome of the perfect model, the “wizard curve”. Half the survivors would be observed in 10/45=22% of the observations. options(yardstick.event_first = FALSE) # set the second level as success augment(m, type.predict = &quot;response&quot;) %&gt;% yardstick::gain_curve(surv, .fitted) %&gt;% autoplot() + labs(title = &quot;Gain Curve&quot;) The ROC (Receiver Operating Characteristics) curve plots sensitivity vs specificity at different cut-off values for the probability, ranging cut-off from 0 to 1. options(yardstick.event_first = FALSE) # set the second level as success augment(m, type.predict = &quot;response&quot;) %&gt;% yardstick::roc_curve(surv, .fitted) %&gt;% autoplot() + labs(title = &quot;ROC Curve&quot;) "],
["multinomial-logistic-regression.html", "5.2 Multinomial Logistic Regression", " 5.2 Multinomial Logistic Regression The following notes rely on the [PSU STAT 504 course notes](https://online.stat.psu.edu/stat504/node/171/. Multinomial logistic regression models the odds the multinomial response variable \\(Y \\sim Mult(n, \\pi)\\) is in level \\(j\\) relative to baseline category \\(j^*\\) for all pairs of categories as a function of \\(k\\) explanatory variables, \\(X = (X_1, X_2, ... X_k)\\). \\[\\log \\left( \\frac{\\pi_{ij}}{\\pi_{ij^*}} \\right) = x_i^T \\beta_j, \\hspace{5mm} j \\ne j^2\\] Interpet the \\(k^{th}\\) element of \\(\\beta_j\\) as the increase in log-odds of falling a response in category \\(j\\) relative to category \\(j^*\\) resulting from a one-unit increase in the \\(k^{th}\\) predictor term, holding the other terms constant. Multinomial model is a type of GLM. Here is an example using multinomial logistic regression. A researcher classified the stomach contents of \\(n = 219\\) alligators according to \\(r = 5\\) categories (fish, Inv., Rept, Bird, Other) as a function of covariates Lake, Sex, and Size.. gator_dat &lt;- tribble( ~profile, ~Gender, ~Size, ~Lake, ~Fish, ~Invertebrate, ~Reptile, ~Bird, ~Other, &quot;1&quot;, &quot;f&quot;, &quot;&lt;2.3&quot;, &quot;george&quot;, 3, 9, 1, 0, 1, &quot;2&quot;, &quot;m&quot;, &quot;&lt;2.3&quot;, &quot;george&quot;, 13, 10, 0, 2, 2, &quot;3&quot;, &quot;f&quot;, &quot;&gt;2.3&quot;, &quot;george&quot;, 8, 1, 0, 0, 1, &quot;4&quot;, &quot;m&quot;, &quot;&gt;2.3&quot;, &quot;george&quot;, 9, 0, 0, 1, 2, &quot;5&quot;, &quot;f&quot;, &quot;&lt;2.3&quot;, &quot;hancock&quot;, 16, 3, 2, 2, 3, &quot;6&quot;, &quot;m&quot;, &quot;&lt;2.3&quot;, &quot;hancock&quot;, 7, 1, 0, 0, 5, &quot;7&quot;, &quot;f&quot;, &quot;&gt;2.3&quot;, &quot;hancock&quot;, 3, 0, 1, 2, 3, &quot;8&quot;, &quot;m&quot;, &quot;&gt;2.3&quot;, &quot;hancock&quot;, 4, 0, 0, 1, 2, &quot;9&quot;, &quot;f&quot;, &quot;&lt;2.3&quot;, &quot;oklawaha&quot;, 3, 9, 1, 0, 2, &quot;10&quot;, &quot;m&quot;, &quot;&lt;2.3&quot;, &quot;oklawaha&quot;, 2, 2, 0, 0, 1, &quot;11&quot;, &quot;f&quot;, &quot;&gt;2.3&quot;, &quot;oklawaha&quot;, 0, 1, 0, 1, 0, &quot;12&quot;, &quot;m&quot;, &quot;&gt;2.3&quot;, &quot;oklawaha&quot;, 13, 7, 6, 0, 0, &quot;13&quot;, &quot;f&quot;, &quot;&lt;2.3&quot;, &quot;trafford&quot;, 2, 4, 1, 1, 4, &quot;14&quot;, &quot;m&quot;, &quot;&lt;2.3&quot;, &quot;trafford&quot;, 3, 7, 1, 0, 1, &quot;15&quot;, &quot;f&quot;, &quot;&gt;2.3&quot;, &quot;trafford&quot;, 0, 1, 0, 0, 0, &quot;16&quot;, &quot;m&quot;, &quot;&gt;2.3&quot;, &quot;trafford&quot;, 8, 6, 6, 3, 5 ) gator_dat &lt;- gator_dat %&gt;% mutate( Gender = as_factor(Gender), Lake = fct_relevel(Lake, &quot;hancock&quot;), Size = as_factor(Size) ) There are 4 equations to estimate: \\[\\log \\left( \\frac{\\pi_j} {\\pi_{j^*}} \\right) = \\beta X\\] where \\(\\pi_{j^*}\\) is the probability of fish, the baseline category. Run a multivariate logistic regression model with VGAM::vglm(). library(VGAM) ## Warning: package &#39;VGAM&#39; was built under R version 4.0.2 vglm() fits 4 logit models. gator_vglm &lt;- vglm( cbind(Bird,Invertebrate,Reptile,Other,Fish) ~ Lake + Size + Gender, data = gator_dat, family = multinomial ) summary(gator_vglm) ## ## Call: ## vglm(formula = cbind(Bird, Invertebrate, Reptile, Other, Fish) ~ ## Lake + Size + Gender, family = multinomial, data = gator_dat) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## log(mu[,1]/mu[,5]) -1.199 -0.548 -0.2242 0.368 3.48 ## log(mu[,2]/mu[,5]) -1.322 -0.461 0.0105 0.381 1.87 ## log(mu[,3]/mu[,5]) -0.703 -0.575 -0.3551 0.261 2.06 ## log(mu[,4]/mu[,5]) -1.694 -0.289 -0.1081 1.124 1.37 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept):1 -1.857 0.581 -3.19 0.0014 ** ## (Intercept):2 -1.611 0.551 -2.93 0.0034 ** ## (Intercept):3 -2.287 0.657 -3.48 0.0005 *** ## (Intercept):4 -0.664 0.380 -1.75 0.0806 . ## Lakegeorge:1 -0.575 0.795 -0.72 0.4694 ## Lakegeorge:2 1.781 0.623 2.86 0.0043 ** ## Lakegeorge:3 -1.129 1.193 -0.95 0.3437 ## Lakegeorge:4 -0.767 0.569 -1.35 0.1776 ## Lakeoklawaha:1 -1.126 1.192 -0.94 0.3451 ## Lakeoklawaha:2 2.694 0.669 4.02 0.000057 *** ## Lakeoklawaha:3 1.401 0.810 1.73 0.0839 . ## Lakeoklawaha:4 -0.741 0.742 -1.00 0.3184 ## Laketrafford:1 0.662 0.846 0.78 0.4341 ## Laketrafford:2 2.936 0.687 4.27 0.000019 *** ## Laketrafford:3 1.932 0.825 2.34 0.0193 * ## Laketrafford:4 0.791 0.588 1.35 0.1784 ## Size&gt;2.3:1 0.730 0.652 1.12 0.2629 ## Size&gt;2.3:2 -1.336 0.411 -3.25 0.0012 ** ## Size&gt;2.3:3 0.557 0.647 0.86 0.3890 ## Size&gt;2.3:4 -0.291 0.460 -0.63 0.5275 ## Genderm:1 -0.606 0.689 -0.88 0.3787 ## Genderm:2 -0.463 0.396 -1.17 0.2418 ## Genderm:3 -0.628 0.685 -0.92 0.3598 ## Genderm:4 -0.253 0.466 -0.54 0.5881 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Names of linear predictors: log(mu[,1]/mu[,5]), log(mu[,2]/mu[,5]), ## log(mu[,3]/mu[,5]), log(mu[,4]/mu[,5]) ## ## Residual deviance: 50 on 40 degrees of freedom ## ## Log-likelihood: -73 on 40 degrees of freedom ## ## Number of Fisher scoring iterations: 5 ## ## No Hauck-Donner effect found in any of the estimates ## ## ## Reference group is level 5 of the response The residual deviance is 50.2637 on 40 degrees of freedom. Residual deviance tests the current model fit versus the saturated model. The saturated model, which fits a separate multinomial distribution to each of the 16 profiles (unique combinations of lake, sex and size), has 16 × 4 = 64 parameters. The current model has an intercept, three lake coefficients, one sex coefficient and one size coefficient for each of the four logit equations, for a total of 24 parameters. Therefore, the overall fit statistics have 64 − 24 = 40 degrees of freedom. E &lt;- data.frame(fitted(gator_vglm) * rowSums(gator_dat[, 5:9])) O &lt;- gator_dat %&gt;% select(Bird, Invertebrate, Reptile, Other, Fish) + .000001 (g2 &lt;- 2 * sum(O * log(O / E))) ## [1] 50 indicates the model fits okay, but not great. The Residual Deviance of 50.26 with 40 df from the table above output is reasonable, with p-value of 0.1282 and the statistics/df is close to 1 that is 1.256. "],
["ordinal-logistic-regression.html", "5.3 Ordinal Logistic Regression", " 5.3 Ordinal Logistic Regression These notes rely on UVA, PSU STAT 504 class notes, and Laerd Statistics. The ordinal logistic regression model is \\[logit[P(Y \\le j)] = \\log \\left[ \\frac{P(Y \\le j)}{P(Y \\gt j)} \\right] = \\alpha_j - \\beta X, \\hspace{5mm} j \\in [1, J-1]\\] where \\(j \\in [1, J-1]\\) are the levels of the ordinal outcome variable \\(Y\\). The proportional odds model assumes there is a common set of slope parameters \\(\\beta\\) for the predictors. The ordinal outcomes are distinguished by the \\(J-1\\) intercepts \\(\\alpha_j\\). The benchmark level is \\(J\\). Technically, the model could be written \\(logit[P(Y \\le j)] = \\alpha_j + \\zeta X\\), replacing beta with zeta because the model fits \\(\\alpha_j - \\beta X\\) instead of \\(\\alpha_j + \\beta X\\). Suppose you want to model the probability a respondent holds a political ideology [“Socialist”, “Liberal”, “Moderate”, “Conservative”, “Libertarian”] given their party affiliation [“Republican”, “Democrat”]. table(ideology) ## ideo ## party Socialist Liberal Moderate Conservative Libertarian ## Rep 30 46 148 84 99 ## Dem 80 81 171 41 55 5.3.1 Assumptions Ordinal regression makes four assumptions about the underlying data. One is that the response variable is ordinal (duh). The second is that the explanatory variables are continuous or categorical. You can include ordinal variables, but you need to treat them either as continous or categorical. Third, there is no multicollinearity. Fourth, the odds are proportional, meaning each independent variable has an identical effect at each cumulative split of the ordinal dependent variable. Test for proportionality with a full likelihood ratio test comparing the fitted location model to a model with varying location parameters. This test can sometimes flag violations that do not exist, so can also run separate binomial logistic regressions on cumulative dichotomous dependent variables to further determine if this assumption is met. 5.3.2 Modeling Fit a proportional odds logistic regression. pom &lt;- MASS::polr(ideo ~ party, data = ideology) summary(pom) ## Call: ## MASS::polr(formula = ideo ~ party, data = ideology) ## ## Coefficients: ## Value Std. Error t value ## partyDem -0.975 0.129 -7.54 ## ## Intercepts: ## Value Std. Error t value ## Socialist|Liberal -2.469 0.132 -18.736 ## Liberal|Moderate -1.475 0.109 -13.531 ## Moderate|Conservative 0.237 0.094 2.516 ## Conservative|Libertarian 1.070 0.104 10.292 ## ## Residual Deviance: 2474.98 ## AIC: 2484.98 The log-odds a Democrat identifies as Socialist vs &gt;Socialist, or equivalently, the log-odds a Democrat identifies as &lt;=Socialist vs &gt;=Liberal is \\[logit[P(Y \\le 1)] = -2.4690 - (-0.9745)(1) = -1.4945\\] which translates into an odds of \\[odds(Y&lt;=1) = exp(logit[P(Y \\le 1)]) = \\frac{exp(-2.469)}{exp(-0.9745)} = 0.2244\\] It is the same for Republicans, except multiply the slope coefficient by zero. \\[logit[P(Y \\le 1)] = -2.4690 - (-0.9745)(0) = -2.4690\\] \\[odds(Y&lt;=1) = exp(logit[P(Y \\le 1)]) = \\frac{exp(-2.469)}{exp(0)} = -2.4690\\] The “proportional odds” part of the proportional odds model is that the ratios of the \\(J - 1\\) odds are identical for each level of the predictors. The numerators are always the same, and the denominators differ only by the exponent of the slope coefficient, \\(-0.9745\\). For all \\(j \\in [1, J-1]\\), the odds a Democrat’s ideology is \\(\\le j\\) vs \\(&gt;j\\) is \\(exp(-0.9745) = 2.6498\\) times that of a Republican’s odds. You can translate the cumulative odds to cumulative probabilities by taking the ratio \\(\\pi = exp(odds) / (1 + exp(odds))\\). The probability a Democrat identifies as &lt;=Socialist vs &gt;Socialist is \\[P(Y \\le 1) = \\frac{exp(-1.4945)} {(1 + exp(-1.4945))} = 0.183.\\] The individual probabilities are just the successive differences in the cumulative probabilities. The log odds a Democrat identifies as &lt;=Liberal vs &gt;Liberal are \\(logit[P(Y \\lt 2)] = -1.4745 - (-0.9745)(1) = -0.500\\), which translates into a probability of \\(P(Y \\le 2) = exp(-0.5) / (1 + exp(-0.5)) = 0.378\\). The probability a Democrat identifies as Liberal is the difference in adjacent cumulative probabilities, \\(P(Y \\le 2) - P(Y \\le 1) = 0.378 = 0.183 = 0.194\\). This is how the model to predicts the level probabilities. x &lt;- predict(pom, newdata = data.frame(party = c(&quot;Dem&quot;, &quot;Rep&quot;)), type = &quot;probs&quot;) rownames(x) &lt;- c(&quot;Dem&quot;, &quot;Rep&quot;) print(x) ## Socialist Liberal Moderate Conservative Libertarian ## Dem 0.183 0.19 0.39 0.11 0.11 ## Rep 0.078 0.11 0.37 0.19 0.26 Always check the assumption of proportional odds. One way to do this is by comparing the proportional odds model with a multinomial logit model, also called an unconstrained baseline logit model. The multinomial logit model models unordered responses and fits a slope to each level of the \\(J – 1\\) responses. The proportional odds model is nested in the multinomial model, so you can use a likelihood ratio test to see if the models are statistically different. mlm &lt;- nnet::multinom(ideo ~ party, data = ideology) ## # weights: 15 (8 variable) ## initial value 1343.880657 ## iter 10 value 1239.866743 ## final value 1235.648615 ## converged Calculate the difference in the deviance test statistics \\(D = -2 loglik(\\beta)\\). G &lt;- -2 * (logLik(pom)[1] - logLik(mlm)[1]) pchisq(G, df = length(pom$zeta) - 1, lower.tail = FALSE) ## [1] 0.3 The p-value is high, so do not reject the null hypothesis that the proportional odds model fits differently than the more complex multinomial logit model. 5.3.3 Case Study The General Social Survey for year 1972, 1973, and 1974 surveyed caucasian Christians about their attitudes att toward abortion. Respondents were classified by years of education edu and religious group att. abort %&gt;% pivot_wider(names_from = att, values_from = cnt) ## # A tibble: 27 x 6 ## year rel edu Neg Mix Pos ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1972 Prot Low 9 12 48 ## 2 1972 Prot Med 13 43 197 ## 3 1972 Prot High 4 9 139 ## 4 1972 SProt Low 9 17 30 ## 5 1972 SProt Med 6 10 97 ## 6 1972 SProt High 1 8 68 ## 7 1972 Cath Low 14 12 32 ## 8 1972 Cath Med 18 50 131 ## 9 1972 Cath High 8 13 64 ## 10 1973 Prot Low 4 16 59 ## # ... with 17 more rows Fit a proportional-odds cumulative logit model with just main effects. There are two main effects for year, two for rel, and two for edu, plus two logit equations for the response for a total of eight parameters. abort_mdl &lt;- MASS::polr(att ~ year + rel + edu, data = abort, weights = cnt) summary(abort_mdl) ## Call: ## MASS::polr(formula = att ~ year + rel + edu, data = abort, weights = cnt) ## ## Coefficients: ## Value Std. Error t value ## year1973 0.221 0.105 2.11 ## year1974 0.233 0.106 2.20 ## relSProt -0.249 0.113 -2.19 ## relCath -0.796 0.100 -7.93 ## eduMed 0.717 0.109 6.59 ## eduHigh 1.127 0.128 8.79 ## ## Intercepts: ## Value Std. Error t value ## Neg|Mix -2.33 0.14 -17.27 ## Mix|Pos -0.79 0.12 -6.49 ## ## Residual Deviance: 4059.32 ## AIC: 4075.32 All predictors are significant. Now fit the saturated model. abort_mdl_sat &lt;- MASS::polr(att ~ year*rel*edu, weights = cnt, data = abort) summary(abort_mdl_sat) ## Call: ## MASS::polr(formula = att ~ year * rel * edu, data = abort, weights = cnt) ## ## Coefficients: ## Value Std. Error t value ## year1973 0.3422 0.365 0.9370 ## year1974 -0.0144 0.361 -0.0398 ## relSProt -0.6274 0.366 -1.7141 ## relCath -0.7238 0.369 -1.9620 ## eduMed 0.5036 0.301 1.6743 ## eduHigh 1.6007 0.390 4.1068 ## year1973:relSProt 0.1093 0.524 0.2088 ## year1974:relSProt 0.4387 0.521 0.8421 ## year1973:relCath 0.3895 0.539 0.7228 ## year1974:relCath 0.8696 0.559 1.5543 ## year1973:eduMed 0.2805 0.441 0.6365 ## year1974:eduMed 0.5487 0.430 1.2761 ## year1973:eduHigh -0.5961 0.540 -1.1031 ## year1974:eduHigh -0.1175 0.541 -0.2171 ## relSProt:eduMed 1.1421 0.479 2.3849 ## relCath:eduMed 0.1275 0.424 0.3004 ## relSProt:eduHigh 0.3039 0.586 0.5186 ## relCath:eduHigh -0.5623 0.532 -1.0572 ## year1973:relSProt:eduMed -1.2710 0.667 -1.9069 ## year1974:relSProt:eduMed -1.3331 0.668 -1.9952 ## year1973:relCath:eduMed -0.8360 0.629 -1.3291 ## year1974:relCath:eduMed -0.8973 0.647 -1.3879 ## year1973:relSProt:eduHigh 0.9047 0.878 1.0309 ## year1974:relSProt:eduHigh -0.4321 0.805 -0.5369 ## year1973:relCath:eduHigh -0.2265 0.749 -0.3021 ## year1974:relCath:eduHigh -0.9896 0.766 -1.2915 ## ## Intercepts: ## Value Std. Error t value ## Neg|Mix -2.311 0.267 -8.647 ## Mix|Pos -0.761 0.261 -2.922 ## ## Residual Deviance: 4018.27 ## AIC: 4074.27 Compare the two models. anova(abort_mdl, abort_mdl_sat) ## Likelihood ratio tests of ordinal regression models ## ## Response: att ## Model Resid. df Resid. Dev Test Df LR stat. Pr(Chi) ## 1 year + rel + edu 3229 4059 ## 2 year * rel * edu 3209 4018 1 vs 2 20 41 0.0037 The likelihood ratio test indicates the main-effects model fits poorly in comparison to the saturated model (LR = 41.0, df = 20, p &lt; 0.01). From the table of coefficients,the effects of religion and education appear to be much more powerful than the year, so consider modeling an interaction between rel and edu. This is also what the stepwise AIC algorithm recommends. summary(abort_mdl_step) ## Call: ## MASS::polr(formula = att ~ year + rel + edu + rel:edu, data = abort, ## weights = cnt) ## ## Coefficients: ## Value Std. Error t value ## year1973 0.228 0.105 2.171 ## year1974 0.241 0.106 2.271 ## relSProt -0.450 0.213 -2.110 ## relCath -0.348 0.223 -1.556 ## eduMed 0.750 0.177 4.230 ## eduHigh 1.369 0.219 6.244 ## relSProt:eduMed 0.253 0.267 0.946 ## relCath:eduMed -0.389 0.260 -1.496 ## relSProt:eduHigh 0.385 0.336 1.146 ## relCath:eduHigh -0.944 0.307 -3.079 ## ## Intercepts: ## Value Std. Error t value ## Neg|Mix -2.26 0.17 -13.28 ## Mix|Pos -0.72 0.16 -4.49 ## ## Residual Deviance: 4040.44 ## AIC: 4064.44 Compare the model with the rel:edu interaction to the saturated model. anova(abort_mdl_step, abort_mdl_sat) ## Likelihood ratio tests of ordinal regression models ## ## Response: att ## Model Resid. df Resid. Dev Test Df LR stat. Pr(Chi) ## 1 year + rel + edu + rel:edu 3225 4040 ## 2 year * rel * edu 3209 4018 1 vs 2 16 22 0.14 Great, this time they are not significantly different (LR = 22.2, df = 16, p = 0.138). Interpret the results. Positive coefficients mean attitudes toward legalizing abortion are more positive relative to the reference year, 1972. The odds of supporting legalization in 1973 compared to 1972 were \\(exp(0.2281) = 1.26\\). The odds for 1974 vs 1972 were \\(exp(0.2410) = 1.27\\), so attitudes toward abortion became more positive from 1972 to 1973, but remained nearly unchanged from 1973 to 1974. Among Protestants (reference religious group), increasing education is associated with more positive attitudes toward abortion legalization. The odds of a Protestant with medium education vs low education supporting legalization are \\(exp(0.7504) = 2.12\\). Among Southern Protestants, odds are \\(exp(0.7504 + 0.2526) = 2.73\\). Therefore, the estimated effects of education for Southern Protestants are in the same direction as for Protestants but are somewhat larger. Note, however, that the interaction effect coefficient is not not significantly different from zero, so the effect of education among Protestants and Southern Protestants is not significantly different. Among Catholics, the medium vs low education odds are \\(exp(0.7504- 0.3892) = 1.44\\). And the high vs low education odds are \\(exp(1.3689 - 0.9442) = 1.53\\). Increasing education is still associated with more positive attitudes, but the effects are smaller than they are among Protestants and Southern Protestants. Example Summarization We used logistic regression to investigate whether groups with the Christian religion might moderate the effects of education on attitudes toward abortion legalization. For Protestants, higher education education was associated with higher, significant, increase of odds of a more positive attitude toward abortion legalization, b = 0.7504, SE = 0.1774, OR = 2.12, p &lt; .01. There was a significant interaction for Catholics at high levels of education, b = -0.9442, SE = 0.3066, p &lt; .01, relative to the Protestant reference group, but no significant interaction at medium education, and no interaction at all for Southern Protestants relative to the reference group. The figure above graphs the interaction, showing the change in the expected probability of positive attitude by education level for Protestant, Southern Protestant, and Catholic religious groups. Overall, the significant interaction for Catholics at high levels of education suggests that education has a different relationship to attitudes toward abortion depending on the individual’s religious group, but the difference between Protestant and Southern Protestant is minimal. "],
["poisson-regression.html", "5.4 Poisson Regression", " 5.4 Poisson Regression Poisson models count data, like “traffic tickets per day”, or “website hits per day”. The response is an expected rate or intensity. For count data, specify the generalized model, this time with family = poisson or family = quasipoisson. Recall that the probability of achieving a count \\(y\\) when the expected rate is \\(\\lambda\\) is distributed \\[P(Y = y|\\lambda) = \\frac{e^{-\\lambda} \\lambda^y}{y!}.\\] The poisson regression model is \\[\\lambda = \\exp(X \\beta).\\] You can solve this for \\(y\\) to get \\[y = X\\beta = \\ln(\\lambda).\\] That is, the model predicts the log of the response rate. For a sample of size n, the likelihood function is \\[L(\\beta; y, X) = \\prod_{i=1}^n \\frac{e^{-\\exp({X_i\\beta})}\\exp({X_i\\beta})^{y_i}}{y_i!}.\\] The log-likelihood is \\[l(\\beta) = \\sum_{i=1}^n (y_i X_i \\beta - \\sum_{i=1}^n\\exp(X_i\\beta) - \\sum_{i=1}^n\\log(y_i!).\\] Maximizing the log-likelihood has no closed-form solution, so the coefficient estimates are found through interatively reweighted least squares. Poisson processes assume the variance of the response variable equals its mean. “Equals” means the mean and variance are of a similar order of magnitude. If that assumption does not hold, use the quasi-poisson. Use Poisson regression for large datasets. If the predicted counts are much greater than zero (&gt;30), the linear regression will work fine. Whereas RMSE is not useful for logistic models, it is a good metric in Poisson. Dataset fire contains response variable injuries counting the number of injuries during the month and one explanatory variable, the month mo. fire &lt;- read_csv(file = &quot;C:/Users/mpfol/OneDrive/Documents/Data Science/Data/CivilInjury_0.csv&quot;) ## Parsed with column specification: ## cols( ## ID = col_double(), ## `Injury Date` = col_datetime(format = &quot;&quot;), ## `Total Injuries` = col_double() ## ) fire &lt;- fire %&gt;% mutate(mo = as.POSIXlt(`Injury Date`)$mon + 1) %&gt;% rename(dt = `Injury Date`, injuries = `Total Injuries`) str(fire) ## tibble [300 x 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ ID : num [1:300] 1 2 3 4 5 6 7 8 9 10 ... ## $ dt : POSIXct[1:300], format: &quot;2005-01-10&quot; &quot;2005-01-11&quot; ... ## $ injuries: num [1:300] 1 1 1 5 2 1 1 1 1 1 ... ## $ mo : num [1:300] 1 1 1 1 1 1 2 2 2 4 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. ID = col_double(), ## .. `Injury Date` = col_datetime(format = &quot;&quot;), ## .. `Total Injuries` = col_double() ## .. ) In a situation like this where there the relationship is bivariate, start with a visualization. ggplot(fire, aes(x = mo, y = injuries)) + geom_jitter() + geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;poisson&quot;)) + labs(title = &quot;Injuries by Month&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Fit a poisson regression in R using glm(formula, data, family = poisson). But first, check whether the mean and variance of injuries are the same magnitude? If not, then use family = quasipoisson. mean(fire$injuries) ## [1] 1.4 var(fire$injuries) ## [1] 1 They are of the same magnitude, so fit the regression with family = poisson. m2 &lt;- glm(injuries ~ mo, family = poisson, data = fire) summary(m2) ## ## Call: ## glm(formula = injuries ~ mo, family = poisson, data = fire) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.399 -0.347 -0.303 -0.250 4.318 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.2280 0.1048 2.18 0.03 * ## mo 0.0122 0.0140 0.87 0.38 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 139.87 on 299 degrees of freedom ## Residual deviance: 139.11 on 298 degrees of freedom ## AIC: 792.1 ## ## Number of Fisher Scoring iterations: 5 The predicted value \\(\\hat{y}\\) is the estimated log of the response variable, \\[\\hat{y} = X \\hat{\\beta} = \\ln (\\lambda).\\] Suppose mo is January (mo = ), then the log ofinjuries` is \\(\\hat{y} = 0.323787\\). Or, more intuitively, the expected count of injuries is \\(\\exp(0.323787) = 1.38\\) predict(m2, newdata = data.frame(mo=1)) ## 1 ## 0.24 predict(m2, newdata = data.frame(mo=1), type = &quot;response&quot;) ## 1 ## 1.3 Here is a plot of the predicted counts in red. augment(m2, type.predict = &quot;response&quot;) %&gt;% ggplot(aes(x = mo, y = injuries)) + geom_point() + geom_point(aes(y = .fitted), color = &quot;red&quot;) + scale_y_continuous(limits = c(0, NA)) + labs(x = &quot;Month&quot;, y = &quot;Injuries&quot;, title = &quot;Poisson Fitted Line Plot&quot;) Evaluate a logistic model fit with an analysis of deviance. (perf &lt;- glance(m2)) ## # A tibble: 1 x 7 ## null.deviance df.null logLik AIC BIC deviance df.residual ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 140. 299 -394. 792. 799. 139. 298 (pseudoR2 &lt;- 1 - perf$deviance / perf$null.deviance) ## [1] 0.0054 The deviance of the null model (no regressors) is 139.9. The deviance of the full model is 132.2. The psuedo-R2 is very low at .05. How about the RMSE? RMSE(pred = predict(m2, type = &quot;response&quot;), obs = fire$injuries) ## [1] 1 The average prediction error is about 0.99. That’s almost as much as the variance of injuries - i.e., just predicting the mean of injuries would be almost as good! Use the GainCurvePlot() function to plot the gain curve. augment(m2, type.predict = &quot;response&quot;) %&gt;% ggplot(aes(x = injuries, y = .fitted)) + geom_point() + geom_smooth(method =&quot;lm&quot;) + labs(x = &quot;Actual&quot;, y = &quot;Predicted&quot;, title = &quot;Poisson Fitted vs Actual&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; augment(m2) %&gt;% data.frame() %&gt;% GainCurvePlot(xvar = &quot;.fitted&quot;, truthVar = &quot;injuries&quot;, title = &quot;Poisson Model&quot;) It seems that mo was a poor predictor of injuries. "],
["multivariate-statistical-analysis.html", "Chapter 6 Multivariate Statistical Analysis", " Chapter 6 Multivariate Statistical Analysis These notes are structured from the PSU STAT 504 course. "],
["background.html", "6.1 Background", " 6.1 Background "],
["manova.html", "6.2 MANOVA", " 6.2 MANOVA "],
["repeated-measures.html", "6.3 Repeated Measures", " 6.3 Repeated Measures "],
["lda.html", "6.4 LDA", " 6.4 LDA Linear Discriminant Analysis (LDA) is a supervised machine learning classification (binary or multimonial) and dimension reduction method. LDA finds linear combinations of variables that best “discriminate” the response classes. One approach (Welch) to LDA assumes the predictor variables are continuous random variables normally distributed and with equal variance. You will typically scale the data to meet these conditions. For a response variable of \\(k\\) levels, LDA produces \\(k-1\\) discriminants using Bayes Theorem. \\[Pr[Y = C_l | X] = \\frac{P[Y = C_l] P[X | Y = C_l]}{\\sum_{l=1}^C Pr[Y = C_l] Pr[X | Y = C_l]}\\] The probability that \\(Y\\) equals class level \\(C_l\\) given the predictors \\(X\\) equals the prior probability of \\(Y\\) multiplied by the probability of observing \\(X\\) if \\(Y = C_l\\) divided by the sum of all priors and probabilities of \\(X\\) given the priors. The predicted value for any \\(X\\) is just the \\(C_l\\) with the maximimum probability. One way to calculate the probabilities is by assuming \\(X\\) has a multivariate normal distribution with means \\(\\mu_l\\) and common variance \\(\\Sigma\\). Then the linear discriminant function group \\(l\\) is \\[X&#39;\\Sigma^{-1}\\mu_l - 0.5 \\mu_l^{&#39;}\\Sigma^{-1}\\mu_l + \\log(Pr[Y = C_l])\\] The theoretical means and covariance matrix is estimated by the sample mean \\(\\mu = \\bar{x}_l\\) and covariance \\(\\Sigma = S\\), and the population predictors \\(X\\) are replaced with the sample predictors \\(u\\). Another approach (Fisher) to LDA is to find a linear combination of predictors that maximizes the between-group covariance matrix \\(B\\) relative to the within-group covariance matrix \\(W\\). \\[\\frac{b&#39;Bb}{b&#39;Wb}\\] The solution to this optimization problem is the eigenvector corresponding to the largest eigenvalue of \\(W^{-1}B\\). This vector is a linear discrminant. Solving for two-group setting gives the discriminant function \\(S^{-1}(\\bar{x}_1 - \\bar{x}_2)\\) where \\(S^{-1}\\) is the inverse of the covariance matrix of the data and \\(\\bar{x}_1\\) and \\(\\bar{x}_2\\) are the means of each predictor in response groups 1 and 2. In practice, a new sample, \\(u\\), is projected onto the discriminant function as \\(uS^{-1}(\\bar{x}_1 - \\bar{x}_2)\\), which returns a discriminant score. A new sample is then classified into group 1 if the sample is closer to the group 1 mean than the group 2 mean in the projection: \\[\\left| b&#39;(u - \\bar{x}_1) \\right| - \\left| b&#39;(u - \\bar{x}_2) \\right| &lt; 0\\] In general, the model requires \\(CP + P(P + 1)/2\\) parameters with \\(P\\) predictors and \\(C\\) classes. The value of the extra parameters in LDA models is that the between-predictor correlations are explicitly handled by the model. This should provide some advantage to LDA over logistic regression when there are substantial correlations, although both models will break down when the multicollinearity becomes extreme. Fisher’s formulation is intuitive, easy to solve mathematically, and, unlike Welch’s approach, involves no assumptions about the underlying distributions of the data. In practice, it is best to center and scale predictors and remove near-zero-variance predictors. If the matrix is still not invertible, use PLS or regularization. "],
["regularization.html", "Chapter 7 Regularization", " Chapter 7 Regularization These notes are from this tutorial on DataCamp, the Machine Learning Toolbox DataCamp class, and Interpretable Machine Learning (Molnar 2020). Regularization is a set of methods that manage the bias-variance trade-off problem in linear regression. The linear regression model is \\(Y = X \\beta + \\epsilon\\), where \\(\\epsilon \\sim N(0, \\sigma^2)\\). OLS estimates the coefficients by minimizing the loss function \\[L = \\sum_{i = 1}^n \\left(y_i - x_i^{&#39;} \\hat\\beta \\right)^2.\\] The resulting estimate for the coefficients is \\[\\hat{\\beta} = \\left(X&#39;X\\right)^{-1}\\left(X&#39;Y\\right).\\] There are two important characteristics of any estimator: its bias and its variance. For OLS, these are \\[Bias(\\hat{\\beta}) = E(\\hat{\\beta}) - \\beta = 0\\] and \\[Var(\\hat{\\beta}) = \\sigma^2(X&#39;X)^{-1}\\] where the unknown population variance \\(\\sigma^2\\) is estimated from the residuals \\[\\hat\\sigma^2 = \\frac{\\epsilon&#39; \\epsilon}{n - k}.\\] The OLS estimator is unbiased, but can have a large variance when the predictor variables are highly correlated with each other, or when there are many predictors (notice how \\(\\hat{\\sigma}^2\\) increases as \\(k \\rightarrow n\\)). Stepwise selection balances the trade-off by eliminating variables, but this throws away information. Regularization keeps all the predictors, but reduces coefficient magnitudes to reduce variance at the expense of some bias. In the sections below, I’ll use the mtcars data set to predict mpg from the other variables using the caret::glmnet() function. glmnet() uses penalized maximum likelihood to fit generalized linear models such as ridge, lasso, and elastic net. I’ll compare the model performances by creating a training and validation set, and a common trainControl object to make sure the models use the same observations in the cross-validation folds. library(tidyverse) library(caret) data(&quot;mtcars&quot;) set.seed(123) partition &lt;- createDataPartition(mtcars$mpg, p = 0.8, list = FALSE) training &lt;- mtcars[partition, ] testing &lt;- mtcars[-partition, ] train_control &lt;- trainControl( method = &quot;repeatedcv&quot;, number = 5, repeats = 5, savePredictions = &quot;final&quot; # saves predictions from optimal tuning parameters ) References "],
["ridge.html", "7.1 Ridge", " 7.1 Ridge Ridge regression estimates the linear model coefficients by minimizing an augmented loss function which includes a term, \\(\\lambda\\), that penalizes the magnitude of the coefficient estimates, \\[L = \\sum_{i = 1}^n \\left(y_i - x_i^{&#39;} \\hat\\beta \\right)^2 + \\lambda \\sum_{j=1}^k \\hat{\\beta}_j^2.\\] The resulting estimate for the coefficients is \\[\\hat{\\beta} = \\left(X&#39;X + \\lambda I\\right)^{-1}\\left(X&#39;Y \\right).\\] As \\(\\lambda \\rightarrow 0\\), ridge regression approaches OLS. The bias and variance for the ridge estimator are \\[Bias(\\hat{\\beta}) = -\\lambda \\left(X&#39;X + \\lambda I \\right)^{-1} \\beta\\] \\[Var(\\hat{\\beta}) = \\sigma^2 \\left(X&#39;X + \\lambda I \\right)^{-1}X&#39;X \\left(X&#39;X + \\lambda I \\right)^{-1}\\] The estimator bias increases with \\(\\lambda\\) and the estimator variance decreases with \\(\\lambda\\). The optimal level for \\(\\lambda\\) is the one that minimizes the root mean squared error (RMSE) or the Akaike or Bayesian Information Criterion (AIC or BIC), or R-squared. Example Specify alpha = 0 in a tuning grid for ridge regression (the following sections reveal how alpha distinguishes ridge, lasso, and elastic net). Note that I standardize the predictors in the preProcess step - ridge regression requires standardization. set.seed(1234) mdl_ridge &lt;- train( mpg ~ ., data = training, method = &quot;glmnet&quot;, metric = &quot;RMSE&quot;, # Choose from RMSE, RSquared, AIC, BIC, ...others? preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneGrid = expand.grid( .alpha = 0, # optimize a ridge regression .lambda = seq(0, 5, length.out = 101) ), trControl = train_control ) mdl_ridge ## glmnet ## ## 28 samples ## 10 predictors ## ## Pre-processing: centered (10), scaled (10) ## Resampling: Cross-Validated (5 fold, repeated 5 times) ## Summary of sample sizes: 21, 24, 22, 21, 24, 21, ... ## Resampling results across tuning parameters: ## ## lambda RMSE Rsquared MAE ## 0.00 2.6 0.88 2.2 ## 0.05 2.6 0.88 2.2 ## 0.10 2.6 0.88 2.2 ## 0.15 2.6 0.88 2.2 ## 0.20 2.6 0.88 2.2 ## 0.25 2.6 0.88 2.2 ## 0.30 2.6 0.88 2.2 ## 0.35 2.6 0.88 2.2 ## 0.40 2.6 0.88 2.2 ## 0.45 2.6 0.88 2.2 ## 0.50 2.6 0.88 2.2 ## 0.55 2.5 0.88 2.2 ## 0.60 2.5 0.88 2.2 ## 0.65 2.5 0.88 2.2 ## 0.70 2.5 0.88 2.2 ## 0.75 2.5 0.88 2.2 ## 0.80 2.5 0.88 2.2 ## 0.85 2.5 0.88 2.2 ## 0.90 2.5 0.88 2.2 ## 0.95 2.5 0.88 2.2 ## 1.00 2.5 0.88 2.2 ## 1.05 2.5 0.88 2.2 ## 1.10 2.5 0.88 2.2 ## 1.15 2.5 0.88 2.1 ## 1.20 2.4 0.88 2.1 ## 1.25 2.4 0.88 2.1 ## 1.30 2.4 0.88 2.1 ## 1.35 2.4 0.88 2.1 ## 1.40 2.4 0.88 2.1 ## 1.45 2.4 0.88 2.1 ## 1.50 2.4 0.88 2.1 ## 1.55 2.4 0.88 2.1 ## 1.60 2.4 0.88 2.1 ## 1.65 2.4 0.88 2.1 ## 1.70 2.4 0.88 2.1 ## 1.75 2.4 0.88 2.1 ## 1.80 2.4 0.88 2.1 ## 1.85 2.4 0.88 2.1 ## 1.90 2.4 0.88 2.1 ## 1.95 2.4 0.88 2.1 ## 2.00 2.4 0.88 2.1 ## 2.05 2.4 0.88 2.1 ## 2.10 2.4 0.88 2.1 ## 2.15 2.4 0.88 2.1 ## 2.20 2.4 0.88 2.1 ## 2.25 2.4 0.88 2.1 ## 2.30 2.4 0.88 2.1 ## 2.35 2.4 0.88 2.1 ## 2.40 2.4 0.88 2.1 ## 2.45 2.4 0.88 2.1 ## 2.50 2.4 0.88 2.1 ## 2.55 2.4 0.88 2.1 ## 2.60 2.4 0.88 2.1 ## 2.65 2.4 0.88 2.1 ## 2.70 2.4 0.88 2.1 ## 2.75 2.4 0.88 2.1 ## 2.80 2.4 0.88 2.1 ## 2.85 2.4 0.88 2.1 ## 2.90 2.4 0.88 2.1 ## 2.95 2.4 0.88 2.1 ## 3.00 2.4 0.88 2.1 ## 3.05 2.4 0.88 2.1 ## 3.10 2.4 0.88 2.1 ## 3.15 2.4 0.88 2.1 ## 3.20 2.4 0.88 2.1 ## 3.25 2.4 0.88 2.1 ## 3.30 2.4 0.88 2.1 ## 3.35 2.4 0.88 2.1 ## 3.40 2.4 0.88 2.1 ## 3.45 2.4 0.88 2.1 ## 3.50 2.4 0.88 2.1 ## 3.55 2.4 0.88 2.1 ## 3.60 2.4 0.88 2.1 ## 3.65 2.4 0.88 2.1 ## 3.70 2.4 0.88 2.1 ## 3.75 2.4 0.88 2.1 ## 3.80 2.4 0.88 2.1 ## 3.85 2.4 0.88 2.1 ## 3.90 2.4 0.88 2.1 ## 3.95 2.4 0.88 2.1 ## 4.00 2.4 0.88 2.1 ## 4.05 2.4 0.88 2.1 ## 4.10 2.4 0.88 2.1 ## 4.15 2.4 0.88 2.1 ## 4.20 2.4 0.88 2.1 ## 4.25 2.4 0.88 2.1 ## 4.30 2.4 0.88 2.1 ## 4.35 2.4 0.88 2.1 ## 4.40 2.4 0.88 2.1 ## 4.45 2.4 0.88 2.1 ## 4.50 2.4 0.88 2.1 ## 4.55 2.4 0.88 2.1 ## 4.60 2.4 0.88 2.1 ## 4.65 2.4 0.88 2.1 ## 4.70 2.4 0.88 2.1 ## 4.75 2.4 0.88 2.1 ## 4.80 2.4 0.88 2.1 ## 4.85 2.4 0.88 2.1 ## 4.90 2.4 0.88 2.1 ## 4.95 2.4 0.88 2.1 ## 5.00 2.4 0.88 2.1 ## ## Tuning parameter &#39;alpha&#39; was held constant at a value of 0 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were alpha = 0 and lambda = 2.8. The model printout shows the RMSE, R-Squared, and mean absolute error (MAE) values at each lambda specified in the tuning grid. The final three lines summarize what happened. It did not tune alpha because I held it at 0 for ridge regression; it optimized using RMSE; and the optimal tuning values (at the minimum RMSE) were alpha = 0 and lambda = 2.75. You plot the model to see the tuning results. ggplot(mdl_ridge) + labs(title = &quot;Ridge Regression Parameter Tuning&quot;, x = &quot;lambda&quot;) varImp() ranks the predictors by the absolute value of the coefficients in the tuned model. The most important variables here were wt, disp, and am. plot(varImp(mdl_ridge)) "],
["lasso.html", "7.2 Lasso", " 7.2 Lasso Lasso stands for “least absolute shrinkage and selection operator”. Like ridge, lasso adds a penalty for coefficients, but instead of penalizing the sum of squared coefficients (L2 penalty), lasso penalizes the sum of absolute values (L1 penalty). As a result, for high values of \\(\\lambda\\), coefficients can be zeroed under lasso. The loss function for lasso is \\[L = \\sum_{i = 1}^n \\left(y_i - x_i^{&#39;} \\hat\\beta \\right)^2 + \\lambda \\sum_{j=1}^k \\left| \\hat{\\beta}_j \\right|.\\] Example Continuing with prediction of mpg from the other variables in the mtcars data set, follow the same steps as before, but with ridge regression. This time specify parameter alpha = 1 for ridge regression (it was 0 for ridge, and for elastic net it will be something in between and require optimization). set.seed(1234) mdl_lasso &lt;- train( mpg ~ ., data = training, method = &quot;glmnet&quot;, metric = &quot;RMSE&quot;, preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneGrid = expand.grid( .alpha = 1, # optimize a lasso regression .lambda = seq(0, 5, length.out = 101) ), trControl = train_control ) ## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : ## There were missing values in resampled performance measures. mdl_lasso$bestTune ## alpha lambda ## 14 1 0.65 The summary output shows the model did not tune alpha because I held it at 1 for lasso regression. The optimal tuning values (at the minimum RMSE) were alpha = 1 and lambda = 0.65. You can see the RMSE minimum on the the plot. ggplot(mdl_ridge) + labs(title = &quot;Lasso Regression Parameter Tuning&quot;, x = &quot;lambda&quot;) "],
["elastic-net.html", "7.3 Elastic Net", " 7.3 Elastic Net Elastic Net combines the penalties of ridge and lasso to get the best of both worlds. The loss function for elastic net is \\[L = \\frac{\\sum_{i = 1}^n \\left(y_i - x_i^{&#39;} \\hat\\beta \\right)^2}{2n} + \\lambda \\frac{1 - \\alpha}{2}\\sum_{j=1}^k \\hat{\\beta}_j^2 + \\lambda \\alpha\\left| \\hat{\\beta}_j \\right|.\\] In this loss function, new parameter \\(\\alpha\\) is a “mixing” parameter that balances the two approaches. If \\(\\alpha\\) is zero, you are back to ridge regression, and if \\(\\alpha\\) is one, you are back to lasso. Example Continuing with prediction of mpg from the other variables in the mtcars data set, follow the same steps as before, but with elastic net regression there are two parameters to optimize: \\(\\lambda\\) and \\(\\alpha\\). set.seed(1234) mdl_elnet &lt;- train( mpg ~ ., data = training, method = &quot;glmnet&quot;, metric = &quot;RMSE&quot;, preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneGrid = expand.grid( .alpha = seq(0, 1, length.out = 10), # optimize an elnet regression .lambda = seq(0, 5, length.out = 101) ), trControl = train_control ) ## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : ## There were missing values in resampled performance measures. mdl_elnet$bestTune ## alpha lambda ## 56 0 2.8 The optimal tuning values (at the mininum RMSE) were alpha = 0.0 and lambda = 2.75, so the mix is 100% ridge, 0% lasso. You can see the RMSE minimum on the the plot. Alpha is on the horizontal axis and the different lambdas are shown as separate series. ggplot(mdl_elnet) + labs(title = &quot;Elastic Net Regression Parameter Tuning&quot;, x = &quot;lambda&quot;) ## Warning: The shape palette can deal with a maximum of 6 discrete values because ## more than 6 becomes difficult to discriminate; you have 10. Consider ## specifying shapes manually if you must have them. ## Warning: Removed 404 rows containing missing values (geom_point). "],
["model-summary.html", "Model Summary", " Model Summary Make predictions on the validation data set for each of the three models. pr_ridge &lt;- postResample(pred = predict(mdl_ridge, newdata = testing), obs = testing$mpg) pr_lasso &lt;- postResample(pred = predict(mdl_lasso, newdata = testing), obs = testing$mpg) pr_elnet &lt;- postResample(pred = predict(mdl_elnet, newdata = testing), obs = testing$mpg) rbind(pr_ridge, pr_lasso, pr_elnet) ## RMSE Rsquared MAE ## pr_ridge 3.7 0.90 2.8 ## pr_lasso 4.0 0.97 3.0 ## pr_elnet 3.7 0.90 2.8 It looks like ridge/elnet was the big winner today based on RMSE and MAE. Lasso had the best Rsquared though. On average, ridge/elnet will miss the true value of mpg by 3.75 mpg (RMSE) or 2.76 mpg (MAE). The model explains about 90% of the variation in mpg. You can also compare the models by resampling. model.resamples &lt;- resamples(list(Ridge = mdl_ridge, Lasso = mdl_lasso, ELNet = mdl_elnet)) summary(model.resamples) ## ## Call: ## summary.resamples(object = model.resamples) ## ## Models: Ridge, Lasso, ELNet ## Number of resamples: 25 ## ## MAE ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## Ridge 0.66 1.6 2.2 2.1 2.5 3.5 0 ## Lasso 0.81 1.9 2.2 2.3 2.6 4.0 0 ## ELNet 0.66 1.6 2.2 2.1 2.5 3.5 0 ## ## RMSE ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## Ridge 0.73 1.9 2.5 2.4 2.8 4.3 0 ## Lasso 0.91 2.1 2.5 2.6 2.9 4.5 0 ## ELNet 0.73 1.9 2.5 2.4 2.8 4.3 0 ## ## Rsquared ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## Ridge 0.69 0.84 0.89 0.88 0.94 0.98 0 ## Lasso 0.63 0.81 0.87 0.86 0.94 1.00 0 ## ELNet 0.69 0.84 0.89 0.88 0.94 0.98 0 You want the smallest mean RMSE, and a small range of RMSEs. Ridge/elnet had the smallest mean, and a relatively small range. Boxplots are a common way to visualize this information. bwplot(model.resamples, metric = &quot;RMSE&quot;, main = &quot;Model Comparison on Resamples&quot;) Now that you have identified the optimal model, capture its tuning parameters and refit the model to the entire data set. set.seed(123) mdl_final &lt;- train( mpg ~ ., data = training, method = &quot;glmnet&quot;, metric = &quot;RMSE&quot;, preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneGrid = data.frame( .alpha = mdl_ridge$bestTune$alpha, # optimized hyperparameters .lambda = mdl_ridge$bestTune$lambda), # optimized hyperparameters trControl = train_control ) mdl_final ## glmnet ## ## 28 samples ## 10 predictors ## ## Pre-processing: centered (10), scaled (10) ## Resampling: Cross-Validated (5 fold, repeated 5 times) ## Summary of sample sizes: 22, 22, 23, 22, 23, 23, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 2.4 0.89 2.1 ## ## Tuning parameter &#39;alpha&#39; was held constant at a value of 0 ## Tuning ## parameter &#39;lambda&#39; was held constant at a value of 2.8 The model is ready to predict on new data! Here are some final conclusions on the models. Lasso can set some coefficients to zero, thus performing variable selection. Lasso and Ridge address multicollinearity differently: in ridge regression, the coefficients of correlated predictors are similar; In lasso, one of the correlated predictors has a larger coefficient, while the rest are (nearly) zeroed. Lasso tends to do well if there are a small number of significant parameters and the others are close to zero. Ridge tends to work well if there are many large parameters of about the same value. In practice, you don’t know which will be best, so run cross-validation pick the best. "],
["decision-trees.html", "Chapter 8 Decision Trees", " Chapter 8 Decision Trees Decision trees, also known as classification and regression tree (CART) models, are tree-based methods for supervised machine learning. Simple classification trees and regression trees are easy to use and interpret, but are not competitive with the best machine learning methods. However, they form the foundation for ensemble models such as bagged trees, random forests, and boosted trees, which although less interpretable, are very accurate. CART models segment the predictor space into \\(K\\) non-overlapping terminal nodes (leaves). Each node is described by a set of rules which can be used to predict new responses. The predicted value \\(\\hat{y}\\) for each node is the mode (classification) or mean (regression). CART models define the nodes through a top-down greedy process called recursive binary splitting. The process is top-down because it begins at the top of the tree with all observations in a single region and successively splits the predictor space. It is greedy because at each splitting step, the best split is made at that particular step without consideration to subsequent splits. The best split is the predictor variable and cutpoint that minimizes a cost function. The most common cost function for regression trees is the sum of squared residuals, \\[RSS = \\sum_{k=1}^K\\sum_{i \\in A_k}{\\left(y_i - \\hat{y}_{A_k} \\right)^2}.\\] For classification trees, it is the Gini index, \\[G = \\sum_{c=1}^C{\\hat{p}_{kc}(1 - \\hat{p}_{kc})},\\] and the entropy (aka information statistic) \\[D = - \\sum_{c=1}^C{\\hat{p}_{kc} \\log \\hat{p}_{kc}}\\] where \\(\\hat{p}_{kc}\\) is the proportion of training observations in node \\(k\\) that are class \\(c\\). A completely pure node in a binary tree would have \\(\\hat{p} \\in \\{ 0, 1 \\}\\) and \\(G = D = 0\\). A completely impure node in a binary tree would have \\(\\hat{p} = 0.5\\) and \\(G = 0.5^2 \\cdot 2 = 0.25\\) and \\(D = -(0.5 \\log(0.5)) \\cdot 2 = 0.69\\). CART repeats the splitting process for each child node until a stopping criterion is satisfied, usually when no node size surpasses a predefined maximum, or continued splitting does not improve the model significantly. CART may also impose a minimum number of observations in each node. The resulting tree likely over-fits the training data and therefore does not generalize well to test data, so CART prunes the tree, minimizing the cross-validated prediction error. Rather than cross-validating every possible subtree to find the one with minimum error, CART uses cost-complexity pruning. Cost-complexity is the tradeoff between error (cost) and tree size (complexity) where the tradeoff is quantified with cost-complexity parameter \\(c_p\\). The cost complexity of the tree, \\(R_{c_p}(T)\\), is the sum of its risk (error) plus a “cost complexity” factor \\(c_p\\) multiple of the tree size \\(|T|\\). \\[R_{c_p}(T) = R(T) + c_p|T|\\] \\(c_p\\) can take on any value from \\([0..\\infty]\\), but it turns out there is an optimal tree for ranges of \\(c_p\\) values, so there are only a finite set of interesting values for \\(c_p\\) (James et al. 2013) (Therneau and Atkinson 2019) (Kuhn and Johnson 2016). A parametric algorithm identifies the interesting \\(c_p\\) values and their associated pruned trees, \\(T_{c_p}\\). CART uses cross-validation to determine which \\(c_p\\) is optimal. References "],
["classification-tree.html", "8.1 Classification Tree", " 8.1 Classification Tree You don’t usually build a simple classification tree on its own, but it is a good way to build understanding, and the ensemble models build on the logic. I’ll learn by example, using the ISLR::OJ data set to predict which brand of orange juice, Citrus Hill (CH) or Minute Maid = (MM), customers Purchase from its 17 predictor variables. library(tidyverse) library(caret) library(rpart) # classification and regression trees library(rpart.plot) # better formatted plots than the ones in rpart oj_dat &lt;- ISLR::OJ skimr::skim(oj_dat) Table 8.1: Data summary Name oj_dat Number of rows 1070 Number of columns 18 _______________________ Column type frequency: factor 2 numeric 16 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts Purchase 0 1 FALSE 2 CH: 653, MM: 417 Store7 0 1 FALSE 2 No: 714, Yes: 356 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist WeekofPurchase 0 1 254.38 15.56 227.00 240.00 257.00 268.00 278.00 ▆▅▅▇▇ StoreID 0 1 3.96 2.31 1.00 2.00 3.00 7.00 7.00 ▇▅▃▁▇ PriceCH 0 1 1.87 0.10 1.69 1.79 1.86 1.99 2.09 ▅▂▇▆▁ PriceMM 0 1 2.09 0.13 1.69 1.99 2.09 2.18 2.29 ▂▁▃▇▆ DiscCH 0 1 0.05 0.12 0.00 0.00 0.00 0.00 0.50 ▇▁▁▁▁ DiscMM 0 1 0.12 0.21 0.00 0.00 0.00 0.23 0.80 ▇▁▂▁▁ SpecialCH 0 1 0.15 0.35 0.00 0.00 0.00 0.00 1.00 ▇▁▁▁▂ SpecialMM 0 1 0.16 0.37 0.00 0.00 0.00 0.00 1.00 ▇▁▁▁▂ LoyalCH 0 1 0.57 0.31 0.00 0.33 0.60 0.85 1.00 ▅▃▆▆▇ SalePriceMM 0 1 1.96 0.25 1.19 1.69 2.09 2.13 2.29 ▁▂▂▂▇ SalePriceCH 0 1 1.82 0.14 1.39 1.75 1.86 1.89 2.09 ▂▁▇▇▅ PriceDiff 0 1 0.15 0.27 -0.67 0.00 0.23 0.32 0.64 ▁▂▃▇▂ PctDiscMM 0 1 0.06 0.10 0.00 0.00 0.00 0.11 0.40 ▇▁▂▁▁ PctDiscCH 0 1 0.03 0.06 0.00 0.00 0.00 0.00 0.25 ▇▁▁▁▁ ListPriceDiff 0 1 0.22 0.11 0.00 0.14 0.24 0.30 0.44 ▂▃▆▇▁ STORE 0 1 1.63 1.43 0.00 0.00 2.00 3.00 4.00 ▇▃▅▅▃ I’ll split oj_dat (n = 1,070) into oj_train (80%, n = 857) to fit various models, and oj_test (20%, n = 213) to compare their performance on new data. set.seed(12345) partition &lt;- createDataPartition(y = oj_dat$Purchase, p = 0.8, list = FALSE) oj_train &lt;- oj_dat[partition, ] oj_test &lt;- oj_dat[-partition, ] Function rpart::rpart() builds a full tree, minimizing the Gini index \\(G\\) by default (parms = list(split = \"gini\")), until the stopping criterion is satisfied. The default stopping criterion is only attempt a split if the current node has at least minsplit = 20 observations, and only accept a split if the resulting nodes have at least minbucket = round(minsplit/3) observations, and the resulting overall fit improves by cp = 0.01 (i.e., \\(\\Delta G &lt;= 0.01\\)). # Use method = &quot;class&quot; for classification, method = &quot;anova&quot; for regression set.seed(123) oj_mdl_cart_full &lt;- rpart(formula = Purchase ~ ., data = oj_train, method = &quot;class&quot;) print(oj_mdl_cart_full) ## n= 857 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 857 330 CH (0.610 0.390) ## 2) LoyalCH&gt;=0.48 537 94 CH (0.825 0.175) ## 4) LoyalCH&gt;=0.76 271 13 CH (0.952 0.048) * ## 5) LoyalCH&lt; 0.76 266 81 CH (0.695 0.305) ## 10) PriceDiff&gt;=-0.16 226 50 CH (0.779 0.221) * ## 11) PriceDiff&lt; -0.16 40 9 MM (0.225 0.775) * ## 3) LoyalCH&lt; 0.48 320 80 MM (0.250 0.750) ## 6) LoyalCH&gt;=0.28 146 58 MM (0.397 0.603) ## 12) SalePriceMM&gt;=2 71 31 CH (0.563 0.437) * ## 13) SalePriceMM&lt; 2 75 18 MM (0.240 0.760) * ## 7) LoyalCH&lt; 0.28 174 22 MM (0.126 0.874) * The output starts with the root node. The predicted class at the root is CH and this prediction produces 334 errors on the 857 observations for a success rate (accuracy) of 61% (0.61026838) and an error rate of 39% (0.38973162). The child nodes of node “x” are labeled 2x) and 2x+1), so the child nodes of 1) are 2) and 3), and the child nodes of 2) are 4) and 5). Terminal nodes are labeled with an asterisk (*). Surprisingly, only 3 of the 17 features were used the in full tree: LoyalCH (Customer brand loyalty for CH), PriceDiff (relative price of MM over CH), and SalePriceMM (absolute price of MM). The first split is at LoyalCH = 0.48285. Here is a diagram of the full (unpruned) tree. rpart.plot(oj_mdl_cart_full, yesno = TRUE) The boxes show the node classification (based on mode), the proportion of observations that are not CH, and the proportion of observations included in the node. rpart() not only grew the full tree, it identified the set of cost complexity parameters, and measured the model performance of each corresponding tree using cross-validation. printcp() displays the candidate \\(c_p\\) values. You can use this table to decide how to prune the tree. printcp(oj_mdl_cart_full) ## ## Classification tree: ## rpart(formula = Purchase ~ ., data = oj_train, method = &quot;class&quot;) ## ## Variables actually used in tree construction: ## [1] LoyalCH PriceDiff SalePriceMM ## ## Root node error: 334/857 = 0 ## ## n= 857 ## ## CP nsplit rel error xerror xstd ## 1 0 0 1 1 0 ## 2 0 1 1 1 0 ## 3 0 3 0 0 0 ## 4 0 5 0 0 0 There are 4 \\(c_p\\) values in this model. The model with the smallest complexity parameter allows the most splits (nsplit). The highest complexity parameter corresponds to a tree with just a root node. rel error is the error rate relative to the root node. The root node absolute error is 0.38973162 (the proportion of MM), so its rel error is 0.38973162/0.38973162 = 1.0. That means the absolute error of the full tree (at CP = 0.01) is 0.42814 * 0.38973162 = 0.1669. You can verify that by calculating the error rate of the predicted values: data.frame(pred = predict(oj_mdl_cart_full, newdata = oj_train, type = &quot;class&quot;)) %&gt;% mutate(obs = oj_train$Purchase, err = if_else(pred != obs, 1, 0)) %&gt;% summarize(mean_err = mean(err)) ## mean_err ## 1 0.17 Finishing the CP table tour, xerror is the relative cross-validated error rate and xstd is its standard error. If you want the lowest possible error, then prune to the tree with the smallest relative CV error, \\(c_p\\) = 0.01. If you want to balance predictive power with simplicity, prune to the smallest tree within 1 SE of the one with the smallest relative error. The CP table is not super-helpful for finding that tree, so add a column to find it. oj_mdl_cart_full$cptable %&gt;% data.frame() %&gt;% mutate( min_idx = which.min(oj_mdl_cart_full$cptable[, &quot;xerror&quot;]), rownum = row_number(), xerror_cap = oj_mdl_cart_full$cptable[min_idx, &quot;xerror&quot;] + oj_mdl_cart_full$cptable[min_idx, &quot;xstd&quot;], eval = case_when(rownum == min_idx ~ &quot;min xerror&quot;, xerror &lt; xerror_cap ~ &quot;under cap&quot;, TRUE ~ &quot;&quot;) ) %&gt;% select(-rownum, -min_idx) ## CP nsplit rel.error xerror xstd xerror_cap eval ## 1 0.479 0 1.00 1.00 0.043 0.5 ## 2 0.033 1 0.52 0.54 0.036 0.5 ## 3 0.013 3 0.46 0.47 0.034 0.5 under cap ## 4 0.010 5 0.43 0.46 0.034 0.5 min xerror The simplest tree using the 1-SE rule is $c_p = 0.01347305, CV error = 0.18). Fortunately, plotcp() presents a nice graphical representation of the relationship between xerror and cp. plotcp(oj_mdl_cart_full, upper = &quot;splits&quot;) The dashed line is set at the minimum xerror + xstd. The top axis shows the number of splits in the tree. I’m not sure why the CP values are not the same as in the table (they are close, but not the same). The figure suggests I should prune to 5 or 3 splits. I see this curve never really hits a minimum - it is still decreasing at 5 splits. The default tuning parameter value cp = 0.01 may be too large, so I’ll set it to cp = 0.001 and start over. set.seed(123) oj_mdl_cart_full &lt;- rpart( formula = Purchase ~ ., data = oj_train, method = &quot;class&quot;, cp = 0.001 ) print(oj_mdl_cart_full) ## n= 857 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 857 330 CH (0.610 0.390) ## 2) LoyalCH&gt;=0.48 537 94 CH (0.825 0.175) ## 4) LoyalCH&gt;=0.76 271 13 CH (0.952 0.048) * ## 5) LoyalCH&lt; 0.76 266 81 CH (0.695 0.305) ## 10) PriceDiff&gt;=-0.16 226 50 CH (0.779 0.221) ## 20) ListPriceDiff&gt;=0.26 115 11 CH (0.904 0.096) * ## 21) ListPriceDiff&lt; 0.26 111 39 CH (0.649 0.351) ## 42) PriceMM&gt;=2.2 19 2 CH (0.895 0.105) * ## 43) PriceMM&lt; 2.2 92 37 CH (0.598 0.402) ## 86) DiscCH&gt;=0.12 7 0 CH (1.000 0.000) * ## 87) DiscCH&lt; 0.12 85 37 CH (0.565 0.435) ## 174) ListPriceDiff&gt;=0.22 45 15 CH (0.667 0.333) * ## 175) ListPriceDiff&lt; 0.22 40 18 MM (0.450 0.550) ## 350) LoyalCH&gt;=0.53 28 13 CH (0.536 0.464) ## 700) WeekofPurchase&lt; 2.7e+02 21 8 CH (0.619 0.381) * ## 701) WeekofPurchase&gt;=2.7e+02 7 2 MM (0.286 0.714) * ## 351) LoyalCH&lt; 0.53 12 3 MM (0.250 0.750) * ## 11) PriceDiff&lt; -0.16 40 9 MM (0.225 0.775) * ## 3) LoyalCH&lt; 0.48 320 80 MM (0.250 0.750) ## 6) LoyalCH&gt;=0.28 146 58 MM (0.397 0.603) ## 12) SalePriceMM&gt;=2 71 31 CH (0.563 0.437) ## 24) LoyalCH&lt; 0.3 7 0 CH (1.000 0.000) * ## 25) LoyalCH&gt;=0.3 64 31 CH (0.516 0.484) ## 50) WeekofPurchase&gt;=2.5e+02 52 22 CH (0.577 0.423) ## 100) PriceCH&lt; 1.9 35 11 CH (0.686 0.314) ## 200) StoreID&lt; 1.5 9 1 CH (0.889 0.111) * ## 201) StoreID&gt;=1.5 26 10 CH (0.615 0.385) ## 402) LoyalCH&lt; 0.41 17 4 CH (0.765 0.235) * ## 403) LoyalCH&gt;=0.41 9 3 MM (0.333 0.667) * ## 101) PriceCH&gt;=1.9 17 6 MM (0.353 0.647) * ## 51) WeekofPurchase&lt; 2.5e+02 12 3 MM (0.250 0.750) * ## 13) SalePriceMM&lt; 2 75 18 MM (0.240 0.760) ## 26) SpecialCH&gt;=0.5 14 6 CH (0.571 0.429) * ## 27) SpecialCH&lt; 0.5 61 10 MM (0.164 0.836) * ## 7) LoyalCH&lt; 0.28 174 22 MM (0.126 0.874) ## 14) LoyalCH&gt;=0.035 117 21 MM (0.179 0.821) ## 28) WeekofPurchase&lt; 2.7e+02 104 21 MM (0.202 0.798) ## 56) PriceCH&gt;=1.9 20 9 MM (0.450 0.550) ## 112) WeekofPurchase&gt;=2.5e+02 12 5 CH (0.583 0.417) * ## 113) WeekofPurchase&lt; 2.5e+02 8 2 MM (0.250 0.750) * ## 57) PriceCH&lt; 1.9 84 12 MM (0.143 0.857) * ## 29) WeekofPurchase&gt;=2.7e+02 13 0 MM (0.000 1.000) * ## 15) LoyalCH&lt; 0.035 57 1 MM (0.018 0.982) * This is a much larger tree. Did I find a cp value that produces a local min? plotcp(oj_mdl_cart_full, upper = &quot;splits&quot;) Yes, the min is at CP = 0.011 with 5 splits. The min + 1 SE is at CP = 0.021 with 3 splits. I’ll prune the tree to 3 splits. oj_mdl_cart &lt;- prune( oj_mdl_cart_full, cp = oj_mdl_cart_full$cptable[oj_mdl_cart_full$cptable[, 2] == 3, &quot;CP&quot;] ) rpart.plot(oj_mdl_cart, yesno = TRUE) The most “important” indicator of Purchase appears to be LoyalCH. From the rpart vignette (page 12), “An overall measure of variable importance is the sum of the goodness of split measures for each split for which it was the primary variable, plus goodness (adjusted agreement) for all splits in which it was a surrogate.” Surrogates refer to alternative features for a node to handle missing data. For each split, CART evaluates a variety of alternative “surrogate” splits to use when the feature value for the primary split is NA. Surrogate splits are splits that produce results similar to the original split. A variable’s importance is the sum of the improvement in the overall Gini (or RMSE) measure produced by the nodes in which it appears. Here is the variable importance for this model. oj_mdl_cart$variable.importance %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;Feature&quot;) %&gt;% rename(Overall = &#39;.&#39;) %&gt;% ggplot(aes(x = fct_reorder(Feature, Overall), y = Overall)) + geom_pointrange(aes(ymin = 0, ymax = Overall), color = &quot;cadetblue&quot;, size = .3) + theme_minimal() + coord_flip() + labs(x = &quot;&quot;, y = &quot;&quot;, title = &quot;Variable Importance with Simple Classication&quot;) LoyalCH is by far the most important variable, as expected from its position at the top of the tree, and one level down. You can see how the surrogates appear in the model with the summary() function. summary(oj_mdl_cart) ## Call: ## rpart(formula = Purchase ~ ., data = oj_train, method = &quot;class&quot;, ## cp = 0.001) ## n= 857 ## ## CP nsplit rel error xerror xstd ## 1 0.479 0 1.00 1.00 0.043 ## 2 0.033 1 0.52 0.54 0.036 ## 3 0.013 3 0.46 0.47 0.034 ## ## Variable importance ## LoyalCH PriceDiff SalePriceMM StoreID WeekofPurchase ## 67 9 5 4 4 ## DiscMM PriceMM PctDiscMM PriceCH ## 3 3 3 1 ## ## Node number 1: 857 observations, complexity param=0.48 ## predicted class=CH expected loss=0.39 P(node) =1 ## class counts: 523 334 ## probabilities: 0.610 0.390 ## left son=2 (537 obs) right son=3 (320 obs) ## Primary splits: ## LoyalCH &lt; 0.48 to the right, improve=130, (0 missing) ## StoreID &lt; 3.5 to the right, improve= 40, (0 missing) ## PriceDiff &lt; 0.015 to the right, improve= 24, (0 missing) ## ListPriceDiff &lt; 0.26 to the right, improve= 23, (0 missing) ## SalePriceMM &lt; 1.8 to the right, improve= 20, (0 missing) ## Surrogate splits: ## StoreID &lt; 3.5 to the right, agree=0.65, adj=0.053, (0 split) ## PriceMM &lt; 1.9 to the right, agree=0.64, adj=0.031, (0 split) ## WeekofPurchase &lt; 230 to the right, agree=0.63, adj=0.016, (0 split) ## DiscMM &lt; 0.77 to the left, agree=0.63, adj=0.006, (0 split) ## SalePriceMM &lt; 1.4 to the right, agree=0.63, adj=0.006, (0 split) ## ## Node number 2: 537 observations, complexity param=0.033 ## predicted class=CH expected loss=0.18 P(node) =0.63 ## class counts: 443 94 ## probabilities: 0.825 0.175 ## left son=4 (271 obs) right son=5 (266 obs) ## Primary splits: ## LoyalCH &lt; 0.76 to the right, improve=18.0, (0 missing) ## PriceDiff &lt; 0.015 to the right, improve=15.0, (0 missing) ## SalePriceMM &lt; 1.8 to the right, improve=14.0, (0 missing) ## ListPriceDiff &lt; 0.26 to the right, improve=11.0, (0 missing) ## DiscMM &lt; 0.15 to the left, improve= 7.8, (0 missing) ## Surrogate splits: ## WeekofPurchase &lt; 260 to the right, agree=0.59, adj=0.18, (0 split) ## PriceCH &lt; 1.8 to the right, agree=0.59, adj=0.17, (0 split) ## StoreID &lt; 3.5 to the right, agree=0.59, adj=0.16, (0 split) ## PriceMM &lt; 2 to the right, agree=0.59, adj=0.16, (0 split) ## SalePriceMM &lt; 2 to the right, agree=0.59, adj=0.16, (0 split) ## ## Node number 3: 320 observations ## predicted class=MM expected loss=0.25 P(node) =0.37 ## class counts: 80 240 ## probabilities: 0.250 0.750 ## ## Node number 4: 271 observations ## predicted class=CH expected loss=0.048 P(node) =0.32 ## class counts: 258 13 ## probabilities: 0.952 0.048 ## ## Node number 5: 266 observations, complexity param=0.033 ## predicted class=CH expected loss=0.3 P(node) =0.31 ## class counts: 185 81 ## probabilities: 0.695 0.305 ## left son=10 (226 obs) right son=11 (40 obs) ## Primary splits: ## PriceDiff &lt; -0.16 to the right, improve=21, (0 missing) ## ListPriceDiff &lt; 0.24 to the right, improve=21, (0 missing) ## SalePriceMM &lt; 1.8 to the right, improve=17, (0 missing) ## DiscMM &lt; 0.15 to the left, improve=10, (0 missing) ## PctDiscMM &lt; 0.073 to the left, improve=10, (0 missing) ## Surrogate splits: ## SalePriceMM &lt; 1.6 to the right, agree=0.91, adj=0.38, (0 split) ## DiscMM &lt; 0.57 to the left, agree=0.90, adj=0.30, (0 split) ## PctDiscMM &lt; 0.26 to the left, agree=0.90, adj=0.30, (0 split) ## WeekofPurchase &lt; 270 to the left, agree=0.87, adj=0.15, (0 split) ## SalePriceCH &lt; 2.1 to the left, agree=0.86, adj=0.05, (0 split) ## ## Node number 10: 226 observations ## predicted class=CH expected loss=0.22 P(node) =0.26 ## class counts: 176 50 ## probabilities: 0.779 0.221 ## ## Node number 11: 40 observations ## predicted class=MM expected loss=0.22 P(node) =0.047 ## class counts: 9 31 ## probabilities: 0.225 0.775 I’ll evaluate the predictions and record the accuracy (correct classification percentage) for comparison to other models. Two ways to evaluate the model are the confusion matrix, and the ROC curve. 8.1.1 Measuring Performance 8.1.1.1 Confusion Matrix Print the confusion matrix with caret::confusionMatrix() to see how well does this model performs against the holdout set. oj_preds_cart &lt;- bind_cols( predict(oj_mdl_cart, newdata = oj_test, type = &quot;prob&quot;), predicted = predict(oj_mdl_cart, newdata = oj_test, type = &quot;class&quot;), actual = oj_test$Purchase ) oj_cm_cart &lt;- confusionMatrix(oj_preds_cart$predicted, reference = oj_preds_cart$actual) oj_cm_cart ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 113 13 ## MM 17 70 ## ## Accuracy : 0.859 ## 95% CI : (0.805, 0.903) ## No Information Rate : 0.61 ## P-Value [Acc &gt; NIR] : 0.00000000000000126 ## ## Kappa : 0.706 ## ## Mcnemar&#39;s Test P-Value : 0.584 ## ## Sensitivity : 0.869 ## Specificity : 0.843 ## Pos Pred Value : 0.897 ## Neg Pred Value : 0.805 ## Prevalence : 0.610 ## Detection Rate : 0.531 ## Detection Prevalence : 0.592 ## Balanced Accuracy : 0.856 ## ## &#39;Positive&#39; Class : CH ## The confusion matrix is at the top. It also includes a lot of statistics. It’s worth getting familiar with the stats. The model accuracy and 95% CI are calculated from the binomial test. binom.test(x = 113 + 70, n = 213) ## ## Exact binomial test ## ## data: 113 + 70 and 213 ## number of successes = 183, number of trials = 213, p-value ## &lt;0.0000000000000002 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.81 0.90 ## sample estimates: ## probability of success ## 0.86 The “No Information Rate” (NIR) statistic is the class rate for the largest class. In this case CH is the largest class, so NIR = 130/213 = 0.6103. “P-Value [Acc &gt; NIR]” is the binomial test that the model accuracy is significantly better than the NIR (i.e., significantly better than just always guessing CH). binom.test(x = 113 + 70, n = 213, p = 130/213, alternative = &quot;greater&quot;) ## ## Exact binomial test ## ## data: 113 + 70 and 213 ## number of successes = 183, number of trials = 213, p-value = ## 0.000000000000001 ## alternative hypothesis: true probability of success is greater than 0.61 ## 95 percent confidence interval: ## 0.81 1.00 ## sample estimates: ## probability of success ## 0.86 The “Accuracy” statistic indicates the model predicts 0.8590 of the observations correctly. That’s good, but less impressive when you consider the prevalence of CH is 0.6103 - you could achieve 61% accuracy just by predicting CH every time. A measure that controls for the prevalence is Cohen’s kappa statistic. The kappa statistic is explained here. It compares the accuracy to the accuracy of a “random system”. It is defined as \\[\\kappa = \\frac{Acc - RA}{1-RA}\\] where \\[RA = \\frac{ActFalse \\times PredFalse + ActTrue \\times PredTrue}{Total \\times Total}\\] is the hypothetical probability of a chance agreement. ActFalse will be the number of “MM” (13 + 70 = 83) and actual true will be the number of “CH” (113 + 17 = 130). The predicted counts are table(oj_preds_cart$predicted) ## ## CH MM ## 126 87 So, \\(RA = (83*87 + 130*126) / 213^2 = 0.5202\\) and \\(\\kappa = (0.8592 - 0.5202)/(1 - 0.5202) = 0.7064\\). The kappa statistic varies from 0 to 1 where 0 means accurate predictions occur merely by chance, and 1 means the predictions are in perfect agreement with the observations. In this case, a kappa statistic of 0.7064 is “substantial”. See chart here. The other measures from the confusionMatrix() output are various proportions and you can remind yourself of their definitions in the documentation with ?confusionMatrix. Visuals are almost always helpful. Here is a plot of the confusion matrix. plot(oj_preds_cart$actual, oj_preds_cart$predicted, main = &quot;Simple Classification: Predicted vs. Actual&quot;, xlab = &quot;Actual&quot;, ylab = &quot;Predicted&quot;) 8.1.1.2 ROC Curve The ROC (receiver operating characteristics) curve (Fawcett 2005) is another measure of accuracy. The ROC curve is a plot of the true positive rate (TPR, sensitivity) versus the false positive rate (FPR, 1 - specificity) for a set of thresholds. By default, the threshold for predicting the default classification is 0.50, but it could be any threshold. precrec::evalmod() calculates the confusion matrix values from the model using the holdout data set. The AUC on the holdout set is 0.8848. pRoc::plot.roc(), plotROC::geom_roc(), and yardstick::roc_curve() are all options for plotting a ROC curve. mdl_auc &lt;- Metrics::auc(actual = oj_preds_cart$actual == &quot;CH&quot;, oj_preds_cart$CH) yardstick::roc_curve(oj_preds_cart, actual, CH) %&gt;% autoplot() + labs( title = &quot;OJ CART ROC Curve&quot;, subtitle = paste0(&quot;AUC = &quot;, round(mdl_auc, 4)) ) A few points on the ROC space are helpful for understanding how to use it. The lower left point (0, 0) is the result of always predicting “negative” or in this case “MM” if “CH” is taken as the default class. No false positives, but no true positives either. The upper right point (1, 1) is the result of always predicting “positive” (“CH” here). You catch all true positives, but miss all the true negatives. The upper left point (0, 1) is the result of perfect accuracy. The lower right point (1, 0) is the result of perfect imbecility. You made the exact wrong prediction every time. The 45 degree diagonal is the result of randomly guessing positive (CH) X percent of the time. If you guess positive 90% of the time and the prevalence is 50%, your TPR will be 90% and your FPR will also be 90%, etc. The goal is for all nodes to bunch up in the upper left. Points to the left of the diagonal with a low TPR can be thought of as “conservative” predictors - they only make positive (CH) predictions with strong evidence. Points to the left of the diagonal with a high TPR can be thought of as “liberal” predictors - they make positive (CH) predictions with weak evidence. 8.1.1.3 Gain Curve The gain curve plots the cumulative summed true outcome versus the fraction of items seen when sorted by the predicted value. The “wizard” curve is the gain curve when the data is sorted by the true outcome. If the model’s gain curve is close to the wizard curve, then the model predicted the response variable well. The gray area is the “gain” over a random prediction. 130 of the 213 consumers in the holdout set purchased CH. The gain curve encountered 77 CH purchasers (59%) within the first 79 observations (37%). It encountered all 130 CH purchasers on the 213th observation (100%). The bottom of the gray area is the outcome of a random model. Only half the CH purchasers would be observed within 50% of the observations. The top of the gray area is the outcome of the perfect model, the “wizard curve”. Half the CH purchasers would be observed in 65/213=31% of the observations. yardstick::gain_curve(oj_preds_cart, actual, CH) %&gt;% autoplot() + labs( title = &quot;OJ CART Gain Curve&quot; ) 8.1.2 Training with Caret I can also fit the model with caret::train(). There are two ways to tune hyperparameters in train(): set the number of tuning parameter values to consider by setting tuneLength, or set particular values to consider for each parameter by defining a tuneGrid. I’ll build the model using 10-fold cross-validation to optimize the hyperparameter CP. If you have no idea what is the optimal tuning parameter, start with tuneLength to get close, then fine-tune with tuneGrid. That’s what I’ll do. I’ll create a training control object that I can re-use in other model builds. oj_trControl = trainControl( method = &quot;cv&quot;, number = 10, savePredictions = &quot;final&quot;, # save preds for the optimal tuning parameter classProbs = TRUE, # class probs in addition to preds summaryFunction = twoClassSummary ) Now fit the model. set.seed(1234) oj_mdl_cart2 &lt;- train( Purchase ~ ., data = oj_train, method = &quot;rpart&quot;, tuneLength = 5, metric = &quot;ROC&quot;, trControl = oj_trControl ) caret built a full tree using rpart’s default parameters: gini splitting index, at least 20 observations in a node in order to consider splitting it, and at least 6 observations in each node. Caret then calculated the accuracy for each candidate value of \\(\\alpha\\). Here is the results. print(oj_mdl_cart2) ## CART ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... ## Resampling results across tuning parameters: ## ## cp ROC Sens Spec ## 0.006 0.85 0.86 0.73 ## 0.009 0.85 0.86 0.73 ## 0.013 0.85 0.85 0.74 ## 0.033 0.78 0.85 0.68 ## 0.479 0.59 0.92 0.26 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.006. The second cp (0.008982036) produced the highest accuracy. I can drill into the best value of cp using a tuning grid. set.seed(1234) oj_mdl_cart2 &lt;- train( Purchase ~ ., data = oj_train, method = &quot;rpart&quot;, tuneGrid = expand.grid(cp = seq(from = 0.001, to = 0.010, length = 11)), metric = &quot;ROC&quot;, trControl = oj_trControl ) print(oj_mdl_cart2) ## CART ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... ## Resampling results across tuning parameters: ## ## cp ROC Sens Spec ## 0.0010 0.85 0.85 0.72 ## 0.0019 0.85 0.85 0.72 ## 0.0028 0.85 0.85 0.73 ## 0.0037 0.85 0.85 0.74 ## 0.0046 0.85 0.85 0.73 ## 0.0055 0.85 0.86 0.73 ## 0.0064 0.85 0.86 0.73 ## 0.0073 0.85 0.86 0.73 ## 0.0082 0.85 0.86 0.73 ## 0.0091 0.85 0.86 0.73 ## 0.0100 0.85 0.85 0.74 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.0055. The best model is at cp = 0.0082. Here are the cross-validated accuracies for the candidate cp values. plot(oj_mdl_cart2) Here are the rules in the final model. oj_mdl_cart2$finalModel ## n= 857 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 857 330 CH (0.610 0.390) ## 2) LoyalCH&gt;=0.48 537 94 CH (0.825 0.175) ## 4) LoyalCH&gt;=0.76 271 13 CH (0.952 0.048) * ## 5) LoyalCH&lt; 0.76 266 81 CH (0.695 0.305) ## 10) PriceDiff&gt;=-0.16 226 50 CH (0.779 0.221) * ## 11) PriceDiff&lt; -0.16 40 9 MM (0.225 0.775) * ## 3) LoyalCH&lt; 0.48 320 80 MM (0.250 0.750) ## 6) LoyalCH&gt;=0.28 146 58 MM (0.397 0.603) ## 12) SalePriceMM&gt;=2 71 31 CH (0.563 0.437) ## 24) LoyalCH&lt; 0.3 7 0 CH (1.000 0.000) * ## 25) LoyalCH&gt;=0.3 64 31 CH (0.516 0.484) ## 50) WeekofPurchase&gt;=2.5e+02 52 22 CH (0.577 0.423) ## 100) PriceCH&lt; 1.9 35 11 CH (0.686 0.314) * ## 101) PriceCH&gt;=1.9 17 6 MM (0.353 0.647) * ## 51) WeekofPurchase&lt; 2.5e+02 12 3 MM (0.250 0.750) * ## 13) SalePriceMM&lt; 2 75 18 MM (0.240 0.760) ## 26) SpecialCH&gt;=0.5 14 6 CH (0.571 0.429) * ## 27) SpecialCH&lt; 0.5 61 10 MM (0.164 0.836) * ## 7) LoyalCH&lt; 0.28 174 22 MM (0.126 0.874) * rpart.plot(oj_mdl_cart2$finalModel) Let’s look at the performance on the holdout data set. oj_preds_cart2 &lt;- bind_cols( predict(oj_mdl_cart2, newdata = oj_test, type = &quot;prob&quot;), Predicted = predict(oj_mdl_cart2, newdata = oj_test, type = &quot;raw&quot;), Actual = oj_test$Purchase ) oj_cm_cart2 &lt;- confusionMatrix(oj_preds_cart2$Predicted, oj_preds_cart2$Actual) oj_cm_cart2 ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 117 18 ## MM 13 65 ## ## Accuracy : 0.854 ## 95% CI : (0.8, 0.899) ## No Information Rate : 0.61 ## P-Value [Acc &gt; NIR] : 0.00000000000000483 ## ## Kappa : 0.691 ## ## Mcnemar&#39;s Test P-Value : 0.472 ## ## Sensitivity : 0.900 ## Specificity : 0.783 ## Pos Pred Value : 0.867 ## Neg Pred Value : 0.833 ## Prevalence : 0.610 ## Detection Rate : 0.549 ## Detection Prevalence : 0.634 ## Balanced Accuracy : 0.842 ## ## &#39;Positive&#39; Class : CH ## The accuracy is 0.8451 - a little worse than the 0.8592 from the direct method. The AUC is 0.9102. mdl_auc &lt;- Metrics::auc(actual = oj_preds_cart2$Actual == &quot;CH&quot;, oj_preds_cart2$CH) yardstick::roc_curve(oj_preds_cart2, Actual, CH) %&gt;% autoplot() + labs( title = &quot;OJ CART ROC Curve (caret)&quot;, subtitle = paste0(&quot;AUC = &quot;, round(mdl_auc, 4)) ) yardstick::gain_curve(oj_preds_cart2, Actual, CH) %&gt;% autoplot() + labs(title = &quot;OJ CART Gain Curve (caret)&quot;) Finally, here is the variable importance plot. Brand loyalty is most important, followed by price difference. plot(varImp(oj_mdl_cart2), main=&quot;Variable Importance with CART (caret)&quot;) Looks like the manual effort fared best. Here is a summary the accuracy rates of the two models. oj_scoreboard &lt;- rbind( data.frame(Model = &quot;Single Tree&quot;, Accuracy = oj_cm_cart$overall[&quot;Accuracy&quot;]), data.frame(Model = &quot;Single Tree (caret)&quot;, Accuracy = oj_cm_cart2$overall[&quot;Accuracy&quot;]) ) %&gt;% arrange(desc(Accuracy)) scoreboard(oj_scoreboard) ModelAccuracySingle Tree0.8592Single Tree (caret)0.8545 References "],
["regression-tree.html", "8.2 Regression Tree", " 8.2 Regression Tree A simple regression tree is built in a manner similar to a simple classification tree, and like the simple classification tree, it is rarely invoked on its own; the bagged, random forest, and gradient boosting methods build on this logic. I’ll learn by example again. Using the ISLR::Carseats data set, and predict Sales using from the 10 feature variables. cs_dat &lt;- ISLR::Carseats skimr::skim(cs_dat) Table 8.2: Data summary Name cs_dat Number of rows 400 Number of columns 11 _______________________ Column type frequency: factor 3 numeric 8 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts ShelveLoc 0 1 FALSE 3 Med: 219, Bad: 96, Goo: 85 Urban 0 1 FALSE 2 Yes: 282, No: 118 US 0 1 FALSE 2 Yes: 258, No: 142 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist Sales 0 1 7.5 2.8 0 5.4 7.5 9.3 16 ▁▆▇▃▁ CompPrice 0 1 125.0 15.3 77 115.0 125.0 135.0 175 ▁▅▇▃▁ Income 0 1 68.7 28.0 21 42.8 69.0 91.0 120 ▇▆▇▆▅ Advertising 0 1 6.6 6.6 0 0.0 5.0 12.0 29 ▇▃▃▁▁ Population 0 1 264.8 147.4 10 139.0 272.0 398.5 509 ▇▇▇▇▇ Price 0 1 115.8 23.7 24 100.0 117.0 131.0 191 ▁▂▇▆▁ Age 0 1 53.3 16.2 25 39.8 54.5 66.0 80 ▇▆▇▇▇ Education 0 1 13.9 2.6 10 12.0 14.0 16.0 18 ▇▇▃▇▇ Split careseats_dat (n = 400) into cs_train (80%, n = 321) and cs_test (20%, n = 79). set.seed(12345) partition &lt;- createDataPartition(y = cs_dat$Sales, p = 0.8, list = FALSE) cs_train &lt;- cs_dat[partition, ] cs_test &lt;- cs_dat[-partition, ] The first step is to build a full tree, then perform k-fold cross-validation to help select the optimal cost complexity (cp). The only difference here is the rpart() parameter method = \"anova\" to produce a regression tree. set.seed(1234) cs_mdl_cart_full &lt;- rpart(Sales ~ ., cs_train, method = &quot;anova&quot;) print(cs_mdl_cart_full) ## n= 321 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 321 2600 7.5 ## 2) ShelveLoc=Bad,Medium 251 1500 6.8 ## 4) Price&gt;=1.1e+02 168 720 6.0 ## 8) ShelveLoc=Bad 50 170 4.7 ## 16) Population&lt; 2e+02 20 48 3.6 * ## 17) Population&gt;=2e+02 30 81 5.4 * ## 9) ShelveLoc=Medium 118 430 6.5 ## 18) Advertising&lt; 12 88 290 6.1 ## 36) CompPrice&lt; 1.4e+02 69 190 5.8 ## 72) Price&gt;=1.3e+02 16 51 4.5 * ## 73) Price&lt; 1.3e+02 53 110 6.2 * ## 37) CompPrice&gt;=1.4e+02 19 58 7.4 * ## 19) Advertising&gt;=12 30 83 7.8 * ## 5) Price&lt; 1.1e+02 83 440 8.4 ## 10) Age&gt;=64 32 150 6.9 ## 20) Price&gt;=85 25 67 6.2 ## 40) ShelveLoc=Bad 9 18 4.8 * ## 41) ShelveLoc=Medium 16 21 6.9 * ## 21) Price&lt; 85 7 20 9.6 * ## 11) Age&lt; 64 51 180 9.3 ## 22) Income&lt; 58 12 28 7.7 * ## 23) Income&gt;=58 39 120 9.7 ## 46) Age&gt;=50 14 21 8.5 * ## 47) Age&lt; 50 25 60 10.0 * ## 3) ShelveLoc=Good 70 420 10.0 ## 6) Price&gt;=1.1e+02 49 240 9.4 ## 12) Advertising&lt; 14 41 160 8.9 ## 24) Age&gt;=61 17 53 7.8 * ## 25) Age&lt; 61 24 69 9.8 * ## 13) Advertising&gt;=14 8 13 12.0 * ## 7) Price&lt; 1.1e+02 21 61 12.0 * The predicted Sales at the root is the mean Sales for the training data set, 7.535950 (values are $000s). The deviance at the root is the SSE, 2567.768. The first split is at ShelveLoc = [Bad, Medium] vs Good. Here is the unpruned tree diagram. rpart.plot(cs_mdl_cart_full, yesno = TRUE) The boxes show the node predicted value (mean) and the proportion of observations that are in the node (or child nodes). rpart() grew the full tree, and used cross-validation to test the performance of the possible complexity hyperparameters. printcp() displays the candidate cp values. You can use this table to decide how to prune the tree. printcp(cs_mdl_cart_full) ## ## Regression tree: ## rpart(formula = Sales ~ ., data = cs_train, method = &quot;anova&quot;) ## ## Variables actually used in tree construction: ## [1] Advertising Age CompPrice Income Population Price ## [7] ShelveLoc ## ## Root node error: 2568/321 = 8 ## ## n= 321 ## ## CP nsplit rel error xerror xstd ## 1 0 0 1 1 0 ## 2 0 1 1 1 0 ## 3 0 2 1 1 0 ## 4 0 3 1 1 0 ## 5 0 4 1 1 0 ## 6 0 5 0 1 0 ## 7 0 6 0 1 0 ## 8 0 7 0 1 0 ## 9 0 8 0 1 0 ## 10 0 9 0 1 0 ## 11 0 10 0 1 0 ## 12 0 11 0 1 0 ## 13 0 12 0 1 0 ## 14 0 13 0 1 0 ## 15 0 14 0 1 0 ## 16 0 15 0 1 0 There were 16 possible cp values in this model. The model with the smallest complexity parameter allows the most splits (nsplit). The highest complexity parameter corresponds to a tree with just a root node. rel error is the SSE relative to the root node. The root node SSE is 2567.76800, so its rel error is 2567.76800/2567.76800 = 1.0. That means the absolute error of the full tree (at CP = 0.01) is 0.30963 * 2567.76800 = 795.058. You can verify that by calculating the SSE of the model predicted values: data.frame(pred = predict(cs_mdl_cart_full, newdata = cs_train)) %&gt;% mutate(obs = cs_train$Sales, sq_err = (obs - pred)^2) %&gt;% summarize(sse = sum(sq_err)) ## sse ## 1 795 Finishing the CP table tour, xerror is the cross-validated SSE and xstd is its standard error. If you want the lowest possible error, then prune to the tree with the smallest relative SSE (xerror). If you want to balance predictive power with simplicity, prune to the smallest tree within 1 SE of the one with the smallest relative SSE. The CP table is not super-helpful for finding that tree. I’ll add a column to find it. cs_mdl_cart_full$cptable %&gt;% data.frame() %&gt;% mutate(min_xerror_idx = which.min(cs_mdl_cart_full$cptable[, &quot;xerror&quot;]), rownum = row_number(), xerror_cap = cs_mdl_cart_full$cptable[min_xerror_idx, &quot;xerror&quot;] + cs_mdl_cart_full$cptable[min_xerror_idx, &quot;xstd&quot;], eval = case_when(rownum == min_xerror_idx ~ &quot;min xerror&quot;, xerror &lt; xerror_cap ~ &quot;under cap&quot;, TRUE ~ &quot;&quot;)) %&gt;% select(-rownum, -min_xerror_idx) ## CP nsplit rel.error xerror xstd xerror_cap eval ## 1 0.263 0 1.00 1.01 0.077 0.59 ## 2 0.121 1 0.74 0.75 0.059 0.59 ## 3 0.046 2 0.62 0.65 0.051 0.59 ## 4 0.045 3 0.57 0.67 0.052 0.59 ## 5 0.042 4 0.52 0.66 0.051 0.59 ## 6 0.026 5 0.48 0.62 0.049 0.59 ## 7 0.026 6 0.46 0.62 0.048 0.59 ## 8 0.024 7 0.43 0.62 0.048 0.59 ## 9 0.015 8 0.41 0.58 0.042 0.59 under cap ## 10 0.015 9 0.39 0.56 0.041 0.59 under cap ## 11 0.015 10 0.38 0.56 0.041 0.59 under cap ## 12 0.014 11 0.36 0.56 0.041 0.59 under cap ## 13 0.014 12 0.35 0.56 0.038 0.59 min xerror ## 14 0.014 13 0.33 0.56 0.038 0.59 under cap ## 15 0.011 14 0.32 0.57 0.039 0.59 under cap ## 16 0.010 15 0.31 0.57 0.038 0.59 under cap Okay, so the simplest tree is the one with CP = 0.02599265 (5 splits). Fortunately, plotcp() presents a nice graphical representation of the relationship between xerror and cp. plotcp(cs_mdl_cart_full, upper = &quot;splits&quot;) The dashed line is set at the minimum xerror + xstd. The top axis shows the number of splits in the tree. I’m not sure why the CP values are not the same as in the table (they are close, but not the same). The smallest relative error is at CP = 0.01000000 (15 splits), but the maximum CP below the dashed line (one standard deviation above the minimum error) is at CP = 0.02599265 (5 splits). Use the prune() function to prune the tree by specifying the associated cost-complexity cp. cs_mdl_cart &lt;- prune( cs_mdl_cart_full, cp = cs_mdl_cart_full$cptable[cs_mdl_cart_full$cptable[, 2] == 5, &quot;CP&quot;] ) rpart.plot(cs_mdl_cart, yesno = TRUE) The most “important” indicator of Sales is ShelveLoc. Here are the importance values from the model. cs_mdl_cart$variable.importance %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;Feature&quot;) %&gt;% rename(Overall = &#39;.&#39;) %&gt;% ggplot(aes(x = fct_reorder(Feature, Overall), y = Overall)) + geom_pointrange(aes(ymin = 0, ymax = Overall), color = &quot;cadetblue&quot;, size = .3) + theme_minimal() + coord_flip() + labs(x = &quot;&quot;, y = &quot;&quot;, title = &quot;Variable Importance with Simple Regression&quot;) The most important indicator of Sales is ShelveLoc, then Price, then Age, all of which appear in the final model. CompPrice was also important. The last step is to make predictions on the validation data set. The root mean squared error (\\(RMSE = \\sqrt{(1/2) \\sum{(actual - pred)^2}})\\) and mean absolute error (\\(MAE = (1/n) \\sum{|actual - pred|}\\)) are the two most common measures of predictive accuracy. The key difference is that RMSE punishes large errors more harshly. For a regression tree, set argument type = \"vector\" (or do not specify at all). cs_preds_cart &lt;- predict(cs_mdl_cart, cs_test, type = &quot;vector&quot;) cs_rmse_cart &lt;- RMSE( pred = cs_preds_cart, obs = cs_test$Sales ) cs_rmse_cart ## [1] 2.4 The pruning process leads to an average prediction error of 2.363 in the test data set. Not too bad considering the standard deviation of Sales is 2.8. Here is a predicted vs actual plot. data.frame(Predicted = cs_preds_cart, Actual = cs_test$Sales) %&gt;% ggplot(aes(x = Actual, y = Predicted)) + geom_point(alpha = 0.6, color = &quot;cadetblue&quot;) + geom_smooth() + geom_abline(intercept = 0, slope = 1, linetype = 2) + labs(title = &quot;Carseats CART, Predicted vs Actual&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; The 6 possible predicted values do a decent job of binning the observations. 8.2.1 Training with Caret I can also fit the model with caret::train(), specifying method = \"rpart\". I’ll build the model using 10-fold cross-validation to optimize the hyperparameter CP. cs_trControl = trainControl( method = &quot;cv&quot;, number = 10, savePredictions = &quot;final&quot; # save predictions for the optimal tuning parameter ) I’ll let the model look for the best CP tuning parameter with tuneLength to get close, then fine-tune with tuneGrid. set.seed(1234) cs_mdl_cart2 = train( Sales ~ ., data = cs_train, method = &quot;rpart&quot;, tuneLength = 5, metric = &quot;RMSE&quot;, trControl = cs_trControl ) ## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : ## There were missing values in resampled performance measures. print(cs_mdl_cart2) ## CART ## ## 321 samples ## 10 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... ## Resampling results across tuning parameters: ## ## cp RMSE Rsquared MAE ## 0.042 2.2 0.41 1.8 ## 0.045 2.2 0.38 1.8 ## 0.046 2.3 0.37 1.8 ## 0.121 2.4 0.29 1.9 ## 0.263 2.7 0.19 2.2 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was cp = 0.042. The first cp (0.04167149) produced the smallest RMSE. I can drill into the best value of cp using a tuning grid. I’ll try that now. set.seed(1234) cs_mdl_cart2 = train( Sales ~ ., data = cs_train, method = &quot;rpart&quot;, tuneGrid = expand.grid(cp = seq(from = 0, to = 0.1, by = 0.01)), metric = &quot;RMSE&quot;, trControl = cs_trControl ) print(cs_mdl_cart2) ## CART ## ## 321 samples ## 10 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... ## Resampling results across tuning parameters: ## ## cp RMSE Rsquared MAE ## 0.00 2.1 0.50 1.7 ## 0.01 2.1 0.46 1.7 ## 0.02 2.1 0.47 1.7 ## 0.03 2.1 0.45 1.7 ## 0.04 2.1 0.44 1.7 ## 0.05 2.3 0.36 1.8 ## 0.06 2.3 0.37 1.8 ## 0.07 2.3 0.36 1.8 ## 0.08 2.3 0.36 1.8 ## 0.09 2.3 0.36 1.8 ## 0.10 2.3 0.36 1.8 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was cp = 0. It looks like the best performing tree is the unpruned one. plot(cs_mdl_cart2) Let’s see the final model. rpart.plot(cs_mdl_cart2$finalModel) What were the most important variables? plot(varImp(cs_mdl_cart2), main=&quot;Variable Importance with Simple Regression&quot;) Evaluate the model by making predictions with the test data set. cs_preds_cart2 &lt;- predict(cs_mdl_cart2, cs_test, type = &quot;raw&quot;) data.frame(Actual = cs_test$Sales, Predicted = cs_preds_cart2) %&gt;% ggplot(aes(x = Actual, y = Predicted)) + geom_point(alpha = 0.6, color = &quot;cadetblue&quot;) + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;) + geom_abline(intercept = 0, slope = 1, linetype = 2) + labs(title = &quot;Carseats CART, Predicted vs Actual (caret)&quot;) The model over-estimates at the low end and underestimates at the high end. Calculate the test data set RMSE. (cs_rmse_cart2 &lt;- RMSE(pred = cs_preds_cart2, obs = cs_test$Sales)) ## [1] 2.3 Caret performed better in this model. Here is a summary the RMSE values of the two models. cs_scoreboard &lt;- rbind( data.frame(Model = &quot;Single Tree&quot;, RMSE = cs_rmse_cart), data.frame(Model = &quot;Single Tree (caret)&quot;, RMSE = cs_rmse_cart2) ) %&gt;% arrange(RMSE) scoreboard(cs_scoreboard) ModelRMSESingle Tree (caret)2.2983Single Tree2.3632 "],
["bagged-trees.html", "8.3 Bagged Trees", " 8.3 Bagged Trees One drawback of decision trees is that they are high-variance estimators. A small number of additional training observations can dramatically alter the prediction performance of a learned tree. Bootstrap aggregation, or bagging, is a general-purpose procedure for reducing the variance of a statistical learning method. The algorithm constructs B regression trees using B bootstrapped training sets, and averages the resulting predictions. These trees are grown deep, and are not pruned. Hence each individual tree has high variance, but low bias. Averaging the B trees reduces the variance. The predicted value for an observation is the mode (classification) or mean (regression) of the trees. B usually equals ~25. To test the model accuracy, the out-of-bag observations are predicted from the models. For a training set of size n, each tree is composed of \\(\\sim (1 - e^{-1})n = .632n\\) unique observations in-bag and \\(.368n\\) out-of-bag. For each tree in the ensemble, bagging makes predictions on the tree’s out-of-bag observations. I think (see page 197 of (Kuhn and Johnson 2016)) bagging measures the performance (RMSE, Accuracy, ROC, etc.) of each tree in the ensemble and averages them to produce an overall performance estimate. (This makes no sense to me. If each tree has poor performance, then the average performance of many trees will still be poor. An ensemble of B trees will produce \\(\\sim .368 B\\) predictions per unique observation. Seems like you should take the mean/mode of each observation’s prediction as the final prediction. Then you have n predictions to compare to n actuals, and you assess performance on that.) The downside to bagging is that there is no single tree with a set of rules to interpret. It becomes unclear which variables are more important than others. The next section explains how bagged trees are a special case of random forests. 8.3.1 Bagged Classification Tree Leaning by example, I’ll predict Purchase from the OJ data set again, this time using the bagging method by specifying method = \"treebag\". Caret has no hyperparameters to tune with this model, so I won’t set tuneLegth or tuneGrid. The ensemble size defaults to nbagg = 25, but you can override it (I didn’t). set.seed(1234) oj_mdl_bag &lt;- train( Purchase ~ ., data = oj_train, method = &quot;treebag&quot;, trControl = oj_trControl, metric = &quot;ROC&quot; ) oj_mdl_bag$finalModel ## ## Bagging classification trees with 25 bootstrap replications oj_mdl_bag ## Bagged CART ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... ## Resampling results: ## ## ROC Sens Spec ## 0.86 0.84 0.72 # summary(oj_mdl_bag) If you review the summary(oj_mdl_bag), you’ll see that caret built B = 25 trees from 25 bootstrapped training sets of 857 samples (the size of oj_train). I think caret started by splitting the training set into 10 folds, then using 9 of the folds to run the bagging algorithm and collect performance measures on the hold-out fold. After repeating the process for all 10 folds, it averaged the performance measures to produce the resampling results shown above. Had there been hyperparameters to tune, caret would have repeated this process for all hyperparameter combinations and the resampling results above would be from the best performing combination. Then caret ran the bagging algorithm again on the entire data set, and the trees you see in summary(oj_mdl_bag) are what it produces. (It seems inefficient to cross-validate a bagging algorithm given that the out-of-bag samples are there for performance testing.) Let’s look at the performance on the holdout data set. oj_preds_bag &lt;- bind_cols( predict(oj_mdl_bag, newdata = oj_test, type = &quot;prob&quot;), Predicted = predict(oj_mdl_bag, newdata = oj_test, type = &quot;raw&quot;), Actual = oj_test$Purchase ) oj_cm_bag &lt;- confusionMatrix(oj_preds_bag$Predicted, reference = oj_preds_bag$Actual) oj_cm_bag ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 113 16 ## MM 17 67 ## ## Accuracy : 0.845 ## 95% CI : (0.789, 0.891) ## No Information Rate : 0.61 ## P-Value [Acc &gt; NIR] : 0.0000000000000631 ## ## Kappa : 0.675 ## ## Mcnemar&#39;s Test P-Value : 1 ## ## Sensitivity : 0.869 ## Specificity : 0.807 ## Pos Pred Value : 0.876 ## Neg Pred Value : 0.798 ## Prevalence : 0.610 ## Detection Rate : 0.531 ## Detection Prevalence : 0.606 ## Balanced Accuracy : 0.838 ## ## &#39;Positive&#39; Class : CH ## The accuracy is 0.8451 - surprisingly worse than the 0.85915 of the single tree model, but that is a difference of three predictions in a set of 213. Here are the ROC and gain curves. mdl_auc &lt;- Metrics::auc(actual = oj_preds_bag$Actual == &quot;CH&quot;, oj_preds_bag$CH) yardstick::roc_curve(oj_preds_bag, Actual, CH) %&gt;% autoplot() + labs( title = &quot;OJ Bagging ROC Curve&quot;, subtitle = paste0(&quot;AUC = &quot;, round(mdl_auc, 4)) ) yardstick::gain_curve(oj_preds_bag, Actual, CH) %&gt;% autoplot() + labs(title = &quot;OJ Bagging Gain Curve&quot;) Let’s see what are the most important variables. plot(varImp(oj_mdl_bag), main=&quot;Variable Importance with Bagging&quot;) Finally, let’s check out the scoreboard. Bagging fared worse than the single tree models. oj_scoreboard &lt;- rbind(oj_scoreboard, data.frame(Model = &quot;Bagging&quot;, Accuracy = oj_cm_bag$overall[&quot;Accuracy&quot;]) ) %&gt;% arrange(desc(Accuracy)) scoreboard(oj_scoreboard) ModelAccuracySingle Tree0.8592Single Tree (caret)0.8545Bagging0.8451 8.3.2 Bagging Regression Tree I’ll predict Sales from the Carseats data set again, this time using the bagging method by specifying method = \"treebag\". set.seed(1234) cs_mdl_bag &lt;- train( Sales ~ ., data = cs_train, method = &quot;treebag&quot;, trControl = cs_trControl ) cs_mdl_bag ## Bagged CART ## ## 321 samples ## 10 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 1.7 0.68 1.3 Let’s look at the performance on the holdout data set. The RMSE is 1.9185, but the model over-predicts at low end of Sales and under-predicts at high end. cs_preds_bag &lt;- bind_cols( Predicted = predict(cs_mdl_bag, newdata = cs_test), Actual = cs_test$Sales ) (cs_rmse_bag &lt;- RMSE(pred = cs_preds_bag$Predicted, obs = cs_preds_bag$Actual)) ## [1] 1.9 cs_preds_bag %&gt;% ggplot(aes(x = Actual, y = Predicted)) + geom_point(alpha = 0.6, color = &quot;cadetblue&quot;) + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;) + geom_abline(intercept = 0, slope = 1, linetype = 2) + labs(title = &quot;Carseats Bagging, Predicted vs Actual (caret)&quot;) Now the variable importance. plot(varImp(cs_mdl_bag), main=&quot;Variable Importance with Bagging&quot;) Before moving on, check in with the scoreboard. cs_scoreboard &lt;- rbind(cs_scoreboard, data.frame(Model = &quot;Bagging&quot;, RMSE = cs_rmse_bag) ) %&gt;% arrange(RMSE) scoreboard(cs_scoreboard) ModelRMSEBagging1.9185Single Tree (caret)2.2983Single Tree2.3632 References "],
["random-forests.html", "8.4 Random Forests", " 8.4 Random Forests Random forests improve bagged trees by way of a small tweak that de-correlates the trees. As in bagging, the algorithm builds a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of mtry predictors is chosen as split candidates from the full set of p predictors. A fresh sample of mtry predictors is taken at each split. Typically \\(mtry \\sim \\sqrt{p}\\). Bagged trees are thus a special case of random forests where mtry = p. 8.4.0.1 Random Forest Classification Tree Now I’ll try it with the random forest method by specifying method = \"rf\". Hyperparameter mtry can take any value from 1 to 17 (the number of predictors) and I expect the best value to be near \\(\\sqrt{17} \\sim 4\\). set.seed(1234) oj_mdl_rf &lt;- train( Purchase ~ ., data = oj_train, method = &quot;rf&quot;, metric = &quot;ROC&quot;, tuneGrid = expand.grid(mtry = 1:10), # searching around mtry=4 trControl = oj_trControl ) oj_mdl_rf ## Random Forest ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... ## Resampling results across tuning parameters: ## ## mtry ROC Sens Spec ## 1 0.84 0.90 0.55 ## 2 0.86 0.88 0.70 ## 3 0.87 0.86 0.72 ## 4 0.87 0.85 0.72 ## 5 0.87 0.85 0.71 ## 6 0.87 0.84 0.73 ## 7 0.87 0.84 0.72 ## 8 0.87 0.84 0.72 ## 9 0.87 0.83 0.72 ## 10 0.87 0.84 0.72 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 7. The largest ROC score was at mtry = 7 - higher than I expected. plot(oj_mdl_rf) Use the model to make predictions on the test set. oj_preds_rf &lt;- bind_cols( predict(oj_mdl_rf, newdata = oj_test, type = &quot;prob&quot;), Predicted = predict(oj_mdl_rf, newdata = oj_test, type = &quot;raw&quot;), Actual = oj_test$Purchase ) oj_cm_rf &lt;- confusionMatrix(oj_preds_rf$Predicted, reference = oj_preds_rf$Actual) oj_cm_rf ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 110 16 ## MM 20 67 ## ## Accuracy : 0.831 ## 95% CI : (0.774, 0.879) ## No Information Rate : 0.61 ## P-Value [Acc &gt; NIR] : 0.0000000000023 ## ## Kappa : 0.648 ## ## Mcnemar&#39;s Test P-Value : 0.617 ## ## Sensitivity : 0.846 ## Specificity : 0.807 ## Pos Pred Value : 0.873 ## Neg Pred Value : 0.770 ## Prevalence : 0.610 ## Detection Rate : 0.516 ## Detection Prevalence : 0.592 ## Balanced Accuracy : 0.827 ## ## &#39;Positive&#39; Class : CH ## The accuracy on the holdout set is 0.8310. The AUC is 0.9244. Here are the ROC and gain curves. # AUC is 0.9190 mdl_auc &lt;- Metrics::auc(actual = oj_preds_rf$Actual == &quot;CH&quot;, oj_preds_rf$CH) yardstick::roc_curve(oj_preds_rf, Actual, CH) %&gt;% autoplot() + labs( title = &quot;OJ Random Forest ROC Curve&quot;, subtitle = paste0(&quot;AUC = &quot;, round(mdl_auc, 4)) ) yardstick::gain_curve(oj_preds_rf, Actual, CH) %&gt;% autoplot() + labs(title = &quot;OJ Random Forest Gain Curve&quot;) What are the most important variables? plot(varImp(oj_mdl_rf), main=&quot;Variable Importance with Random Forest&quot;) Let’s update the scoreboard. The bagging and random forest models did pretty well, but the manual classification tree is still in first place. There’s still gradient boosting to investigate! oj_scoreboard &lt;- rbind(oj_scoreboard, data.frame(Model = &quot;Random Forest&quot;, Accuracy = oj_cm_rf$overall[&quot;Accuracy&quot;]) ) %&gt;% arrange(desc(Accuracy)) scoreboard(oj_scoreboard) ModelAccuracySingle Tree0.8592Single Tree (caret)0.8545Bagging0.8451Random Forest0.8310 8.4.0.2 Random Forest Regression Tree Now I’ll try it with the random forest method by specifying method = \"rf\". Hyperparameter mtry can take any value from 1 to 10 (the number of predictors) and I expect the best value to be near \\(\\sqrt{10} \\sim 3\\). set.seed(1234) cs_mdl_rf &lt;- train( Sales ~ ., data = cs_train, method = &quot;rf&quot;, tuneGrid = expand.grid(mtry = 1:10), # searching around mtry=3 trControl = cs_trControl ) cs_mdl_rf ## Random Forest ## ## 321 samples ## 10 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... ## Resampling results across tuning parameters: ## ## mtry RMSE Rsquared MAE ## 1 2.2 0.64 1.7 ## 2 1.8 0.73 1.4 ## 3 1.7 0.75 1.3 ## 4 1.6 0.75 1.3 ## 5 1.5 0.76 1.2 ## 6 1.5 0.75 1.2 ## 7 1.5 0.75 1.2 ## 8 1.5 0.75 1.2 ## 9 1.5 0.74 1.2 ## 10 1.5 0.74 1.2 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was mtry = 7. The minimum RMSE is at mtry = 7. plot(cs_mdl_rf) Make predictions on the test set. Like the bagged tree model, this one also over-predicts at low end of Sales and under-predicts at high end. The RMSE of 1.7184 is better than bagging’s 1.9185. cs_preds_rf &lt;- bind_cols( Predicted = predict(cs_mdl_rf, newdata = cs_test), Actual = cs_test$Sales ) (cs_rmse_rf &lt;- RMSE(pred = cs_preds_rf$Predicted, obs = cs_preds_rf$Actual)) ## [1] 1.7 cs_preds_rf %&gt;% ggplot(aes(x = Actual, y = Predicted)) + geom_point(alpha = 0.6, color = &quot;cadetblue&quot;) + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;) + geom_abline(intercept = 0, slope = 1, linetype = 2) + labs(title = &quot;Carseats Random Forest, Predicted vs Actual (caret)&quot;) plot(varImp(cs_mdl_rf), main=&quot;Variable Importance with Random Forest&quot;) Let’s check in with the scoreboard. cs_scoreboard &lt;- rbind(cs_scoreboard, data.frame(Model = &quot;Random Forest&quot;, RMSE = cs_rmse_rf) ) %&gt;% arrange(RMSE) scoreboard(cs_scoreboard) ModelRMSERandom Forest1.7184Bagging1.9185Single Tree (caret)2.2983Single Tree2.3632 The bagging and random forest models did very well - they took over the top positions! "],
["gradient-boosting.html", "8.5 Gradient Boosting", " 8.5 Gradient Boosting Note: I learned gradient boosting from explained.ai. Gradient boosting machine (GBM) is an additive modeling algorithm that gradually builds a composite model by iteratively adding M weak sub-models based on the performance of the prior iteration’s composite, \\[F_M(x) = \\sum_m^M f_m(x).\\] The idea is to fit a weak model, then replace the response values with the residuals from that model, and fit another model. Adding the residual prediction model to the original response prediction model produces a more accurate model. GBM repeats this process over and over, running new models to predict the residuals of the previous composite models, and adding the results to produce new composites. With each iteration, the model becomes stronger and stronger. The successive trees are usually weighted to slow down the learning rate. “Shrinkage” reduces the influence of each individual tree and leaves space for future trees to improve the model. \\[F_M(x) = f_0 + \\eta\\sum_{m = 1}^M f_m(x).\\] The smaller the learning rate, \\(\\eta\\), the larger the number of trees, \\(M\\). \\(\\eta\\) and \\(M\\) are hyperparameters. Other constraints to the trees are usually applied as additional hyperparameters, including, tree depth, number of nodes, minimum observations per split, and minimum improvement to loss. The name “gradient boosting” refers to the boosting of a model with a gradient. Each round of training builds a weak learner and uses the residuals to calculate a gradient, the partial derivative of the loss function. Gradient boosting “descends the gradient” to adjust the model parameters to reduce the error in the next round of training. In the case of classification problems, the loss function is the log-loss; for regression problems, the loss function is mean squared error. GBM continues until it reaches maximum number of trees or an acceptable error level. 8.5.0.1 Gradient Boosting Classification Tree In addition to the gradient boosting machine algorithm, implemented in caret with method = gbm, there is a variable called Extreme Gradient Boosting, XGBoost, which frankly I don’t know anything about other than it is supposed to work extremely well. Let’s try them both! 8.5.0.1.1 GBM I’ll predict Purchase from the OJ data set again, this time using the GBM method by specifying method = \"gbm\". gbm has the following tuneable hyperparameters (see modelLookup(\"gbm\")). n.trees: number of boosting iterations, \\(M\\) interaction.depth: maximum tree depth shrinkage: shrinkage, \\(\\eta\\) n.minobsinnode: minimum terminal node size I’ll use tuneLength = 5. set.seed(1234) garbage &lt;- capture.output( oj_mdl_gbm &lt;- train( Purchase ~ ., data = oj_train, method = &quot;gbm&quot;, metric = &quot;ROC&quot;, tuneLength = 5, trControl = oj_trControl )) oj_mdl_gbm ## Stochastic Gradient Boosting ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... ## Resampling results across tuning parameters: ## ## interaction.depth n.trees ROC Sens Spec ## 1 50 0.88 0.87 0.72 ## 1 100 0.89 0.87 0.74 ## 1 150 0.88 0.87 0.75 ## 1 200 0.88 0.87 0.75 ## 1 250 0.88 0.87 0.74 ## 2 50 0.88 0.87 0.75 ## 2 100 0.89 0.87 0.75 ## 2 150 0.88 0.86 0.75 ## 2 200 0.88 0.86 0.75 ## 2 250 0.88 0.85 0.75 ## 3 50 0.89 0.87 0.76 ## 3 100 0.88 0.86 0.77 ## 3 150 0.88 0.86 0.76 ## 3 200 0.87 0.87 0.77 ## 3 250 0.87 0.86 0.76 ## 4 50 0.88 0.85 0.75 ## 4 100 0.88 0.86 0.76 ## 4 150 0.87 0.85 0.77 ## 4 200 0.87 0.85 0.76 ## 4 250 0.87 0.85 0.75 ## 5 50 0.89 0.86 0.76 ## 5 100 0.88 0.85 0.74 ## 5 150 0.88 0.85 0.76 ## 5 200 0.87 0.85 0.74 ## 5 250 0.87 0.84 0.75 ## ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## ## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10 ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were n.trees = 50, interaction.depth = ## 5, shrinkage = 0.1 and n.minobsinnode = 10. train() tuned n.trees ($M) and interaction.depth, holding shrinkage = 0.1 (), and n.minobsinnode = 10. The optimal hyperparameter values were n.trees = 50, and interaction.depth = 5. You can see from the tuning plot that accuracy is maximized at \\(M=50\\) for tree depth of 5, but \\(M=50\\) with tree depth of 3 worked nearly as well. plot(oj_mdl_gbm) Let’s see how the model performed on the holdout set. The accuracy was 0.8451. oj_preds_gbm &lt;- bind_cols( predict(oj_mdl_gbm, newdata = oj_test, type = &quot;prob&quot;), Predicted = predict(oj_mdl_gbm, newdata = oj_test, type = &quot;raw&quot;), Actual = oj_test$Purchase ) oj_cm_gbm &lt;- confusionMatrix(oj_preds_gbm$Predicted, reference = oj_preds_gbm$Actual) oj_cm_gbm ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 113 16 ## MM 17 67 ## ## Accuracy : 0.845 ## 95% CI : (0.789, 0.891) ## No Information Rate : 0.61 ## P-Value [Acc &gt; NIR] : 0.0000000000000631 ## ## Kappa : 0.675 ## ## Mcnemar&#39;s Test P-Value : 1 ## ## Sensitivity : 0.869 ## Specificity : 0.807 ## Pos Pred Value : 0.876 ## Neg Pred Value : 0.798 ## Prevalence : 0.610 ## Detection Rate : 0.531 ## Detection Prevalence : 0.606 ## Balanced Accuracy : 0.838 ## ## &#39;Positive&#39; Class : CH ## AUC was 0.9386. Here are the ROC and gain curves. mdl_auc &lt;- Metrics::auc(actual = oj_preds_gbm$Actual == &quot;CH&quot;, oj_preds_gbm$CH) yardstick::roc_curve(oj_preds_gbm, Actual, CH) %&gt;% autoplot() + labs( title = &quot;OJ GBM ROC Curve&quot;, subtitle = paste0(&quot;AUC = &quot;, round(mdl_auc, 4)) ) yardstick::gain_curve(oj_preds_gbm, Actual, CH) %&gt;% autoplot() + labs(title = &quot;OJ GBM Gain Curve&quot;) Now the variable importance. Just a few variables. LoyalCH is at the top again. #plot(varImp(oj_mdl_gbm), main=&quot;Variable Importance with Gradient Boosting&quot;) 8.5.0.1.2 XGBoost I’ll predict Purchase from the OJ data set again, this time using the XGBoost method by specifying method = \"xgbTree\". xgbTree has the following tuneable hyperparameters (see modelLookup(\"xgbTree\")). The first three are the same as xgb. nrounds: number of boosting iterations, \\(M\\) max_depth: maximum tree depth eta: shrinkage, \\(\\eta\\) gamma: minimum loss reduction colsamle_bytree: subsample ratio of columns min_child_weight: minimum size of instance weight substample: subsample percentage I’ll use tuneLength = 5 again. set.seed(1234) garbage &lt;- capture.output( oj_mdl_xgb &lt;- train( Purchase ~ ., data = oj_train, method = &quot;xgbTree&quot;, metric = &quot;ROC&quot;, tuneLength = 5, trControl = oj_trControl )) oj_mdl_xgb ## eXtreme Gradient Boosting ## ## 857 samples ## 17 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... ## Resampling results across tuning parameters: ## ## eta max_depth colsample_bytree subsample nrounds ROC Sens Spec ## 0.3 1 0.6 0.50 50 0.88 0.88 0.75 ## 0.3 1 0.6 0.50 100 0.88 0.87 0.75 ## 0.3 1 0.6 0.50 150 0.88 0.87 0.73 ## 0.3 1 0.6 0.50 200 0.88 0.86 0.73 ## 0.3 1 0.6 0.50 250 0.88 0.86 0.74 ## 0.3 1 0.6 0.62 50 0.89 0.88 0.75 ## 0.3 1 0.6 0.62 100 0.88 0.87 0.76 ## 0.3 1 0.6 0.62 150 0.88 0.86 0.75 ## 0.3 1 0.6 0.62 200 0.88 0.86 0.74 ## 0.3 1 0.6 0.62 250 0.88 0.86 0.73 ## 0.3 1 0.6 0.75 50 0.89 0.88 0.75 ## 0.3 1 0.6 0.75 100 0.88 0.87 0.75 ## 0.3 1 0.6 0.75 150 0.88 0.88 0.72 ## 0.3 1 0.6 0.75 200 0.88 0.87 0.73 ## 0.3 1 0.6 0.75 250 0.88 0.86 0.74 ## 0.3 1 0.6 0.88 50 0.89 0.87 0.75 ## 0.3 1 0.6 0.88 100 0.88 0.87 0.74 ## 0.3 1 0.6 0.88 150 0.88 0.86 0.75 ## 0.3 1 0.6 0.88 200 0.88 0.86 0.74 ## 0.3 1 0.6 0.88 250 0.88 0.86 0.75 ## 0.3 1 0.6 1.00 50 0.89 0.88 0.75 ## 0.3 1 0.6 1.00 100 0.89 0.88 0.74 ## 0.3 1 0.6 1.00 150 0.89 0.88 0.74 ## 0.3 1 0.6 1.00 200 0.88 0.88 0.75 ## 0.3 1 0.6 1.00 250 0.88 0.87 0.75 ## 0.3 1 0.8 0.50 50 0.88 0.87 0.75 ## 0.3 1 0.8 0.50 100 0.88 0.86 0.74 ## 0.3 1 0.8 0.50 150 0.88 0.87 0.74 ## 0.3 1 0.8 0.50 200 0.87 0.85 0.74 ## 0.3 1 0.8 0.50 250 0.87 0.86 0.74 ## 0.3 1 0.8 0.62 50 0.88 0.87 0.74 ## 0.3 1 0.8 0.62 100 0.88 0.86 0.76 ## 0.3 1 0.8 0.62 150 0.88 0.87 0.74 ## 0.3 1 0.8 0.62 200 0.88 0.86 0.74 ## 0.3 1 0.8 0.62 250 0.88 0.87 0.73 ## 0.3 1 0.8 0.75 50 0.89 0.88 0.74 ## 0.3 1 0.8 0.75 100 0.88 0.87 0.74 ## 0.3 1 0.8 0.75 150 0.88 0.88 0.74 ## 0.3 1 0.8 0.75 200 0.88 0.87 0.75 ## 0.3 1 0.8 0.75 250 0.88 0.86 0.74 ## 0.3 1 0.8 0.88 50 0.89 0.87 0.74 ## 0.3 1 0.8 0.88 100 0.88 0.88 0.74 ## 0.3 1 0.8 0.88 150 0.88 0.87 0.74 ## 0.3 1 0.8 0.88 200 0.88 0.86 0.75 ## 0.3 1 0.8 0.88 250 0.88 0.86 0.73 ## 0.3 1 0.8 1.00 50 0.89 0.87 0.74 ## 0.3 1 0.8 1.00 100 0.89 0.87 0.74 ## 0.3 1 0.8 1.00 150 0.88 0.87 0.75 ## 0.3 1 0.8 1.00 200 0.88 0.88 0.75 ## 0.3 1 0.8 1.00 250 0.88 0.87 0.75 ## 0.3 2 0.6 0.50 50 0.88 0.86 0.76 ## 0.3 2 0.6 0.50 100 0.87 0.86 0.75 ## 0.3 2 0.6 0.50 150 0.87 0.85 0.75 ## 0.3 2 0.6 0.50 200 0.86 0.85 0.74 ## 0.3 2 0.6 0.50 250 0.86 0.84 0.73 ## 0.3 2 0.6 0.62 50 0.89 0.86 0.74 ## 0.3 2 0.6 0.62 100 0.88 0.84 0.76 ## 0.3 2 0.6 0.62 150 0.87 0.84 0.76 ## 0.3 2 0.6 0.62 200 0.87 0.83 0.76 ## 0.3 2 0.6 0.62 250 0.86 0.84 0.74 ## 0.3 2 0.6 0.75 50 0.88 0.87 0.77 ## 0.3 2 0.6 0.75 100 0.88 0.86 0.75 ## 0.3 2 0.6 0.75 150 0.87 0.86 0.74 ## 0.3 2 0.6 0.75 200 0.87 0.86 0.73 ## 0.3 2 0.6 0.75 250 0.86 0.85 0.74 ## 0.3 2 0.6 0.88 50 0.89 0.86 0.77 ## 0.3 2 0.6 0.88 100 0.88 0.85 0.74 ## 0.3 2 0.6 0.88 150 0.87 0.85 0.76 ## 0.3 2 0.6 0.88 200 0.87 0.85 0.75 ## 0.3 2 0.6 0.88 250 0.87 0.84 0.75 ## 0.3 2 0.6 1.00 50 0.89 0.87 0.76 ## 0.3 2 0.6 1.00 100 0.88 0.85 0.76 ## 0.3 2 0.6 1.00 150 0.88 0.85 0.75 ## 0.3 2 0.6 1.00 200 0.87 0.86 0.75 ## 0.3 2 0.6 1.00 250 0.87 0.85 0.75 ## 0.3 2 0.8 0.50 50 0.88 0.85 0.74 ## 0.3 2 0.8 0.50 100 0.87 0.86 0.75 ## 0.3 2 0.8 0.50 150 0.87 0.85 0.74 ## 0.3 2 0.8 0.50 200 0.86 0.83 0.74 ## 0.3 2 0.8 0.50 250 0.86 0.83 0.75 ## 0.3 2 0.8 0.62 50 0.88 0.86 0.76 ## 0.3 2 0.8 0.62 100 0.87 0.85 0.76 ## 0.3 2 0.8 0.62 150 0.87 0.85 0.76 ## 0.3 2 0.8 0.62 200 0.87 0.85 0.76 ## 0.3 2 0.8 0.62 250 0.86 0.84 0.74 ## 0.3 2 0.8 0.75 50 0.88 0.85 0.76 ## 0.3 2 0.8 0.75 100 0.88 0.84 0.74 ## 0.3 2 0.8 0.75 150 0.87 0.85 0.75 ## 0.3 2 0.8 0.75 200 0.87 0.85 0.75 ## 0.3 2 0.8 0.75 250 0.87 0.84 0.74 ## 0.3 2 0.8 0.88 50 0.88 0.86 0.75 ## 0.3 2 0.8 0.88 100 0.88 0.85 0.75 ## 0.3 2 0.8 0.88 150 0.87 0.86 0.75 ## 0.3 2 0.8 0.88 200 0.87 0.85 0.75 ## 0.3 2 0.8 0.88 250 0.86 0.85 0.74 ## 0.3 2 0.8 1.00 50 0.88 0.86 0.76 ## 0.3 2 0.8 1.00 100 0.88 0.85 0.75 ## 0.3 2 0.8 1.00 150 0.87 0.85 0.75 ## 0.3 2 0.8 1.00 200 0.87 0.85 0.75 ## 0.3 2 0.8 1.00 250 0.87 0.85 0.74 ## 0.3 3 0.6 0.50 50 0.88 0.86 0.76 ## 0.3 3 0.6 0.50 100 0.87 0.84 0.73 ## 0.3 3 0.6 0.50 150 0.85 0.84 0.72 ## 0.3 3 0.6 0.50 200 0.85 0.84 0.72 ## 0.3 3 0.6 0.50 250 0.85 0.84 0.73 ## 0.3 3 0.6 0.62 50 0.88 0.86 0.76 ## 0.3 3 0.6 0.62 100 0.87 0.84 0.74 ## 0.3 3 0.6 0.62 150 0.86 0.84 0.73 ## 0.3 3 0.6 0.62 200 0.86 0.85 0.73 ## 0.3 3 0.6 0.62 250 0.85 0.85 0.74 ## 0.3 3 0.6 0.75 50 0.88 0.85 0.75 ## 0.3 3 0.6 0.75 100 0.86 0.84 0.74 ## 0.3 3 0.6 0.75 150 0.86 0.84 0.73 ## 0.3 3 0.6 0.75 200 0.86 0.84 0.74 ## 0.3 3 0.6 0.75 250 0.85 0.84 0.74 ## 0.3 3 0.6 0.88 50 0.87 0.86 0.77 ## 0.3 3 0.6 0.88 100 0.87 0.85 0.75 ## 0.3 3 0.6 0.88 150 0.86 0.85 0.75 ## 0.3 3 0.6 0.88 200 0.86 0.84 0.75 ## 0.3 3 0.6 0.88 250 0.86 0.85 0.75 ## 0.3 3 0.6 1.00 50 0.88 0.86 0.76 ## 0.3 3 0.6 1.00 100 0.87 0.86 0.75 ## 0.3 3 0.6 1.00 150 0.87 0.86 0.74 ## 0.3 3 0.6 1.00 200 0.87 0.84 0.75 ## 0.3 3 0.6 1.00 250 0.86 0.85 0.73 ## 0.3 3 0.8 0.50 50 0.87 0.84 0.75 ## 0.3 3 0.8 0.50 100 0.86 0.85 0.73 ## 0.3 3 0.8 0.50 150 0.86 0.84 0.71 ## 0.3 3 0.8 0.50 200 0.85 0.83 0.74 ## 0.3 3 0.8 0.50 250 0.84 0.83 0.71 ## 0.3 3 0.8 0.62 50 0.88 0.87 0.75 ## 0.3 3 0.8 0.62 100 0.87 0.85 0.73 ## 0.3 3 0.8 0.62 150 0.87 0.85 0.74 ## 0.3 3 0.8 0.62 200 0.86 0.84 0.74 ## 0.3 3 0.8 0.62 250 0.85 0.84 0.72 ## 0.3 3 0.8 0.75 50 0.88 0.86 0.75 ## 0.3 3 0.8 0.75 100 0.87 0.85 0.72 ## 0.3 3 0.8 0.75 150 0.86 0.84 0.73 ## 0.3 3 0.8 0.75 200 0.86 0.84 0.73 ## 0.3 3 0.8 0.75 250 0.86 0.84 0.72 ## 0.3 3 0.8 0.88 50 0.88 0.86 0.74 ## 0.3 3 0.8 0.88 100 0.87 0.85 0.74 ## 0.3 3 0.8 0.88 150 0.86 0.85 0.75 ## 0.3 3 0.8 0.88 200 0.86 0.84 0.75 ## 0.3 3 0.8 0.88 250 0.86 0.84 0.74 ## 0.3 3 0.8 1.00 50 0.88 0.85 0.75 ## 0.3 3 0.8 1.00 100 0.87 0.86 0.75 ## 0.3 3 0.8 1.00 150 0.87 0.85 0.74 ## 0.3 3 0.8 1.00 200 0.86 0.85 0.75 ## 0.3 3 0.8 1.00 250 0.86 0.84 0.75 ## 0.3 4 0.6 0.50 50 0.87 0.85 0.74 ## 0.3 4 0.6 0.50 100 0.85 0.84 0.74 ## 0.3 4 0.6 0.50 150 0.86 0.84 0.73 ## 0.3 4 0.6 0.50 200 0.85 0.84 0.72 ## 0.3 4 0.6 0.50 250 0.85 0.85 0.72 ## 0.3 4 0.6 0.62 50 0.87 0.85 0.75 ## 0.3 4 0.6 0.62 100 0.87 0.85 0.74 ## 0.3 4 0.6 0.62 150 0.86 0.83 0.75 ## 0.3 4 0.6 0.62 200 0.86 0.83 0.75 ## 0.3 4 0.6 0.62 250 0.86 0.84 0.74 ## 0.3 4 0.6 0.75 50 0.87 0.85 0.74 ## 0.3 4 0.6 0.75 100 0.86 0.85 0.72 ## 0.3 4 0.6 0.75 150 0.86 0.84 0.75 ## 0.3 4 0.6 0.75 200 0.86 0.83 0.73 ## 0.3 4 0.6 0.75 250 0.85 0.83 0.72 ## 0.3 4 0.6 0.88 50 0.87 0.86 0.75 ## 0.3 4 0.6 0.88 100 0.86 0.85 0.74 ## 0.3 4 0.6 0.88 150 0.85 0.85 0.73 ## 0.3 4 0.6 0.88 200 0.85 0.84 0.73 ## 0.3 4 0.6 0.88 250 0.85 0.84 0.72 ## 0.3 4 0.6 1.00 50 0.88 0.86 0.76 ## 0.3 4 0.6 1.00 100 0.86 0.85 0.73 ## 0.3 4 0.6 1.00 150 0.86 0.85 0.73 ## 0.3 4 0.6 1.00 200 0.86 0.83 0.73 ## 0.3 4 0.6 1.00 250 0.85 0.82 0.72 ## 0.3 4 0.8 0.50 50 0.86 0.85 0.72 ## 0.3 4 0.8 0.50 100 0.85 0.85 0.72 ## 0.3 4 0.8 0.50 150 0.85 0.85 0.73 ## 0.3 4 0.8 0.50 200 0.85 0.84 0.72 ## 0.3 4 0.8 0.50 250 0.84 0.83 0.73 ## 0.3 4 0.8 0.62 50 0.87 0.85 0.76 ## 0.3 4 0.8 0.62 100 0.86 0.85 0.74 ## 0.3 4 0.8 0.62 150 0.86 0.84 0.71 ## 0.3 4 0.8 0.62 200 0.85 0.83 0.72 ## 0.3 4 0.8 0.62 250 0.85 0.83 0.71 ## 0.3 4 0.8 0.75 50 0.87 0.84 0.75 ## 0.3 4 0.8 0.75 100 0.86 0.84 0.74 ## 0.3 4 0.8 0.75 150 0.86 0.84 0.73 ## 0.3 4 0.8 0.75 200 0.85 0.84 0.72 ## 0.3 4 0.8 0.75 250 0.85 0.83 0.72 ## 0.3 4 0.8 0.88 50 0.87 0.84 0.75 ## 0.3 4 0.8 0.88 100 0.86 0.84 0.74 ## 0.3 4 0.8 0.88 150 0.86 0.83 0.73 ## 0.3 4 0.8 0.88 200 0.85 0.83 0.75 ## 0.3 4 0.8 0.88 250 0.85 0.83 0.74 ## 0.3 4 0.8 1.00 50 0.87 0.85 0.74 ## 0.3 4 0.8 1.00 100 0.86 0.86 0.73 ## 0.3 4 0.8 1.00 150 0.86 0.85 0.73 ## 0.3 4 0.8 1.00 200 0.86 0.84 0.73 ## 0.3 4 0.8 1.00 250 0.86 0.84 0.72 ## 0.3 5 0.6 0.50 50 0.86 0.85 0.73 ## 0.3 5 0.6 0.50 100 0.85 0.84 0.71 ## 0.3 5 0.6 0.50 150 0.85 0.83 0.72 ## 0.3 5 0.6 0.50 200 0.85 0.82 0.73 ## 0.3 5 0.6 0.50 250 0.85 0.81 0.72 ## 0.3 5 0.6 0.62 50 0.86 0.84 0.73 ## 0.3 5 0.6 0.62 100 0.86 0.84 0.72 ## 0.3 5 0.6 0.62 150 0.85 0.84 0.73 ## 0.3 5 0.6 0.62 200 0.85 0.85 0.72 ## 0.3 5 0.6 0.62 250 0.85 0.84 0.70 ## 0.3 5 0.6 0.75 50 0.86 0.84 0.75 ## 0.3 5 0.6 0.75 100 0.86 0.83 0.73 ## 0.3 5 0.6 0.75 150 0.85 0.82 0.73 ## 0.3 5 0.6 0.75 200 0.85 0.82 0.71 ## 0.3 5 0.6 0.75 250 0.85 0.83 0.71 ## 0.3 5 0.6 0.88 50 0.86 0.85 0.75 ## 0.3 5 0.6 0.88 100 0.86 0.84 0.74 ## 0.3 5 0.6 0.88 150 0.85 0.84 0.73 ## 0.3 5 0.6 0.88 200 0.85 0.83 0.72 ## 0.3 5 0.6 0.88 250 0.85 0.82 0.73 ## 0.3 5 0.6 1.00 50 0.87 0.85 0.74 ## 0.3 5 0.6 1.00 100 0.86 0.84 0.72 ## 0.3 5 0.6 1.00 150 0.86 0.84 0.73 ## 0.3 5 0.6 1.00 200 0.85 0.83 0.73 ## 0.3 5 0.6 1.00 250 0.85 0.83 0.73 ## 0.3 5 0.8 0.50 50 0.86 0.84 0.74 ## 0.3 5 0.8 0.50 100 0.85 0.84 0.73 ## 0.3 5 0.8 0.50 150 0.85 0.84 0.73 ## 0.3 5 0.8 0.50 200 0.85 0.84 0.72 ## 0.3 5 0.8 0.50 250 0.85 0.83 0.71 ## 0.3 5 0.8 0.62 50 0.86 0.85 0.72 ## 0.3 5 0.8 0.62 100 0.86 0.84 0.74 ## 0.3 5 0.8 0.62 150 0.86 0.83 0.72 ## 0.3 5 0.8 0.62 200 0.85 0.83 0.73 ## 0.3 5 0.8 0.62 250 0.85 0.83 0.72 ## 0.3 5 0.8 0.75 50 0.86 0.85 0.75 ## 0.3 5 0.8 0.75 100 0.85 0.84 0.73 ## 0.3 5 0.8 0.75 150 0.85 0.82 0.72 ## 0.3 5 0.8 0.75 200 0.84 0.83 0.72 ## 0.3 5 0.8 0.75 250 0.84 0.82 0.71 ## 0.3 5 0.8 0.88 50 0.87 0.85 0.75 ## 0.3 5 0.8 0.88 100 0.86 0.84 0.74 ## 0.3 5 0.8 0.88 150 0.85 0.84 0.72 ## 0.3 5 0.8 0.88 200 0.85 0.82 0.72 ## 0.3 5 0.8 0.88 250 0.85 0.83 0.73 ## 0.3 5 0.8 1.00 50 0.87 0.85 0.73 ## 0.3 5 0.8 1.00 100 0.86 0.85 0.75 ## 0.3 5 0.8 1.00 150 0.86 0.84 0.74 ## 0.3 5 0.8 1.00 200 0.85 0.84 0.72 ## 0.3 5 0.8 1.00 250 0.85 0.84 0.72 ## 0.4 1 0.6 0.50 50 0.88 0.86 0.74 ## 0.4 1 0.6 0.50 100 0.88 0.86 0.72 ## 0.4 1 0.6 0.50 150 0.87 0.85 0.74 ## 0.4 1 0.6 0.50 200 0.87 0.86 0.73 ## 0.4 1 0.6 0.50 250 0.87 0.85 0.74 ## 0.4 1 0.6 0.62 50 0.89 0.87 0.73 ## 0.4 1 0.6 0.62 100 0.88 0.86 0.75 ## 0.4 1 0.6 0.62 150 0.88 0.85 0.75 ## 0.4 1 0.6 0.62 200 0.87 0.85 0.75 ## 0.4 1 0.6 0.62 250 0.88 0.86 0.74 ## 0.4 1 0.6 0.75 50 0.88 0.87 0.74 ## 0.4 1 0.6 0.75 100 0.88 0.87 0.75 ## 0.4 1 0.6 0.75 150 0.88 0.87 0.76 ## 0.4 1 0.6 0.75 200 0.88 0.87 0.73 ## 0.4 1 0.6 0.75 250 0.88 0.86 0.74 ## 0.4 1 0.6 0.88 50 0.89 0.88 0.74 ## 0.4 1 0.6 0.88 100 0.88 0.88 0.75 ## 0.4 1 0.6 0.88 150 0.88 0.87 0.74 ## 0.4 1 0.6 0.88 200 0.88 0.87 0.73 ## 0.4 1 0.6 0.88 250 0.87 0.86 0.74 ## 0.4 1 0.6 1.00 50 0.89 0.87 0.74 ## 0.4 1 0.6 1.00 100 0.89 0.87 0.74 ## 0.4 1 0.6 1.00 150 0.88 0.87 0.75 ## 0.4 1 0.6 1.00 200 0.88 0.87 0.75 ## 0.4 1 0.6 1.00 250 0.88 0.87 0.74 ## 0.4 1 0.8 0.50 50 0.88 0.88 0.75 ## 0.4 1 0.8 0.50 100 0.88 0.87 0.74 ## 0.4 1 0.8 0.50 150 0.87 0.86 0.72 ## 0.4 1 0.8 0.50 200 0.87 0.86 0.73 ## 0.4 1 0.8 0.50 250 0.87 0.85 0.73 ## 0.4 1 0.8 0.62 50 0.88 0.88 0.75 ## 0.4 1 0.8 0.62 100 0.88 0.87 0.74 ## 0.4 1 0.8 0.62 150 0.88 0.86 0.77 ## 0.4 1 0.8 0.62 200 0.87 0.85 0.73 ## 0.4 1 0.8 0.62 250 0.87 0.85 0.75 ## 0.4 1 0.8 0.75 50 0.89 0.87 0.75 ## 0.4 1 0.8 0.75 100 0.88 0.86 0.74 ## 0.4 1 0.8 0.75 150 0.88 0.86 0.73 ## 0.4 1 0.8 0.75 200 0.88 0.86 0.73 ## 0.4 1 0.8 0.75 250 0.87 0.85 0.73 ## 0.4 1 0.8 0.88 50 0.89 0.88 0.74 ## 0.4 1 0.8 0.88 100 0.88 0.87 0.75 ## 0.4 1 0.8 0.88 150 0.88 0.86 0.74 ## 0.4 1 0.8 0.88 200 0.88 0.87 0.75 ## 0.4 1 0.8 0.88 250 0.87 0.86 0.74 ## 0.4 1 0.8 1.00 50 0.89 0.87 0.75 ## 0.4 1 0.8 1.00 100 0.88 0.88 0.75 ## 0.4 1 0.8 1.00 150 0.88 0.88 0.74 ## 0.4 1 0.8 1.00 200 0.88 0.87 0.74 ## 0.4 1 0.8 1.00 250 0.88 0.87 0.75 ## 0.4 2 0.6 0.50 50 0.88 0.86 0.75 ## 0.4 2 0.6 0.50 100 0.87 0.85 0.76 ## 0.4 2 0.6 0.50 150 0.87 0.85 0.75 ## 0.4 2 0.6 0.50 200 0.86 0.85 0.71 ## 0.4 2 0.6 0.50 250 0.86 0.85 0.72 ## 0.4 2 0.6 0.62 50 0.88 0.85 0.76 ## 0.4 2 0.6 0.62 100 0.87 0.85 0.75 ## 0.4 2 0.6 0.62 150 0.87 0.85 0.76 ## 0.4 2 0.6 0.62 200 0.86 0.84 0.74 ## 0.4 2 0.6 0.62 250 0.86 0.83 0.73 ## 0.4 2 0.6 0.75 50 0.87 0.86 0.77 ## 0.4 2 0.6 0.75 100 0.87 0.86 0.73 ## 0.4 2 0.6 0.75 150 0.87 0.84 0.76 ## 0.4 2 0.6 0.75 200 0.86 0.85 0.75 ## 0.4 2 0.6 0.75 250 0.86 0.85 0.73 ## 0.4 2 0.6 0.88 50 0.88 0.85 0.75 ## 0.4 2 0.6 0.88 100 0.87 0.85 0.77 ## 0.4 2 0.6 0.88 150 0.87 0.85 0.74 ## 0.4 2 0.6 0.88 200 0.87 0.84 0.74 ## 0.4 2 0.6 0.88 250 0.86 0.85 0.74 ## 0.4 2 0.6 1.00 50 0.88 0.86 0.77 ## 0.4 2 0.6 1.00 100 0.87 0.85 0.74 ## 0.4 2 0.6 1.00 150 0.87 0.84 0.73 ## 0.4 2 0.6 1.00 200 0.87 0.84 0.74 ## 0.4 2 0.6 1.00 250 0.86 0.85 0.74 ## 0.4 2 0.8 0.50 50 0.87 0.85 0.74 ## 0.4 2 0.8 0.50 100 0.87 0.85 0.72 ## 0.4 2 0.8 0.50 150 0.86 0.86 0.73 ## 0.4 2 0.8 0.50 200 0.85 0.84 0.71 ## 0.4 2 0.8 0.50 250 0.86 0.84 0.70 ## 0.4 2 0.8 0.62 50 0.87 0.85 0.75 ## 0.4 2 0.8 0.62 100 0.87 0.85 0.75 ## 0.4 2 0.8 0.62 150 0.86 0.84 0.75 ## 0.4 2 0.8 0.62 200 0.85 0.84 0.73 ## 0.4 2 0.8 0.62 250 0.85 0.84 0.73 ## 0.4 2 0.8 0.75 50 0.87 0.85 0.75 ## 0.4 2 0.8 0.75 100 0.87 0.85 0.75 ## 0.4 2 0.8 0.75 150 0.86 0.84 0.75 ## 0.4 2 0.8 0.75 200 0.86 0.85 0.73 ## 0.4 2 0.8 0.75 250 0.86 0.83 0.74 ## 0.4 2 0.8 0.88 50 0.88 0.87 0.75 ## 0.4 2 0.8 0.88 100 0.87 0.85 0.74 ## 0.4 2 0.8 0.88 150 0.87 0.85 0.73 ## 0.4 2 0.8 0.88 200 0.87 0.85 0.74 ## 0.4 2 0.8 0.88 250 0.86 0.85 0.74 ## 0.4 2 0.8 1.00 50 0.88 0.85 0.75 ## 0.4 2 0.8 1.00 100 0.87 0.85 0.74 ## 0.4 2 0.8 1.00 150 0.87 0.85 0.74 ## 0.4 2 0.8 1.00 200 0.87 0.85 0.75 ## 0.4 2 0.8 1.00 250 0.86 0.85 0.75 ## 0.4 3 0.6 0.50 50 0.86 0.85 0.74 ## 0.4 3 0.6 0.50 100 0.86 0.84 0.75 ## 0.4 3 0.6 0.50 150 0.86 0.85 0.73 ## 0.4 3 0.6 0.50 200 0.85 0.83 0.74 ## 0.4 3 0.6 0.50 250 0.85 0.84 0.72 ## 0.4 3 0.6 0.62 50 0.88 0.86 0.76 ## 0.4 3 0.6 0.62 100 0.86 0.85 0.73 ## 0.4 3 0.6 0.62 150 0.86 0.83 0.72 ## 0.4 3 0.6 0.62 200 0.86 0.84 0.72 ## 0.4 3 0.6 0.62 250 0.85 0.84 0.71 ## 0.4 3 0.6 0.75 50 0.87 0.85 0.74 ## 0.4 3 0.6 0.75 100 0.87 0.85 0.73 ## 0.4 3 0.6 0.75 150 0.86 0.83 0.72 ## 0.4 3 0.6 0.75 200 0.86 0.85 0.71 ## 0.4 3 0.6 0.75 250 0.86 0.84 0.72 ## 0.4 3 0.6 0.88 50 0.87 0.84 0.75 ## 0.4 3 0.6 0.88 100 0.87 0.86 0.75 ## 0.4 3 0.6 0.88 150 0.86 0.85 0.73 ## 0.4 3 0.6 0.88 200 0.86 0.84 0.73 ## 0.4 3 0.6 0.88 250 0.85 0.84 0.72 ## 0.4 3 0.6 1.00 50 0.88 0.86 0.76 ## 0.4 3 0.6 1.00 100 0.87 0.85 0.73 ## 0.4 3 0.6 1.00 150 0.86 0.85 0.74 ## 0.4 3 0.6 1.00 200 0.86 0.84 0.74 ## 0.4 3 0.6 1.00 250 0.86 0.84 0.72 ## 0.4 3 0.8 0.50 50 0.87 0.84 0.74 ## 0.4 3 0.8 0.50 100 0.86 0.85 0.74 ## 0.4 3 0.8 0.50 150 0.85 0.84 0.73 ## 0.4 3 0.8 0.50 200 0.85 0.83 0.72 ## 0.4 3 0.8 0.50 250 0.84 0.83 0.72 ## 0.4 3 0.8 0.62 50 0.87 0.85 0.74 ## 0.4 3 0.8 0.62 100 0.86 0.83 0.73 ## 0.4 3 0.8 0.62 150 0.85 0.83 0.72 ## 0.4 3 0.8 0.62 200 0.85 0.82 0.72 ## 0.4 3 0.8 0.62 250 0.85 0.83 0.72 ## 0.4 3 0.8 0.75 50 0.87 0.86 0.73 ## 0.4 3 0.8 0.75 100 0.86 0.84 0.73 ## 0.4 3 0.8 0.75 150 0.85 0.85 0.73 ## 0.4 3 0.8 0.75 200 0.86 0.84 0.72 ## 0.4 3 0.8 0.75 250 0.85 0.85 0.72 ## 0.4 3 0.8 0.88 50 0.87 0.86 0.74 ## 0.4 3 0.8 0.88 100 0.86 0.85 0.75 ## 0.4 3 0.8 0.88 150 0.85 0.83 0.74 ## 0.4 3 0.8 0.88 200 0.85 0.84 0.75 ## 0.4 3 0.8 0.88 250 0.85 0.83 0.73 ## 0.4 3 0.8 1.00 50 0.87 0.85 0.73 ## 0.4 3 0.8 1.00 100 0.87 0.84 0.74 ## 0.4 3 0.8 1.00 150 0.86 0.83 0.74 ## 0.4 3 0.8 1.00 200 0.86 0.84 0.74 ## 0.4 3 0.8 1.00 250 0.86 0.83 0.73 ## 0.4 4 0.6 0.50 50 0.86 0.84 0.75 ## 0.4 4 0.6 0.50 100 0.85 0.83 0.75 ## 0.4 4 0.6 0.50 150 0.85 0.83 0.73 ## 0.4 4 0.6 0.50 200 0.84 0.84 0.73 ## 0.4 4 0.6 0.50 250 0.84 0.84 0.74 ## 0.4 4 0.6 0.62 50 0.86 0.84 0.72 ## 0.4 4 0.6 0.62 100 0.85 0.84 0.73 ## 0.4 4 0.6 0.62 150 0.85 0.83 0.73 ## 0.4 4 0.6 0.62 200 0.84 0.82 0.72 ## 0.4 4 0.6 0.62 250 0.84 0.83 0.71 ## 0.4 4 0.6 0.75 50 0.86 0.84 0.75 ## 0.4 4 0.6 0.75 100 0.86 0.84 0.74 ## 0.4 4 0.6 0.75 150 0.85 0.83 0.74 ## 0.4 4 0.6 0.75 200 0.85 0.83 0.73 ## 0.4 4 0.6 0.75 250 0.85 0.83 0.72 ## 0.4 4 0.6 0.88 50 0.87 0.85 0.73 ## 0.4 4 0.6 0.88 100 0.86 0.84 0.75 ## 0.4 4 0.6 0.88 150 0.86 0.84 0.72 ## 0.4 4 0.6 0.88 200 0.85 0.83 0.73 ## 0.4 4 0.6 0.88 250 0.85 0.83 0.72 ## 0.4 4 0.6 1.00 50 0.87 0.84 0.73 ## 0.4 4 0.6 1.00 100 0.86 0.83 0.73 ## 0.4 4 0.6 1.00 150 0.86 0.83 0.73 ## 0.4 4 0.6 1.00 200 0.85 0.82 0.72 ## 0.4 4 0.6 1.00 250 0.85 0.81 0.72 ## 0.4 4 0.8 0.50 50 0.86 0.85 0.73 ## 0.4 4 0.8 0.50 100 0.86 0.84 0.72 ## 0.4 4 0.8 0.50 150 0.85 0.84 0.71 ## 0.4 4 0.8 0.50 200 0.85 0.83 0.71 ## 0.4 4 0.8 0.50 250 0.85 0.83 0.71 ## 0.4 4 0.8 0.62 50 0.86 0.84 0.77 ## 0.4 4 0.8 0.62 100 0.85 0.83 0.73 ## 0.4 4 0.8 0.62 150 0.85 0.83 0.72 ## 0.4 4 0.8 0.62 200 0.84 0.82 0.73 ## 0.4 4 0.8 0.62 250 0.85 0.82 0.73 ## 0.4 4 0.8 0.75 50 0.87 0.85 0.72 ## 0.4 4 0.8 0.75 100 0.85 0.85 0.72 ## 0.4 4 0.8 0.75 150 0.85 0.83 0.72 ## 0.4 4 0.8 0.75 200 0.85 0.83 0.71 ## 0.4 4 0.8 0.75 250 0.85 0.82 0.72 ## 0.4 4 0.8 0.88 50 0.86 0.85 0.74 ## 0.4 4 0.8 0.88 100 0.85 0.84 0.74 ## 0.4 4 0.8 0.88 150 0.85 0.84 0.74 ## 0.4 4 0.8 0.88 200 0.85 0.82 0.73 ## 0.4 4 0.8 0.88 250 0.85 0.83 0.73 ## 0.4 4 0.8 1.00 50 0.87 0.85 0.75 ## 0.4 4 0.8 1.00 100 0.86 0.85 0.73 ## 0.4 4 0.8 1.00 150 0.86 0.85 0.71 ## 0.4 4 0.8 1.00 200 0.86 0.84 0.72 ## 0.4 4 0.8 1.00 250 0.85 0.84 0.70 ## 0.4 5 0.6 0.50 50 0.86 0.84 0.75 ## 0.4 5 0.6 0.50 100 0.85 0.84 0.72 ## 0.4 5 0.6 0.50 150 0.84 0.83 0.72 ## 0.4 5 0.6 0.50 200 0.84 0.84 0.71 ## 0.4 5 0.6 0.50 250 0.84 0.84 0.72 ## 0.4 5 0.6 0.62 50 0.86 0.83 0.72 ## 0.4 5 0.6 0.62 100 0.85 0.83 0.73 ## 0.4 5 0.6 0.62 150 0.85 0.84 0.74 ## 0.4 5 0.6 0.62 200 0.85 0.84 0.72 ## 0.4 5 0.6 0.62 250 0.84 0.84 0.72 ## 0.4 5 0.6 0.75 50 0.86 0.84 0.72 ## 0.4 5 0.6 0.75 100 0.85 0.85 0.71 ## 0.4 5 0.6 0.75 150 0.84 0.83 0.70 ## 0.4 5 0.6 0.75 200 0.84 0.84 0.71 ## 0.4 5 0.6 0.75 250 0.84 0.83 0.71 ## 0.4 5 0.6 0.88 50 0.86 0.85 0.73 ## 0.4 5 0.6 0.88 100 0.86 0.84 0.73 ## 0.4 5 0.6 0.88 150 0.85 0.83 0.73 ## 0.4 5 0.6 0.88 200 0.85 0.84 0.73 ## 0.4 5 0.6 0.88 250 0.85 0.82 0.72 ## 0.4 5 0.6 1.00 50 0.86 0.84 0.74 ## 0.4 5 0.6 1.00 100 0.85 0.83 0.74 ## 0.4 5 0.6 1.00 150 0.85 0.82 0.73 ## 0.4 5 0.6 1.00 200 0.85 0.82 0.72 ## 0.4 5 0.6 1.00 250 0.84 0.82 0.72 ## 0.4 5 0.8 0.50 50 0.87 0.85 0.71 ## 0.4 5 0.8 0.50 100 0.86 0.85 0.71 ## 0.4 5 0.8 0.50 150 0.85 0.84 0.72 ## 0.4 5 0.8 0.50 200 0.85 0.84 0.73 ## 0.4 5 0.8 0.50 250 0.84 0.83 0.70 ## 0.4 5 0.8 0.62 50 0.86 0.85 0.73 ## 0.4 5 0.8 0.62 100 0.86 0.84 0.73 ## 0.4 5 0.8 0.62 150 0.85 0.84 0.74 ## 0.4 5 0.8 0.62 200 0.85 0.83 0.73 ## 0.4 5 0.8 0.62 250 0.85 0.83 0.73 ## 0.4 5 0.8 0.75 50 0.86 0.85 0.74 ## 0.4 5 0.8 0.75 100 0.85 0.84 0.72 ## 0.4 5 0.8 0.75 150 0.85 0.83 0.72 ## 0.4 5 0.8 0.75 200 0.84 0.82 0.72 ## 0.4 5 0.8 0.75 250 0.84 0.82 0.72 ## 0.4 5 0.8 0.88 50 0.86 0.85 0.73 ## 0.4 5 0.8 0.88 100 0.85 0.83 0.74 ## 0.4 5 0.8 0.88 150 0.85 0.83 0.73 ## 0.4 5 0.8 0.88 200 0.85 0.83 0.73 ## 0.4 5 0.8 0.88 250 0.85 0.83 0.73 ## 0.4 5 0.8 1.00 50 0.87 0.85 0.72 ## 0.4 5 0.8 1.00 100 0.86 0.85 0.71 ## 0.4 5 0.8 1.00 150 0.86 0.84 0.72 ## 0.4 5 0.8 1.00 200 0.85 0.84 0.72 ## 0.4 5 0.8 1.00 250 0.85 0.84 0.72 ## ## Tuning parameter &#39;gamma&#39; was held constant at a value of 0 ## Tuning ## parameter &#39;min_child_weight&#39; was held constant at a value of 1 ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were nrounds = 50, max_depth = 1, eta ## = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample ## = 1. train() tuned eta (\\(\\eta\\)), max_depth, colsample_bytree, subsample, and nrounds, holding gamma = 0, and min_child_weight = 1. The optimal hyperparameter values were eta = 0.3, max_depth - 1, colsample_bytree = 0.6, subsample = 1, and nrounds = 50. With so many hyperparameters, the tuning plot is nearly unreadable. #plot(oj_mdl_xgb) Let’s see how the model performed on the holdout set. The accuracy was 0.8732 - much better than the 0.8451 from regular gradient boosting. oj_preds_xgb &lt;- bind_cols( predict(oj_mdl_xgb, newdata = oj_test, type = &quot;prob&quot;), Predicted = predict(oj_mdl_xgb, newdata = oj_test, type = &quot;raw&quot;), Actual = oj_test$Purchase ) oj_cm_xgb &lt;- confusionMatrix(oj_preds_xgb$Predicted, reference = oj_preds_xgb$Actual) oj_cm_xgb ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 120 17 ## MM 10 66 ## ## Accuracy : 0.873 ## 95% CI : (0.821, 0.915) ## No Information Rate : 0.61 ## P-Value [Acc &gt; NIR] : &lt;0.0000000000000002 ## ## Kappa : 0.729 ## ## Mcnemar&#39;s Test P-Value : 0.248 ## ## Sensitivity : 0.923 ## Specificity : 0.795 ## Pos Pred Value : 0.876 ## Neg Pred Value : 0.868 ## Prevalence : 0.610 ## Detection Rate : 0.563 ## Detection Prevalence : 0.643 ## Balanced Accuracy : 0.859 ## ## &#39;Positive&#39; Class : CH ## AUC was 0.9386 for gradient boosting, and here it is 0.9323. Here are the ROC and gain curves. mdl_auc &lt;- Metrics::auc(actual = oj_preds_xgb$Actual == &quot;CH&quot;, oj_preds_xgb$CH) yardstick::roc_curve(oj_preds_xgb, Actual, CH) %&gt;% autoplot() + labs( title = &quot;OJ XGBoost ROC Curve&quot;, subtitle = paste0(&quot;AUC = &quot;, round(mdl_auc, 4)) ) yardstick::gain_curve(oj_preds_xgb, Actual, CH) %&gt;% autoplot() + labs(title = &quot;OJ XGBoost Gain Curve&quot;) Now the variable importance. Nothing jumps out at me here. It’s the same top variables as regular gradient boosting. plot(varImp(oj_mdl_xgb), main=&quot;Variable Importance with XGBoost&quot;) Okay, let’s check in with the leader board. Wow, XGBoost is extreme. oj_scoreboard &lt;- rbind(oj_scoreboard, data.frame(Model = &quot;Gradient Boosting&quot;, Accuracy = oj_cm_gbm$overall[&quot;Accuracy&quot;])) %&gt;% rbind(data.frame(Model = &quot;XGBoost&quot;, Accuracy = oj_cm_xgb$overall[&quot;Accuracy&quot;])) %&gt;% arrange(desc(Accuracy)) scoreboard(oj_scoreboard) ModelAccuracyXGBoost0.8732Single Tree0.8592Single Tree (caret)0.8545Bagging0.8451Gradient Boosting0.8451Random Forest0.8310 8.5.0.2 Gradient Boosting Regression Tree 8.5.0.2.1 GBM I’ll predict Sales from the Carseats data set again, this time using the bagging method by specifying method = \"gbm\" set.seed(1234) garbage &lt;- capture.output( cs_mdl_gbm &lt;- train( Sales ~ ., data = cs_train, method = &quot;gbm&quot;, tuneLength = 5, trControl = cs_trControl )) cs_mdl_gbm ## Stochastic Gradient Boosting ## ## 321 samples ## 10 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... ## Resampling results across tuning parameters: ## ## interaction.depth n.trees RMSE Rsquared MAE ## 1 50 1.8 0.67 1.50 ## 1 100 1.5 0.78 1.24 ## 1 150 1.3 0.83 1.06 ## 1 200 1.2 0.84 0.99 ## 1 250 1.2 0.85 0.94 ## 2 50 1.5 0.78 1.22 ## 2 100 1.2 0.84 1.01 ## 2 150 1.2 0.84 0.96 ## 2 200 1.2 0.84 0.95 ## 2 250 1.2 0.84 0.95 ## 3 50 1.4 0.81 1.13 ## 3 100 1.2 0.83 0.99 ## 3 150 1.2 0.83 0.97 ## 3 200 1.2 0.83 0.98 ## 3 250 1.2 0.82 0.99 ## 4 50 1.3 0.81 1.09 ## 4 100 1.3 0.82 0.99 ## 4 150 1.2 0.82 0.99 ## 4 200 1.3 0.82 0.99 ## 4 250 1.3 0.81 1.01 ## 5 50 1.3 0.81 1.06 ## 5 100 1.3 0.82 1.00 ## 5 150 1.2 0.82 0.99 ## 5 200 1.3 0.82 1.01 ## 5 250 1.3 0.81 1.02 ## ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## ## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were n.trees = 250, interaction.depth = ## 1, shrinkage = 0.1 and n.minobsinnode = 10. The optimal tuning parameters were at \\(M = 250\\) and interation.depth = 1. plot(cs_mdl_gbm) Here is the holdout set performance. cs_preds_gbm &lt;- bind_cols( Predicted = predict(cs_mdl_gbm, newdata = cs_test), Actual = cs_test$Sales ) # Model over-predicts at low end of Sales and under-predicts at high end cs_preds_gbm %&gt;% ggplot(aes(x = Actual, y = Predicted)) + geom_point(alpha = 0.6, color = &quot;cadetblue&quot;) + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;) + geom_abline(intercept = 0, slope = 1, linetype = 2) + labs(title = &quot;Carseats GBM, Predicted vs Actual&quot;) #plot(varImp(cs_mdl_gbm), main=&quot;Variable Importance with GBM&quot;) The RMSE is 1.438 - the best of the bunch. cs_rmse_gbm &lt;- RMSE(pred = cs_preds_gbm$Predicted, obs = cs_preds_gbm$Actual) cs_scoreboard &lt;- rbind(cs_scoreboard, data.frame(Model = &quot;GBM&quot;, RMSE = cs_rmse_gbm) ) %&gt;% arrange(RMSE) scoreboard(cs_scoreboard) ModelRMSEGBM1.4381Random Forest1.7184Bagging1.9185Single Tree (caret)2.2983Single Tree2.3632 8.5.0.2.2 XGBoost I’ll predict Sales from the Carseats data set again, this time using the bagging method by specifying method = \"xgb\" set.seed(1234) garbage &lt;- capture.output( cs_mdl_xgb &lt;- train( Sales ~ ., data = cs_train, method = &quot;xgbTree&quot;, tuneLength = 5, trControl = cs_trControl )) cs_mdl_xgb ## eXtreme Gradient Boosting ## ## 321 samples ## 10 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... ## Resampling results across tuning parameters: ## ## eta max_depth colsample_bytree subsample nrounds RMSE Rsquared MAE ## 0.3 1 0.6 0.50 50 1.4 0.80 1.11 ## 0.3 1 0.6 0.50 100 1.2 0.84 0.92 ## 0.3 1 0.6 0.50 150 1.1 0.85 0.91 ## 0.3 1 0.6 0.50 200 1.1 0.85 0.90 ## 0.3 1 0.6 0.50 250 1.1 0.85 0.90 ## 0.3 1 0.6 0.62 50 1.4 0.81 1.13 ## 0.3 1 0.6 0.62 100 1.2 0.85 0.96 ## 0.3 1 0.6 0.62 150 1.1 0.85 0.91 ## 0.3 1 0.6 0.62 200 1.1 0.85 0.92 ## 0.3 1 0.6 0.62 250 1.1 0.85 0.91 ## 0.3 1 0.6 0.75 50 1.4 0.81 1.14 ## 0.3 1 0.6 0.75 100 1.2 0.84 0.97 ## 0.3 1 0.6 0.75 150 1.1 0.85 0.92 ## 0.3 1 0.6 0.75 200 1.1 0.85 0.92 ## 0.3 1 0.6 0.75 250 1.1 0.85 0.92 ## 0.3 1 0.6 0.88 50 1.4 0.81 1.13 ## 0.3 1 0.6 0.88 100 1.2 0.85 0.95 ## 0.3 1 0.6 0.88 150 1.1 0.86 0.89 ## 0.3 1 0.6 0.88 200 1.1 0.86 0.89 ## 0.3 1 0.6 0.88 250 1.1 0.86 0.89 ## 0.3 1 0.6 1.00 50 1.4 0.81 1.15 ## 0.3 1 0.6 1.00 100 1.2 0.85 0.96 ## 0.3 1 0.6 1.00 150 1.1 0.86 0.90 ## 0.3 1 0.6 1.00 200 1.1 0.86 0.88 ## 0.3 1 0.6 1.00 250 1.1 0.86 0.88 ## 0.3 1 0.8 0.50 50 1.4 0.80 1.14 ## 0.3 1 0.8 0.50 100 1.2 0.84 0.96 ## 0.3 1 0.8 0.50 150 1.2 0.84 0.95 ## 0.3 1 0.8 0.50 200 1.2 0.84 0.94 ## 0.3 1 0.8 0.50 250 1.2 0.84 0.93 ## 0.3 1 0.8 0.62 50 1.4 0.81 1.11 ## 0.3 1 0.8 0.62 100 1.2 0.85 0.95 ## 0.3 1 0.8 0.62 150 1.1 0.85 0.91 ## 0.3 1 0.8 0.62 200 1.1 0.85 0.90 ## 0.3 1 0.8 0.62 250 1.2 0.84 0.92 ## 0.3 1 0.8 0.75 50 1.4 0.81 1.12 ## 0.3 1 0.8 0.75 100 1.2 0.85 0.95 ## 0.3 1 0.8 0.75 150 1.1 0.86 0.91 ## 0.3 1 0.8 0.75 200 1.1 0.85 0.90 ## 0.3 1 0.8 0.75 250 1.1 0.85 0.90 ## 0.3 1 0.8 0.88 50 1.4 0.80 1.15 ## 0.3 1 0.8 0.88 100 1.2 0.84 0.96 ## 0.3 1 0.8 0.88 150 1.1 0.85 0.91 ## 0.3 1 0.8 0.88 200 1.1 0.85 0.91 ## 0.3 1 0.8 0.88 250 1.1 0.85 0.90 ## 0.3 1 0.8 1.00 50 1.4 0.81 1.15 ## 0.3 1 0.8 1.00 100 1.2 0.85 0.96 ## 0.3 1 0.8 1.00 150 1.1 0.86 0.91 ## 0.3 1 0.8 1.00 200 1.1 0.86 0.89 ## 0.3 1 0.8 1.00 250 1.1 0.86 0.89 ## 0.3 2 0.6 0.50 50 1.3 0.81 1.04 ## 0.3 2 0.6 0.50 100 1.3 0.80 1.04 ## 0.3 2 0.6 0.50 150 1.3 0.79 1.06 ## 0.3 2 0.6 0.50 200 1.4 0.79 1.05 ## 0.3 2 0.6 0.50 250 1.4 0.77 1.09 ## 0.3 2 0.6 0.62 50 1.3 0.81 1.02 ## 0.3 2 0.6 0.62 100 1.3 0.82 0.99 ## 0.3 2 0.6 0.62 150 1.3 0.81 1.02 ## 0.3 2 0.6 0.62 200 1.3 0.80 1.04 ## 0.3 2 0.6 0.62 250 1.3 0.79 1.05 ## 0.3 2 0.6 0.75 50 1.2 0.83 0.98 ## 0.3 2 0.6 0.75 100 1.2 0.83 0.96 ## 0.3 2 0.6 0.75 150 1.3 0.82 0.98 ## 0.3 2 0.6 0.75 200 1.3 0.81 1.00 ## 0.3 2 0.6 0.75 250 1.3 0.80 1.02 ## 0.3 2 0.6 0.88 50 1.3 0.83 1.02 ## 0.3 2 0.6 0.88 100 1.2 0.82 1.00 ## 0.3 2 0.6 0.88 150 1.3 0.82 0.99 ## 0.3 2 0.6 0.88 200 1.3 0.81 1.00 ## 0.3 2 0.6 0.88 250 1.3 0.80 1.03 ## 0.3 2 0.6 1.00 50 1.3 0.83 1.02 ## 0.3 2 0.6 1.00 100 1.2 0.83 1.01 ## 0.3 2 0.6 1.00 150 1.3 0.82 1.01 ## 0.3 2 0.6 1.00 200 1.3 0.80 1.04 ## 0.3 2 0.6 1.00 250 1.3 0.80 1.04 ## 0.3 2 0.8 0.50 50 1.3 0.82 1.00 ## 0.3 2 0.8 0.50 100 1.3 0.81 1.03 ## 0.3 2 0.8 0.50 150 1.3 0.80 1.06 ## 0.3 2 0.8 0.50 200 1.3 0.80 1.05 ## 0.3 2 0.8 0.50 250 1.4 0.79 1.08 ## 0.3 2 0.8 0.62 50 1.3 0.81 1.02 ## 0.3 2 0.8 0.62 100 1.3 0.80 1.02 ## 0.3 2 0.8 0.62 150 1.3 0.79 1.05 ## 0.3 2 0.8 0.62 200 1.3 0.79 1.06 ## 0.3 2 0.8 0.62 250 1.4 0.78 1.08 ## 0.3 2 0.8 0.75 50 1.3 0.81 1.00 ## 0.3 2 0.8 0.75 100 1.3 0.80 1.02 ## 0.3 2 0.8 0.75 150 1.4 0.78 1.06 ## 0.3 2 0.8 0.75 200 1.4 0.77 1.08 ## 0.3 2 0.8 0.75 250 1.4 0.77 1.11 ## 0.3 2 0.8 0.88 50 1.2 0.83 1.00 ## 0.3 2 0.8 0.88 100 1.2 0.82 0.99 ## 0.3 2 0.8 0.88 150 1.3 0.81 1.01 ## 0.3 2 0.8 0.88 200 1.3 0.80 1.03 ## 0.3 2 0.8 0.88 250 1.3 0.79 1.04 ## 0.3 2 0.8 1.00 50 1.2 0.82 1.01 ## 0.3 2 0.8 1.00 100 1.3 0.82 1.01 ## 0.3 2 0.8 1.00 150 1.3 0.81 1.02 ## 0.3 2 0.8 1.00 200 1.3 0.80 1.04 ## 0.3 2 0.8 1.00 250 1.3 0.79 1.05 ## 0.3 3 0.6 0.50 50 1.5 0.75 1.16 ## 0.3 3 0.6 0.50 100 1.4 0.75 1.16 ## 0.3 3 0.6 0.50 150 1.5 0.74 1.16 ## 0.3 3 0.6 0.50 200 1.5 0.75 1.16 ## 0.3 3 0.6 0.50 250 1.5 0.74 1.17 ## 0.3 3 0.6 0.62 50 1.4 0.75 1.12 ## 0.3 3 0.6 0.62 100 1.5 0.74 1.15 ## 0.3 3 0.6 0.62 150 1.5 0.74 1.17 ## 0.3 3 0.6 0.62 200 1.5 0.73 1.18 ## 0.3 3 0.6 0.62 250 1.5 0.73 1.18 ## 0.3 3 0.6 0.75 50 1.4 0.76 1.13 ## 0.3 3 0.6 0.75 100 1.4 0.76 1.13 ## 0.3 3 0.6 0.75 150 1.5 0.75 1.15 ## 0.3 3 0.6 0.75 200 1.5 0.74 1.16 ## 0.3 3 0.6 0.75 250 1.5 0.74 1.16 ## 0.3 3 0.6 0.88 50 1.4 0.79 1.09 ## 0.3 3 0.6 0.88 100 1.4 0.78 1.09 ## 0.3 3 0.6 0.88 150 1.4 0.77 1.09 ## 0.3 3 0.6 0.88 200 1.4 0.77 1.10 ## 0.3 3 0.6 0.88 250 1.4 0.77 1.10 ## 0.3 3 0.6 1.00 50 1.4 0.78 1.06 ## 0.3 3 0.6 1.00 100 1.4 0.78 1.05 ## 0.3 3 0.6 1.00 150 1.4 0.77 1.06 ## 0.3 3 0.6 1.00 200 1.4 0.77 1.07 ## 0.3 3 0.6 1.00 250 1.4 0.77 1.08 ## 0.3 3 0.8 0.50 50 1.5 0.74 1.15 ## 0.3 3 0.8 0.50 100 1.5 0.75 1.18 ## 0.3 3 0.8 0.50 150 1.5 0.74 1.19 ## 0.3 3 0.8 0.50 200 1.5 0.74 1.19 ## 0.3 3 0.8 0.50 250 1.5 0.73 1.19 ## 0.3 3 0.8 0.62 50 1.4 0.77 1.08 ## 0.3 3 0.8 0.62 100 1.4 0.76 1.12 ## 0.3 3 0.8 0.62 150 1.5 0.75 1.14 ## 0.3 3 0.8 0.62 200 1.5 0.74 1.15 ## 0.3 3 0.8 0.62 250 1.5 0.74 1.15 ## 0.3 3 0.8 0.75 50 1.3 0.80 1.05 ## 0.3 3 0.8 0.75 100 1.4 0.78 1.09 ## 0.3 3 0.8 0.75 150 1.4 0.78 1.11 ## 0.3 3 0.8 0.75 200 1.4 0.77 1.12 ## 0.3 3 0.8 0.75 250 1.4 0.77 1.13 ## 0.3 3 0.8 0.88 50 1.4 0.78 1.08 ## 0.3 3 0.8 0.88 100 1.4 0.77 1.10 ## 0.3 3 0.8 0.88 150 1.4 0.77 1.12 ## 0.3 3 0.8 0.88 200 1.4 0.77 1.13 ## 0.3 3 0.8 0.88 250 1.4 0.76 1.13 ## 0.3 3 0.8 1.00 50 1.4 0.77 1.12 ## 0.3 3 0.8 1.00 100 1.4 0.76 1.13 ## 0.3 3 0.8 1.00 150 1.4 0.76 1.14 ## 0.3 3 0.8 1.00 200 1.5 0.75 1.15 ## 0.3 3 0.8 1.00 250 1.5 0.75 1.16 ## 0.3 4 0.6 0.50 50 1.6 0.71 1.23 ## 0.3 4 0.6 0.50 100 1.6 0.70 1.25 ## 0.3 4 0.6 0.50 150 1.6 0.70 1.27 ## 0.3 4 0.6 0.50 200 1.6 0.70 1.27 ## 0.3 4 0.6 0.50 250 1.6 0.70 1.27 ## 0.3 4 0.6 0.62 50 1.5 0.73 1.19 ## 0.3 4 0.6 0.62 100 1.5 0.73 1.19 ## 0.3 4 0.6 0.62 150 1.5 0.72 1.19 ## 0.3 4 0.6 0.62 200 1.5 0.72 1.19 ## 0.3 4 0.6 0.62 250 1.5 0.72 1.19 ## 0.3 4 0.6 0.75 50 1.5 0.72 1.22 ## 0.3 4 0.6 0.75 100 1.6 0.71 1.23 ## 0.3 4 0.6 0.75 150 1.6 0.71 1.23 ## 0.3 4 0.6 0.75 200 1.6 0.71 1.23 ## 0.3 4 0.6 0.75 250 1.6 0.71 1.23 ## 0.3 4 0.6 0.88 50 1.5 0.75 1.16 ## 0.3 4 0.6 0.88 100 1.5 0.75 1.17 ## 0.3 4 0.6 0.88 150 1.5 0.74 1.17 ## 0.3 4 0.6 0.88 200 1.5 0.74 1.17 ## 0.3 4 0.6 0.88 250 1.5 0.74 1.17 ## 0.3 4 0.6 1.00 50 1.5 0.73 1.20 ## 0.3 4 0.6 1.00 100 1.5 0.73 1.20 ## 0.3 4 0.6 1.00 150 1.5 0.73 1.21 ## 0.3 4 0.6 1.00 200 1.5 0.73 1.21 ## 0.3 4 0.6 1.00 250 1.5 0.73 1.21 ## 0.3 4 0.8 0.50 50 1.6 0.70 1.23 ## 0.3 4 0.8 0.50 100 1.6 0.70 1.24 ## 0.3 4 0.8 0.50 150 1.6 0.70 1.25 ## 0.3 4 0.8 0.50 200 1.6 0.70 1.25 ## 0.3 4 0.8 0.50 250 1.6 0.70 1.25 ## 0.3 4 0.8 0.62 50 1.5 0.74 1.19 ## 0.3 4 0.8 0.62 100 1.5 0.74 1.20 ## 0.3 4 0.8 0.62 150 1.5 0.74 1.20 ## 0.3 4 0.8 0.62 200 1.5 0.74 1.20 ## 0.3 4 0.8 0.62 250 1.5 0.74 1.20 ## 0.3 4 0.8 0.75 50 1.5 0.74 1.20 ## 0.3 4 0.8 0.75 100 1.5 0.74 1.21 ## 0.3 4 0.8 0.75 150 1.5 0.74 1.21 ## 0.3 4 0.8 0.75 200 1.5 0.74 1.21 ## 0.3 4 0.8 0.75 250 1.5 0.74 1.21 ## 0.3 4 0.8 0.88 50 1.5 0.75 1.13 ## 0.3 4 0.8 0.88 100 1.5 0.75 1.15 ## 0.3 4 0.8 0.88 150 1.5 0.75 1.15 ## 0.3 4 0.8 0.88 200 1.5 0.75 1.15 ## 0.3 4 0.8 0.88 250 1.5 0.75 1.15 ## 0.3 4 0.8 1.00 50 1.5 0.75 1.16 ## 0.3 4 0.8 1.00 100 1.5 0.75 1.16 ## 0.3 4 0.8 1.00 150 1.5 0.75 1.17 ## 0.3 4 0.8 1.00 200 1.5 0.75 1.17 ## 0.3 4 0.8 1.00 250 1.5 0.75 1.17 ## 0.3 5 0.6 0.50 50 1.7 0.66 1.32 ## 0.3 5 0.6 0.50 100 1.7 0.66 1.32 ## 0.3 5 0.6 0.50 150 1.7 0.66 1.32 ## 0.3 5 0.6 0.50 200 1.7 0.66 1.32 ## 0.3 5 0.6 0.50 250 1.7 0.66 1.32 ## 0.3 5 0.6 0.62 50 1.6 0.71 1.25 ## 0.3 5 0.6 0.62 100 1.6 0.70 1.25 ## 0.3 5 0.6 0.62 150 1.6 0.70 1.25 ## 0.3 5 0.6 0.62 200 1.6 0.70 1.25 ## 0.3 5 0.6 0.62 250 1.6 0.70 1.25 ## 0.3 5 0.6 0.75 50 1.6 0.69 1.27 ## 0.3 5 0.6 0.75 100 1.6 0.69 1.27 ## 0.3 5 0.6 0.75 150 1.6 0.69 1.27 ## 0.3 5 0.6 0.75 200 1.6 0.69 1.27 ## 0.3 5 0.6 0.75 250 1.6 0.69 1.27 ## 0.3 5 0.6 0.88 50 1.6 0.71 1.23 ## 0.3 5 0.6 0.88 100 1.6 0.71 1.23 ## 0.3 5 0.6 0.88 150 1.6 0.71 1.23 ## 0.3 5 0.6 0.88 200 1.6 0.71 1.23 ## 0.3 5 0.6 0.88 250 1.6 0.71 1.23 ## 0.3 5 0.6 1.00 50 1.6 0.71 1.24 ## 0.3 5 0.6 1.00 100 1.6 0.71 1.24 ## 0.3 5 0.6 1.00 150 1.6 0.71 1.24 ## 0.3 5 0.6 1.00 200 1.6 0.71 1.24 ## 0.3 5 0.6 1.00 250 1.6 0.71 1.24 ## 0.3 5 0.8 0.50 50 1.5 0.75 1.18 ## 0.3 5 0.8 0.50 100 1.5 0.74 1.19 ## 0.3 5 0.8 0.50 150 1.5 0.74 1.19 ## 0.3 5 0.8 0.50 200 1.5 0.74 1.19 ## 0.3 5 0.8 0.50 250 1.5 0.74 1.19 ## 0.3 5 0.8 0.62 50 1.5 0.74 1.15 ## 0.3 5 0.8 0.62 100 1.5 0.74 1.16 ## 0.3 5 0.8 0.62 150 1.5 0.74 1.16 ## 0.3 5 0.8 0.62 200 1.5 0.74 1.16 ## 0.3 5 0.8 0.62 250 1.5 0.74 1.16 ## 0.3 5 0.8 0.75 50 1.5 0.74 1.20 ## 0.3 5 0.8 0.75 100 1.5 0.74 1.20 ## 0.3 5 0.8 0.75 150 1.5 0.74 1.21 ## 0.3 5 0.8 0.75 200 1.5 0.74 1.21 ## 0.3 5 0.8 0.75 250 1.5 0.74 1.21 ## 0.3 5 0.8 0.88 50 1.6 0.72 1.24 ## 0.3 5 0.8 0.88 100 1.6 0.72 1.24 ## 0.3 5 0.8 0.88 150 1.6 0.72 1.24 ## 0.3 5 0.8 0.88 200 1.6 0.72 1.24 ## 0.3 5 0.8 0.88 250 1.6 0.72 1.24 ## 0.3 5 0.8 1.00 50 1.5 0.73 1.17 ## 0.3 5 0.8 1.00 100 1.5 0.73 1.18 ## 0.3 5 0.8 1.00 150 1.5 0.73 1.18 ## 0.3 5 0.8 1.00 200 1.5 0.73 1.18 ## 0.3 5 0.8 1.00 250 1.5 0.73 1.18 ## 0.4 1 0.6 0.50 50 1.3 0.81 1.04 ## 0.4 1 0.6 0.50 100 1.2 0.84 0.95 ## 0.4 1 0.6 0.50 150 1.2 0.83 0.95 ## 0.4 1 0.6 0.50 200 1.2 0.83 0.96 ## 0.4 1 0.6 0.50 250 1.2 0.83 0.96 ## 0.4 1 0.6 0.62 50 1.3 0.82 1.02 ## 0.4 1 0.6 0.62 100 1.1 0.85 0.91 ## 0.4 1 0.6 0.62 150 1.1 0.85 0.91 ## 0.4 1 0.6 0.62 200 1.2 0.84 0.93 ## 0.4 1 0.6 0.62 250 1.2 0.84 0.94 ## 0.4 1 0.6 0.75 50 1.3 0.83 1.04 ## 0.4 1 0.6 0.75 100 1.1 0.85 0.92 ## 0.4 1 0.6 0.75 150 1.1 0.85 0.90 ## 0.4 1 0.6 0.75 200 1.1 0.85 0.90 ## 0.4 1 0.6 0.75 250 1.1 0.85 0.90 ## 0.4 1 0.6 0.88 50 1.3 0.82 1.09 ## 0.4 1 0.6 0.88 100 1.2 0.84 0.95 ## 0.4 1 0.6 0.88 150 1.1 0.85 0.92 ## 0.4 1 0.6 0.88 200 1.1 0.85 0.93 ## 0.4 1 0.6 0.88 250 1.1 0.85 0.93 ## 0.4 1 0.6 1.00 50 1.3 0.83 1.04 ## 0.4 1 0.6 1.00 100 1.1 0.86 0.92 ## 0.4 1 0.6 1.00 150 1.1 0.86 0.89 ## 0.4 1 0.6 1.00 200 1.1 0.86 0.89 ## 0.4 1 0.6 1.00 250 1.1 0.86 0.89 ## 0.4 1 0.8 0.50 50 1.3 0.83 1.01 ## 0.4 1 0.8 0.50 100 1.2 0.84 0.96 ## 0.4 1 0.8 0.50 150 1.2 0.84 0.94 ## 0.4 1 0.8 0.50 200 1.2 0.83 0.96 ## 0.4 1 0.8 0.50 250 1.2 0.83 0.95 ## 0.4 1 0.8 0.62 50 1.3 0.83 1.02 ## 0.4 1 0.8 0.62 100 1.2 0.83 0.97 ## 0.4 1 0.8 0.62 150 1.2 0.83 0.96 ## 0.4 1 0.8 0.62 200 1.2 0.83 0.97 ## 0.4 1 0.8 0.62 250 1.2 0.83 0.97 ## 0.4 1 0.8 0.75 50 1.3 0.83 1.03 ## 0.4 1 0.8 0.75 100 1.1 0.85 0.92 ## 0.4 1 0.8 0.75 150 1.2 0.84 0.92 ## 0.4 1 0.8 0.75 200 1.2 0.84 0.93 ## 0.4 1 0.8 0.75 250 1.2 0.84 0.94 ## 0.4 1 0.8 0.88 50 1.3 0.83 1.04 ## 0.4 1 0.8 0.88 100 1.2 0.85 0.92 ## 0.4 1 0.8 0.88 150 1.1 0.85 0.92 ## 0.4 1 0.8 0.88 200 1.1 0.85 0.92 ## 0.4 1 0.8 0.88 250 1.2 0.84 0.93 ## 0.4 1 0.8 1.00 50 1.3 0.84 1.03 ## 0.4 1 0.8 1.00 100 1.1 0.86 0.91 ## 0.4 1 0.8 1.00 150 1.1 0.86 0.88 ## 0.4 1 0.8 1.00 200 1.1 0.86 0.88 ## 0.4 1 0.8 1.00 250 1.1 0.86 0.88 ## 0.4 2 0.6 0.50 50 1.4 0.77 1.12 ## 0.4 2 0.6 0.50 100 1.4 0.77 1.12 ## 0.4 2 0.6 0.50 150 1.4 0.77 1.16 ## 0.4 2 0.6 0.50 200 1.5 0.75 1.19 ## 0.4 2 0.6 0.50 250 1.5 0.75 1.20 ## 0.4 2 0.6 0.62 50 1.3 0.80 1.03 ## 0.4 2 0.6 0.62 100 1.3 0.79 1.06 ## 0.4 2 0.6 0.62 150 1.3 0.79 1.06 ## 0.4 2 0.6 0.62 200 1.4 0.79 1.07 ## 0.4 2 0.6 0.62 250 1.4 0.77 1.10 ## 0.4 2 0.6 0.75 50 1.3 0.80 1.03 ## 0.4 2 0.6 0.75 100 1.3 0.78 1.06 ## 0.4 2 0.6 0.75 150 1.4 0.77 1.08 ## 0.4 2 0.6 0.75 200 1.4 0.76 1.12 ## 0.4 2 0.6 0.75 250 1.4 0.76 1.12 ## 0.4 2 0.6 0.88 50 1.3 0.81 1.02 ## 0.4 2 0.6 0.88 100 1.3 0.81 1.01 ## 0.4 2 0.6 0.88 150 1.3 0.80 1.03 ## 0.4 2 0.6 0.88 200 1.3 0.79 1.05 ## 0.4 2 0.6 0.88 250 1.3 0.78 1.06 ## 0.4 2 0.6 1.00 50 1.2 0.82 1.01 ## 0.4 2 0.6 1.00 100 1.3 0.81 1.03 ## 0.4 2 0.6 1.00 150 1.3 0.81 1.05 ## 0.4 2 0.6 1.00 200 1.3 0.80 1.07 ## 0.4 2 0.6 1.00 250 1.3 0.79 1.08 ## 0.4 2 0.8 0.50 50 1.4 0.78 1.09 ## 0.4 2 0.8 0.50 100 1.4 0.76 1.13 ## 0.4 2 0.8 0.50 150 1.5 0.75 1.16 ## 0.4 2 0.8 0.50 200 1.5 0.73 1.18 ## 0.4 2 0.8 0.50 250 1.6 0.72 1.22 ## 0.4 2 0.8 0.62 50 1.3 0.80 1.06 ## 0.4 2 0.8 0.62 100 1.3 0.79 1.05 ## 0.4 2 0.8 0.62 150 1.3 0.78 1.08 ## 0.4 2 0.8 0.62 200 1.4 0.78 1.08 ## 0.4 2 0.8 0.62 250 1.4 0.78 1.09 ## 0.4 2 0.8 0.75 50 1.3 0.79 1.04 ## 0.4 2 0.8 0.75 100 1.4 0.78 1.07 ## 0.4 2 0.8 0.75 150 1.4 0.77 1.10 ## 0.4 2 0.8 0.75 200 1.5 0.75 1.13 ## 0.4 2 0.8 0.75 250 1.4 0.76 1.13 ## 0.4 2 0.8 0.88 50 1.3 0.80 1.05 ## 0.4 2 0.8 0.88 100 1.3 0.78 1.08 ## 0.4 2 0.8 0.88 150 1.4 0.77 1.09 ## 0.4 2 0.8 0.88 200 1.4 0.76 1.12 ## 0.4 2 0.8 0.88 250 1.4 0.75 1.14 ## 0.4 2 0.8 1.00 50 1.3 0.81 1.02 ## 0.4 2 0.8 1.00 100 1.3 0.80 1.03 ## 0.4 2 0.8 1.00 150 1.3 0.79 1.07 ## 0.4 2 0.8 1.00 200 1.4 0.78 1.10 ## 0.4 2 0.8 1.00 250 1.4 0.77 1.12 ## 0.4 3 0.6 0.50 50 1.5 0.75 1.18 ## 0.4 3 0.6 0.50 100 1.5 0.74 1.20 ## 0.4 3 0.6 0.50 150 1.5 0.73 1.21 ## 0.4 3 0.6 0.50 200 1.5 0.73 1.22 ## 0.4 3 0.6 0.50 250 1.5 0.73 1.22 ## 0.4 3 0.6 0.62 50 1.4 0.75 1.14 ## 0.4 3 0.6 0.62 100 1.5 0.73 1.18 ## 0.4 3 0.6 0.62 150 1.5 0.73 1.19 ## 0.4 3 0.6 0.62 200 1.5 0.73 1.19 ## 0.4 3 0.6 0.62 250 1.5 0.73 1.19 ## 0.4 3 0.6 0.75 50 1.4 0.77 1.12 ## 0.4 3 0.6 0.75 100 1.5 0.75 1.16 ## 0.4 3 0.6 0.75 150 1.5 0.75 1.17 ## 0.4 3 0.6 0.75 200 1.5 0.74 1.18 ## 0.4 3 0.6 0.75 250 1.5 0.74 1.18 ## 0.4 3 0.6 0.88 50 1.4 0.77 1.07 ## 0.4 3 0.6 0.88 100 1.4 0.75 1.11 ## 0.4 3 0.6 0.88 150 1.5 0.74 1.12 ## 0.4 3 0.6 0.88 200 1.5 0.74 1.13 ## 0.4 3 0.6 0.88 250 1.5 0.74 1.13 ## 0.4 3 0.6 1.00 50 1.4 0.78 1.11 ## 0.4 3 0.6 1.00 100 1.4 0.77 1.11 ## 0.4 3 0.6 1.00 150 1.4 0.77 1.12 ## 0.4 3 0.6 1.00 200 1.4 0.76 1.12 ## 0.4 3 0.6 1.00 250 1.4 0.76 1.13 ## 0.4 3 0.8 0.50 50 1.6 0.71 1.25 ## 0.4 3 0.8 0.50 100 1.6 0.69 1.29 ## 0.4 3 0.8 0.50 150 1.6 0.69 1.30 ## 0.4 3 0.8 0.50 200 1.6 0.69 1.31 ## 0.4 3 0.8 0.50 250 1.6 0.69 1.31 ## 0.4 3 0.8 0.62 50 1.4 0.76 1.13 ## 0.4 3 0.8 0.62 100 1.5 0.75 1.16 ## 0.4 3 0.8 0.62 150 1.5 0.74 1.18 ## 0.4 3 0.8 0.62 200 1.5 0.74 1.18 ## 0.4 3 0.8 0.62 250 1.5 0.74 1.18 ## 0.4 3 0.8 0.75 50 1.5 0.75 1.18 ## 0.4 3 0.8 0.75 100 1.5 0.72 1.22 ## 0.4 3 0.8 0.75 150 1.6 0.71 1.24 ## 0.4 3 0.8 0.75 200 1.6 0.71 1.25 ## 0.4 3 0.8 0.75 250 1.6 0.71 1.25 ## 0.4 3 0.8 0.88 50 1.4 0.78 1.10 ## 0.4 3 0.8 0.88 100 1.4 0.78 1.10 ## 0.4 3 0.8 0.88 150 1.4 0.77 1.12 ## 0.4 3 0.8 0.88 200 1.4 0.77 1.12 ## 0.4 3 0.8 0.88 250 1.4 0.77 1.12 ## 0.4 3 0.8 1.00 50 1.3 0.79 1.07 ## 0.4 3 0.8 1.00 100 1.4 0.78 1.10 ## 0.4 3 0.8 1.00 150 1.4 0.78 1.11 ## 0.4 3 0.8 1.00 200 1.4 0.77 1.12 ## 0.4 3 0.8 1.00 250 1.4 0.77 1.12 ## 0.4 4 0.6 0.50 50 1.6 0.71 1.25 ## 0.4 4 0.6 0.50 100 1.6 0.70 1.26 ## 0.4 4 0.6 0.50 150 1.6 0.70 1.27 ## 0.4 4 0.6 0.50 200 1.6 0.70 1.27 ## 0.4 4 0.6 0.50 250 1.6 0.70 1.27 ## 0.4 4 0.6 0.62 50 1.6 0.70 1.26 ## 0.4 4 0.6 0.62 100 1.6 0.70 1.27 ## 0.4 4 0.6 0.62 150 1.6 0.69 1.27 ## 0.4 4 0.6 0.62 200 1.6 0.69 1.27 ## 0.4 4 0.6 0.62 250 1.6 0.69 1.27 ## 0.4 4 0.6 0.75 50 1.6 0.70 1.27 ## 0.4 4 0.6 0.75 100 1.6 0.70 1.28 ## 0.4 4 0.6 0.75 150 1.6 0.70 1.28 ## 0.4 4 0.6 0.75 200 1.6 0.70 1.28 ## 0.4 4 0.6 0.75 250 1.6 0.70 1.28 ## 0.4 4 0.6 0.88 50 1.6 0.72 1.25 ## 0.4 4 0.6 0.88 100 1.6 0.72 1.27 ## 0.4 4 0.6 0.88 150 1.6 0.72 1.27 ## 0.4 4 0.6 0.88 200 1.6 0.71 1.27 ## 0.4 4 0.6 0.88 250 1.6 0.71 1.28 ## 0.4 4 0.6 1.00 50 1.6 0.71 1.23 ## 0.4 4 0.6 1.00 100 1.6 0.70 1.24 ## 0.4 4 0.6 1.00 150 1.6 0.70 1.24 ## 0.4 4 0.6 1.00 200 1.6 0.70 1.24 ## 0.4 4 0.6 1.00 250 1.6 0.70 1.24 ## 0.4 4 0.8 0.50 50 1.7 0.66 1.37 ## 0.4 4 0.8 0.50 100 1.7 0.64 1.42 ## 0.4 4 0.8 0.50 150 1.7 0.64 1.42 ## 0.4 4 0.8 0.50 200 1.8 0.64 1.43 ## 0.4 4 0.8 0.50 250 1.8 0.64 1.43 ## 0.4 4 0.8 0.62 50 1.6 0.69 1.24 ## 0.4 4 0.8 0.62 100 1.6 0.68 1.25 ## 0.4 4 0.8 0.62 150 1.6 0.68 1.25 ## 0.4 4 0.8 0.62 200 1.6 0.68 1.25 ## 0.4 4 0.8 0.62 250 1.6 0.68 1.25 ## 0.4 4 0.8 0.75 50 1.6 0.72 1.21 ## 0.4 4 0.8 0.75 100 1.6 0.72 1.22 ## 0.4 4 0.8 0.75 150 1.6 0.71 1.22 ## 0.4 4 0.8 0.75 200 1.6 0.71 1.22 ## 0.4 4 0.8 0.75 250 1.6 0.71 1.22 ## 0.4 4 0.8 0.88 50 1.6 0.71 1.22 ## 0.4 4 0.8 0.88 100 1.6 0.71 1.23 ## 0.4 4 0.8 0.88 150 1.6 0.71 1.23 ## 0.4 4 0.8 0.88 200 1.6 0.71 1.23 ## 0.4 4 0.8 0.88 250 1.6 0.71 1.23 ## 0.4 4 0.8 1.00 50 1.6 0.71 1.24 ## 0.4 4 0.8 1.00 100 1.6 0.71 1.25 ## 0.4 4 0.8 1.00 150 1.6 0.70 1.25 ## 0.4 4 0.8 1.00 200 1.6 0.70 1.25 ## 0.4 4 0.8 1.00 250 1.6 0.70 1.25 ## 0.4 5 0.6 0.50 50 1.9 0.59 1.52 ## 0.4 5 0.6 0.50 100 1.9 0.59 1.53 ## 0.4 5 0.6 0.50 150 1.9 0.59 1.53 ## 0.4 5 0.6 0.50 200 1.9 0.59 1.53 ## 0.4 5 0.6 0.50 250 1.9 0.59 1.53 ## 0.4 5 0.6 0.62 50 1.7 0.67 1.35 ## 0.4 5 0.6 0.62 100 1.7 0.67 1.35 ## 0.4 5 0.6 0.62 150 1.7 0.67 1.36 ## 0.4 5 0.6 0.62 200 1.7 0.67 1.36 ## 0.4 5 0.6 0.62 250 1.7 0.67 1.36 ## 0.4 5 0.6 0.75 50 1.7 0.66 1.30 ## 0.4 5 0.6 0.75 100 1.7 0.66 1.30 ## 0.4 5 0.6 0.75 150 1.7 0.66 1.30 ## 0.4 5 0.6 0.75 200 1.7 0.66 1.30 ## 0.4 5 0.6 0.75 250 1.7 0.66 1.30 ## 0.4 5 0.6 0.88 50 1.7 0.65 1.37 ## 0.4 5 0.6 0.88 100 1.7 0.65 1.37 ## 0.4 5 0.6 0.88 150 1.7 0.65 1.37 ## 0.4 5 0.6 0.88 200 1.7 0.65 1.37 ## 0.4 5 0.6 0.88 250 1.7 0.65 1.37 ## 0.4 5 0.6 1.00 50 1.6 0.68 1.30 ## 0.4 5 0.6 1.00 100 1.6 0.68 1.30 ## 0.4 5 0.6 1.00 150 1.6 0.68 1.30 ## 0.4 5 0.6 1.00 200 1.6 0.68 1.30 ## 0.4 5 0.6 1.00 250 1.6 0.68 1.30 ## 0.4 5 0.8 0.50 50 1.6 0.69 1.27 ## 0.4 5 0.8 0.50 100 1.6 0.69 1.27 ## 0.4 5 0.8 0.50 150 1.6 0.69 1.27 ## 0.4 5 0.8 0.50 200 1.6 0.69 1.27 ## 0.4 5 0.8 0.50 250 1.6 0.69 1.27 ## 0.4 5 0.8 0.62 50 1.6 0.69 1.26 ## 0.4 5 0.8 0.62 100 1.6 0.69 1.27 ## 0.4 5 0.8 0.62 150 1.6 0.69 1.27 ## 0.4 5 0.8 0.62 200 1.6 0.69 1.27 ## 0.4 5 0.8 0.62 250 1.6 0.69 1.27 ## 0.4 5 0.8 0.75 50 1.7 0.69 1.29 ## 0.4 5 0.8 0.75 100 1.7 0.69 1.30 ## 0.4 5 0.8 0.75 150 1.7 0.69 1.30 ## 0.4 5 0.8 0.75 200 1.7 0.69 1.30 ## 0.4 5 0.8 0.75 250 1.7 0.69 1.30 ## 0.4 5 0.8 0.88 50 1.6 0.71 1.24 ## 0.4 5 0.8 0.88 100 1.6 0.71 1.24 ## 0.4 5 0.8 0.88 150 1.6 0.71 1.24 ## 0.4 5 0.8 0.88 200 1.6 0.71 1.24 ## 0.4 5 0.8 0.88 250 1.6 0.71 1.24 ## 0.4 5 0.8 1.00 50 1.6 0.69 1.27 ## 0.4 5 0.8 1.00 100 1.6 0.69 1.27 ## 0.4 5 0.8 1.00 150 1.6 0.69 1.27 ## 0.4 5 0.8 1.00 200 1.6 0.69 1.27 ## 0.4 5 0.8 1.00 250 1.6 0.69 1.27 ## ## Tuning parameter &#39;gamma&#39; was held constant at a value of 0 ## Tuning ## parameter &#39;min_child_weight&#39; was held constant at a value of 1 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were nrounds = 150, max_depth = 1, eta ## = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample ## = 1. The optimal tuning parameters were at \\(M = 250\\) and interation.depth = 1. train() tuned eta (\\(\\eta\\)), max_depth, gamma, colsample_bytree, subsample, and nrounds, holding gamma = 0, and min_child_weight = 1. The optimal hyperparameter values were eta = 0.4, max_depth = 1, gamma = 0, colsample_bytree = 0.8, subsample = 1, and nrounds = 150. With so many hyperparameters, the tuning plot is nearly unreadable. #plot(cs_mdl_xgb) Here is the holdout set performance. cs_preds_xgb &lt;- bind_cols( Predicted = predict(cs_mdl_xgb, newdata = cs_test), Actual = cs_test$Sales ) # Model over-predicts at low end of Sales and under-predicts at high end cs_preds_xgb %&gt;% ggplot(aes(x = Actual, y = Predicted)) + geom_point(alpha = 0.6, color = &quot;cadetblue&quot;) + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;) + geom_abline(intercept = 0, slope = 1, linetype = 2) + labs(title = &quot;Carseats XGBoost, Predicted vs Actual&quot;) plot(varImp(cs_mdl_xgb), main=&quot;Variable Importance with XGBoost&quot;) The RMSE is 1.438 - the best of the bunch. cs_rmse_xgb &lt;- RMSE(pred = cs_preds_xgb$Predicted, obs = cs_preds_xgb$Actual) cs_scoreboard &lt;- rbind(cs_scoreboard, data.frame(Model = &quot;XGBoost&quot;, RMSE = cs_rmse_xgb) ) %&gt;% arrange(RMSE) scoreboard(cs_scoreboard) ModelRMSEXGBoost1.3952GBM1.4381Random Forest1.7184Bagging1.9185Single Tree (caret)2.2983Single Tree2.3632 "],
["summary.html", "8.6 Summary", " 8.6 Summary I created four classification trees using the ISLR::OJ data set and four regression trees using the ISLR::Carseats data set. Let’s compare their performance. 8.6.1 Classification Trees The caret::resamples() function summarizes the resampling performance on the final model produced in train(). It creates summary statistics (mean, min, max, etc.) for each performance metric (ROC, RMSE, etc.) for a list of models. oj_resamples &lt;- resamples(list(&quot;Single Tree (caret)&quot; = oj_mdl_cart2, &quot;Bagged&quot; = oj_mdl_bag, &quot;Random Forest&quot; = oj_mdl_rf, &quot;GBM&quot; = oj_mdl_gbm, &quot;XGBoost&quot; = oj_mdl_xgb)) summary(oj_resamples) ## ## Call: ## summary.resamples(object = oj_resamples) ## ## Models: Single Tree (caret), Bagged, Random Forest, GBM, XGBoost ## Number of resamples: 10 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## Single Tree (caret) 0.74 0.84 0.86 0.85 0.88 0.92 0 ## Bagged 0.79 0.84 0.86 0.86 0.88 0.89 0 ## Random Forest 0.80 0.87 0.88 0.87 0.89 0.91 0 ## GBM 0.82 0.87 0.91 0.89 0.91 0.94 0 ## XGBoost 0.82 0.88 0.90 0.89 0.92 0.94 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## Single Tree (caret) 0.77 0.85 0.85 0.86 0.89 0.92 0 ## Bagged 0.77 0.83 0.83 0.84 0.85 0.92 0 ## Random Forest 0.79 0.83 0.83 0.84 0.85 0.92 0 ## GBM 0.79 0.85 0.87 0.86 0.88 0.88 0 ## XGBoost 0.81 0.87 0.89 0.88 0.90 0.92 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## Single Tree (caret) 0.61 0.63 0.76 0.73 0.82 0.85 0 ## Bagged 0.62 0.67 0.71 0.72 0.76 0.82 0 ## Random Forest 0.58 0.65 0.74 0.72 0.81 0.85 0 ## GBM 0.61 0.68 0.79 0.76 0.84 0.91 0 ## XGBoost 0.61 0.64 0.78 0.75 0.82 0.88 0 The mean ROC value is the performance measure I used to evaluate the models in train() (you can compare the Mean column to each model’s object (e.g., print(oj_mdl_cart2))). The best performing model on resamples based on the mean ROC score was XGBoost. It also had the highest mean sensitivity. GBM had the highest specificity. Here is a box plot of the distributions. bwplot(oj_resamples) One way to evaluate the box plots is with a post-hoc test of differences. The single tree was about the same as the random forest and bagged models. GBM and XGBoost were also statistically equivalent. (oj_diff &lt;- diff(oj_resamples)) ## ## Call: ## diff.resamples(x = oj_resamples) ## ## Models: Single Tree (caret), Bagged, Random Forest, GBM, XGBoost ## Metrics: ROC, Sens, Spec ## Number of differences: 10 ## p-value adjustment: bonferroni # summary(oj_diff) dotplot(oj_diff) 8.6.2 Regression Trees Here is the summary of resampling metrics for the Carseats models. cs_resamples &lt;- resamples(list(&quot;Single Tree (caret)&quot; = cs_mdl_cart2, &quot;Bagged&quot; = cs_mdl_bag, &quot;Random Forest&quot; = cs_mdl_rf, &quot;GBM&quot; = cs_mdl_gbm, &quot;XGBoost&quot; = cs_mdl_xgb)) summary(cs_resamples) ## ## Call: ## summary.resamples(object = cs_resamples) ## ## Models: Single Tree (caret), Bagged, Random Forest, GBM, XGBoost ## Number of resamples: 10 ## ## MAE ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## Single Tree (caret) 1.37 1.59 1.73 1.70 1.80 2.1 0 ## Bagged 0.93 1.13 1.35 1.34 1.57 1.7 0 ## Random Forest 0.85 1.04 1.26 1.21 1.39 1.5 0 ## GBM 0.72 0.86 0.92 0.94 1.04 1.1 0 ## XGBoost 0.66 0.79 0.90 0.88 0.96 1.1 0 ## ## RMSE ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## Single Tree (caret) 1.69 1.90 2.0 2.1 2.2 2.5 0 ## Bagged 1.21 1.41 1.7 1.7 2.0 2.1 0 ## Random Forest 1.07 1.28 1.6 1.5 1.7 1.9 0 ## GBM 0.93 1.08 1.2 1.2 1.3 1.4 0 ## XGBoost 0.86 0.98 1.1 1.1 1.2 1.3 0 ## ## Rsquared ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## Single Tree (caret) 0.39 0.50 0.52 0.50 0.53 0.59 0 ## Bagged 0.53 0.63 0.68 0.68 0.73 0.78 0 ## Random Forest 0.68 0.70 0.76 0.75 0.80 0.83 0 ## GBM 0.82 0.83 0.85 0.85 0.86 0.87 0 ## XGBoost 0.84 0.86 0.87 0.86 0.87 0.88 0 The best performing model on resamples based on the mean RMSE score was XGBoost. It also had the lowest mean absolute error (MAE) and highest R-squared. Here is a box plot of the distributions. bwplot(cs_resamples) The post-hoc test indicates all models differed from each other except for GBM and XGBoost. (cs_diff &lt;- diff(cs_resamples)) ## ## Call: ## diff.resamples(x = cs_resamples) ## ## Models: Single Tree (caret), Bagged, Random Forest, GBM, XGBoost ## Metrics: MAE, RMSE, Rsquared ## Number of differences: 10 ## p-value adjustment: bonferroni # summary(cs_diff) dotplot(cs_diff) "],
["non-linear-models.html", "Chapter 9 Non-linear Models", " Chapter 9 Non-linear Models Linear methods can model nonlinear relationships by including polynomial terms, interaction effects, and variable transformations. However, it is often difficult to identify how to formulate the model. Nonlinear models may be preferable because you do not need to know the the exact form of the nonlinearity prior to model training. "],
["splines.html", "9.1 Splines", " 9.1 Splines A regression spline fits a piecewise polynomial to the range of X partitioned by knots (K knots produce K + 1 piecewise polynomials) James et al (James et al. 2013). The polynomials can be of any degree d, but are usually in the range [0, 3], most commonly 3 (a cubic spline). To avoid discontinuities in the fit, a degree-d spline is constrained to have continuity in derivatives up to degree d−1 at each knot. A cubic spline fit to a data set with K knots, performs least squares regression with an intercept and 3 + K predictors, of the form \\[y_i = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\beta_4h(X, \\xi_1) + \\beta_5h(X, \\xi_2) + \\dots + \\beta_{K+3}h(X, \\xi_K)\\] where \\(\\xi_1, \\dots, \\xi_K\\) are the knots are truncated power basis functions \\(h(X, \\xi) = (X - \\xi)^3\\) if \\(X &gt; \\xi\\), else 0. Splines can have high variance at the outer range of the predictors. A natural spline is a regression spline additionally constrained to be linear at the boundaries. How many knots should there be, and Where should the knots be placed? It is common to place knots in a uniform fashion, with equal numbers of points between each knot. The number of knots is typically chosen by trial and error using cross-validation to minimize the RSS. The number of knots is usually expressed in terms of degrees of freedom. A cubic spline will have K + 3 + 1 degrees of freedom. A natural spline has K + 3 + 1 - 5 degrees of freedom due to the constraints at the endpoints. A further constraint can be added to reduce overfitting by enforcing smoothness in the spline. Instead of minimizing the loss function \\(\\sum{(y - g(x))^2}\\) where \\(g(x)\\) is a natural spline, minimize a loss function with an additional penalty for variability: \\[L = \\sum{(y_i - g(x_i))^2 + \\lambda \\int g&#39;&#39;(t)^2dt}.\\] The function \\(g(x)\\) that minimizes the loss function is a natural cubic spline with knots at each \\(x_1, \\dots, x_n\\). This is called a smoothing spline. The larger g is, the greater the penalty on variation in the spline. In a smoothing spline, you do not optimize the number or location of the knots – there is a knot at each training observation. Instead, you optimize \\(\\lambda\\). One way to optimze \\(\\lambda\\) is cross-validation to minimize RSS. Leave-one-out cross-validation (LOOCV) can be computed efficiently for smoothing splines. References "],
["mars.html", "9.2 MARS", " 9.2 MARS Multivariate adaptive regression splines (MARS) is a non-parametric algorithm that creates a piecewise linear model to capture nonlinearities and interactions effects. The resulting model is a weighted sum of basis functions \\(B_i(X)\\): \\[\\hat{y} = \\sum_{i=1}^{k}{w_iB_i(x)}\\] The basis functions are either a constant (for the intercept), a hinge function of the form \\(\\max(0, x - x_0)\\) or \\(\\max(0, x_0 - x)\\) (a more concise representation is \\([\\pm(x - x_0)]_+\\)), or products of two or more hinge functions (for interactions). MARS automatically selects which predictors to use and what predictor values to serve as the knots of the hinge functions. MARS builds a model in two phases: the forward pass and the backward pass, similar to growing and pruning of tree models. MARS starts with a model consisting of just the intercept term equaling the mean of the response values. It then asseses every predictor to find a basis function pair consisting of opposing sides of a mirrored hinge function which produces the maximum improvement in the model error. MARS repeats the process until either it reaches a predefined limit of terms or the error improvement reaches a predefined limit. MARS generalizes the model by removing terms according to the generalized cross validation (GCV) criterion. GCV is a form of regularization: it trades off goodness-of-fit against model complexity. The earth::earth() function (documentation) performs the MARS algorithm (the term “MARS” is trademarked, so open-source implementations use “Earth” instead). The caret implementation tunes two parameters: nprune and degree. nprune is the maximum number of terms in the pruned model. degree is the maximum degree of interaction (default is 1 (no interactions)). However, there are other hyperparameters in the model that may improve performance, including minspan which regulates the number of knots in the predictors. Here is an example using the Ames housing data set (following this tutorial. library(tidyverse) library(earth) library(caret) # set up ames &lt;- AmesHousing::make_ames() set.seed(12345) idx &lt;- createDataPartition(ames$Sale_Price, p = 0.80, list = FALSE) ames_train &lt;- ames[idx, ] %&gt;% as.data.frame() ames_test &lt;- ames[-idx, ] m &lt;- train( x = subset(ames_train, select = -Sale_Price), y = ames_train$Sale_Price, method = &quot;earth&quot;, metric = &quot;RMSE&quot;, minspan = -15, trControl = trainControl(method = &quot;cv&quot;, number = 10), tuneGrid = expand.grid( degree = 1:3, nprune = seq(2, 100, length.out = 10) %&gt;% floor() ) ) The model plot shows the best tuning parameter combination. plot(m, main = &quot;MARS Parameter Tuning&quot;) m$bestTune ## nprune degree ## 25 45 3 How does this model perform against the holdout data? caret::postResample( pred = log(predict(m, newdata = ames_test)), obs = log(ames_test$Sale_Price) ) ## RMSE Rsquared MAE ## 0.165 0.855 0.093 "],
["gam.html", "9.3 GAM", " 9.3 GAM Generalized additive models (GAM) allow for non-linear relationships between each feature and the response by replacing each linear component \\(\\beta_j x_{ij}\\) with a nonlinear function \\(f_j(x_{ij})\\). The GAM model is of the form \\[y_i = \\beta_0 + \\sum{f_j(x_{ij})} + \\epsilon_i.\\] It is called an additive model because we calculate a separate \\(f_j\\) for each \\(X_j\\), and then add together all of their contributions. The advantage of GAMs is that they automatically model non-linear relationships so you do not need to manually try out many diﬀerent transformations on each variable individually. And because the model is additive, you can still examine the eﬀect of each \\(X_j\\) on \\(Y\\) individually while holding all of the other variables ﬁxed. The main limitation of GAMs is that the model is restricted to be additive, so important interactions can be missed unless you explicitly add them. "],
["support-vector-machines.html", "Chapter 10 Support Vector Machines", " Chapter 10 Support Vector Machines These notes rely on (James et al. 2013), (Hastie, Tibshirani, and Friedman 2017), (Kuhn and Johnson 2016), PSU STAT 508, and the e1071 SVM vignette. Support Vector Machines (SVM) is a classification model that maps observations as points in space so that the categories are divided by as wide a gap as possible. New observations can then be mapped into the space for prediction. The SVM algorithm finds the optimal separating hyperplane using a nonlinear mapping to a sufficiently high dimension. The hyperplane is defined by the observations that lie within a margin optimized by a cost hyperparameter. These observations are called the support vectors. SVM is an extension of the support vector classifier which in turn is a generalization of the simple and intuitive maximal margin classifier. The best way to understand the SVM is to start with the maximal margin classifier and work up. References "],
["maximal-margin-classifier.html", "10.1 Maximal Margin Classifier", " 10.1 Maximal Margin Classifier The maximal margin classifier is the optimal hyperplane defined in the (rare) case where two classes are linearly separable. Given an \\(n \\times p\\) data matrix \\(X\\) with a binary response variable defined as \\(y \\in [-1, 1]\\) it might be possible to define a p-dimensional hyperplane \\(h(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 \\dots + \\beta_pX_p = x_i^T \\beta + \\beta_0 = 0\\) such that all observations of each class fall on opposite sides of the hyperplane. This separating hyperplane has the property that if \\(\\beta\\) is constrained to be a unit vector, \\(||\\beta|| = \\sum\\beta^2 = 1\\), then the product of the hyperplane and response variables are positive perpendicular distances from the hyperplane, the smallest of which may be termed the hyperplane margin, \\(M\\), \\[y_i (x_i^{&#39;} \\beta + \\beta_0) \\ge M.\\] The maximal margin classifier is the hyperplane with the maximum margin, \\(\\max \\{M\\}\\) subject to \\(||\\beta|| = 1\\). A separating hyperplane rarely exists. In fact, even if a separating hyperplane does exist, its resulting margin is probably undesirably narrow. Here is the maximal margin classifier.1 Maximum marginal classifier The data set has two linearly separable classes, \\(y \\in [-1, 1]\\) described by two features, \\(X1\\) and \\(X2\\)2. The code is unimportant - just trying to produce the visualization. Image from https://nlp.stanford.edu/IR-book/pdf/15svm.pdf.↩︎ Any more features and we won’t be able to visualize it↩︎ "],
["support-vector-classifier.html", "10.2 Support Vector Classifier", " 10.2 Support Vector Classifier The maximal margin classifier can be generalized to non-separable cases using a so-called soft margin. The generalization is called the support vector classifier. The soft margin allows some misclassification in the interest of greater robustness to individual observations. The support vector classifier optimizes \\[y_i (x_i^{&#39;} \\beta + \\beta_0) \\ge M(1 - \\xi_i)\\] where the \\(\\xi_i\\) are positive slack variables whose sum is bounded by some constant tuning parameter \\(\\sum{\\xi_i} \\le \\Xi\\). The slack variable values indicate where the observation lies: \\(\\xi_i = 0\\) observations lie on the correct side of the margin; \\(\\xi_i &gt; 0\\) observation lie on the wrong side of the margin; \\(\\xi_i &gt; 1\\) observations lie on the wrong side of the hyperplane. \\(\\Xi\\) sets the tolerance for margin violation. If \\(\\Xi = 0\\), then all observations must reside on the correct side of the margin, as in the maximal margin classifier. \\(\\Xi\\) controls the bias-variance trade-off: as \\(\\Xi\\) increases, the margin widens and allows more violations, increasing bias and decreasing variance. The support vector classifier is usually defined by dropping the \\(||\\beta|| = 1\\) constraint, and defining \\(M = 1 / ||\\beta||\\). The optimization problem then becomes \\[ \\min ||\\beta|| \\hspace{2mm} s.t. \\hspace{2mm} \\begin{cases} y_i(x_i^T\\beta + \\beta_0) \\ge 1 - \\xi_i, \\hspace{2mm} \\forall i &amp; \\\\ \\xi_i \\ge 0, \\hspace{2mm} \\sum \\xi_i \\le \\Xi. \\end{cases} \\] This is a quadratic equation with linear inequality constraints, so it is a convex optimization problem which can be solved using Lagrange multipliers. Re-express the optimization problem as \\[ \\min_{\\beta_0, \\beta} \\frac{1}{2}||\\beta||^2 = C\\sum_{i = 1}^N \\xi_i \\\\ s.t. \\xi_i \\ge 0, \\hspace{2mm} y_i(x_i^T\\beta + \\beta_0) \\ge 1 - \\xi_i, \\hspace{2mm} \\forall i \\] where the “cost” parameter \\(C\\) replaces the constant and penalizes large residuals. This optimization problem is equivalent to another optimization problem, the familiar loss + penalty formulation: \\[\\min_{\\beta_0, \\beta} \\sum_{i=1}^N{[1 - y_if(x_i)]_+} + \\frac{\\lambda}{2} ||\\beta||^2 \\] where \\(\\lambda = 1 / C\\) and \\([1 - y_if(x_i)]_+\\) is a “hinge” loss function with \\(f(x_i) = sign[Pr(Y = +1|x) - 1 / 2]\\). The parameter estimates can be written as functions of a set of unknown parameters \\((\\alpha_i)\\) and data points. The solution to the optimization problem requires only the inner products of the observations, represented as \\(\\langle x_i, x_j \\rangle\\), \\[f(x) = \\beta_0 + \\sum_{i = 1}^n {\\alpha_i \\langle x, x_i \\rangle}\\] The solution has the interesting property that only observations on or within the margin affect the hyperplane. These observations are known as support vectors. As the constant increases, the number of violating observations increase, and thus the number of support vectors increases. This property makes the algorithm robust to the extreme observations far away from the hyperplane. The parameter estimators for \\(\\alpha_i\\) are nonzero only for the support vectors in the solution—that is, if a training observation is not a support vector, then its \\(\\alpha_i\\) equals zero. The only shortcoming with the algorithm is that it presumes a linear decision boundary. "],
["support-vector-machines-1.html", "10.3 Support Vector Machines", " 10.3 Support Vector Machines Enlarging the feature space of the support vector classifier accommodates nonlinar relationships. Support vector machines do this in a specific way, using kernals. The kernal is a generalization of the inner product with form \\(K(x_i, x_i^{&#39;})\\). So the linear kernal is simply \\[K(x_i, x_i^{&#39;}) = \\langle x, x_i \\rangle\\] and the solution is \\[f(x) = \\beta_0 + \\sum_{i = 1}^n {\\alpha_i K(x_i, x_i^{&#39;})}\\] \\(K\\) can take onother form instead, such as polynomial \\[K(x, x&#39;) = (\\gamma \\langle x, x&#39; \\rangle + c_0)^d\\] or radial \\[K(x, x&#39;) = \\exp\\{-\\gamma ||x - x&#39;||^2\\}.\\] "],
["my-svm-example.html", "10.4 My svm Example", " 10.4 My svm Example Here is a data set of two classes \\(y \\in [-1, 1]\\) described by two features \\(X1\\) and \\(X2\\). library(tidyverse) set.seed(1) x &lt;- matrix(rnorm (20*2), ncol=2) y &lt;- c(rep(-1, 10), rep(1, 10)) x[y==1, ] &lt;- x[y==1, ] + 1 train_data &lt;- data.frame(x, y) train_data$y &lt;- as.factor(y) A scatter plot reveals whether the classes are linearly separable. ggplot(train_data, aes(x = X1, y = X2, color = y)) + geom_point(size = 2) + labs(title = &quot;Binary response with two features&quot;) + theme(legend.position = &quot;top&quot;) No, they are not linearly separable. Now fit a support vector machine. The e1071 library implements the SVM algorithm. svm(..., kernel=\"linear\") fits a support vector classifier. Change the kernal to c(\"polynomial\", \"radial\") for SVM. Try a cost of 10. m &lt;- svm( y ~ ., data = train_data, kernel = &quot;linear&quot;, type = &quot;C-classification&quot;, # (default) for classification cost = 10, # default is 1 scale = FALSE # do not standardize features ) plot(m, train_data) The support vectors are plotted as “x’s”. There are seven of them. m$index ## [1] 1 2 5 7 14 16 17 The summary shows adds additional information, including the distribution of the support vector classes. summary(m) ## ## Call: ## svm(formula = y ~ ., data = train_data, kernel = &quot;linear&quot;, type = &quot;C-classification&quot;, ## cost = 10, scale = FALSE) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: linear ## cost: 10 ## ## Number of Support Vectors: 7 ## ## ( 4 3 ) ## ## ## Number of Classes: 2 ## ## Levels: ## -1 1 The seven support vectors are comprised of four in one class, three in the other. What if we lower the cost of margin violations? This will increase bias and lower variance. m &lt;- svm( y ~ ., data = train_data, kernel = &quot;linear&quot;, type = &quot;C-classification&quot;, cost = 0.1, scale = FALSE ) plot(m, train_data) There are many more support vectors now. (In case you hoped to see the linear decision boundary formulation, or at least a graphical representation of the margins, keep hoping. The model is generalized beyond two features, so it evidently does not worry too much about supporting sanitized two-feature demos.) Which cost level yields the best predictive performance on holdout data? Use cross validation to find out. SVM defaults to 10-fold CV. I’ll try seven candidate values for cost. set.seed(1) m_tune &lt;- tune( svm, y ~ ., data = train_data, kernel =&quot;linear&quot;, ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)) ) summary(m_tune) ## ## Parameter tuning of &#39;svm&#39;: ## ## - sampling method: 10-fold cross validation ## ## - best parameters: ## cost ## 0.1 ## ## - best performance: 0.05 ## ## - Detailed performance results: ## cost error dispersion ## 1 0.001 0.55 0.44 ## 2 0.010 0.55 0.44 ## 3 0.100 0.05 0.16 ## 4 1.000 0.15 0.24 ## 5 5.000 0.15 0.24 ## 6 10.000 0.15 0.24 ## 7 100.000 0.15 0.24 The lowest cross-validation error rate is 0.10 with cost = 0.1. tune() saves the best tuning parameter value. m_best &lt;- m_tune$best.model summary(m_best) ## ## Call: ## best.tune(method = svm, train.x = y ~ ., data = train_data, ranges = list(cost = c(0.001, ## 0.01, 0.1, 1, 5, 10, 100)), kernel = &quot;linear&quot;) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: linear ## cost: 0.1 ## ## Number of Support Vectors: 16 ## ## ( 8 8 ) ## ## ## Number of Classes: 2 ## ## Levels: ## -1 1 There are 16 support vectors, 8 in each class. This is a pretty wide margin. plot(m_best, train_data) To fit an SVM, use a different kernel. You can use kernal = c(\"polynomial\", \"radial\", \"sigmoid\"). For a polynomial model, also specify the polynomial degree. For a radial model, include the gamma value. set.seed(1) m3_tune &lt;- tune( svm, y ~ ., data = train_data, kernel =&quot;polynomial&quot;, ranges = list( cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100), degree = c(1, 2, 3) ) ) summary(m3_tune) ## ## Parameter tuning of &#39;svm&#39;: ## ## - sampling method: 10-fold cross validation ## ## - best parameters: ## cost degree ## 1 1 ## ## - best performance: 0.1 ## ## - Detailed performance results: ## cost degree error dispersion ## 1 0.001 1 0.55 0.44 ## 2 0.010 1 0.55 0.44 ## 3 0.100 1 0.30 0.26 ## 4 1.000 1 0.10 0.21 ## 5 5.000 1 0.10 0.21 ## 6 10.000 1 0.15 0.24 ## 7 100.000 1 0.15 0.24 ## 8 0.001 2 0.70 0.42 ## 9 0.010 2 0.70 0.42 ## 10 0.100 2 0.70 0.42 ## 11 1.000 2 0.65 0.24 ## 12 5.000 2 0.50 0.33 ## 13 10.000 2 0.50 0.33 ## 14 100.000 2 0.50 0.33 ## 15 0.001 3 0.65 0.34 ## 16 0.010 3 0.65 0.34 ## 17 0.100 3 0.50 0.33 ## 18 1.000 3 0.40 0.32 ## 19 5.000 3 0.35 0.34 ## 20 10.000 3 0.35 0.34 ## 21 100.000 3 0.35 0.34 The lowest cross-validation error rate is 0.10 with cost = 1, polynomial degree 1. m3_best &lt;- m3_tune$best.model summary(m3_best) ## ## Call: ## best.tune(method = svm, train.x = y ~ ., data = train_data, ranges = list(cost = c(0.001, ## 0.01, 0.1, 1, 5, 10, 100), degree = c(1, 2, 3)), kernel = &quot;polynomial&quot;) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: polynomial ## cost: 1 ## degree: 1 ## coef.0: 0 ## ## Number of Support Vectors: 12 ## ## ( 6 6 ) ## ## ## Number of Classes: 2 ## ## Levels: ## -1 1 There are 12 support vectors, 6 in each class. This is a pretty wide margin. plot(m3_best, train_data) "],
["using-caret.html", "10.5 Using Caret", " 10.5 Using Caret The model can also be fit using caret. I’ll used LOOCV since the data set is so small. Normalize the variables to make their scale comparable. library(caret) library(kernlab) train_data_3 &lt;- train_data %&gt;% mutate(y = factor(y, labels = c(&quot;A&quot;, &quot;B&quot;))) m4 &lt;- train( y ~ ., data = train_data_3, method = &quot;svmPoly&quot;, preProcess = c(&quot;center&quot;, &quot;scale&quot;), trControl = trainControl( method = &quot;cv&quot;, number = 5, summaryFunction = twoClassSummary, # Use AUC to pick the best model classProbs=TRUE ) ) m4$bestTune ## degree scale C ## 8 1 0.1 0.5 #plot(m4) "],
["part-3-unupervised-machine-learning.html", "PART 3: Unupervised Machine Learning", " PART 3: Unupervised Machine Learning Unsupervised machine learning searches for structure in unlabeled data (data without a response variable). The goal of unsupervised learning is clustering into homogenous subgroups, and dimensionality reduction. Examples of cluster analysis are k-means clustering, hierarchical cluster analysis (HCA), and PCA (others here). "],
["dimensionality-reduction.html", "Chapter 11 Dimensionality Reduction ", " Chapter 11 Dimensionality Reduction "],
["pca.html", "11.1 PCA", " 11.1 PCA "],
["t-sne.html", "11.2 t-SNE", " 11.2 t-SNE https://www.datacamp.com/community/tutorials/introduction-t-sne "],
["svd.html", "11.3 SVD", " 11.3 SVD "],
["cluster-analysis.html", "Chapter 12 Cluster Analysis", " Chapter 12 Cluster Analysis These notes are primarily taken from studying DataCamp courses Cluster Analysis in R and Unsupervised Learning in R, AIHR, UC Business Analytics R Programming Guide, and PSU STAT-505. Cluster analysis is a data exploration (mining) tool for dividing features into clusters, distinct populations with no a priori defining characteristics. The goal is to describe those populations with the observed data. Popular uses of clustering are audience segmentation, creating personas, detecting anomalies, and pattern recognition in images. There are two common approaches to cluster analysis. Agglomerative hierarchical algorithms start by defining each data point as a cluster, then repeatedly combine the two closest clusters into a new cluster until all data points are merged into a single cluster. Non-hierarchical methods such as K-means initially randomly partitions the data into a set of K clusters, then iteratively moves observations into different clusters until there is no sensible reassignment possible. Setup I will learn by example, using the IBM HR Analytics Employee Attrition &amp; Performance data set from Kaggle to discover which factors are associated with employee turnover and whether distinct clusters of employees are more susceptible to turnover. The clusters can help personalize employee experience (AIHR). This data set includes 1,470 employee records consisting of the EmployeeNumber, a flag for Attrition during some time frame, and 32 other descriptive variables. library(tidyverse) library(plotly) # interactive graphing library(cluster) # daisy and pam library(Rtsne) # dimensionality reduction and visualization library(dendextend) # color_branches set.seed(1234) # reproducibility dat &lt;- read_csv(&quot;./input/WA_Fn-UseC_-HR-Employee-Attrition.csv&quot;) dat &lt;- dat %&gt;% mutate_if(is.character, as_factor) %&gt;% select(EmployeeNumber, Attrition, everything()) my_skim &lt;- skimr::skim_with(numeric = skimr::sfl(p25 = NULL, p50 = NULL, p75 = NULL, hist = NULL)) my_skim(dat) Table 12.1: Data summary Name dat Number of rows 1470 Number of columns 35 _______________________ Column type frequency: factor 9 numeric 26 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts Attrition 0 1 FALSE 2 No: 1233, Yes: 237 BusinessTravel 0 1 FALSE 3 Tra: 1043, Tra: 277, Non: 150 Department 0 1 FALSE 3 Res: 961, Sal: 446, Hum: 63 EducationField 0 1 FALSE 6 Lif: 606, Med: 464, Mar: 159, Tec: 132 Gender 0 1 FALSE 2 Mal: 882, Fem: 588 JobRole 0 1 FALSE 9 Sal: 326, Res: 292, Lab: 259, Man: 145 MaritalStatus 0 1 FALSE 3 Mar: 673, Sin: 470, Div: 327 Over18 0 1 FALSE 1 Y: 1470 OverTime 0 1 FALSE 2 No: 1054, Yes: 416 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p100 EmployeeNumber 0 1 1024.87 602.02 1 2068 Age 0 1 36.92 9.14 18 60 DailyRate 0 1 802.49 403.51 102 1499 DistanceFromHome 0 1 9.19 8.11 1 29 Education 0 1 2.91 1.02 1 5 EmployeeCount 0 1 1.00 0.00 1 1 EnvironmentSatisfaction 0 1 2.72 1.09 1 4 HourlyRate 0 1 65.89 20.33 30 100 JobInvolvement 0 1 2.73 0.71 1 4 JobLevel 0 1 2.06 1.11 1 5 JobSatisfaction 0 1 2.73 1.10 1 4 MonthlyIncome 0 1 6502.93 4707.96 1009 19999 MonthlyRate 0 1 14313.10 7117.79 2094 26999 NumCompaniesWorked 0 1 2.69 2.50 0 9 PercentSalaryHike 0 1 15.21 3.66 11 25 PerformanceRating 0 1 3.15 0.36 3 4 RelationshipSatisfaction 0 1 2.71 1.08 1 4 StandardHours 0 1 80.00 0.00 80 80 StockOptionLevel 0 1 0.79 0.85 0 3 TotalWorkingYears 0 1 11.28 7.78 0 40 TrainingTimesLastYear 0 1 2.80 1.29 0 6 WorkLifeBalance 0 1 2.76 0.71 1 4 YearsAtCompany 0 1 7.01 6.13 0 40 YearsInCurrentRole 0 1 4.23 3.62 0 18 YearsSinceLastPromotion 0 1 2.19 3.22 0 15 YearsWithCurrManager 0 1 4.12 3.57 0 17 You would normally start a cluster analysis with an exploration of the data to determine which variables are interesting and relevant to your goal. I’ll bypass that rigor and just use a binary correlation analysis. Binary correlation analysis converts features into binary format by binning the continuous features and one-hot encoding the binary features. correlate() calculates the correlation coefficient for each binary feature to the response variable. A Correlation Funnel is an tornado plot that lists the highest correlation features (based on absolute magnitude) at the top of the and the lowest correlation features at the bottom. Read more on the correlationfunel GitHub README. Using binary correlation, I’ll include just the variables with a correlation coefficient of at least 0.10. For our employee attrition data set, OverTime (Y|N) has the largest correlation, JobLevel = 1, MonthlyIncome &lt;= 2,695.80, etc. dat %&gt;% select(-EmployeeNumber) %&gt;% correlationfunnel::binarize(n_bins = 5, thresh_infreq = 0.01) %&gt;% correlationfunnel::correlate(Attrition__Yes) %&gt;% correlationfunnel::plot_correlation_funnel(interactive = FALSE) %&gt;% ggplotly() # Makes prettier, but drops the labels Using the cutoff of 0.1 leaves 14 features for the analysis. vars &lt;- c( &quot;EmployeeNumber&quot;, &quot;Attrition&quot;, &quot;OverTime&quot;, &quot;JobLevel&quot;, &quot;MonthlyIncome&quot;, &quot;YearsAtCompany&quot;, &quot;StockOptionLevel&quot;, &quot;YearsWithCurrManager&quot;, &quot;TotalWorkingYears&quot;, &quot;MaritalStatus&quot;, &quot;Age&quot;, &quot;YearsInCurrentRole&quot;, &quot;JobRole&quot;, &quot;EnvironmentSatisfaction&quot;, &quot;JobInvolvement&quot;, &quot;BusinessTravel&quot; ) dat_2 &lt;- dat %&gt;% select(one_of(vars)) Data Preparation The concept of distance is central to clustering. Two observations are similar if the distance between their features is relatively small. To compare feature distances, they should be on a similar scale. There are many ways to define distance (see options in ?dist), but the two most common are Euclidean, \\(d = \\sqrt{\\sum{(x_i - y_i)^2}}\\), and binary, 1 minus the proportion of shared features (Wikipedia, PSU-505). If you have a mix a feature types, use the Gower distance (Analytics Vidhya) range-normalizes the quantitative variables, one-hot encodes the nominal variables, and ranks the ordinal variables. Then it calculates distances using the Manhattan distance for quantitative and ordinal variables, and the Dice coefficient for nominal variables. Gower’s distance is computationally expensive, so you could one-hot encode the data and standardize the variables as \\((x - \\bar{x}) / sd(x)\\) so that each feature has a mean of 0 and standard deviation of 1, like this: dat_2_mtrx &lt;- mltools::one_hot(data.table::as.data.table(dat_2[, 2:16])) %&gt;% as.matrix() row.names(dat_2_mtrx) &lt;- dat_2$EmployeeNumber dat_2_mtrx &lt;- na.omit(dat_2_mtrx) dat_2_mtrx &lt;- scale(dat_2_mtrx) dat_2_dist &lt;- dist(dat_2_mtrx) But normally you would go ahead and calculate Gower’s distance using daisy(). dat_2_gwr &lt;- cluster::daisy(dat_2[, 2:16], metric = &quot;gower&quot;) As a sanity check, let’s see the most similar and dissimilar pairs of employees according to their Gower distance. Here are the most similar employees. x &lt;- as.matrix(dat_2_gwr) dat_2[which(x == min(x[x != 0]), arr.ind = TRUE)[1, ], ] %&gt;% t() %&gt;% as.data.frame() %&gt;% rownames_to_column() %&gt;% flextable::flextable() %&gt;% flextable::autofit() rownameV1V2EmployeeNumber1624 614AttritionYesYesOverTimeYesYesJobLevel11MonthlyIncome15691878YearsAtCompany00StockOptionLevel00YearsWithCurrManager00TotalWorkingYears00MaritalStatusSingleSingleAge1818YearsInCurrentRole00JobRoleSales RepresentativeSales RepresentativeEnvironmentSatisfaction22JobInvolvement33BusinessTravelTravel_FrequentlyTravel_Frequently They are identical except for MonthlyIncome. Here are the most dissimilar employees. dat_2[which(x == max(x), arr.ind = TRUE)[1, ], ] %&gt;% t() %&gt;% as.data.frame() %&gt;% rownames_to_column() %&gt;% flextable::flextable() %&gt;% flextable::autofit() rownameV1V2EmployeeNumber1094 825AttritionNoYesOverTimeNoYesJobLevel15MonthlyIncome 462119246YearsAtCompany 331StockOptionLevel30YearsWithCurrManager28TotalWorkingYears 340MaritalStatusMarriedSingleAge2758YearsInCurrentRole 215JobRoleLaboratory TechnicianResearch DirectorEnvironmentSatisfaction14JobInvolvement13BusinessTravelNon-TravelTravel_Rarely These two employees have nothing in common. With the data preparation complete, we can finally perform our cluster analysis. I’ll try K-means and HCA. "],
["k-means.html", "12.1 K-Means", " 12.1 K-Means The k-means clustering algorithm randomly assigns all observations to one of \\(k\\) clusters. K-means then iteratively calculates the cluster centroids and reassigns the observations to their nearest centroid. The centroid is the mean of the points in the cluster (Hence the name “k-means”). The iterations continue until either the centroids stabilize or the iterations reach a set maximum, iter.max (typically 50). The result is k clusters with the minimum total intra-cluster variation. The centroid of cluster \\(c_i \\in C\\) is the mean of the cluster observations \\(S_i\\): \\(c_i = \\frac{1}{|S_i|} \\sum_{x_i \\in S_i}{x_i}\\). The nearest centroid is the minimum squared Euclidean distance, \\(\\underset{c_i \\in C}{\\operatorname{arg min}} dist(c_i, x)^2\\).3 The algorithm will converge to a result, but the result may only be a local optimum. Other random starting centroids may yield a different local optimum. Common practice is to run the k-means algorithm nstart times and select the lowest within-cluster sum of squared distances among the cluster members. A typical number of runs is nstart = 20. Choosing K What is the best number of clusters? You may have a preference in advance, but more likely you will use a scree plot or use the silhouette method. The scree plot is a plot of the total within-cluster sum of squared distances as a function of k. The sum of squares always decreases as k increases, but at a declining rate. The optimal k is at the “elbow” in the curve - the point at which the curve flattens. kmeans() returns an object of class kmeans, a list in which one of the components is the model sum of squares tot.withinss. In the scree plot below, the elbow may be k = 5. wss &lt;- map_dbl(2:10, ~ kmeans(dat_2_gwr, centers = .x)$tot.withinss) wss_tbl &lt;- tibble(k = 2:10, wss) ggplot(wss_tbl, aes(x = k, y = wss)) + geom_point(size = 2) + geom_line() + scale_x_continuous(breaks = 2:10) + labs(title = &quot;Scree Plot&quot;) The silhouette method calculates the within-cluster distance \\(C(i)\\) for each observation, and its distance to the nearest cluster \\(N(i)\\). The silhouette width is \\(S = 1 - C(i) / N(i)\\) for \\(C(i) &lt; N(i)\\) and \\(S = N(i) / C(i) - 1\\) for \\(C(i) &gt; N(i)\\). A value close to 1 means the observation is well-matched to its current cluster; A value near 0 means the observation is on the border between the two clusters; and a value near -1 means the observation is better-matched to the other cluster. The optimal number of clusters is the number that maximizes the total silhouette width. cluster::pam() returns a list in which one of the components is the average width silinfo$avg.width. In the silhouette plot below, the maximum width is at k = 6. sil &lt;- map_dbl(2:10, ~ pam(dat_2_gwr, k = .x)$silinfo$avg.width) sil_tbl &lt;- tibble(k = 2:10, sil) ggplot(sil_tbl, aes(x = k, y = sil)) + geom_point(size = 2) + geom_line() + scale_x_continuous(breaks = 2:10) + labs(title = &quot;Silhouette Plot&quot;) Run pam() again and attach the results to the original table for visualization and summary statistics. mdl &lt;- pam(dat_2_gwr, k = 6) dat_3 &lt;- dat_2 %&gt;% mutate(cluster = as.factor(mdl$clustering)) Here are the six medoids from our data set. dat_2[mdl$medoids, ] ## # A tibble: 6 x 16 ## EmployeeNumber Attrition OverTime JobLevel MonthlyIncome YearsAtCompany ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1171 No No 2 5155 6 ## 2 35 No No 2 6825 9 ## 3 65 Yes Yes 1 3441 2 ## 4 221 No No 1 2713 5 ## 5 747 No No 2 5304 8 ## 6 1408 No No 4 16799 20 ## # ... with 10 more variables: StockOptionLevel &lt;dbl&gt;, ## # YearsWithCurrManager &lt;dbl&gt;, TotalWorkingYears &lt;dbl&gt;, MaritalStatus &lt;fct&gt;, ## # Age &lt;dbl&gt;, YearsInCurrentRole &lt;dbl&gt;, JobRole &lt;fct&gt;, ## # EnvironmentSatisfaction &lt;dbl&gt;, JobInvolvement &lt;dbl&gt;, BusinessTravel &lt;fct&gt; We’re most concerned about attrition. Do high-attrition employees fall into a particular cluster? Yes! 79.7% of cluster 3 left the company - that’s 59.5% of all turnover in the company. dat_3_smry &lt;- dat_3 %&gt;% count(cluster, Attrition) %&gt;% group_by(cluster) %&gt;% mutate(cluster_n = sum(n), turnover_rate = scales::percent(n / sum(n), accuracy = 0.1)) %&gt;% ungroup() %&gt;% filter(Attrition == &quot;Yes&quot;) %&gt;% mutate(pct_of_turnover = scales::percent(n / sum(n), accuracy = 0.1)) %&gt;% select(cluster, turnover_rate, turnover_n = n, cluster_n, pct_of_turnover) dat_3_smry ## # A tibble: 6 x 5 ## cluster turnover_rate turnover_n cluster_n pct_of_turnover ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 8.6% 23 268 9.7% ## 2 2 8.6% 24 280 10.1% ## 3 3 79.7% 141 177 59.5% ## 4 4 8.0% 29 364 12.2% ## 5 5 4.0% 8 202 3.4% ## 6 6 6.7% 12 179 5.1% You can get some sense of the quality of clustering by constructing the Barnes-Hut t-Distributed Stochastic Neighbor Embedding (t-SNE). dat_4 &lt;- dat_3 %&gt;% left_join(dat_3_smry, by = &quot;cluster&quot;) %&gt;% rename(Cluster = cluster) %&gt;% mutate( MonthlyIncome = MonthlyIncome %&gt;% scales::dollar(), description = str_glue(&quot;Turnover = {Attrition} MaritalDesc = {MaritalStatus} Age = {Age} Job Role = {JobRole} Job Level {JobLevel} Overtime = {OverTime} Current Role Tenure = {YearsInCurrentRole} Professional Tenure = {TotalWorkingYears} Monthly Income = {MonthlyIncome} Cluster: {Cluster} Cluster Size: {cluster_n} Cluster Turnover Rate: {turnover_rate} Cluster Turnover Count: {turnover_n} &quot;)) tsne_obj &lt;- Rtsne(dat_2_gwr, is_distance = TRUE) tsne_tbl &lt;- tsne_obj$Y %&gt;% as_tibble() %&gt;% setNames(c(&quot;X&quot;, &quot;Y&quot;)) %&gt;% bind_cols(dat_4) %&gt;% mutate(Cluster = as_factor(Cluster)) g &lt;- tsne_tbl %&gt;% ggplot(aes(x = X, y = Y, colour = Cluster, text = description)) + geom_point() ggplotly(g) Another common approach is to take summary statistics and draw your own conclusions. dat_3[, 2:17] %&gt;% group_by(cluster, Attrition) %&gt;% summarise_if(is.numeric, mean) ## # A tibble: 12 x 12 ## # Groups: cluster [6] ## cluster Attrition JobLevel MonthlyIncome YearsAtCompany StockOptionLevel ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Yes 2.26 6530. 7.17 0 ## 2 1 No 2.12 6324. 7.29 0 ## 3 2 Yes 2.42 7171. 7.92 1.04 ## 4 2 No 2.25 6621. 7.33 1.09 ## 5 3 Yes 1.28 3547. 3.62 0.348 ## 6 3 No 1.25 3477. 3.67 0.194 ## 7 4 Yes 1.10 2900. 2.83 1.03 ## 8 4 No 1.14 3106. 3.98 0.973 ## 9 5 Yes 2.38 7296. 3.12 1.5 ## 10 5 No 2.16 6492. 7.74 1.49 ## 11 6 Yes 3.83 14134 20.3 0.75 ## 12 6 No 4.23 16498. 14.7 0.844 ## # ... with 6 more variables: YearsWithCurrManager &lt;dbl&gt;, ## # TotalWorkingYears &lt;dbl&gt;, Age &lt;dbl&gt;, YearsInCurrentRole &lt;dbl&gt;, ## # EnvironmentSatisfaction &lt;dbl&gt;, JobInvolvement &lt;dbl&gt; dat_3 %&gt;% ggplot(aes(x = cluster, y = TotalWorkingYears)) + geom_boxplot() + geom_jitter(data = dat_3, aes(x = cluster, y = TotalWorkingYears, color = Attrition), alpha = 0.2) km_aov &lt;- aov(MonthlyIncome ~ cluster, data = dat_3) anova(km_aov) ## Analysis of Variance Table ## ## Response: MonthlyIncome ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## cluster 5 23136635762 4627327152 719 &lt;0.0000000000000002 *** ## Residuals 1464 9423539276 6436844 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #TukeyHSD(km_aov) Euclidean distances are appropriate for quantitative variables. What about categorical variables? This discussion at StackExchange explains that k-modes is suitable for categorical data. It may be okay to convert categorical variables into binary values and treating them as numeric.↩︎ "],
["hca.html", "12.2 HCA", " 12.2 HCA Hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which builds a hierarchy of clusters (usually presented in a dendrogram). The HCA process is: Calculate the distance between each observation with dist() or daisy(). We did that above when we created dat_2_gwr. Cluster the two closest observations into a cluster with hclust(). Then calculate the cluster’s distance to the remaining observations. If the shortest distance is between two observations, define a second cluster, otherwise adds the observation as a new level to the cluster. The process repeats until all observations belong to a single cluster. The “distance” to a cluster can be defined as: complete: distance to the furthest member of the cluster, single: distance to the closest member of the cluster, average: average distance to all members of the cluster, or centroid: distance between the centroids of each cluster. Complete and average distances tend to produce more balanced trees and are most common. Pruning an unbalanced tree can result in most observations assigned to one cluster and only a few observations assigned to other clusters. This is useful for identifying outliers. mdl_hc &lt;- hclust(dat_2_gwr, method = &quot;complete&quot;) Evaluate the hclust tree with a dendogram, principal component analysis (PCA), and/or summary statistics. The vertical lines in a dendogram indicate the distance between nodes and their associated cluster. Choose the number of clusters to keep by identifying a cut point that creates a reasonable number of clusters with a substantial number of observations per cluster (I know, “reasonable” and “substantial” are squishy terms). Below, cutting at height 0.65 to create 7 clusters seems good. # Inspect the tree to choose a size. plot(color_branches(as.dendrogram(mdl_hc), k = 7)) abline(h = .65, col = &quot;red&quot;) “Cut” the hierarchical tree into the desired number of clusters (k) or height h with cutree(hclust, k = NULL, h = NULL). cutree() returns a vector of cluster memberships. Attach this vector back to the original dataframe for visualization and summary statistics. dat_2_clstr_hca &lt;- dat_2 %&gt;% mutate(cluster = cutree(mdl_hc, k = 7)) Calculate summary statistics and draw conclusions. Useful summary statistics are typically membership count, and feature averages (or proportions). dat_2_clstr_hca %&gt;% group_by(cluster) %&gt;% summarise_if(is.numeric, funs(mean(.))) ## # A tibble: 7 x 12 ## cluster EmployeeNumber JobLevel MonthlyIncome YearsAtCompany StockOptionLevel ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1051. 2.59 8056. 8.21 0.515 ## 2 2 1029. 1.76 5146. 5.98 0.964 ## 3 3 974. 1.59 4450. 5.29 0 ## 4 4 1088. 1.46 3961. 4.60 1.14 ## 5 5 980. 4.23 16534. 14.2 1.02 ## 6 6 1036. 3.56 12952. 14.9 0 ## 7 7 993. 3.67 13733. 12.8 0.955 ## # ... with 6 more variables: YearsWithCurrManager &lt;dbl&gt;, ## # TotalWorkingYears &lt;dbl&gt;, Age &lt;dbl&gt;, YearsInCurrentRole &lt;dbl&gt;, ## # EnvironmentSatisfaction &lt;dbl&gt;, JobInvolvement &lt;dbl&gt; 12.2.0.1 K-Means vs HCA Hierarchical clustering has some advantages over k-means. It can use any distance method - not just euclidean. The results are stable - k-means can produce different results each time. While they can both be evaluated with the silhouette and elbow plots, hierachical clustering can also be evaluated with a dendogram. But hierarchical clusters has one significant drawback: it is computationally complex compared to k-means. For this last reason, k-means is more common. "],
["text-mining.html", "Chapter 13 Text Mining", " Chapter 13 Text Mining Text mining may be thought of as the process of distilling actionable insights from text, typically by identifying patterns with statistical pattern learning. Typical text mining tasks include text categorization, sentiment analysis, and topic modeling. There are two approaches to text mining. Semantic parsing identifies words by type and order (sentences, phrases, nouns/verbs, proper nouns, etc.). Bag of Words treats words as simply attributes of the document. Bag of words is obviously the simpler way to go. The qdap package provides parsing tools for preparing transcript data. library(qdap) For example, freq_terms() parses text and counts the terms. freq_terms(DATA$state, top = 5) ## WORD FREQ ## 1 you 4 ## 2 we 3 ## 3 fun 2 ## 4 i 2 ## 5 is 2 ## 6 it&#39;s 2 ## 7 no 2 ## 8 not 2 ## 9 what 2 You can also plot the terms. plot(freq_terms(DATA$state, top = 5)) There are two kinds of the corpus data types, the permanent corpus, PCorpus, and the volatile corpus, VCorpus. The volatile corpus is held in RAM rather than saved to disk. Create a volatile corpus with tm::vCorpous(). vCorpous() takes either a text source created with tm::VectorSource() or a dataframe source created with Dataframe Source() where the input dataframe has cols doc_id, text_id and zero or more metadata columns. tweets &lt;- read_csv(file = &quot;https://assets.datacamp.com/production/repositories/19/datasets/27a2a8587eff17add54f4ba288e770e235ea3325/coffee.csv&quot;) ## Parsed with column specification: ## cols( ## num = col_double(), ## text = col_character(), ## favorited = col_logical(), ## replyToSN = col_character(), ## created = col_character(), ## truncated = col_logical(), ## replyToSID = col_double(), ## id = col_double(), ## replyToUID = col_double(), ## statusSource = col_character(), ## screenName = col_character(), ## retweetCount = col_double(), ## retweeted = col_logical(), ## longitude = col_logical(), ## latitude = col_logical() ## ) coffee_tweets &lt;- tweets$text coffee_source &lt;- VectorSource(coffee_tweets) coffee_corpus &lt;- VCorpus(coffee_source) # Text of first tweet coffee_corpus[[1]][1] ## $content ## [1] &quot;@ayyytylerb that is so true drink lots of coffee&quot; In bag of words text mining, cleaning helps aggregate terms, especially words with common stems like “miner” and “mining”. There are several functions useful for preprocessing: tolower(), tm::removePunctuation(), tm::removeNumbers(), tm::stripWhiteSpace(), and removeWords(). Apply these functions to the documents in a VCorpus object with tm_map(). If the function is not one of the pre-defined functions, wrap it in content_transformer(). Another preprocessing function is stemDocument(). # Create the object: text text &lt;- &quot;&lt;b&gt;She&lt;/b&gt; woke up at 6 A.M. It\\&#39;s so early! She was only 10% awake and began drinking coffee in front of her computer.&quot; tolower(text) ## [1] &quot;&lt;b&gt;she&lt;/b&gt; woke up at 6 a.m. it&#39;s so early! she was only 10% awake and began drinking coffee in front of her computer.&quot; removePunctuation(text) ## [1] &quot;bSheb woke up at 6 AM Its so early She was only 10 awake and began drinking coffee in front of her computer&quot; removeNumbers(text) ## [1] &quot;&lt;b&gt;She&lt;/b&gt; woke up at A.M. It&#39;s so early! She was only % awake and began drinking coffee in front of her computer.&quot; stripWhitespace(text) ## [1] &quot;&lt;b&gt;She&lt;/b&gt; woke up at 6 A.M. It&#39;s so early! She was only 10% awake and began drinking coffee in front of her computer.&quot; The qdap package offers other preprocessing functions. text &lt;- &quot;&lt;b&gt;She&lt;/b&gt; woke up at 6 A.M. It\\&#39;s so early! She was only 10% awake and began drinking coffee in front of her computer.&quot; bracketX(text) ## [1] &quot;She woke up at 6 A.M. It&#39;s so early! She was only 10% awake and began drinking coffee in front of her computer.&quot; replace_number(text) ## [1] &quot;&lt;b&gt;She&lt;/b&gt; woke up at six A.M. It&#39;s so early! She was only ten% awake and began drinking coffee in front of her computer.&quot; replace_abbreviation(text) ## [1] &quot;&lt;b&gt;She&lt;/b&gt; woke up at 6 AM It&#39;s so early! She was only 10% awake and began drinking coffee in front of her computer.&quot; replace_contraction(text) ## [1] &quot;&lt;b&gt;She&lt;/b&gt; woke up at 6 A.M. it is so early! She was only 10% awake and began drinking coffee in front of her computer.&quot; replace_symbol(text) ## [1] &quot;&lt;b&gt;She&lt;/b&gt; woke up at 6 A.M. It&#39;s so early! She was only 10 percent awake and began drinking coffee in front of her computer.&quot; tm::stopwords(\"en\") returns a vector of stop words. You can add to the list with concatenation. new_stops &lt;- c(&quot;coffee&quot;, &quot;bean&quot;, stopwords(&quot;en&quot;)) removeWords(text, new_stops) ## [1] &quot;&lt;b&gt;She&lt;/b&gt; woke 6 A.M. It&#39;s early! She 10% awake began drinking front computer.&quot; tm::stemDocument() and tm::stemCompletion() reduce the variation in terms. complicate &lt;- c(&quot;complicated&quot;, &quot;complication&quot;, &quot;complicatedly&quot;) stem_doc &lt;- stemDocument(complicate) comp_dict &lt;- c(&quot;complicate&quot;) complete_text &lt;- stemCompletion(stem_doc, comp_dict) complete_text ## complic complic complic ## &quot;complicate&quot; &quot;complicate&quot; &quot;complicate&quot; clean_corpus &lt;- function(corpus){ corpus &lt;- tm_map(corpus, removePunctuation) corpus &lt;- tm_map(corpus, content_transformer(tolower)) corpus &lt;- tm_map(corpus, removeWords, words = c(stopwords(&quot;en&quot;), &quot;coffee&quot;, &quot;mug&quot;)) corpus &lt;- tm_map(corpus, stripWhitespace) return(corpus) } clean_corp &lt;- clean_corpus(coffee_corpus) content(clean_corp[[1]]) ## [1] &quot;ayyytylerb true drink lots &quot; # compare to original content(coffee_corpus[[1]]) ## [1] &quot;@ayyytylerb that is so true drink lots of coffee&quot; To perform the analysis of the tweets, convert the corpus into either a document term matrix (DTM, documents as rows, terms as cols), or a term document matrix (TDM, terms as rows, documents as cols). coffee_tdm &lt;- TermDocumentMatrix(clean_corp) # Print coffee_tdm data coffee_tdm ## &lt;&lt;TermDocumentMatrix (terms: 3075, documents: 1000)&gt;&gt; ## Non-/sparse entries: 7384/3067616 ## Sparsity : 100% ## Maximal term length: 27 ## Weighting : term frequency (tf) # Convert coffee_tdm to a matrix coffee_m &lt;- as.matrix(coffee_tdm) # Print the dimensions of the matrix dim(coffee_m) ## [1] 3075 1000 # Review a portion of the matrix coffee_m[c(&quot;star&quot;, &quot;starbucks&quot;), 25:35] ## Docs ## Terms 25 26 27 28 29 30 31 32 33 34 35 ## star 0 0 0 0 0 0 0 0 0 0 0 ## starbucks 0 1 1 0 0 0 0 0 0 1 0 "],
["tidy-text.html", "13.1 Tidy Text", " 13.1 Tidy Text The tidy text format is a table with one token (meaningful unit of text, such as a word) per row. The tidytext package assists with the major tasks in text analysis. A typical text analysis using tidy data principles unnests tokens with unnest_tokens(), removes unimportant “stop words” tokens anti_join(stop_words), and summarizes token counts count(). Here are four Jane Austin books from the janeaustenr. “Sense &amp; Sensibility” acts as the baseline count, and the other books are faceted for comparison. Note the use of the “tribble” of custom stop words. custom_stop_words &lt;- tribble( ~word, ~lexicon, &quot;edward&quot;, &quot;CUSTOM&quot;, &quot;frank&quot;, &quot;CUSTOM&quot;, &quot;thomas&quot;, &quot;CUSTOM&quot;, &quot;fanny&quot;, &quot;CUSTOM&quot;, &quot;anne&quot;, &quot;CUSTOM&quot; ) austin_tidy &lt;- austen_books() %&gt;% group_by(book) %&gt;% mutate( linenumber = row_number(), chapter = cumsum(str_detect(text, regex(&quot;^chapter [\\\\divxlc]&quot;, ignore_case = TRUE))) ) %&gt;% ungroup() %&gt;% unnest_tokens(output = word, input = text) %&gt;% anti_join(stop_words) %&gt;% anti_join(custom_stop_words) ## Joining, by = &quot;word&quot; ## Joining, by = &quot;word&quot; austin_tidy %&gt;% count(book, word) %&gt;% group_by(book) %&gt;% mutate(proportion = n / sum(n)) %&gt;% select(-n) %&gt;% pivot_wider(names_from = book, values_from = proportion) %&gt;% pivot_longer(cols = `Pride &amp; Prejudice`:`Persuasion`, names_to = &quot;book&quot;, values_to = &quot;proportion&quot;) %&gt;% ggplot(aes(x = proportion, y = `Sense &amp; Sensibility`, color = abs(`Sense &amp; Sensibility` - proportion))) + geom_abline(color = &quot;gray40&quot;, lty = 2) + geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) + geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) + scale_x_log10(labels = percent_format()) + scale_y_log10(labels = percent_format()) + scale_color_gradient(limits = c(0, 0.001), low = &quot;darkslategray4&quot;, high = &quot;gray75&quot;) + facet_wrap(~book, ncol = 2) + theme(legend.position = &quot;none&quot;) + labs(y = &quot;Sense &amp; Sensibility&quot;, x = NULL) ## Warning: Removed 50931 rows containing missing values (geom_point). ## Warning: Removed 50931 rows containing missing values (geom_text). Words close to the 45-degree lines have similar frequencies in both books. Words far from the line are found more in one book more than the other. If there are few points near the low frequencies, then few infrequent words are shared. Emma is similar to Sense &amp; Sensibility because the points are fairly narrow on the 45-degree line, and they extend all the way to the origin. A common way to visualize words is with a word cloud. The wordcloud library is helpful. NOte that word clouds do not contain any information not already in a bar plot. austin_word_cnt &lt;- austin_tidy %&gt;% filter(book == &quot;Sense &amp; Sensibility&quot;) %&gt;% count(word) pal &lt;- brewer.pal(9,&quot;BuGn&quot;) pal &lt;- pal[-(1:4)] wordcloud( words = austin_word_cnt$word, freq = austin_word_cnt$n, max.words = 30, colors = pal ) # pa_file &lt;- readxl::read_excel(&quot;./../../PeoplaAnalyticsCloud.xlsx&quot;) # pa_tidy &lt;- pa_file %&gt;% # mutate( # linenumber = row_number(), # ) %&gt;% # ungroup() %&gt;% # unnest_tokens(output = word, input = comment) %&gt;% # # unnest_tokens(output = bigram, input = comment, token = &quot;ngrams&quot;, n = 2) # anti_join(stop_words) %&gt;% # anti_join(custom_stop_words) # pa_tidy_n &lt;- pa_tidy %&gt;% # count(word) # pal &lt;- brewer.pal(9,&quot;BuGn&quot;) # pal &lt;- pal[-(1:4)] # wordcloud( # words = pa_tidy_n$word, # freq = pa_tidy_n$n, # max.words = 30, # colors = pal # ) # # pa_2gram &lt;- pa_tidy %&gt;% # separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% # filter(!word1 %in% stop_words$word &amp; # !word2 %in% stop_words$word &amp; # !is.na(word1) &amp; !is.na(word2)) %&gt;% # unite(bigram, word1, word2, sep = &quot; &quot;) # # # pa_2gram %&gt;% # count(book, bigram, sort = TRUE) # # pa_2gram2 &lt;- pa_2gram %&gt;% # count(bigram) # pal &lt;- brewer.pal(9,&quot;BuGn&quot;) # pal &lt;- pal[-(1:4)] # wordcloud( # words = pa_2gram2$bigram, # freq = pa_2gram2$n, # max.words = 30, # colors = pal # ) "],
["bag-of-words.html", "13.2 Bag of Words", " 13.2 Bag of Words "],
["sentiment-analysis.html", "13.3 Sentiment Analysis", " 13.3 Sentiment Analysis A typical sentiment analysis involves unnesting tokens with unnest_tokens(), assigning sentiments with inner_join(sentiments), counting tokens with count(), and summarizing and visualizing. The tidytext package contains four sentiment lexicons, all based on unigrams. nrc. binary “yes”/“no” for categories positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. bing. “positive”/“negative” classification. AFINN. score between -5 (most negative) and 5 (most positive). loughran. “positive”/“negative”/“litigious”/“uncertainty”/“constraining”/“superflous” classification. You can view the sentiment assignments with get_sentiments(lexicon = c(\"afinn\", \"bing\", nrc\", \"laughlin\")) x1 &lt;- get_sentiments(lexicon = &quot;nrc&quot;) %&gt;% count(sentiment) %&gt;% mutate(lexicon = &quot;nrc&quot;) x2 &lt;- get_sentiments(lexicon = &quot;bing&quot;) %&gt;% count(sentiment) %&gt;% mutate(lexicon = &quot;bing&quot;) x3 &lt;- get_sentiments(lexicon = &quot;afinn&quot;) %&gt;% count(value) %&gt;% mutate(lexicon = &quot;afinn&quot;) %&gt;% mutate(sentiment = as.character(value)) %&gt;% select(-value) x4 &lt;- get_sentiments(lexicon = &quot;loughran&quot;) %&gt;% count(sentiment) %&gt;% mutate(lexicon = &quot;loughran&quot;) x &lt;- bind_rows(x1, x2, x3, x4) ggplot(x, aes(x = fct_reorder(sentiment, n), y = n, fill = lexicon)) + geom_col(show.legend = FALSE) + coord_flip() + labs(title = &quot;Sentiment Counts&quot;, x = &quot;&quot;, y = &quot;&quot;) + facet_wrap(~ lexicon, scales = &quot;free&quot;) Here is a sentiment analysis of sections of 80 lines of Jane Austin’s books. (Small sections may not have enough words to get a good estimate of sentiment, and large sections can wash out the narrative structure. 80 lines seems about right.) austin_tidy %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% count(book, index = linenumber %/% 80, sentiment) %&gt;% pivot_wider(names_from = sentiment, values_from = n, values_fill = list(n = 0)) %&gt;% mutate(sentiment = positive - negative) %&gt;% ggplot(aes(x = index, y = sentiment, fill = book)) + geom_col(show.legend = FALSE) + facet_wrap(~book, ncol = 2, scales = &quot;free_x&quot;) ## Joining, by = &quot;word&quot; Fair to say Jane Austin novels tend to have a happy ending? The three sentiment lexicons provide different views of THE data. Here is a comparison of the lexicons using one of Jane Austin’s novels, “Pride and Prejudice”. # AFINN lexicon measures sentiment with a numeric score between -5 and 5. afinn &lt;- austin_tidy %&gt;% filter(book == &quot;Pride &amp; Prejudice&quot;) %&gt;% inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;% group_by(index = linenumber %/% 80) %&gt;% summarise(sentiment = sum(value)) %&gt;% mutate(method = &quot;AFINN&quot;) ## `summarise()` ungrouping output (override with `.groups` argument) # Bing and nrc categorize words in a binary fashion, either positive or negative. bing &lt;- austin_tidy %&gt;% filter(book == &quot;Pride &amp; Prejudice&quot;) %&gt;% inner_join(get_sentiments(&quot;bing&quot;), by = &quot;word&quot;) %&gt;% count(index = linenumber %/% 80, sentiment) %&gt;% pivot_wider(names_from = sentiment, values_from = n, values_fill = list(n = 0)) %&gt;% mutate(sentiment = positive - negative) %&gt;% mutate(method = &quot;Bing&quot;) %&gt;% select(index, sentiment, method) nrc &lt;- austin_tidy %&gt;% filter(book == &quot;Pride &amp; Prejudice&quot;) %&gt;% inner_join(get_sentiments(&quot;nrc&quot;) %&gt;% filter(sentiment %in% c(&quot;positive&quot;, &quot;negative&quot;)), by = &quot;word&quot;) %&gt;% count(index = linenumber %/% 80, sentiment) %&gt;% pivot_wider(names_from = sentiment, values_from = n, values_fill = list(n = 0)) %&gt;% mutate(sentiment = positive - negative) %&gt;% mutate(method = &quot;NRC&quot;) %&gt;% select(index, sentiment, method) bind_rows(afinn, bing, nrc) %&gt;% ggplot(aes(index, sentiment, fill = method)) + geom_col(show.legend = FALSE) + facet_wrap(~method, ncol = 1, scales = &quot;free_y&quot;) In this example, and in general, NRC sentiment tends to be high, AFINN sentiment has more variance, and Bing sentiment finds longer stretches of similar text. However, all three agree roughly on the overall trends in the sentiment through a narrative arc. What are the top-10 positive and negative words? Using the Bing lexicon, get the counts, then group_by(sentiment) and top_n() to the top 10 in each category. austin_tidy %&gt;% filter(book == &quot;Pride &amp; Prejudice&quot;) %&gt;% inner_join(get_sentiments(&quot;bing&quot;), by = &quot;word&quot;) %&gt;% count(word, sentiment, sort = TRUE) %&gt;% group_by(sentiment) %&gt;% top_n(n = 10, wt = n) %&gt;% ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales = &quot;free_y&quot;) + coord_flip() + labs(y = &quot;Contribution to Sentiment&quot;, x = &quot;&quot;) Uh oh, “miss” is a red-herring - in Jane Austin novels it often refers to an unmarried woman. Drop it from the analysis by appending it to the stop-words list. austin_tidy %&gt;% anti_join(bind_rows(stop_words, tibble(word = c(&quot;miss&quot;), lexicon = c(&quot;custom&quot;)))) %&gt;% filter(book == &quot;Pride &amp; Prejudice&quot;) %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% count(word, sentiment, sort = TRUE) %&gt;% group_by(sentiment) %&gt;% top_n(n = 10, wt = n) %&gt;% ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales = &quot;free_y&quot;) + coord_flip() + labs(y = &quot;Contribution to Sentiment&quot;, x = &quot;&quot;) ## Joining, by = &quot;word&quot; ## Joining, by = &quot;word&quot; Better! A common way to visualize sentiments is with a word cloud. austin_tidy %&gt;% anti_join(bind_rows(stop_words, tibble(word = c(&quot;miss&quot;), lexicon = c(&quot;custom&quot;)))) %&gt;% filter(book == &quot;Pride &amp; Prejudice&quot;) %&gt;% count(word) %&gt;% with(wordcloud(word, n, max.words = 100)) ## Joining, by = &quot;word&quot; comparison.cloud is another implementation of a word cloud. It takes a matrix input. x &lt;- austin_tidy %&gt;% anti_join(bind_rows(stop_words, tibble(word = c(&quot;miss&quot;), lexicon = c(&quot;custom&quot;)))) %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% filter(book == &quot;Pride &amp; Prejudice&quot;) %&gt;% count(word, sentiment, sort = TRUE) %&gt;% pivot_wider(names_from = sentiment, values_from = n, values_fill = list(n = 0)) %&gt;% as.data.frame() ## Joining, by = &quot;word&quot; ## Joining, by = &quot;word&quot; rownames(x) &lt;- x[,1] comparison.cloud(x[, 2:3]) ## Warning in comparison.cloud(x[, 2:3]): agreeable could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): pleased could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): handsome could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): amiable could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): advantage could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): satisfied could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): instantly could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): astonishment could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): respect could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): compliment could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): exceedingly could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): admiration could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): master could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): pretty could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): delighted could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): fair could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): favour could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): pleasing could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): praise could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): proud could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): pleasant could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): promised could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): charming could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): excellent could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): proper could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): ready could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): disappointment could not be fit on page. ## It will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): indifference could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): resentment could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): delightful could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): fancy could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): intelligence could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): strong could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): confess could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): distress could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): excuse could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): ignorant could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): admire could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): capable could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): felicity could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): superior could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): objection could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): wrong could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): affectionate could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): ease could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): perfect could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): politeness could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): promise could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): readily could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): disagreeable could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): misery could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): painful could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): extraordinary could not be fit on page. ## It will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): pardon could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): distressed could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): interference could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): partiality could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): unhappy could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): worse could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): assurance could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): beautiful could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): earnest could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): worth could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): apprehension could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): contempt could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): disappointed could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): astonished could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): compassion could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): eagerly could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): elegant could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): gratified could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): reasonable could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): respectable could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): tolerable could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): eager could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): enjoyment could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): grateful could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): intimate could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): relief could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): secure could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): confusion could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): difficult could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): disgrace could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): dreadful could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): excessively could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): imprudent could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): neglect could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): objections could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): accomplished could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): comfortable could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): congratulations could not be fit on page. ## It will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): courage could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): generous could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): recommend could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): steady could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): awkward could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): debts could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): embarrassment could not be fit on page. ## It will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): impatience could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): insufficient could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): interruption could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): shame could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): strange could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): unlucky could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): unwilling could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): vexation could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): wretched could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): confidence could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): delicacy could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): earnestly could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): gained could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): noble could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): satisfy could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): fears could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): haste could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): hastily could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): imprudence could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): misfortune could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): mistress could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): refuse could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): suspicion could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): violent could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): attentive could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): encouragement could not be fit on page. ## It will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): excited could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): flatter could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): goodness could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): heartily could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): heaven could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): intimacy could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): rational could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): remarkably could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): sincere could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): smiles could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): smiling could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): triumph could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): virtue could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): alarmed could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): concerns could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): death could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): deny could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): difficulty could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): evils could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): foolish could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): impatient could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): incapable could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): indignation could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): mistake could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): nonsense could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): protested could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): reproach could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): severe could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): shocked could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): solicitude could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): suffered could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): uneasy could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): unexpected could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): unknown could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): unpleasant could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): elegance could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): fresh could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): humble could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): improvement could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): patience could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): recommended could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): sincerely could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): valuable could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): broken could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): condescension could not be fit on page. ## It will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): disapprobation could not be fit on page. ## It will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): falsehood could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): fault could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): hopeless could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): ignorance could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): improbable could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): refusal could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): refusing could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): selfish could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): temptation could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): uncomfortable could not be fit on page. ## It will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): uneasiness could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): unreasonable could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): weak could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): worst could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): assurances could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): eagerness could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): enjoy could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): gracious could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): indebted could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): openly could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): properly could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): quiet could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): thankful could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): warmly could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): warmth could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): abominable could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): delayed could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): disdain could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): displeasure could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): distressing could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): hurt could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): impertinent could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): inconvenience could not be fit on page. ## It will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): miserable could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): mortification could not be fit on page. ## It will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): mortifying could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): prejudice could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): refused could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): resigned could not be fit on page. It ## will not be plotted. ## Warning in comparison.cloud(x[, 2:3]): sick could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): useless could not be fit on page. It will ## not be plotted. ## Warning in comparison.cloud(x[, 2:3]): violently could not be fit on page. It ## will not be plotted. rm(x) Sometimes it makes more sense to analyze entire sentences. Specify unnest_tokens(..., token = \"sentences\") to override the default token = \"word\". austen_books() %&gt;% group_by(book) %&gt;% mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex(&quot;^chapter [\\\\divxlc]&quot;, ignore_case = TRUE)))) %&gt;% ungroup() %&gt;% unnest_tokens(output = word, input = text, token = &quot;sentences&quot;) ## # A tibble: 87,906 x 4 ## book linenumber chapter word ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 Sense &amp; Sensi~ 1 0 sense and sensibility ## 2 Sense &amp; Sensi~ 3 0 by jane austen ## 3 Sense &amp; Sensi~ 5 0 (1811) ## 4 Sense &amp; Sensi~ 10 1 chapter 1 ## 5 Sense &amp; Sensi~ 13 1 the family of dashwood had long been settl~ ## 6 Sense &amp; Sensi~ 13 1 their estate ## 7 Sense &amp; Sensi~ 14 1 was large, and their residence was at norl~ ## 8 Sense &amp; Sensi~ 15 1 their property, where, for many generation~ ## 9 Sense &amp; Sensi~ 16 1 respectable a manner as to engage the gene~ ## 10 Sense &amp; Sensi~ 17 1 surrounding acquaintance. ## # ... with 87,896 more rows 13.3.1 N-Grams Create n-grams by specifying unnest_tokens(..., token = \"ngrams\", n) where n = 2 is a bigram, etc. To remove the stop words, separate the n-grams, then filter on the stop_words data set. austin.2gram &lt;- austen_books() %&gt;% group_by(book) %&gt;% mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex(&quot;^chapter [\\\\divxlc]&quot;, ignore_case = TRUE)))) %&gt;% ungroup() %&gt;% unnest_tokens(output = bigram, input = text, token = &quot;ngrams&quot;, n = 2) austin.2gram &lt;- austin.2gram %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(!word1 %in% stop_words$word &amp; !word2 %in% stop_words$word &amp; !is.na(word1) &amp; !is.na(word2)) %&gt;% unite(bigram, word1, word2, sep = &quot; &quot;) austin.2gram %&gt;% count(book, bigram, sort = TRUE) ## # A tibble: 31,391 x 3 ## book bigram n ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 Mansfield Park sir thomas 266 ## 2 Mansfield Park miss crawford 196 ## 3 Emma miss woodhouse 143 ## 4 Persuasion captain wentworth 143 ## 5 Emma frank churchill 114 ## 6 Persuasion lady russell 110 ## 7 Persuasion sir walter 108 ## 8 Mansfield Park lady bertram 101 ## 9 Emma miss fairfax 98 ## 10 Sense &amp; Sensibility colonel brandon 96 ## # ... with 31,381 more rows Here are the most commonly mentioned streets in Austin’s novels. austin.2gram %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(word2 == &quot;street&quot;) %&gt;% count(book, word1, sort = TRUE) ## # A tibble: 33 x 3 ## book word1 n ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 Sense &amp; Sensibility harley 16 ## 2 Sense &amp; Sensibility berkeley 15 ## 3 Northanger Abbey milsom 10 ## 4 Northanger Abbey pulteney 10 ## 5 Mansfield Park wimpole 9 ## 6 Pride &amp; Prejudice gracechurch 8 ## 7 Persuasion milsom 5 ## 8 Sense &amp; Sensibility bond 4 ## 9 Sense &amp; Sensibility conduit 4 ## 10 Persuasion rivers 4 ## # ... with 23 more rows Bind the tf-idf statistics. Tf-idf is short for term frequency–inverse document frequency. It is a statistic that indicates how important a word is to a document in a collection or corpus. Tf–idf increases with the number of times a word appears in the document and decreases with the number of documents in the corpus that contain the word. The idf adjusts for the fact that some words appear more frequently in general. austen_books() %&gt;% group_by(book) %&gt;% mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex(&quot;^chapter [\\\\divxlc]&quot;, ignore_case = TRUE)))) %&gt;% ungroup() %&gt;% unnest_tokens(output = bigram, input = text, token = &quot;ngrams&quot;, n = 2) %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(!word1 %in% stop_words$word &amp; !word2 %in% stop_words$word &amp; !is.na(word1) &amp; !is.na(word2)) %&gt;% unite(bigram, word1, word2, sep = &quot; &quot;) ## # A tibble: 38,913 x 4 ## book linenumber chapter bigram ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 Sense &amp; Sensibility 3 0 jane austen ## 2 Sense &amp; Sensibility 10 1 chapter 1 ## 3 Sense &amp; Sensibility 14 1 norland park ## 4 Sense &amp; Sensibility 17 1 surrounding acquaintance ## 5 Sense &amp; Sensibility 17 1 late owner ## 6 Sense &amp; Sensibility 18 1 advanced age ## 7 Sense &amp; Sensibility 19 1 constant companion ## 8 Sense &amp; Sensibility 20 1 happened ten ## 9 Sense &amp; Sensibility 22 1 henry dashwood ## 10 Sense &amp; Sensibility 23 1 norland estate ## # ... with 38,903 more rows austin.2gram %&gt;% count(book, bigram) %&gt;% bind_tf_idf(bigram, book, n) %&gt;% group_by(book) %&gt;% top_n(n = 10, wt = tf_idf) %&gt;% ggplot(aes(x = fct_reorder(bigram, n), y = tf_idf, fill = book)) + geom_col(show.legend = FALSE) + facet_wrap(~book, scales = &quot;free_y&quot;, ncol = 2) + labs(y = &quot;tf-idf of bigram to novel&quot;) + coord_flip() A good way to visualize bigrams is with a network graph. Packages igraph and ggraph provides tools for this purpose. set.seed(2016) austen_books() %&gt;% group_by(book) %&gt;% mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex(&quot;^chapter [\\\\divxlc]&quot;, ignore_case = TRUE)))) %&gt;% ungroup() %&gt;% unnest_tokens(output = bigram, input = text, token = &quot;ngrams&quot;, n = 2) %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(!word1 %in% stop_words$word &amp; !word2 %in% stop_words$word &amp; !is.na(word1) &amp; !is.na(word2)) %&gt;% count(word1, word2) %&gt;% filter(n &gt; 20) %&gt;% graph_from_data_frame() %&gt;% # creates unformatted &quot;graph&quot; ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = grid::arrow(type = &quot;closed&quot;, length = unit(.15, &quot;inches&quot;)), end_cap = circle(.07, &#39;inches&#39;)) + geom_node_point(color = &quot;lightblue&quot;, size = 5) + geom_node_text(aes(label = name), vjust = 1, hjust = 1) + theme_void() If you want to count the number of times that two words appear within the same document, or to see how correlated they are, widen the data with the widyr package. austen_books() %&gt;% filter(book == &quot;Pride &amp; Prejudice&quot;) %&gt;% mutate(section = row_number() %/% 10) %&gt;% filter(section &gt; 0) %&gt;% unnest_tokens(word, text) %&gt;% filter(!word %in% stop_words$word) %&gt;% pairwise_count(word, section, sort = TRUE) ## Warning: `distinct_()` is deprecated as of dplyr 0.7.0. ## Please use `distinct()` instead. ## See vignette(&#39;programming&#39;) for more help ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. ## Warning: `tbl_df()` is deprecated as of dplyr 1.0.0. ## Please use `tibble::as_tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. ## # A tibble: 796,008 x 3 ## item1 item2 n ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 darcy elizabeth 144 ## 2 elizabeth darcy 144 ## 3 miss elizabeth 110 ## 4 elizabeth miss 110 ## 5 elizabeth jane 106 ## 6 jane elizabeth 106 ## 7 miss darcy 92 ## 8 darcy miss 92 ## 9 elizabeth bingley 91 ## 10 bingley elizabeth 91 ## # ... with 795,998 more rows The correlation among words is how often they appear together relative to how often they appear separately. The phi coefficient is defined \\[\\phi = \\frac{n_{11}n_{00} - n_{10}n_{01}}{\\sqrt{n_{1.}n_{0.}n_{.1}n_{.0}}}\\] where \\(n_{10}\\) means number of times section has word x, but not word y, and \\(n_{1.}\\) means total times section has word x. This lets us pick particular interesting words and find the other words most associated with them. austen_books() %&gt;% filter(book == &quot;Pride &amp; Prejudice&quot;) %&gt;% mutate(section = row_number() %/% 10) %&gt;% filter(section &gt; 0) %&gt;% unnest_tokens(word, text) %&gt;% filter(!word %in% stop_words$word) %&gt;% pairwise_cor(word, section, sort = TRUE) %&gt;% filter(item1 %in% c(&quot;elizabeth&quot;, &quot;pounds&quot;, &quot;married&quot;, &quot;pride&quot;)) %&gt;% group_by(item1) %&gt;% top_n(n = 4) %&gt;% ungroup() %&gt;% mutate(item2 = reorder(item2, correlation)) %&gt;% ggplot(aes(x = item2, y = correlation)) + geom_bar(stat = &quot;identity&quot;) + facet_wrap(~ item1, scales = &quot;free&quot;) + coord_flip() ## Selecting by correlation You can use the correlation to set a threshold for a graph. set.seed(2016) austen_books() %&gt;% filter(book == &quot;Pride &amp; Prejudice&quot;) %&gt;% mutate(section = row_number() %/% 10) %&gt;% filter(section &gt; 0) %&gt;% unnest_tokens(word, text) %&gt;% filter(!word %in% stop_words$word) %&gt;% # filter to relatively common words group_by(word) %&gt;% filter(n() &gt; 20) %&gt;% pairwise_cor(word, section, sort = TRUE) %&gt;% filter(correlation &gt; .15) %&gt;% # relatively correlated words graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) + geom_node_point(color = &quot;lightblue&quot;, size = 5) + geom_node_text(aes(label = name), repel = TRUE) + theme_void() 13.3.2 Converting to and from non-tidy formats One of the most common objects in text mining packages is the document term matrix (DTM) where each row is a document, each column a term, and each value an appearance count. The broom package contains functions to convert between DTM and tidy formats. Convert a DTM object into a tidy data frame with tidy(). Convert a tidy object into a sparse matrix with cast_sparse(), into a DTM with cast_dtm(), and into a “dfm” for quanteda with cast_dfm(). data(&quot;AssociatedPress&quot;, package = &quot;topicmodels&quot;) AssociatedPress ## &lt;&lt;DocumentTermMatrix (documents: 2246, terms: 10473)&gt;&gt; ## Non-/sparse entries: 302031/23220327 ## Sparsity : 99% ## Maximal term length: 18 ## Weighting : term frequency (tf) Terms(AssociatedPress) %&gt;% head() ## [1] &quot;aaron&quot; &quot;abandon&quot; &quot;abandoned&quot; &quot;abandoning&quot; &quot;abbott&quot; ## [6] &quot;abboud&quot; Create a tidy version of AssociatedPress with tidy(). ap_td &lt;- tidy(AssociatedPress) ap_td %&gt;% inner_join(get_sentiments(&quot;bing&quot;), by = c(term = &quot;word&quot;)) %&gt;% count(sentiment, term, wt = count) %&gt;% ungroup() %&gt;% filter(n &gt;= 200) %&gt;% mutate(n = ifelse(sentiment == &quot;negative&quot;, -n, n)) %&gt;% arrange(n) %&gt;% ggplot(aes(x = fct_inorder(term), y = n, fill = sentiment)) + geom_bar(stat = &quot;identity&quot;) + labs(title = &quot;Most Common AP Sentiment Words&quot;, x = &quot;&quot;, y = &quot;Contribution to Sentiment&quot;) + theme(legend.position = &quot;top&quot;, legend.title = element_blank()) + coord_flip() The document-feature matrix dfm class from the quanteda text-mining package is another implementation of a document-term matrix. Here are the terms most specific (highest tf-idf) from each of four selected inaugural addresses. data(&quot;data_corpus_inaugural&quot;, package = &quot;quanteda&quot;) inaug_dfm &lt;- quanteda::dfm(data_corpus_inaugural, verbose = FALSE) head(inaug_dfm) ## Document-feature matrix of: 6 documents, 9,360 features (93.8% sparse) and 4 docvars. ## features ## docs fellow-citizens of the senate and house representatives : ## 1789-Washington 1 71 116 1 48 2 2 1 ## 1793-Washington 0 11 13 0 2 0 0 1 ## 1797-Adams 3 140 163 1 130 0 2 0 ## 1801-Jefferson 2 104 130 0 81 0 0 1 ## 1805-Jefferson 0 101 143 0 93 0 0 0 ## 1809-Madison 1 69 104 0 43 0 0 0 ## features ## docs among vicissitudes ## 1789-Washington 1 1 ## 1793-Washington 0 0 ## 1797-Adams 4 0 ## 1801-Jefferson 1 0 ## 1805-Jefferson 7 0 ## 1809-Madison 0 0 ## [ reached max_nfeat ... 9,350 more features ] inaug_td &lt;- tidy(inaug_dfm) head(inaug_td) ## # A tibble: 6 x 3 ## document term count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1789-Washington fellow-citizens 1 ## 2 1797-Adams fellow-citizens 3 ## 3 1801-Jefferson fellow-citizens 2 ## 4 1809-Madison fellow-citizens 1 ## 5 1813-Madison fellow-citizens 1 ## 6 1817-Monroe fellow-citizens 5 inaug_td %&gt;% bind_tf_idf(term = term, document = document, n = count) %&gt;% group_by(document) %&gt;% top_n(n = 10, wt = tf_idf) %&gt;% ungroup() %&gt;% filter(document %in% c(&quot;1861-Lincoln&quot;, &quot;1933-Roosevelt&quot;, &quot;1961-Kennedy&quot;, &quot;2009-Obama&quot;)) %&gt;% arrange(document, desc(tf_idf)) %&gt;% ggplot(aes(x = fct_rev(fct_inorder(term)), y = tf_idf, fill = document)) + geom_col() + labs(x = &quot;&quot;) + theme(legend.position = &quot;none&quot;) + coord_flip() + facet_wrap(~document, ncol = 2, scales = &quot;free&quot;) And here is word frequency trend ocer time for six selected terms. (problem with extract() below). # inaug_td %&gt;% # extract(document, &quot;year&quot;, &quot;(\\\\d+)&quot;, convert = TRUE) %&gt;% # complete(year, term, fill = list(count = 0)) %&gt;% # group_by(year) %&gt;% # mutate(year_total = sum(count)) %&gt;% # filter(term %in% c(&quot;god&quot;, &quot;america&quot;, &quot;foreign&quot;, &quot;union&quot;, &quot;constitution&quot;, &quot;freedom&quot;)) %&gt;% # ggplot(aes(x = year, y = count / year_total)) + # geom_point() + # geom_smooth() + # facet_wrap(~ term, scales = &quot;free_y&quot;) + # scale_y_continuous(labels = scales::percent_format()) + # labs(y = &quot;&quot;, # title = &quot;% frequency of word in inaugural address&quot;) Cast tidy data into document-term matrix with cast_dtm(), quanteda’s dfm with cast_dfm(), and sparese matrix with cast_sparse(). inaug_dtm &lt;- cast_dtm(data = inaug_td, document = document, term = term, value = count) inaug_dfm &lt;- cast_dfm(data = inaug_td, document = document, term = term, value = count) inaug_sparse &lt;- cast_sparse(data = inaug_td, row = document, column = term, value = count) An untokenized document collection is called a corpus. The corpuse may include metadata, such as ID, date/time, title, language, etc. Corpus metadata is usually stored as lists. Use tidy() to construct a table, one row per document. data(&quot;acq&quot;) print(acq) ## &lt;&lt;VCorpus&gt;&gt; ## Metadata: corpus specific: 0, document level (indexed): 0 ## Content: documents: 50 acq[[1]] # first document ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 15 ## Content: chars: 1287 acq_td &lt;- tidy(acq) acq_td ## # A tibble: 50 x 16 ## author datetimestamp description heading id language origin topics ## &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &lt;NA&gt; 1987-02-26 10:18:06 &quot;&quot; COMPUT~ 10 en Reute~ YES ## 2 &lt;NA&gt; 1987-02-26 10:19:15 &quot;&quot; OHIO M~ 12 en Reute~ YES ## 3 &lt;NA&gt; 1987-02-26 10:49:56 &quot;&quot; MCLEAN~ 44 en Reute~ YES ## 4 By Ca~ 1987-02-26 10:51:17 &quot;&quot; CHEMLA~ 45 en Reute~ YES ## 5 &lt;NA&gt; 1987-02-26 11:08:33 &quot;&quot; &lt;COFAB~ 68 en Reute~ YES ## 6 &lt;NA&gt; 1987-02-26 11:32:37 &quot;&quot; INVEST~ 96 en Reute~ YES ## 7 By Pa~ 1987-02-26 11:43:13 &quot;&quot; AMERIC~ 110 en Reute~ YES ## 8 &lt;NA&gt; 1987-02-26 11:59:25 &quot;&quot; HONG K~ 125 en Reute~ YES ## 9 &lt;NA&gt; 1987-02-26 12:01:28 &quot;&quot; LIEBER~ 128 en Reute~ YES ## 10 &lt;NA&gt; 1987-02-26 12:08:27 &quot;&quot; GULF A~ 134 en Reute~ YES ## # ... with 40 more rows, and 8 more variables: lewissplit &lt;chr&gt;, ## # cgisplit &lt;chr&gt;, oldid &lt;chr&gt;, places &lt;named list&gt;, people &lt;lgl&gt;, orgs &lt;lgl&gt;, ## # exchanges &lt;lgl&gt;, text &lt;chr&gt; 13.3.3 Example Library tm.plugin.webmining connects to online feeds to retrieve news articles based on a keyword. library(tm.plugin.webmining) ## Warning: package &#39;tm.plugin.webmining&#39; was built under R version 4.0.2 ## ## Attaching package: &#39;tm.plugin.webmining&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract ## The following object is masked from &#39;package:base&#39;: ## ## parse library(purrr) company &lt;- c(&quot;Progressive&quot;, &quot;Microsoft&quot;, &quot;Apple&quot;) symbol &lt;- c(&quot;PRG&quot;, &quot;MSFT&quot;, &quot;AAPL&quot;) download_articles &lt;- function(symbol) { WebCorpus(GoogleFinanceSource(paste0(&quot;NASDAQ:&quot;, symbol))) } #stock_articles &lt;- tibble(company = company, # symbol = symbol) %&gt;% # mutate(corpus = map(symbol, download_articles)) #download_articles(&quot;MSFT&quot;) #?GoogleFinanceSource() #corpus &lt;- Corpus(GoogleFinanceSource(&quot;NASDAQ:MSFT&quot;)) "],
["topic-modeling.html", "13.4 Topic Modeling", " 13.4 Topic Modeling Topic modeling is a method for unsupervised classification of documents, similar to clustering on numeric data, which finds natural groups of items. Latent Dirichlet allocation (LDA) is a popular topic modeling algorithm. LDA treats each document as a mixture of topics (X% topic A, Y% topic B, etc.), and each topic as a mixture of words. Each topic is a collection of word probabilities for all of the unique words used in the corpus. LDA is implemented in the topicmodels package. library(topicmodels) Create a topic model with the LDA function. Parameter k specifieds the number of topics. Here is an example using the AssociatedPress data set. data(&quot;AssociatedPress&quot;) ap_lda &lt;- LDA(AssociatedPress, k = 2, control = list(seed = 1234)) The tidytext package provides a tidy method for extracting the per-topic/word probabilities, called \\(\\beta\\) from the model. library(tidytext) ap_topics &lt;- tidy(ap_lda, matrix = &quot;beta&quot;) ap_topics %&gt;% arrange(-beta) ## # A tibble: 20,946 x 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 percent 0.00981 ## 2 2 i 0.00705 ## 3 1 million 0.00684 ## 4 1 new 0.00594 ## 5 1 year 0.00575 ## 6 2 president 0.00489 ## 7 2 government 0.00452 ## 8 1 billion 0.00427 ## 9 2 people 0.00407 ## 10 2 soviet 0.00372 ## # ... with 20,936 more rows The tidied format lends itself to plotting. ap_topics %&gt;% group_by(topic) %&gt;% top_n(n = 10, wt = beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) %&gt;% mutate(term = reorder_within(term, beta, topic)) %&gt;% ggplot(aes(x = term, y = beta, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free&quot;) + coord_flip() + scale_x_reordered() Topic 1 appears to be related to the economy; topic 2 to politics. What is the right number of topics? That’s a matter of subjectivity, but when the topics appear to be duplicative, then you’ve modeled too many topics. Another way to look at the data is to identify terms that had the greatest difference in beta between topic 1 and topic 2. A good way to do this is with the log ratio of the two, \\(log_2(\\beta_2 / \\beta_1)\\). Log ratios are useful because the differences are symmetrical (\\(log_2(2) = 1\\), and \\(log_2(.5) = -1\\)). To constrain the analysis to a set of especially relevant words, filter for relatively common words having a beta greater than 1/1000 in at least one topic. ap_topics %&gt;% mutate(topic = paste0(&quot;topic&quot;, topic)) %&gt;% pivot_wider(names_from = topic, values_from = beta) %&gt;% filter(topic1 &gt; 0.001 | topic2 &gt; 0.001) %&gt;% mutate(log_ratio = log2(topic2 / topic1)) %&gt;% top_n(n = 20, w = abs(log_ratio)) %&gt;% arrange(-log_ratio) %&gt;% ggplot(aes(x = fct_rev(fct_inorder(term)), y = log_ratio)) + geom_col() + coord_flip() Examine the per-document-per-topic probabilities, called gamma with the matrix = \"gamma\" argument to tidy(). (ap_documents &lt;- tidy(ap_lda, matrix = &quot;gamma&quot;)) ## # A tibble: 4,492 x 3 ## document topic gamma ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 0.248 ## 2 2 1 0.362 ## 3 3 1 0.527 ## 4 4 1 0.357 ## 5 5 1 0.181 ## 6 6 1 0.000588 ## 7 7 1 0.773 ## 8 8 1 0.00445 ## 9 9 1 0.967 ## 10 10 1 0.147 ## # ... with 4,482 more rows As an example, use topic modeling to see whether the chapters for four books cluster into the right books. library(gutenbergr) books &lt;- gutenberg_works(title %in% c(&quot;Twenty Thousand Leagues under the Sea&quot;, &quot;The War of the Worlds&quot;, &quot;Pride and Prejudice&quot;, &quot;Great Expectations&quot;)) %&gt;% gutenberg_download(meta_fields = &quot;title&quot;) ## Warning: `filter_()` is deprecated as of dplyr 0.7.0. ## Please use `filter()` instead. ## See vignette(&#39;programming&#39;) for more help ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. ## Determining mirror for Project Gutenberg from http://www.gutenberg.org/robot/harvest ## Using mirror http://aleph.gutenberg.org by_chapter &lt;- books %&gt;% group_by(title) %&gt;% mutate(chapter = cumsum(str_detect(text, regex(&quot;^chapter &quot;, ignore_case = TRUE)))) %&gt;% ungroup() %&gt;% filter(chapter &gt; 0) %&gt;% unite(document, title, chapter) by_chapter_word &lt;- by_chapter %&gt;% unnest_tokens(output = word, input = text, token = &quot;words&quot;) word_counts &lt;- by_chapter_word %&gt;% anti_join(stop_words) %&gt;% count(document, word, sort = TRUE) %&gt;% ungroup() ## Joining, by = &quot;word&quot; word_counts ## # A tibble: 104,722 x 3 ## document word n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Great Expectations_57 joe 88 ## 2 Great Expectations_7 joe 70 ## 3 Great Expectations_17 biddy 63 ## 4 Great Expectations_27 joe 58 ## 5 Great Expectations_38 estella 58 ## 6 Great Expectations_2 joe 56 ## 7 Great Expectations_23 pocket 53 ## 8 Great Expectations_15 joe 50 ## 9 Great Expectations_18 joe 50 ## 10 The War of the Worlds_16 brother 50 ## # ... with 104,712 more rows The topmodels library requires DocumentTermMatrix objects, so cast word_counts. chapters_dtm &lt;- word_counts %&gt;% cast_dtm(document = document, term = word, value = n) chapters_dtm ## &lt;&lt;DocumentTermMatrix (documents: 193, terms: 18215)&gt;&gt; ## Non-/sparse entries: 104722/3410773 ## Sparsity : 97% ## Maximal term length: 19 ## Weighting : term frequency (tf) Create a four-topic model. chapters_lda &lt;- LDA(chapters_dtm, k = 4, control = list(seed = 1234)) chapters_lda ## A LDA_VEM topic model with 4 topics. What are the per-topic/word probabilities? chapter_topics &lt;- tidy(chapters_lda, matrix = &quot;beta&quot;) chapter_topics ## # A tibble: 72,860 x 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 joe 1.44e-17 ## 2 2 joe 5.96e-61 ## 3 3 joe 9.88e-25 ## 4 4 joe 1.45e- 2 ## 5 1 biddy 5.14e-28 ## 6 2 biddy 5.02e-73 ## 7 3 biddy 4.31e-48 ## 8 4 biddy 4.78e- 3 ## 9 1 estella 2.43e- 6 ## 10 2 estella 4.32e-68 ## # ... with 72,850 more rows For each combination, the model computes the probability of that term being generated from that topic. The top 5 terms per topic are: top_terms &lt;- chapter_topics %&gt;% group_by(topic) %&gt;% top_n(n = 5, wt = beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) top_terms %&gt;% ggplot(aes(x = reorder_within(term, beta, topic), y = beta, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free&quot;) + coord_flip() + scale_x_reordered() These topics are pretty clearly associated with the four books! Each “document” in this analysis was a single chapter. Which topics are associated with each document - can we put the chapters back together into the correct books? Examining the per-document-per-topic probabilities, (gamma). Separate the document name into title and chapter, then visualize the per-document-per-topic probability for each. chapters_gamma &lt;- tidy(chapters_lda, matrix = &quot;gamma&quot;) %&gt;% separate(document, c(&quot;title&quot;, &quot;chapter&quot;), sep = &quot;_&quot;, convert = TRUE) chapters_gamma %&gt;% mutate(title = reorder(title, gamma * topic)) %&gt;% ggplot(aes(factor(topic), y = gamma)) + geom_boxplot() + facet_wrap(~ title) Almost all chapters from Pride and Prejudice, War of the Worlds, and Twenty Thousand Leagues Under the Sea were uniquely identified as a single topic each. Some chapters from Great Expectations (topic 4) were somewhat associated with other topics. Are there any cases where the topic most associated with a chapter belonged to another book? First we’d find the topic that was most associated with each chapter using top_n(), which is effectively the “classification” of that chapter. We can then compare each to the “consensus” topic for each book (the most common topic among its chapters), and see which were most often misidentified. chapter_classifications &lt;- chapters_gamma %&gt;% group_by(title, chapter) %&gt;% top_n(1, gamma) %&gt;% ungroup() chapter_classifications ## # A tibble: 193 x 4 ## title chapter topic gamma ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Great Expectations 23 1 0.547 ## 2 Pride and Prejudice 43 1 1.00 ## 3 Pride and Prejudice 18 1 1.00 ## 4 Pride and Prejudice 45 1 1.00 ## 5 Pride and Prejudice 16 1 1.00 ## 6 Pride and Prejudice 29 1 1.00 ## 7 Pride and Prejudice 10 1 1.00 ## 8 Pride and Prejudice 8 1 1.00 ## 9 Pride and Prejudice 56 1 1.00 ## 10 Pride and Prejudice 47 1 1.00 ## # ... with 183 more rows book_topics &lt;- chapter_classifications %&gt;% count(title, topic) %&gt;% group_by(title) %&gt;% top_n(1, n) %&gt;% ungroup() %&gt;% transmute(consensus = title, topic) chapter_classifications %&gt;% inner_join(book_topics, by = &quot;topic&quot;) %&gt;% filter(title != consensus) ## # A tibble: 2 x 5 ## title chapter topic gamma consensus ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Great Expectations 23 1 0.547 Pride and Prejudice ## 2 Great Expectations 54 3 0.481 The War of the Worlds Only two chapters from Great Expectations were misclassified. The augment() function adds model output (token count and topic classification) to the original observations. assignments &lt;- augment(chapters_lda, data = chapters_dtm) assignments ## # A tibble: 104,722 x 4 ## document term count .topic ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Great Expectations_57 joe 88 4 ## 2 Great Expectations_7 joe 70 4 ## 3 Great Expectations_17 joe 5 4 ## 4 Great Expectations_27 joe 58 4 ## 5 Great Expectations_2 joe 56 4 ## 6 Great Expectations_23 joe 1 4 ## 7 Great Expectations_15 joe 50 4 ## 8 Great Expectations_18 joe 50 4 ## 9 Great Expectations_9 joe 44 4 ## 10 Great Expectations_13 joe 40 4 ## # ... with 104,712 more rows Combine with the book_topics summarization to assess the misclassifications. assignments &lt;- assignments %&gt;% separate(document, c(&quot;title&quot;, &quot;chapter&quot;), sep = &quot;_&quot;, convert = TRUE) %&gt;% inner_join(book_topics, by = c(&quot;.topic&quot; = &quot;topic&quot;)) A good way to visualize the misclassifications is with a confusion matrix. library(scales) assignments %&gt;% count(title, consensus, wt = count) %&gt;% group_by(title) %&gt;% mutate(percent = n / sum(n)) %&gt;% ggplot(aes(consensus, title, fill = percent)) + geom_tile() + scale_fill_gradient2(high = &quot;red&quot;, label = percent_format()) + theme_minimal() + theme(axis.text.x = element_text(angle = 90, hjust = 1), panel.grid = element_blank()) + labs(x = &quot;Book words were assigned to&quot;, y = &quot;Book words came from&quot;, fill = &quot;% of assignments&quot;) We notice that almost all the words for Pride and Prejudice, Twenty Thousand Leagues Under the Sea, and War of the Worlds were correctly assigned, while Great Expectations had a fair number of misassigned words (which, as we saw above, led to two chapters getting misclassified). What were the most commmonly mistaken words? wrong_words &lt;- assignments %&gt;% filter(title != consensus) wrong_words %&gt;% count(title, consensus, term, wt = count) %&gt;% ungroup() %&gt;% arrange(desc(n)) ## # A tibble: 3,551 x 4 ## title consensus term n ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Great Expectations Pride and Prejudice love 44 ## 2 Great Expectations Pride and Prejudice sergeant 37 ## 3 Great Expectations Pride and Prejudice lady 32 ## 4 Great Expectations Pride and Prejudice miss 26 ## 5 Great Expectations The War of the Worlds boat 25 ## 6 Great Expectations The War of the Worlds tide 20 ## 7 Great Expectations The War of the Worlds water 20 ## 8 Great Expectations Pride and Prejudice father 19 ## 9 Great Expectations Pride and Prejudice baby 18 ## 10 Great Expectations Pride and Prejudice flopson 18 ## # ... with 3,541 more rows "],
["appendix-string-manipulation.html", "13.5 Appendix: String Manipulation", " 13.5 Appendix: String Manipulation library(stringr) print() outputs strings as you might enter them, so embedded quotes are escaped. Use writeLines() to see text as you might prefer to read them. writeLines() is similar to cat(), but does not attempt to convert non-character objects to strings. Unicode is a standard for representing characters that might not be on your keyboard. Each available character has a Unicode code point: a number that uniquely identifies it. These code points are generally written in hex notation, that is, using base 16 and the digits 0-9 and A-F. You can find the code point for a particular character by looking up a code chart. Format numbers with format. Here is how to format with comma-separations. digits is not the number of decimal places; it’s the number of significant digits x &lt;- c(72.19, 1030.18, 102091.93, 1189192.18) format(x, digits = 2, big.mark = &quot;,&quot;, trim = TRUE, scientific = FALSE) ## [1] &quot;72&quot; &quot;1,030&quot; &quot;102,092&quot; &quot;1,189,192&quot; 13.5.1 stringr package The stringr package is a simple wrapper around the more complete stringi package. There are a ton of functions (see help(package = \"stringr\")), but here are some particularly useful ones. str_c() concatenates strings, similar to with paste() and paste0(). str_c(&quot;hello&quot;, &quot;world&quot;, sep = &quot; &quot;) ## [1] &quot;hello world&quot; str_replace(string, pattern, replacment) replaces pattern with replacement. str_replace(&quot;If the future&#39;s looking bleek&quot;, pattern = fixed(&quot;bleek&quot;), replacement = &quot;dark&quot;) ## [1] &quot;If the future&#39;s looking dark&quot; str_replace_na(string, replacement) replaces NAs. str_replace_na(c(&quot;We&#39;re the ones &quot;, NA, &quot;have to shine&quot;), replacement = &quot;who &quot;) ## [1] &quot;We&#39;re the ones &quot; &quot;who &quot; &quot;have to shine&quot; str_split(string, pattern, simplify = FALSE) splits string by pattern into a list of vectors, or matrix if simplify = TRUE. str_split(&quot;If there&#39;s no one in control&quot;, pattern = &quot; &quot;, simplify = TRUE) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] &quot;If&quot; &quot;there&#39;s&quot; &quot;no&quot; &quot;one&quot; &quot;in&quot; &quot;control&quot; str_c(..., sep) concatenates a vector of strings, separated by sep. str_c(&quot;we&#39;re&quot;, &quot;the&quot;, &quot;ones&quot;, &quot;who&quot;, &quot;draw&quot;, &quot;the&quot;, &quot;line&quot;, sep = &quot; &quot;) ## [1] &quot;we&#39;re the ones who draw the line&quot; str_sub(string, start, end) returns substring of string from start to end. Use negatives to start from the end of the string. my_str &lt;- &quot;Although we live in trying times&quot; str_sub(my_str, start = 1, end = 5) ## [1] &quot;Altho&quot; str_sub(my_str, start = -4, end = -1) ## [1] &quot;imes&quot; str_length(string) returns the number of characters in a string. str_length(&quot;We&#39;re the ones who have to try&quot;) ## [1] 30 str_detect(string, pattern) returns booleans where string matches pattern. str_detect(c(&quot;Although we know&quot;, &quot;that time&quot;, &quot;has wings&quot;), pattern = fixed(&quot;wings&quot;)) ## [1] FALSE FALSE TRUE str_match(string, pattern) returns matching strings where string matches pattern. str_match(c(&quot;Although we know&quot;, &quot;that time&quot;, &quot;has wings&quot;), pattern = &quot;wings&quot;) ## [,1] ## [1,] NA ## [2,] NA ## [3,] &quot;wings&quot; str_subset(string, pattern) returns string matches where string matches pattern. str_subset(c(&quot;Although we know&quot;, &quot;that time&quot;, &quot;has wings&quot;), pattern = fixed(&quot;wings&quot;)) ## [1] &quot;has wings&quot; str_count(string, pattern) returns a count of matches where string matches pattern. str_count(c(&quot;Although we know&quot;, &quot;that time&quot;, &quot;has wings&quot;), pattern = fixed(&quot;wings&quot;)) ## [1] 0 0 1 str_extract(string, pattern) returns the part of the string matching pattern. str_extract(c(&quot;We&#39;re the ones&quot;, &quot;who have to fly&quot;), pattern = &quot; t..&quot;) ## [1] &quot; the&quot; &quot; to &quot; 13.5.2 Regular Expressions The rebus package is a good resource for building regular expressions. library(rebus) ## Warning: package &#39;rebus&#39; was built under R version 4.0.2 ## ## Attaching package: &#39;rebus&#39; ## The following objects are masked from &#39;package:qdapRegex&#39;: ## ## %|%, group, is.regex ## The following objects are masked from &#39;package:igraph&#39;: ## ## %c%, graph ## The following object is masked from &#39;package:scales&#39;: ## ## alpha ## The following object is masked from &#39;package:kernlab&#39;: ## ## alpha ## The following object is masked from &#39;package:VGAM&#39;: ## ## specials ## The following object is masked from &#39;package:stringr&#39;: ## ## regex ## The following object is masked from &#39;package:ggplot2&#39;: ## ## alpha The %R% operator concatenates the regular expression. START represents regex “^” meaning “starting with”. END represents regex “$” meaning “ending with”. x &lt;- austen_books() %&gt;% filter(book == &quot;Sense &amp; Sensibility&quot;) %&gt;% select(text) %&gt;% head(100) %&gt;% pull() Here are lines from Sense &amp; Sensibility that start with “Mr”. str_subset(x, pattern = START %R% &quot;Mr&quot;) ## [1] &quot;Mrs. Henry Dashwood to his wishes, which proceeded not merely from&quot; ## [2] &quot;Mr. Dashwood&#39;s disappointment was, at first, severe; but his temper was&quot; ## [3] &quot;Mr. John Dashwood had not the strong feelings of the rest of the&quot; ANY_CHAR represents regex “.” Here are lines from Sense &amp; Sensibility with pattern “handsome” str_subset(x, pattern = ANY_CHAR %R% &quot;handsome&quot; %R% ANY_CHAR) ## [1] &quot;them three thousand pounds: it would be liberal and handsome! It would&quot; char_class() is similar to ANY_CHAR except that it matches any character from the string parameter. It is the same as regex “[]”. The opposite is negated_char_class(), which is the same as “[^]”. char_class() can accept ranges, such as “0-9”, and “a-z”. DGT is the same thing as “0-9”. str_subset(c(&quot;apple&quot;, &quot;Aardvark&quot;, &quot;Orukidn&quot;), char_class(&quot;Aa&quot;)) ## [1] &quot;apple&quot; &quot;Aardvark&quot; DOT, CARAT, and DOLLAR represent special characters, “.”, “%”, and “$”. Function or() provides alteration. str_match(c(&quot;kittycat&quot;, &quot;doggone&quot;), pattern = or(&quot;dog&quot;, &quot;cat&quot;)) ## [,1] ## [1,] &quot;cat&quot; ## [2,] &quot;dog&quot; Look for repeating patterns with optional(), zero_or_more(), one_or_more(), and repeated(). Wrap a rebus expression in capture() to create a column in the output for the match to each captured part of the regex. phone_string &lt;- c(&quot;555-123-4567&quot;, &quot;(555)123-4567&quot;, &quot;555.123.4567&quot;, &quot;555-123-4567 (M), 555-123-7654 (H)&quot;) phone_pattern &lt;- capture(DGT %R% DGT %R% DGT) %R% zero_or_more(char_class(&quot;()-.&quot;)) %R% capture(DGT %R% DGT %R% DGT) %R% zero_or_more(char_class(&quot;()-.&quot;)) %R% capture(DGT %R% DGT %R% DGT %R% DGT) # first match with str_match() phone_match &lt;- str_match(phone_string, pattern = phone_pattern) str_c(&quot;(&quot;, phone_match[, 2], &quot;)&quot;, phone_match[, 3], &quot;-&quot;, phone_match[, 4]) ## [1] &quot;(555)123-4567&quot; &quot;(555)123-4567&quot; &quot;(555)123-4567&quot; &quot;(555)123-4567&quot; # all matches with str_match_all() and lapply() phone_match_all &lt;- str_match_all(phone_string, pattern = phone_pattern) lapply(phone_match_all, function(x){str_c(&quot;(&quot;, x[, 2], &quot;)&quot;, x[, 3], &quot;-&quot;, x[, 4])}) %&gt;% unlist() ## [1] &quot;(555)123-4567&quot; &quot;(555)123-4567&quot; &quot;(555)123-4567&quot; &quot;(555)123-4567&quot; ## [5] &quot;(555)123-7654&quot; You can refer to captured patterns with REF[0-9]. str_match(c(&quot;hello&quot;, &quot;sweet&quot;, &quot;kitten&quot;), pattern = capture(LOWER) %R% REF1) ## [,1] [,2] ## [1,] &quot;ll&quot; &quot;l&quot; ## [2,] &quot;ee&quot; &quot;e&quot; ## [3,] &quot;tt&quot; &quot;t&quot; Here is an exercise working with the Oscal Wilde play “The Importance of Being Earnest”. earnest &lt;- read_lines(&quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_2922/datasets/importance-of-being-earnest.txt&quot;) The text is between the lines with “START OF THE PROJECT” and “END OF THE PROJECT”. str_which() returns the indices where the string contains the pattern. The text consists of an introduction and the play itself. The play starts at “FIRST ACT”. start &lt;- str_which(earnest, fixed(&quot;START OF THE PROJECT&quot;)) end &lt;- str_which(earnest, fixed(&quot;END OF THE PROJECT&quot;)) earnest_sub &lt;- earnest[(start+1):(end-1)] play_start &lt;- str_which(earnest_sub, &quot;FIRST ACT&quot;) intro_line_index &lt;- 1:(play_start - 1) intro_text &lt;- earnest_sub[intro_line_index] play_text &lt;- earnest_sub[-intro_line_index] # remove the emptly lines play_lines &lt;- play_text[str_length(play_text) &gt; 0] %&gt;% as.character() # print first 20 lines writeLines(play_lines[1:20]) ## FIRST ACT ## SCENE ## Morning-room in Algernon&#39;s flat in Half-Moon Street. The room is ## luxuriously and artistically furnished. The sound of a piano is heard in ## the adjoining room. ## [Lane is arranging afternoon tea on the table, and after the music has ## ceased, Algernon enters.] ## Algernon. Did you hear what I was playing, Lane? ## Lane. I didn&#39;t think it polite to listen, sir. ## Algernon. I&#39;m sorry for that, for your sake. I don&#39;t play ## accurately--any one can play accurately--but I play with wonderful ## expression. As far as the piano is concerned, sentiment is my forte. I ## keep science for Life. ## Lane. Yes, sir. ## Algernon. And, speaking of the science of Life, have you got the ## cucumber sandwiches cut for Lady Bracknell? ## Lane. Yes, sir. [Hands them on a salver.] ## Algernon. [Inspects them, takes two, and sits down on the sofa.] Oh! . . . ## by the way, Lane, I see from your book that on Thursday night, when ## Lord Shoreman and Mr. Worthing were dining with me, eight bottles of How would you identify lines where the character is starting to speak? You might look for a capitalized word followed by a “.”. pattern &lt;- START %R% ascii_upper() %R% one_or_more(WRD) %R% DOT lines &lt;- str_subset(play_lines, pattern) # Extract the matching string (the character speaking) who &lt;- str_extract(lines, pattern) # Let&#39;s see what we have unique(who) ## [1] &quot;Algernon.&quot; &quot;Lane.&quot; &quot;Jack.&quot; &quot;Cecily.&quot; &quot;Ernest.&quot; ## [6] &quot;University.&quot; &quot;Gwendolen.&quot; &quot;July.&quot; &quot;Chasuble.&quot; &quot;Merriman.&quot; ## [11] &quot;Sunday.&quot; &quot;Mr.&quot; &quot;London.&quot; &quot;Cardew.&quot; &quot;Opera.&quot; ## [16] &quot;Markby.&quot; &quot;Oxonian.&quot; Close, but not perfect. If you know the characters, just search for them directly. or1() is like or() but lets you supply a vector of strings. characters &lt;- c(&quot;Algernon&quot;, &quot;Jack&quot;, &quot;Lane&quot;, &quot;Cecily&quot;, &quot;Gwendolen&quot;, &quot;Chasuble&quot;, &quot;Merriman&quot;, &quot;Lady Bracknell&quot;, &quot;Miss Prism&quot;) pattern &lt;- START %R% or1(characters) %R% DOT lines &lt;- str_subset(play_lines, pattern) # Extract the matching string (the character speaking) who &lt;- str_extract(lines, pattern) # Let&#39;s see what we have unique(who) ## [1] &quot;Algernon.&quot; &quot;Lane.&quot; &quot;Jack.&quot; &quot;Cecily.&quot; ## [5] &quot;Gwendolen.&quot; &quot;Lady Bracknell.&quot; &quot;Miss Prism.&quot; &quot;Chasuble.&quot; ## [9] &quot;Merriman.&quot; # Lines per character table(who) ## who ## Algernon. Cecily. Chasuble. Gwendolen. Jack. ## 201 154 42 102 219 ## Lady Bracknell. Lane. Merriman. Miss Prism. ## 84 21 17 41 "],
["reference-links.html", "13.6 Reference Links", " 13.6 Reference Links Silge, J., &amp; Robinson, D. (2019). Text Mining with R. O’Reilly Media. https://www.tidytextmining.com. https://juliasilge.com/blog/evaluating-stm/ Dotson, Marc. “Introduction to Text Analysis in R”. DataCamp. https://www.datacamp.com/courses/introduction-to-text-analysis-in-r., String Manipulation in R with stringr, Text Mining: Bag of Words, Sentiment Analysis in R. Other DataCamp Courses Sentiment Analysis in R: The Tidy Way, Topic Modeling in R, Introduction to Natural Language Processing in R. There are also some projects in DataCamp, and tutorial. "],
["survival-analysis.html", "Chapter 14 Survival Analysis", " Chapter 14 Survival Analysis These notes rely on the Survival Analysis in R DataCamp course, STHDA, and Applied Survival Analysis Using R (Moore 2016). Survival analysis models time to event. Whereas linear regression outcomes are assumed to have a normal distribution, time-to-event outcomes have a Weibull or unknown distribution. Survival analysis models also deal with censoring (unknown starting event and/or ending event). These factors make survival analysis more complicated than linear regression. Most survival analyses use the survival package for modeling and the survminer package for visualization. library(tidyverse) library(survival) library(survminer) A typical survival analysis uses Kaplan-Meier plots to visualize survival curves, log-rank tests to compare survival curves among groups, and Cox proportional hazards regression to describe the effect of variables on survival. References "],
["basic-concepts.html", "14.1 Basic Concepts", " 14.1 Basic Concepts This section reviews the fundamentals of survival analysis, including the hazard probability density, and survival functions. You can specify the survival distribution function either as a survival function or as a hazard function. Define \\(F(t) = Pr(T \\le t), \\hspace{3mm} 0 &lt; t &lt; \\infty\\) as the cumulative risk function, the probability of dying on or before time \\(t\\). Then the survival function is the probability of surviving up to time \\(t\\), \\[S(t) = 1 - F(t) = pr(T &gt; t), \\hspace{3mm} 0 &lt; t &lt; \\infty.\\] The hazard function is the instantaneous death rate given survival up to time \\(t\\), \\[h(t) = \\lim_{\\delta \\rightarrow 0}{\\frac{pr(t &lt; T &lt; t + \\delta|T &gt; t)}{\\delta}}.\\] The survival function and the hazard function are related. The probability of dying during the interval \\((t, t + \\delta)\\), \\(f(t) = F&#39;(t)\\), is the probability of dying during the interval given survival up to point \\(t\\) times the probability of surviving up to point \\(t\\), \\(f(t) = h(t) S(t)\\). \\(S(t)\\) is also the exponent of the negative cumulative hazard function, \\[S(t) = e^{-H(t)}.\\] You can use the survival function to estimate the mean and median survival times. The mean survival time is \\(E(T) = \\int S(t)dt\\). The median survival time is \\(S(t) = 0.5\\). "],
["survival-curve-estimation.html", "14.2 Survival Curve Estimation", " 14.2 Survival Curve Estimation There are parametric and non-parametric methods to estimate a survivor curve. The usual non-parametric method is the Kaplan-Meier (KM) estimator. The usual parametric method is the Weibull distribution, of which the exponential distribution is a special case. In between the two is the Cox proportional hazards model, the most common way to estimate a survivor curve. 14.2.1 Kaplan-Meier The Kaplan-Meier estimator for the survival function is \\[\\hat{S} = \\prod_{i: t_i &lt; t}{\\frac{n_i - d_i}{n_i}}\\] where \\(n_i\\) is the number of persons under observation at time \\(i\\) and \\(d_i\\) is the number of individuals dying at time \\(i\\). The Kaplan-Meier curve falls only when a subject dies, not when a subject is censored. Confidence limits can be calculated using the “delta” method to obtain the variance of \\(\\log \\left(\\hat{S}(t) \\right)\\) (see p27 of Moore). Calculate the Kaplan-Meier survival function estimate with the survfit() function. Here is an example using the lung data from the survival package to death of 228 patients witha advanced lung cancer. Column status indicates whether or not a person in the study has died (1 = censored, 2 = dead). data(lung, package = &quot;survival&quot;) head(lung) ## inst time status age sex ph.ecog ph.karno pat.karno meal.cal wt.loss ## 1 3 306 2 74 1 1 90 100 1175 NA ## 2 3 455 2 68 1 0 90 90 1225 15 ## 3 3 1010 1 56 1 0 90 90 NA 15 ## 4 5 210 2 57 1 1 90 60 1150 11 ## 5 1 883 2 60 1 0 100 90 NA 0 ## 6 12 1022 1 74 1 1 50 80 513 0 survfit() creates survival curves from a formula or from a previously fitted Cox model. The formula below is an intercept-only model. Structure the response variable as a Surv object. km_fit &lt;- survfit(Surv(time, status) ~ sex, data = lung) km_fit ## Call: survfit(formula = Surv(time, status) ~ sex, data = lung) ## ## n events median 0.95LCL 0.95UCL ## sex=1 138 112 270 212 310 ## sex=2 90 53 426 348 550 The printed survfit object shows there were 138 records (patients) for males (sex=1) and 90 records for females (sex=2), and 112 events (deaths) for males and 53 events for females. It also shows the 95% CI in days for the median time to event. The survfit object contains more variables, including detailed time points with the number at risk n.risk, events n.event, and censors n.censor at each time point and strata strata. data.frame( strata = km_fit$strata, time = km_fit$time, n.risk = km_fit$n.risk, n.event = km_fit$n.event, n.censor = km_fit$n.censor, surv = km_fit$surv, upper = km_fit$upper, lower = km_fit$lower ) %&gt;% head() ## strata time n.risk n.event n.censor surv upper lower ## 1 119 11 138 3 0 0.98 1.00 0.95 ## 2 87 12 135 1 0 0.97 1.00 0.94 ## 3 119 13 134 2 0 0.96 0.99 0.92 ## 4 87 15 132 1 0 0.95 0.99 0.91 ## 5 119 26 131 1 0 0.94 0.98 0.90 ## 6 87 30 130 1 0 0.93 0.98 0.89 Plot the fitted model with ggsurvplot(). A vertical drop in the curves indicates an event. The vertical tick mark on the curves means that a patient was censored. ggsurvplot( km_fit, linetype = &quot;strata&quot;, # Change line type by groups pval = TRUE, conf.int = TRUE, risk.table = TRUE, surv.median.line = &quot;hv&quot;, # median horizontal and vertical ref lines ggtheme = theme_bw(), palette = c(&quot;#E7B800&quot;, &quot;#2E9FDF&quot;), title = &quot;Kaplan-Meier Survival Function Estimate&quot; ) Reading the figure, you can see the median survival is 270 days for sex=1 and 426 days for sex=2. These key values are available from the summary() object. summary(km_fit)$table ## records n.max n.start events *rmean *se(rmean) median 0.95LCL 0.95UCL ## sex=1 138 138 138 112 325 23 270 212 310 ## sex=2 90 90 90 53 458 34 426 348 550 ggsurvplot() can plot the cumulative risk function (aka “cumulative incidence” or “cumulative events”), \\(F(t) = 1 - S(t)\\), with argument fun = \"event\", and the cumulative hazard function, \\(H(t) = -\\log(S(t)).\\), with argument fun = \"cumhaz\". ggsurvplot( km_fit, fun = &quot;event&quot;, linetype = &quot;strata&quot;, # Change line type by groups pval = TRUE, conf.int = TRUE, ggtheme = theme_bw(), palette = c(&quot;#E7B800&quot;, &quot;#2E9FDF&quot;), title = &quot;Kaplan-Meier Cumulative Risk Function Estimate&quot; ) ggsurvplot( km_fit, fun = &quot;cumhaz&quot;, linetype = &quot;strata&quot;, # Change line type by groups pval = TRUE, conf.int = TRUE, ggtheme = theme_bw(), palette = c(&quot;#E7B800&quot;, &quot;#2E9FDF&quot;), title = &quot;Kaplan-Meier Cumulative Hazard Function Estimate&quot; ) ggsurvplot() can produce faceted plots for more complicated analyses. For example, fit a survival curve to the survival::colon data set with predictors sex, rx, and adhere. km_fit_colon &lt;- survfit(Surv(time, status) ~ sex + rx + adhere, data = colon) p &lt;- km_fit_colon %&gt;% ggsurvplot(fun = &quot;event&quot;, conf.int = TRUE, ggtheme = theme_bw()) p$plot + theme(legend.position = &quot;right&quot;) + facet_grid(rx ~ adhere) 14.2.1.1 Log-Rank Test Compare survival curves with the log-rank test (\\(H_0\\): no difference). The log rank test is a non-parametric test, so it makes no assumptions about the survival distributions. The log rank test compares the observed and expected (\\(H_0\\)) number of events in each group. The log rank test statistic is approximately chi-square distributed. Function survdiff() performs the log-rank test. km_diff &lt;- survdiff(Surv(time, status) ~ sex, data = lung) km_diff ## Call: ## survdiff(formula = Surv(time, status) ~ sex, data = lung) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## sex=1 138 112 91.6 4.55 10.3 ## sex=2 90 53 73.4 5.68 10.3 ## ## Chisq= 10.3 on 1 degrees of freedom, p= 0.001 The chi-sq test statistic is 10.3 on one d.f., for a p-value of 0.001, so yes, males and females had different survival patterns. 14.2.2 Weibull Kaplan-Meier curves and logrank tests are examples of univariate analysis. They describe the survival as a function of a single categorical factor variable. Weibull and other parametric models describe the function of multiple covariates. Several parametric distributions are available for modeling survival data. The exponential distribution is the easiest to use because it has a time-independent function, \\[\\log h_i(t) = \\alpha + \\beta X_i\\] or \\[h_i(t) = e^{\\left(\\alpha + \\beta X_i \\right)} = \\lambda\\] where \\(i\\) is the observation number. The constant \\(\\alpha\\) is a kind of baseline log-hazard, because \\(\\log h_i(t) = \\alpha\\), or \\(h_i(t) = e^\\alpha\\), when \\(X\\) is zero. The cumulative hazard is \\(H(t) = \\int_0^t \\lambda dt = \\lambda t\\) and the corresponding survival function is \\[S(t) = e^{-H(t)} = e^{-\\lambda t}.\\] The expected survival time is \\(E(T) = \\int_0^\\infty S(t)dt = \\int_0^\\infty d^{-\\lambda t} dt = 1 / \\lambda.\\). The median survival time is \\(S(t) = e^{-\\lambda t} = 0.5\\), or \\(t_{med} = \\log(2) / \\lambda\\). The Weibull distribution is more appropriate for modeling lifetimes, however. The Weibull hazard function is \\(h(t) = \\alpha \\lambda (\\lambda t)^{\\alpha - 1} = \\alpha \\lambda^\\alpha t^{\\alpha-1}\\). data.frame(t = rep(1:80, 3), alpha = c(rep(1.5, 80), rep(1, 80), rep(0.75, 80)), lambda = rep(0.03, 240)) %&gt;% mutate( f = dweibull(x = t, shape = alpha, scale = 1 / 0.03), S = pweibull(q = t, shape = alpha, scale = 1 / 0.03, lower.tail = FALSE), h = f / S # same as alpha * lambda^alpha * t^(alpha-1) ) %&gt;% ggplot(aes(x = t, y = h, color = as.factor(alpha))) + geom_line() + theme(legend.position = &quot;top&quot;) + labs(y = &quot;hazard&quot;, x = &quot;time&quot;, color = &quot;alpha&quot;, title = &quot;Weibul hazard function at varying levels of alpha&quot;, subtitle = &quot;Lambda = 0.03&quot;, caption = &quot;alpha = 1 is special case of exponential function.&quot;) The cumulative hazard function is \\(H(t) = (\\lambda t)^\\alpha\\) and the corresponding survival function is \\[S(t) = e^{-(\\lambda t)^\\alpha}.\\] The exponential distribution is a special case of the Weibull where \\(\\alpha = 1\\). The expected survival time is \\(E(t) = \\frac{\\Gamma (1 + 1 / \\alpha)}{\\lambda}\\). The median survival time is \\(t_{med} = \\frac{[\\log(2)]^{1 / \\alpha}}{\\lambda}\\) The Kaplan-Meier estimate is used mainly as a descriptive tool. The Weibull model produces a smooth survival curve instead of a step function. The Weibull model assumes a Weibull distribution. Fit a Weibull model with the survreg() function. data(GBSG2, package = &quot;TH.data&quot;) wb &lt;- survreg(Surv(time, cens) ~ 1, data = GBSG2) # 90% of patients survive beyond time point 385 # Alternatively, 10% of patients die at time 385 predict(wb, type = &quot;quantile&quot;, p = 1 - 0.9, newdata = data.frame(1)) ## 1 ## 385 # The median survival time is 1694 predict(wb, type = &quot;quantile&quot;, p = 1 - 0.5, newdata = data.frame(1)) ## 1 ## 1694 surv &lt;- seq(.99, .01, by = -.01) t &lt;- predict(wb, type = &quot;quantile&quot;, p = 1 - surv, newdata = data.frame(1)) head(data.frame(time = t, surv = surv)) ## time surv ## 1 61 0.99 ## 2 105 0.98 ## 3 145 0.97 ## 4 183 0.96 ## 5 219 0.95 ## 6 253 0.94 surv_wb &lt;- data.frame(time = t, surv = surv, upper = NA, lower = NA, std.err = NA) ggsurvplot_df(fit = surv_wb, surv.geom = geom_line) Fit a Weibull model controlling for hormonal therapy horTh and tumor size tsize. wbmod &lt;- survreg(Surv(time, cens) ~ horTh + tsize, data = GBSG2) coef(wbmod) ## (Intercept) horThyes tsize ## 7.961 0.312 -0.012 summary(wbmod) ## ## Call: ## survreg(formula = Surv(time, cens) ~ horTh + tsize, data = GBSG2) ## Value Std. Error z p ## (Intercept) 7.96070 0.10413 76.45 &lt; 0.0000000000000002 ## horThyes 0.31176 0.09602 3.25 0.0012 ## tsize -0.01218 0.00272 -4.47 0.000007772 ## Log(scale) -0.26494 0.04952 -5.35 0.000000088 ## ## Scale= 0.767 ## ## Weibull distribution ## Loglik(model)= -2623 Loglik(intercept only)= -2637 ## Chisq= 28 on 2 degrees of freedom, p= 0.00000076 ## Number of Newton-Raphson Iterations: 5 ## n= 686 surv &lt;- seq(.99, .01, by = -.01) newdata &lt;- expand.grid( horTh = levels(GBSG2$horTh), tsize = quantile(GBSG2$tsize, probs = c(0.25, 0.50, 0.75)) ) t &lt;- predict(wbmod, type = &quot;quantile&quot;, p = 1 - surv, newdata = newdata) surv_wbmod &lt;- surv_wbmod_wide &lt;- cbind(newdata, t) %&gt;% pivot_longer(names_to = &quot;surv_id&quot;, values_to = &quot;time&quot;, cols = -c(1:2)) %&gt;% mutate(tsize = as.numeric(tsize), surv_id = as.factor(as.numeric(surv_id))) %&gt;% data.frame() surv_wbmod$surv = surv[as.numeric(surv_wbmod$surv_id)] surv_wbmod$upper = NA surv_wbmod$lower = NA surv_wbmod$std.err = NA surv_wbmod$strata = NA surv_wbmod[, c(&quot;upper&quot;, &quot;lower&quot;, &quot;std.err&quot;, &quot;strata&quot;)] &lt;- NA ggsurvplot_df(surv_wbmod, surv.geom = geom_line, linetype = &quot;horTh&quot;, color = &quot;tsize&quot;, legend.title = NULL) Interpret the coefficient as the probability of surviving falls by 0.012 per unit increase in the tumor size and increases by 0.312 if taking hormonal therapy. You can fit other models with the dist = c(\"lognormal\", \"exponential\") parameter. 14.2.3 Cox Whereas parametric models specify the baseline hazard function, \\(\\alpha(t) = \\log h_0(t)\\), the Cox proportional hazards model is semi-parametric in that it leaves it unspecified. \\[\\log h_i(t) = \\alpha(t) + \\beta X_i\\] or \\[h_i(t) = h_0(t) \\cdot e^{X_i \\beta} = h_0(t) \\psi_i\\] The \\(e^{\\beta_j}\\) values are called the hazard ratios. A positive \\(e^{\\beta_j}\\) means the hazard increases with the covariate. The proportionality of the model comes from the lack of time dependence in the \\(X\\) variables. Two ratio of the hazard functions of two individuals is \\[\\frac{h_i(t)}{h_{i&#39;}(t)} = \\frac{h_0(t) \\cdot e^{X_i \\beta}}{h_0(t) \\cdot e^{X_{i&#39;} \\beta}}\\] Remarkably, even though the baseline hazard is unspecified, the Cox model can still be estimated by the method of partial likelihood. Consider the first failure time, \\(t_1\\). The probability that patient \\(i\\) is the one to fail is the proportion of patient \\(i\\)’s hazard divided by the sum of the hazards of all \\(R_1\\) patients at risk, \\[p_1 = \\frac{h_i(t_1)}{\\sum_{k \\in R_1} h_k(t_1)} = \\frac{h_0(t_1) \\psi_i}{\\sum_{k \\in R_1} h_0(t_1) \\psi_k} = \\frac{\\psi_i}{\\sum_{k \\in R_1} \\psi_k}\\] The next failure event has a reduced \\(R_2\\) patients at risk. The partial likelihood for \\(D\\) failure times is the product \\(L_i = p_1 p_2 \\cdots p_D\\). The Cox model uses maximum partial likelihood estimation to find the value of \\(\\phi\\) that maximizes the likelihood function. Fit a Cox proportional hazards model with coxph(). Here is a simple Cox proportional hazards model for a single covariate, sex. lung_cox &lt;- coxph(Surv(time, status) ~ sex, data = lung) summary(lung_cox) ## Call: ## coxph(formula = Surv(time, status) ~ sex, data = lung) ## ## n= 228, number of events= 165 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## sex -0.531 0.588 0.167 -3.18 0.0015 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## sex 0.588 1.7 0.424 0.816 ## ## Concordance= 0.579 (se = 0.021 ) ## Likelihood ratio test= 10.6 on 1 df, p=0.001 ## Wald test = 10.1 on 1 df, p=0.001 ## Score (logrank) test = 10.3 on 1 df, p=0.001 The Wald statistic (z) is the ratio of each regression coefficient to its standard error (z = coef/se(coef)), just as with linear regression. The negative coefficient estimator sign means that the hazard (risk of death) decreases with increasing values of the variable. sex is encoded as a 1 = male, 2 = female, so sex = -0.5310 means females have a lower risk of death than males. The exponentiated coefficients (exp(coef)) are the hazard ratios, the effect-size of the covariates. Being female (sex=2) reduces the hazard by a factor of 0.5880 (41%). Below the coefficients estimators table is a table of hazard ratio Confidence intervals. The last section of the summary object is three tests for the overall significance of the model: the likelihood-ratio test, Wald test, and score logrank statistics. These three methods are asymptotically equivalent. The Likelihood ratio test has better behavior for small sample sizes, so it is generally preferred. A multivariate analysis works the same way. Here is the Cox model with two additional covariates: age and ph.ecog. lung_cox_2 &lt;- coxph(Surv(time, status) ~ age + sex + ph.ecog, data = lung) summary(lung_cox_2) ## Call: ## coxph(formula = Surv(time, status) ~ age + sex + ph.ecog, data = lung) ## ## n= 227, number of events= 164 ## (1 observation deleted due to missingness) ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## age 0.01107 1.01113 0.00927 1.19 0.23242 ## sex -0.55261 0.57544 0.16774 -3.29 0.00099 *** ## ph.ecog 0.46373 1.58999 0.11358 4.08 0.000044 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## age 1.011 0.989 0.993 1.030 ## sex 0.575 1.738 0.414 0.799 ## ph.ecog 1.590 0.629 1.273 1.986 ## ## Concordance= 0.637 (se = 0.025 ) ## Likelihood ratio test= 30.5 on 3 df, p=0.000001 ## Wald test = 29.9 on 3 df, p=0.000001 ## Score (logrank) test = 30.5 on 3 df, p=0.000001 The p-values for all three overall tests (likelihood, Wald, and score) are significant, indicating that the model is significant (not all \\(\\beta\\) values are 0). Having fit a Cox model to the data, it’s possible to visualize the predicted survival proportion at any given time for a particular risk group. Function survfit() estimates the survival proportion, by default at the mean values of covariates. #ggsurvplot(survfit(lung_cox_2), data = lung, palette = &quot;#2E9FDF&quot;, ggtheme = theme_minimal()) To display the effects of one or more particular covariates, construct a data frame with test cases and pass to survfit() with the newdata argument. newdata &lt;- expand.grid( sex = unique(lung$sex), # age = quantile(lung$age, probs = c(0.25, 0.50, 0.75)), age = median(lung$age), ph.ecog = 1 ) rownames(newdata) &lt;- c(&quot;male&quot;, &quot;female&quot;) # Create survival curves. The rownames show up in the model lung_pred &lt;- survfit(lung_cox_2, newdata = newdata, data = lung) # surv_summary() creates the data.frame with a summary of the survfit() results, including columns like time (survival time) and surv (survival probability). lung_pred0 &lt;- surv_summary(lung_pred) # get the corresponding new_data cols lung_pred1 &lt;- cbind(lung_pred0, newdata[as.character(lung_pred0$strata), ]) ggsurvplot_df( lung_pred1, color = &quot;sex&quot;, legend.labs = c(&quot;M&quot;, &quot;F&quot;), legend.title = &quot;Sex&quot;, conf.int = TRUE, ggtheme = theme_minimal()) "],
["appendix.html", "Appendix", " Appendix Here are miscellaneous skills, knowledge, and technologies I should know. "],
["publishing-to-bookdown.html", "Publishing to BookDown", " Publishing to BookDown The bookdown package, written by Yihui Xie, is built on top of R Markdown and the knitr package. Use it to publish a book or long manuscript where each chapter is a separate file. There are instructions for how to author a book in his bookdown book (Xie 2019). The main advantage of bookdown over R Markdown is that you can produce multi-page HTML output with numbered headers, equations, figures, etc., just like in a book. I’m using bookdown to create a compendium of all my data science notes. The first step to using bookdown is installing the **bookdown* package with install.packages(\"bookdown\"). Next, create an account at bookdown.org, and connect the account to RStudio. Follow the instructions at https://bookdown.org/home/about/. Finally, create a project in R Studio by creating a new project of type Book Project using Bookdown. After creating all of your Markdown pages, knit the book or click the Build Book button in the Build panel. References "],
["shiny-apps.html", "Shiny Apps", " Shiny Apps "],
["packages.html", "Packages", " Packages R Packages (Wickham 2015) by Hadley Wickham is a good manual on packages, but it does not include a full tutorial. The Developing R Packages Data Camp course is also helpful. I will set up my own exercise and present it here. I will create a package for my pretend organization, “MF”. The package will include the following: R Markdown template. My template will integrate code, output, and commentary in a single R Markdown. The template will produce a familiar work product containing standard content (summary, data management, exploratory analysis, methods, results, conclusions), and a standard style (colors, typeface, size, logo). Functions. Common I/O functions for database retrieval, writing to Excel. Common graphing functions for ggplot styling. I am mostly copying the logic and code from the ggthemes economist.R script. Create a package In the RStudio IDE, click File &gt; New Project. Select “New Directory”. Select “R Package”. You can also use devtools::create(\"mfstylr\"). This will create the minimum items for an R package. + R directory: R scripts with function definitions. + man directory: documentation + NAMESPACE file: information about imported functions and functions made available (managed by **roxygen2**) + DESCRIPTION file: metadata about the package Write functions in R scripts in R directory. Document with tags readable by roxygen2 package. Select XYZ &gt; Install and Restart. 14.2.4 Document Functions with roxygen Add roxygen documentation with #' characters. The first three lines are always the title, Description, and Details. They don’t need any tags, but you need to separate them with blank lines. Create Data Add an RData file to your package with use_data() Create Vignette Add a directory and template vignette with use_vignette(name, title). use_vignette(&quot;Creating-Plots-with-mfstylr&quot;, &quot;Creating Plots with mfstylr&quot;) Step 2: Create an R Markdown template I relied on this blog at free range statistics for a lot what follows. There is also good information about R Markdown and templates in Yihui Xie’s R Markdown: The Definitive Guide (Xie, Allaire, and Grolemund 2019). Use usethis::use_rmarkdown_template() to create an Rmd template. I will create a “Kaggle Report” template. In the Console (or a script), enter usethis::use_rmarkdown_template( template_name = &quot;Kaggle Report&quot;, template_dir = &quot;kaggle_report&quot;, template_description = &quot;Template for creating Kaggle reports in RMarkdown.&quot;, template_create_dir = FALSE ) Since my project directory is C:\\Users\\mpfol\\OneDrive\\Documents\\GitHub\\mfstylr, use_rmarkdown_template() creates subdirectories .\\inst\\rmarkdown\\templates\\kaggle_report\\skeleton with three files .\\inst\\rmarkdown\\templates\\kaggle_report\\template.yaml .\\inst\\rmarkdown\\templates\\kaggle_report\\skeleton\\skeleton.Rmd My kaggle report template will include a logo. Looks like there are two ways to embed an image in your document. One is a direct image loading reference !(), but I don’t think you can control the attributes this way. A second way is adding html tags. ![](logo.png) # or for more control &lt;img src=&quot;logo.png&quot; style=&quot;position:absolute;top:0px;right:0px;&quot; /&gt; References "],
["references.html", "References", " References "]
]
