<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9.1 Classification Tree | My Data Science Notes</title>
  <meta name="description" content="This is a compendium of notes from classes, tutorials, etc. that I reference from time to time." />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="9.1 Classification Tree | My Data Science Notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a compendium of notes from classes, tutorials, etc. that I reference from time to time." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9.1 Classification Tree | My Data Science Notes" />
  
  <meta name="twitter:description" content="This is a compendium of notes from classes, tutorials, etc. that I reference from time to time." />
  

<meta name="author" content="Michael Foley" />


<meta name="date" content="2020-06-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="decision-trees.html"/>
<link rel="next" href="regression-trees.html"/>
<script src="assets/jquery-2.2.3/jquery.min.js"></script>
<link href="assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">My Data Science Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Intro</a></li>
<li class="chapter" data-level="1" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>1</b> Probability</a><ul>
<li class="chapter" data-level="1.1" data-path="principles.html"><a href="principles.html"><i class="fa fa-check"></i><b>1.1</b> Principles</a></li>
<li class="chapter" data-level="1.2" data-path="disc-dist.html"><a href="disc-dist.html"><i class="fa fa-check"></i><b>1.2</b> Discrete Distributions</a><ul>
<li class="chapter" data-level="1.2.1" data-path="disc-dist.html"><a href="disc-dist.html#bernoulli"><i class="fa fa-check"></i><b>1.2.1</b> Bernoulli</a></li>
<li class="chapter" data-level="1.2.2" data-path="disc-dist.html"><a href="disc-dist.html#binomial"><i class="fa fa-check"></i><b>1.2.2</b> Binomial</a></li>
<li class="chapter" data-level="1.2.3" data-path="disc-dist.html"><a href="disc-dist.html#poission"><i class="fa fa-check"></i><b>1.2.3</b> Poission</a></li>
<li class="chapter" data-level="1.2.4" data-path="disc-dist.html"><a href="disc-dist.html#multinomial"><i class="fa fa-check"></i><b>1.2.4</b> Multinomial</a></li>
<li class="chapter" data-level="1.2.5" data-path="disc-dist.html"><a href="disc-dist.html#negative-binomial"><i class="fa fa-check"></i><b>1.2.5</b> Negative-Binomial</a></li>
<li class="chapter" data-level="1.2.6" data-path="disc-dist.html"><a href="disc-dist.html#geometric"><i class="fa fa-check"></i><b>1.2.6</b> Geometric</a></li>
<li class="chapter" data-level="1.2.7" data-path="disc-dist.html"><a href="disc-dist.html#hypergeometric"><i class="fa fa-check"></i><b>1.2.7</b> Hypergeometric</a></li>
<li class="chapter" data-level="1.2.8" data-path="disc-dist.html"><a href="disc-dist.html#gamma"><i class="fa fa-check"></i><b>1.2.8</b> Gamma</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="cont-dist.html"><a href="cont-dist.html"><i class="fa fa-check"></i><b>1.3</b> Continuous Distributions</a><ul>
<li class="chapter" data-level="1.3.1" data-path="cont-dist.html"><a href="cont-dist.html#normal"><i class="fa fa-check"></i><b>1.3.1</b> Normal</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="join-distributions.html"><a href="join-distributions.html"><i class="fa fa-check"></i><b>1.4</b> Join Distributions</a></li>
<li class="chapter" data-level="1.5" data-path="likelihood.html"><a href="likelihood.html"><i class="fa fa-check"></i><b>1.5</b> Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="discrete-analysis.html"><a href="discrete-analysis.html"><i class="fa fa-check"></i><b>2</b> Categorical Analysis - Nonmodel</a><ul>
<li class="chapter" data-level="2.1" data-path="chi-square-test.html"><a href="chi-square-test.html"><i class="fa fa-check"></i><b>2.1</b> Chi-Square Test</a></li>
<li class="chapter" data-level="2.2" data-path="one-way-tables.html"><a href="one-way-tables.html"><i class="fa fa-check"></i><b>2.2</b> One-Way Tables</a><ul>
<li class="chapter" data-level="2.2.1" data-path="one-way-tables.html"><a href="one-way-tables.html#chi-square-goodness-of-fit-test"><i class="fa fa-check"></i><b>2.2.1</b> Chi-Square Goodness-of-Fit Test</a></li>
<li class="chapter" data-level="2.2.2" data-path="one-way-tables.html"><a href="one-way-tables.html#proportion-test"><i class="fa fa-check"></i><b>2.2.2</b> Proportion Test</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="two-way-tables.html"><a href="two-way-tables.html"><i class="fa fa-check"></i><b>2.3</b> Two-Way Tables</a><ul>
<li class="chapter" data-level="2.3.1" data-path="two-way-tables.html"><a href="two-way-tables.html#chi-square-independence-test"><i class="fa fa-check"></i><b>2.3.1</b> Chi-Square Independence Test</a></li>
<li class="chapter" data-level="2.3.2" data-path="two-way-tables.html"><a href="two-way-tables.html#residuals-analysis"><i class="fa fa-check"></i><b>2.3.2</b> Residuals Analysis</a></li>
<li class="chapter" data-level="2.3.3" data-path="two-way-tables.html"><a href="two-way-tables.html#difference-in-proportions"><i class="fa fa-check"></i><b>2.3.3</b> Difference in Proportions</a></li>
<li class="chapter" data-level="2.3.4" data-path="two-way-tables.html"><a href="two-way-tables.html#relative-risk"><i class="fa fa-check"></i><b>2.3.4</b> Relative Risk</a></li>
<li class="chapter" data-level="2.3.5" data-path="two-way-tables.html"><a href="two-way-tables.html#odds-ratio"><i class="fa fa-check"></i><b>2.3.5</b> Odds Ratio</a></li>
<li class="chapter" data-level="2.3.6" data-path="two-way-tables.html"><a href="two-way-tables.html#partitioning-chi-square"><i class="fa fa-check"></i><b>2.3.6</b> Partitioning Chi-Square</a></li>
<li class="chapter" data-level="2.3.7" data-path="two-way-tables.html"><a href="two-way-tables.html#correlation"><i class="fa fa-check"></i><b>2.3.7</b> Correlation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="k-way-tables.html"><a href="k-way-tables.html"><i class="fa fa-check"></i><b>2.4</b> K-Way Tables</a><ul>
<li class="chapter" data-level="2.4.1" data-path="k-way-tables.html"><a href="k-way-tables.html#odds-ratio-1"><i class="fa fa-check"></i><b>2.4.1</b> Odds Ratio</a></li>
<li class="chapter" data-level="2.4.2" data-path="k-way-tables.html"><a href="k-way-tables.html#chi-square-independence-test-1"><i class="fa fa-check"></i><b>2.4.2</b> Chi-Square Independence Test</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="continuous-analysis.html"><a href="continuous-analysis.html"><i class="fa fa-check"></i><b>3</b> Continuous Variable Analysis</a><ul>
<li class="chapter" data-level="3.0.1" data-path="continuous-analysis.html"><a href="continuous-analysis.html#correlation-1"><i class="fa fa-check"></i><b>3.0.1</b> Correlation</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="experiment-design.html"><a href="experiment-design.html"><i class="fa fa-check"></i><b>4</b> Experiment Design</a><ul>
<li class="chapter" data-level="4.1" data-path="single-factor.html"><a href="single-factor.html"><i class="fa fa-check"></i><b>4.1</b> Single Factor</a></li>
<li class="chapter" data-level="4.2" data-path="blocking.html"><a href="blocking.html"><i class="fa fa-check"></i><b>4.2</b> Blocking</a></li>
<li class="chapter" data-level="4.3" data-path="nested.html"><a href="nested.html"><i class="fa fa-check"></i><b>4.3</b> Nested</a></li>
<li class="chapter" data-level="4.4" data-path="split-plot.html"><a href="split-plot.html"><i class="fa fa-check"></i><b>4.4</b> Split Plot</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-2-supervised-machine-learning.html"><a href="part-2-supervised-machine-learning.html"><i class="fa fa-check"></i>PART 2: Supervised Machine Learning</a><ul>
<li class="chapter" data-level="4.5" data-path="linear-regression-model.html"><a href="linear-regression-model.html"><i class="fa fa-check"></i><b>4.5</b> Linear Regression Model</a></li>
<li class="chapter" data-level="4.6" data-path="parameter-estimation.html"><a href="parameter-estimation.html"><i class="fa fa-check"></i><b>4.6</b> Parameter Estimation</a></li>
<li class="chapter" data-level="4.7" data-path="model-assumptions.html"><a href="model-assumptions.html"><i class="fa fa-check"></i><b>4.7</b> Model Assumptions</a><ul>
<li class="chapter" data-level="4.7.1" data-path="model-assumptions.html"><a href="model-assumptions.html#linearity"><i class="fa fa-check"></i><b>4.7.1</b> Linearity</a></li>
<li class="chapter" data-level="4.7.2" data-path="model-assumptions.html"><a href="model-assumptions.html#multicollinearity"><i class="fa fa-check"></i><b>4.7.2</b> Multicollinearity</a></li>
<li class="chapter" data-level="4.7.3" data-path="model-assumptions.html"><a href="model-assumptions.html#normality"><i class="fa fa-check"></i><b>4.7.3</b> Normality</a></li>
<li class="chapter" data-level="4.7.4" data-path="model-assumptions.html"><a href="model-assumptions.html#equal-variances"><i class="fa fa-check"></i><b>4.7.4</b> Equal Variances</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="prediction.html"><a href="prediction.html"><i class="fa fa-check"></i><b>4.8</b> Prediction</a></li>
<li class="chapter" data-level="4.9" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>4.9</b> Inference</a><ul>
<li class="chapter" data-level="4.9.1" data-path="inference.html"><a href="inference.html#t-test"><i class="fa fa-check"></i><b>4.9.1</b> <em>t</em>-Test</a></li>
<li class="chapter" data-level="4.9.2" data-path="inference.html"><a href="inference.html#f-test"><i class="fa fa-check"></i><b>4.9.2</b> <em>F</em>-Test</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="interpretation.html"><a href="interpretation.html"><i class="fa fa-check"></i><b>4.10</b> Interpretation</a></li>
<li class="chapter" data-level="4.11" data-path="model-validation.html"><a href="model-validation.html"><i class="fa fa-check"></i><b>4.11</b> Model Validation</a><ul>
<li class="chapter" data-level="4.11.1" data-path="model-validation.html"><a href="model-validation.html#accuracy-metrics"><i class="fa fa-check"></i><b>4.11.1</b> Accuracy Metrics</a></li>
<li class="chapter" data-level="4.11.2" data-path="model-validation.html"><a href="model-validation.html#cross-validation"><i class="fa fa-check"></i><b>4.11.2</b> Cross-Validation</a></li>
<li class="chapter" data-level="4.11.3" data-path="model-validation.html"><a href="model-validation.html#gain-curve"><i class="fa fa-check"></i><b>4.11.3</b> Gain Curve</a></li>
</ul></li>
<li class="chapter" data-level="4.12" data-path="reference.html"><a href="reference.html"><i class="fa fa-check"></i><b>4.12</b> Reference</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>5</b> Generalized Linear Models</a><ul>
<li class="chapter" data-level="5.1" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>5.1</b> Logistic Regression</a></li>
<li class="chapter" data-level="5.2" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html"><i class="fa fa-check"></i><b>5.2</b> Multinomial Logistic Regression</a></li>
<li class="chapter" data-level="5.3" data-path="ordinal-logistic-regression.html"><a href="ordinal-logistic-regression.html"><i class="fa fa-check"></i><b>5.3</b> Ordinal Logistic Regression</a><ul>
<li class="chapter" data-level="5.3.1" data-path="ordinal-logistic-regression.html"><a href="ordinal-logistic-regression.html#assumptions"><i class="fa fa-check"></i><b>5.3.1</b> Assumptions</a></li>
<li class="chapter" data-level="5.3.2" data-path="ordinal-logistic-regression.html"><a href="ordinal-logistic-regression.html#modeling"><i class="fa fa-check"></i><b>5.3.2</b> Modeling</a></li>
<li class="chapter" data-level="5.3.3" data-path="ordinal-logistic-regression.html"><a href="ordinal-logistic-regression.html#case-study"><i class="fa fa-check"></i><b>5.3.3</b> Case Study</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="poisson-regression.html"><a href="poisson-regression.html"><i class="fa fa-check"></i><b>5.4</b> Poisson Regression</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multivariate-statistical-analysis.html"><a href="multivariate-statistical-analysis.html"><i class="fa fa-check"></i><b>6</b> Multivariate Statistical Analysis</a><ul>
<li class="chapter" data-level="6.1" data-path="background.html"><a href="background.html"><i class="fa fa-check"></i><b>6.1</b> Background</a></li>
<li class="chapter" data-level="6.2" data-path="manova.html"><a href="manova.html"><i class="fa fa-check"></i><b>6.2</b> MANOVA</a></li>
<li class="chapter" data-level="6.3" data-path="repeated-measures.html"><a href="repeated-measures.html"><i class="fa fa-check"></i><b>6.3</b> Repeated Measures</a></li>
<li class="chapter" data-level="6.4" data-path="lda.html"><a href="lda.html"><i class="fa fa-check"></i><b>6.4</b> LDA</a></li>
<li class="chapter" data-level="6.5" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>6.5</b> PCA</a></li>
<li class="chapter" data-level="6.6" data-path="factor-analysis.html"><a href="factor-analysis.html"><i class="fa fa-check"></i><b>6.6</b> Factor Analysis</a></li>
<li class="chapter" data-level="6.7" data-path="canonical-correlation.html"><a href="canonical-correlation.html"><i class="fa fa-check"></i><b>6.7</b> Canonical Correlation</a></li>
<li class="chapter" data-level="6.8" data-path="cluster-analysis.html"><a href="cluster-analysis.html"><i class="fa fa-check"></i><b>6.8</b> Cluster Analysis</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>7</b> Classification</a></li>
<li class="chapter" data-level="8" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>8</b> Regularization</a><ul>
<li class="chapter" data-level="8.1" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>8.1</b> Ridge</a></li>
<li class="chapter" data-level="8.2" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>8.2</b> Lasso</a></li>
<li class="chapter" data-level="8.3" data-path="elastic-net.html"><a href="elastic-net.html"><i class="fa fa-check"></i><b>8.3</b> Elastic Net</a></li>
<li class="chapter" data-level="" data-path="model-summary.html"><a href="model-summary.html"><i class="fa fa-check"></i>Model Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>9</b> Decision Trees</a><ul>
<li class="chapter" data-level="9.1" data-path="classification-tree.html"><a href="classification-tree.html"><i class="fa fa-check"></i><b>9.1</b> Classification Tree</a><ul>
<li class="chapter" data-level="9.1.1" data-path="classification-tree.html"><a href="classification-tree.html#confusion-matrix"><i class="fa fa-check"></i><b>9.1.1</b> Confusion Matrix</a></li>
<li class="chapter" data-level="9.1.2" data-path="classification-tree.html"><a href="classification-tree.html#roc-curve"><i class="fa fa-check"></i><b>9.1.2</b> ROC Curve</a></li>
<li class="chapter" data-level="9.1.3" data-path="classification-tree.html"><a href="classification-tree.html#caret-approach"><i class="fa fa-check"></i><b>9.1.3</b> Caret Approach</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="regression-trees.html"><a href="regression-trees.html"><i class="fa fa-check"></i><b>9.2</b> Regression Trees</a><ul>
<li class="chapter" data-level="9.2.1" data-path="regression-trees.html"><a href="regression-trees.html#caret-approach-1"><i class="fa fa-check"></i><b>9.2.1</b> Caret Approach</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="bagging.html"><a href="bagging.html"><i class="fa fa-check"></i><b>9.3</b> Bagging</a></li>
<li class="chapter" data-level="9.4" data-path="random-forests.html"><a href="random-forests.html"><i class="fa fa-check"></i><b>9.4</b> Random Forests</a></li>
<li class="chapter" data-level="9.5" data-path="gradient-boosting.html"><a href="gradient-boosting.html"><i class="fa fa-check"></i><b>9.5</b> Gradient Boosting</a></li>
<li class="chapter" data-level="9.6" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>9.6</b> Summary</a></li>
<li class="chapter" data-level="9.7" data-path="reference-1.html"><a href="reference-1.html"><i class="fa fa-check"></i><b>9.7</b> Reference</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="non-linear-models.html"><a href="non-linear-models.html"><i class="fa fa-check"></i><b>10</b> Non-linear Models</a><ul>
<li class="chapter" data-level="10.1" data-path="splines.html"><a href="splines.html"><i class="fa fa-check"></i><b>10.1</b> Splines</a></li>
<li class="chapter" data-level="10.2" data-path="mars.html"><a href="mars.html"><i class="fa fa-check"></i><b>10.2</b> MARS</a></li>
<li class="chapter" data-level="10.3" data-path="gam.html"><a href="gam.html"><i class="fa fa-check"></i><b>10.3</b> GAM</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>11</b> Support Vector Machines</a><ul>
<li class="chapter" data-level="11.1" data-path="maximal-margin-classifier.html"><a href="maximal-margin-classifier.html"><i class="fa fa-check"></i><b>11.1</b> Maximal Margin Classifier</a></li>
<li class="chapter" data-level="11.2" data-path="support-vector-classifier.html"><a href="support-vector-classifier.html"><i class="fa fa-check"></i><b>11.2</b> Support Vector Classifier</a></li>
<li class="chapter" data-level="11.3" data-path="support-vector-machines-1.html"><a href="support-vector-machines-1.html"><i class="fa fa-check"></i><b>11.3</b> Support Vector Machines</a></li>
<li class="chapter" data-level="11.4" data-path="example-19.html"><a href="example-19.html"><i class="fa fa-check"></i><b>11.4</b> Example</a></li>
<li class="chapter" data-level="11.5" data-path="using-caret.html"><a href="using-caret.html"><i class="fa fa-check"></i><b>11.5</b> Using Caret</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html"><i class="fa fa-check"></i><b>12</b> Principal Components Analysis</a></li>
<li class="chapter" data-level="13" data-path="text-mining.html"><a href="text-mining.html"><i class="fa fa-check"></i><b>13</b> Text Mining</a><ul>
<li class="chapter" data-level="13.1" data-path="n-grams.html"><a href="n-grams.html"><i class="fa fa-check"></i><b>13.1</b> N-Grams</a></li>
<li class="chapter" data-level="13.2" data-path="converting-to-and-from-non-tidy-formats.html"><a href="converting-to-and-from-non-tidy-formats.html"><i class="fa fa-check"></i><b>13.2</b> Converting to and from non-tidy formats</a></li>
<li class="chapter" data-level="13.3" data-path="example-20.html"><a href="example-20.html"><i class="fa fa-check"></i><b>13.3</b> Example</a></li>
<li class="chapter" data-level="13.4" data-path="stringr-package.html"><a href="stringr-package.html"><i class="fa fa-check"></i><b>13.4</b> stringr package</a></li>
<li class="chapter" data-level="13.5" data-path="regular-expressions.html"><a href="regular-expressions.html"><i class="fa fa-check"></i><b>13.5</b> Regular Expressions</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="survival-analysis.html"><a href="survival-analysis.html"><i class="fa fa-check"></i><b>14</b> Survival Analysis</a><ul>
<li class="chapter" data-level="14.1" data-path="basic-concepts.html"><a href="basic-concepts.html"><i class="fa fa-check"></i><b>14.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="14.2" data-path="survival-curve-estimation.html"><a href="survival-curve-estimation.html"><i class="fa fa-check"></i><b>14.2</b> Survival Curve Estimation</a><ul>
<li class="chapter" data-level="14.2.1" data-path="survival-curve-estimation.html"><a href="survival-curve-estimation.html#kaplan-meier"><i class="fa fa-check"></i><b>14.2.1</b> Kaplan-Meier</a></li>
<li class="chapter" data-level="14.2.2" data-path="survival-curve-estimation.html"><a href="survival-curve-estimation.html#weibull"><i class="fa fa-check"></i><b>14.2.2</b> Weibull</a></li>
<li class="chapter" data-level="14.2.3" data-path="survival-curve-estimation.html"><a href="survival-curve-estimation.html#cox"><i class="fa fa-check"></i><b>14.2.3</b> Cox</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="publishing-to-bookdown.html"><a href="publishing-to-bookdown.html"><i class="fa fa-check"></i>Publishing to BookDown</a></li>
<li class="chapter" data-level="" data-path="shiny-apps.html"><a href="shiny-apps.html"><i class="fa fa-check"></i>Shiny Apps</a></li>
<li class="chapter" data-level="" data-path="packages.html"><a href="packages.html"><i class="fa fa-check"></i>Packages</a><ul>
<li class="chapter" data-level="" data-path="packages.html"><a href="packages.html#create-a-package"><i class="fa fa-check"></i>Create a package</a></li>
<li class="chapter" data-level="14.2.4" data-path="packages.html"><a href="packages.html#document-functions-with-roxygen"><i class="fa fa-check"></i><b>14.2.4</b> Document Functions with roxygen</a></li>
<li class="chapter" data-level="" data-path="packages.html"><a href="packages.html#create-data"><i class="fa fa-check"></i>Create Data</a></li>
<li class="chapter" data-level="" data-path="packages.html"><a href="packages.html#create-vignette"><i class="fa fa-check"></i>Create Vignette</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">My Data Science Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification-tree" class="section level2">
<h2><span class="header-section-number">9.1</span> Classification Tree</h2>
<p>A simple classification tree is rarely performed on its own; the bagged, random forest, and gradient boosting methods build on this logic. However, it is good to start here to build understanding. I’ll learn by example. Using the <code>ISLR::OJ</code> data set, I will predict which brand of orange juice, Citrus Hill (CH) or Minute Maid = (MM), customers <code>Purchase</code> from the 17 feature variables.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="classification-tree.html#cb1-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb1-2"><a href="classification-tree.html#cb1-2"></a><span class="kw">library</span>(caret)</span>
<span id="cb1-3"><a href="classification-tree.html#cb1-3"></a><span class="kw">library</span>(rpart)  <span class="co"># classification and regression trees </span></span>
<span id="cb1-4"><a href="classification-tree.html#cb1-4"></a><span class="kw">library</span>(rpart.plot)  <span class="co"># better formatted plots than the ones in rpart</span></span>
<span id="cb1-5"><a href="classification-tree.html#cb1-5"></a><span class="kw">library</span>(plotROC)  <span class="co"># ROC curves</span></span>
<span id="cb1-6"><a href="classification-tree.html#cb1-6"></a><span class="kw">library</span>(ROCR)</span>
<span id="cb1-7"><a href="classification-tree.html#cb1-7"></a></span>
<span id="cb1-8"><a href="classification-tree.html#cb1-8"></a>oj_dat &lt;-<span class="st"> </span>ISLR<span class="op">::</span>OJ</span></code></pre></div>
<p>I’ll split <code>oj_dat</code> (n = 1,070) into <code>oj_train</code> (80%, n = 857) and <code>oj_test</code> (20%, n = 213). I’ll fit a simple decision tree with <code>oj_train</code>, then later a bagged tree, a random forest, and a gradient boosting tree. I’ll compare their predictive performance with <code>oj_test</code>.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="classification-tree.html#cb2-1"></a><span class="kw">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb2-2"><a href="classification-tree.html#cb2-2"></a>partition &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(<span class="dt">y =</span> oj_dat<span class="op">$</span>Purchase, <span class="dt">p =</span> <span class="fl">0.8</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)</span>
<span id="cb2-3"><a href="classification-tree.html#cb2-3"></a>oj_train &lt;-<span class="st"> </span>oj_dat[partition, ]</span>
<span id="cb2-4"><a href="classification-tree.html#cb2-4"></a>oj_test &lt;-<span class="st"> </span>oj_dat[<span class="op">-</span>partition, ]</span></code></pre></div>
<p>Function <code>rpart::rpart()</code> builds a full tree, minimizing the Gini index <span class="math inline">\(G\)</span> by default (<code>parms = list(split = "gini")</code>), until the stopping criterion is satisfied. The default stopping criterion is</p>
<ul>
<li>only attempt a split if the current node has at least <code>minsplit = 20</code> observations,</li>
<li>only accept a split if
<ul>
<li>the resulting nodes have at least <code>minbucket = round(minsplit/3)</code> observations, and</li>
<li>the resulting overall fit improves by <code>cp = 0.01</code> (i.e., <span class="math inline">\(\Delta G &lt;= 0.01\)</span>).</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="classification-tree.html#cb3-1"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb3-2"><a href="classification-tree.html#cb3-2"></a>oj_model_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">rpart</span>(</span>
<span id="cb3-3"><a href="classification-tree.html#cb3-3"></a>   <span class="dt">formula =</span> Purchase <span class="op">~</span><span class="st"> </span>.,</span>
<span id="cb3-4"><a href="classification-tree.html#cb3-4"></a>   <span class="dt">data =</span> oj_train,</span>
<span id="cb3-5"><a href="classification-tree.html#cb3-5"></a>   <span class="dt">method =</span> <span class="st">&quot;class&quot;</span>  <span class="co"># &quot;class&quot; for classification, &quot;anova&quot; for regression</span></span>
<span id="cb3-6"><a href="classification-tree.html#cb3-6"></a>   )</span>
<span id="cb3-7"><a href="classification-tree.html#cb3-7"></a><span class="kw">print</span>(oj_model_<span class="dv">1</span>)</span></code></pre></div>
<pre><code>## n= 857 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 857 334 CH (0.61026838 0.38973162)  
##    2) LoyalCH&gt;=0.48285 537  94 CH (0.82495345 0.17504655)  
##      4) LoyalCH&gt;=0.7648795 271  13 CH (0.95202952 0.04797048) *
##      5) LoyalCH&lt; 0.7648795 266  81 CH (0.69548872 0.30451128)  
##       10) PriceDiff&gt;=-0.165 226  50 CH (0.77876106 0.22123894) *
##       11) PriceDiff&lt; -0.165 40   9 MM (0.22500000 0.77500000) *
##    3) LoyalCH&lt; 0.48285 320  80 MM (0.25000000 0.75000000)  
##      6) LoyalCH&gt;=0.2761415 146  58 MM (0.39726027 0.60273973)  
##       12) SalePriceMM&gt;=2.04 71  31 CH (0.56338028 0.43661972) *
##       13) SalePriceMM&lt; 2.04 75  18 MM (0.24000000 0.76000000) *
##      7) LoyalCH&lt; 0.2761415 174  22 MM (0.12643678 0.87356322) *</code></pre>
<p>The output starts with the root node. The predicted class at the root is <code>CH</code> and this prediction produces 334 errors on the 857 observations for a success rate of 0.61026838 and an error rate of 0.38973162. The child nodes of node “x” are labeled 2x) and 2x+1), so the child nodes of 1) are 2) and 3), and the child nodes of 2) are 4) and 5). Terminal nodes are labeled with an asterisk (*).</p>
<p>Surprisingly, only 3 of the 17 features were used the in full tree: <code>LoyalCH</code> (Customer brand loyalty for CH), <code>PriceDiff</code> (relative price of MM over CH), and <code>SalePriceMM</code> (absolute price of MM). The first split is at <code>LoyalCH</code> = 0.48285. Here is what the full (unpruned) tree looks like.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="classification-tree.html#cb5-1"></a><span class="kw">rpart.plot</span>(oj_model_<span class="dv">1</span>, <span class="dt">yesno =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>The boxes show the node classification (based on mode), the proportion of observations that are <em>not</em> <code>CH</code>, and the proportion of observations included in the node.</p>
<p><code>rpart()</code> not only grew the full tree, it identified the set of cost complexity parameters, and measured the model performance of each corresponding tree using cross-validation. <code>printcp()</code> displays the candidate <span class="math inline">\(c_p\)</span> values. You can use this table to decide how to prune the tree.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="classification-tree.html#cb6-1"></a><span class="kw">printcp</span>(oj_model_<span class="dv">1</span>)</span></code></pre></div>
<pre><code>## 
## Classification tree:
## rpart(formula = Purchase ~ ., data = oj_train, method = &quot;class&quot;)
## 
## Variables actually used in tree construction:
## [1] LoyalCH     PriceDiff   SalePriceMM
## 
## Root node error: 334/857 = 0.38973
## 
## n= 857 
## 
##         CP nsplit rel error  xerror     xstd
## 1 0.479042      0   1.00000 1.00000 0.042745
## 2 0.032934      1   0.52096 0.54192 0.035775
## 3 0.013473      3   0.45509 0.47006 0.033905
## 4 0.010000      5   0.42814 0.46407 0.033736</code></pre>
<p>There are 4 <span class="math inline">\(c_p\)</span> values in this model. The model with the smallest complexity parameter allows the most splits (<code>nsplit</code>). The highest complexity parameter corresponds to a tree with just a root node. <code>rel error</code> is the error rate relative to the root node. The root node absolute error is 0.38973162 (the proportion of MM), so its <code>rel error</code> is 0.38973162/0.38973162 = 1.0. That means the absolute error of the full tree (at CP = 0.01) is 0.42814 * 0.38973162 = 0.1669. You can verify that by calculating the error rate of the predicted values:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="classification-tree.html#cb8-1"></a><span class="kw">data.frame</span>(<span class="dt">pred =</span> <span class="kw">predict</span>(oj_model_<span class="dv">1</span>, <span class="dt">newdata =</span> oj_train, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb8-2"><a href="classification-tree.html#cb8-2"></a><span class="st">   </span><span class="kw">mutate</span>(<span class="dt">obs =</span> oj_train<span class="op">$</span>Purchase,</span>
<span id="cb8-3"><a href="classification-tree.html#cb8-3"></a>          <span class="dt">err =</span> <span class="kw">if_else</span>(pred <span class="op">!=</span><span class="st"> </span>obs, <span class="dv">1</span>, <span class="dv">0</span>)) <span class="op">%&gt;%</span></span>
<span id="cb8-4"><a href="classification-tree.html#cb8-4"></a><span class="st">   </span><span class="kw">summarize</span>(<span class="dt">mean_err =</span> <span class="kw">mean</span>(err))</span></code></pre></div>
<pre><code>##    mean_err
## 1 0.1668611</code></pre>
<p>Finishing the CP table tour, <code>xerror</code> is the relative cross-validated error rate and <code>xstd</code> is its standard error. If you want the lowest possible error, then prune to the tree with the smallest relative CV error, <span class="math inline">\(c_p\)</span> = 0.01. If you want to balance predictive power with simplicity, prune to the smallest tree within 1 SE of the one with the smallest relative error. The CP table is not super-helpful for finding that tree, so add a column to find it.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="classification-tree.html#cb10-1"></a>oj_model_<span class="dv">1</span><span class="op">$</span>cptable <span class="op">%&gt;%</span></span>
<span id="cb10-2"><a href="classification-tree.html#cb10-2"></a><span class="st">   </span><span class="kw">data.frame</span>() <span class="op">%&gt;%</span></span>
<span id="cb10-3"><a href="classification-tree.html#cb10-3"></a><span class="st">   </span><span class="kw">mutate</span>(</span>
<span id="cb10-4"><a href="classification-tree.html#cb10-4"></a>      <span class="dt">min_idx =</span> <span class="kw">which.min</span>(oj_model_<span class="dv">1</span><span class="op">$</span>cptable[, <span class="st">&quot;xerror&quot;</span>]),</span>
<span id="cb10-5"><a href="classification-tree.html#cb10-5"></a>      <span class="dt">rownum =</span> <span class="kw">row_number</span>(),</span>
<span id="cb10-6"><a href="classification-tree.html#cb10-6"></a>      <span class="dt">xerror_cap =</span> oj_model_<span class="dv">1</span><span class="op">$</span>cptable[min_idx, <span class="st">&quot;xerror&quot;</span>] <span class="op">+</span><span class="st"> </span></span>
<span id="cb10-7"><a href="classification-tree.html#cb10-7"></a><span class="st">                   </span>oj_model_<span class="dv">1</span><span class="op">$</span>cptable[min_idx, <span class="st">&quot;xstd&quot;</span>],</span>
<span id="cb10-8"><a href="classification-tree.html#cb10-8"></a>      <span class="dt">eval =</span> <span class="kw">case_when</span>(rownum <span class="op">==</span><span class="st"> </span>min_idx <span class="op">~</span><span class="st"> &quot;min xerror&quot;</span>,</span>
<span id="cb10-9"><a href="classification-tree.html#cb10-9"></a>                       xerror <span class="op">&lt;</span><span class="st"> </span>xerror_cap <span class="op">~</span><span class="st"> &quot;under cap&quot;</span>,</span>
<span id="cb10-10"><a href="classification-tree.html#cb10-10"></a>                       <span class="ot">TRUE</span> <span class="op">~</span><span class="st"> &quot;&quot;</span>)</span>
<span id="cb10-11"><a href="classification-tree.html#cb10-11"></a>   ) <span class="op">%&gt;%</span></span>
<span id="cb10-12"><a href="classification-tree.html#cb10-12"></a><span class="st">   </span><span class="kw">select</span>(<span class="op">-</span>rownum, <span class="op">-</span>min_idx) </span></code></pre></div>
<pre><code>##           CP nsplit rel.error    xerror       xstd xerror_cap       eval
## 1 0.47904192      0 1.0000000 1.0000000 0.04274518  0.4978082           
## 2 0.03293413      1 0.5209581 0.5419162 0.03577468  0.4978082           
## 3 0.01347305      3 0.4550898 0.4700599 0.03390486  0.4978082  under cap
## 4 0.01000000      5 0.4281437 0.4640719 0.03373631  0.4978082 min xerror</code></pre>
<p>The simplest tree using the 1-SE rule is $c_p = 0.01347305, CV error = 0.1832). Fortunately, <code>plotcp()</code> presents a nice graphical representation of the relationship between <code>xerror</code> and <code>cp</code>.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="classification-tree.html#cb12-1"></a><span class="kw">plotcp</span>(oj_model_<span class="dv">1</span>, <span class="dt">upper =</span> <span class="st">&quot;splits&quot;</span>)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>The dashed line is set at the minimum <code>xerror</code> + <code>xstd</code>. The top axis shows the number of splits in the tree. I’m not sure why the CP values are not the same as in the table (they are close, but not the same). The figure suggests I should prune to 5 or 3 splits. I see this curve never really hits a minimum - it is still decreasing at 5 splits. The default tuning parameter value <code>cp = 0.01</code> may be too small, so I’ll set it to <code>cp = 0.001</code> and start over.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="classification-tree.html#cb13-1"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb13-2"><a href="classification-tree.html#cb13-2"></a>oj_model_1b &lt;-<span class="st"> </span><span class="kw">rpart</span>(</span>
<span id="cb13-3"><a href="classification-tree.html#cb13-3"></a>   <span class="dt">formula =</span> Purchase <span class="op">~</span><span class="st"> </span>.,</span>
<span id="cb13-4"><a href="classification-tree.html#cb13-4"></a>   <span class="dt">data =</span> oj_train,</span>
<span id="cb13-5"><a href="classification-tree.html#cb13-5"></a>   <span class="dt">method =</span> <span class="st">&quot;class&quot;</span>,</span>
<span id="cb13-6"><a href="classification-tree.html#cb13-6"></a>   <span class="dt">cp =</span> <span class="fl">0.001</span></span>
<span id="cb13-7"><a href="classification-tree.html#cb13-7"></a>   )</span>
<span id="cb13-8"><a href="classification-tree.html#cb13-8"></a><span class="kw">print</span>(oj_model_1b)</span></code></pre></div>
<pre><code>## n= 857 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##   1) root 857 334 CH (0.61026838 0.38973162)  
##     2) LoyalCH&gt;=0.48285 537  94 CH (0.82495345 0.17504655)  
##       4) LoyalCH&gt;=0.7648795 271  13 CH (0.95202952 0.04797048) *
##       5) LoyalCH&lt; 0.7648795 266  81 CH (0.69548872 0.30451128)  
##        10) PriceDiff&gt;=-0.165 226  50 CH (0.77876106 0.22123894)  
##          20) ListPriceDiff&gt;=0.255 115  11 CH (0.90434783 0.09565217) *
##          21) ListPriceDiff&lt; 0.255 111  39 CH (0.64864865 0.35135135)  
##            42) PriceMM&gt;=2.155 19   2 CH (0.89473684 0.10526316) *
##            43) PriceMM&lt; 2.155 92  37 CH (0.59782609 0.40217391)  
##              86) DiscCH&gt;=0.115 7   0 CH (1.00000000 0.00000000) *
##              87) DiscCH&lt; 0.115 85  37 CH (0.56470588 0.43529412)  
##               174) ListPriceDiff&gt;=0.215 45  15 CH (0.66666667 0.33333333) *
##               175) ListPriceDiff&lt; 0.215 40  18 MM (0.45000000 0.55000000)  
##                 350) LoyalCH&gt;=0.527571 28  13 CH (0.53571429 0.46428571)  
##                   700) WeekofPurchase&lt; 266.5 21   8 CH (0.61904762 0.38095238) *
##                   701) WeekofPurchase&gt;=266.5 7   2 MM (0.28571429 0.71428571) *
##                 351) LoyalCH&lt; 0.527571 12   3 MM (0.25000000 0.75000000) *
##        11) PriceDiff&lt; -0.165 40   9 MM (0.22500000 0.77500000) *
##     3) LoyalCH&lt; 0.48285 320  80 MM (0.25000000 0.75000000)  
##       6) LoyalCH&gt;=0.2761415 146  58 MM (0.39726027 0.60273973)  
##        12) SalePriceMM&gt;=2.04 71  31 CH (0.56338028 0.43661972)  
##          24) LoyalCH&lt; 0.303104 7   0 CH (1.00000000 0.00000000) *
##          25) LoyalCH&gt;=0.303104 64  31 CH (0.51562500 0.48437500)  
##            50) WeekofPurchase&gt;=246.5 52  22 CH (0.57692308 0.42307692)  
##             100) PriceCH&lt; 1.94 35  11 CH (0.68571429 0.31428571)  
##               200) StoreID&lt; 1.5 9   1 CH (0.88888889 0.11111111) *
##               201) StoreID&gt;=1.5 26  10 CH (0.61538462 0.38461538)  
##                 402) LoyalCH&lt; 0.410969 17   4 CH (0.76470588 0.23529412) *
##                 403) LoyalCH&gt;=0.410969 9   3 MM (0.33333333 0.66666667) *
##             101) PriceCH&gt;=1.94 17   6 MM (0.35294118 0.64705882) *
##            51) WeekofPurchase&lt; 246.5 12   3 MM (0.25000000 0.75000000) *
##        13) SalePriceMM&lt; 2.04 75  18 MM (0.24000000 0.76000000)  
##          26) SpecialCH&gt;=0.5 14   6 CH (0.57142857 0.42857143) *
##          27) SpecialCH&lt; 0.5 61  10 MM (0.16393443 0.83606557) *
##       7) LoyalCH&lt; 0.2761415 174  22 MM (0.12643678 0.87356322)  
##        14) LoyalCH&gt;=0.035047 117  21 MM (0.17948718 0.82051282)  
##          28) WeekofPurchase&lt; 273.5 104  21 MM (0.20192308 0.79807692)  
##            56) PriceCH&gt;=1.875 20   9 MM (0.45000000 0.55000000)  
##             112) WeekofPurchase&gt;=252.5 12   5 CH (0.58333333 0.41666667) *
##             113) WeekofPurchase&lt; 252.5 8   2 MM (0.25000000 0.75000000) *
##            57) PriceCH&lt; 1.875 84  12 MM (0.14285714 0.85714286) *
##          29) WeekofPurchase&gt;=273.5 13   0 MM (0.00000000 1.00000000) *
##        15) LoyalCH&lt; 0.035047 57   1 MM (0.01754386 0.98245614) *</code></pre>
<p>This is a much larger tree. Did I find a <code>cp</code> value that produces a local min?</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="classification-tree.html#cb15-1"></a><span class="kw">plotcp</span>(oj_model_1b, <span class="dt">upper =</span> <span class="st">&quot;splits&quot;</span>)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Yes, the min is at CP = 0.011 with 5 splits. The min + 1 SE is at CP = 0.021 with 3 splits. I’ll prune the tree to 3 splits.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="classification-tree.html#cb16-1"></a>oj_model_1b_pruned &lt;-<span class="st"> </span><span class="kw">prune</span>(</span>
<span id="cb16-2"><a href="classification-tree.html#cb16-2"></a>   oj_model_1b,</span>
<span id="cb16-3"><a href="classification-tree.html#cb16-3"></a>   <span class="dt">cp =</span> oj_model_1b<span class="op">$</span>cptable[oj_model_1b<span class="op">$</span>cptable[, <span class="dv">2</span>] <span class="op">==</span><span class="st"> </span><span class="dv">3</span>, <span class="st">&quot;CP&quot;</span>]</span>
<span id="cb16-4"><a href="classification-tree.html#cb16-4"></a>)</span>
<span id="cb16-5"><a href="classification-tree.html#cb16-5"></a><span class="kw">rpart.plot</span>(oj_model_1b_pruned, <span class="dt">yesno =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>The most “important” indicator of <code>Purchase</code> appears to be <code>LoyalCH</code>. From the <strong>rpart</strong> <a href="https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf">vignette</a> (page 12),</p>
<blockquote>
<p>“An overall measure of variable importance is the sum of the goodness of split measures for each split for which it was the primary variable, plus goodness (adjusted agreement) for all splits in which it was a surrogate.”</p>
</blockquote>
<p>Surrogates refer to alternative features for a node to handle missing data. For each split, CART evaluates a variety of alternative “surrogate” splits to use when the feature value for the primary split is NA. Surrogate splits are splits that produce results similar to the original split.</p>
<p>A variable’s importance is the sum of the improvement in the overall Gini (or RMSE) measure produced by the nodes in which it appears. Here is the variable importance for this model.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="classification-tree.html#cb17-1"></a>oj_model_1b_pruned<span class="op">$</span>variable.importance</span></code></pre></div>
<pre><code>##        LoyalCH      PriceDiff    SalePriceMM        StoreID WeekofPurchase 
##     150.237336      20.843067      11.567443       9.965419       8.386282 
##         DiscMM        PriceMM      PctDiscMM        PriceCH    SalePriceCH 
##       7.081470       7.065493       6.252920       3.055594       1.042153</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="classification-tree.html#cb19-1"></a>oj_model_1b_pruned<span class="op">$</span>variable.importance <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb19-2"><a href="classification-tree.html#cb19-2"></a><span class="st">   </span><span class="kw">data.frame</span>() <span class="op">%&gt;%</span></span>
<span id="cb19-3"><a href="classification-tree.html#cb19-3"></a><span class="st">   </span><span class="kw">rownames_to_column</span>(<span class="dt">var =</span> <span class="st">&quot;Feature&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb19-4"><a href="classification-tree.html#cb19-4"></a><span class="st">   </span><span class="kw">rename</span>(<span class="dt">Overall =</span> <span class="st">&#39;.&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb19-5"><a href="classification-tree.html#cb19-5"></a><span class="st">   </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">fct_reorder</span>(Feature, Overall), <span class="dt">y =</span> Overall)) <span class="op">+</span></span>
<span id="cb19-6"><a href="classification-tree.html#cb19-6"></a><span class="st">   </span><span class="kw">geom_pointrange</span>(<span class="kw">aes</span>(<span class="dt">ymin =</span> <span class="dv">0</span>, <span class="dt">ymax =</span> Overall), <span class="dt">color =</span> <span class="st">&quot;cadetblue&quot;</span>, <span class="dt">size =</span> <span class="fl">.3</span>) <span class="op">+</span></span>
<span id="cb19-7"><a href="classification-tree.html#cb19-7"></a><span class="st">   </span><span class="kw">theme_minimal</span>() <span class="op">+</span></span>
<span id="cb19-8"><a href="classification-tree.html#cb19-8"></a><span class="st">   </span><span class="kw">coord_flip</span>() <span class="op">+</span></span>
<span id="cb19-9"><a href="classification-tree.html#cb19-9"></a><span class="st">   </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;&quot;</span>, <span class="dt">title =</span> <span class="st">&quot;Variable Importance with Simple Classication&quot;</span>)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p><code>LoyalCH</code> is by far the most important variable, as expected from its position at the top of the tree, and one level down.</p>
<p>You can see how the surrogates appear in the model with the <code>summary()</code> function.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="classification-tree.html#cb20-1"></a><span class="kw">summary</span>(oj_model_1b_pruned)</span></code></pre></div>
<pre><code>## Call:
## rpart(formula = Purchase ~ ., data = oj_train, method = &quot;class&quot;, 
##     cp = 0.001)
##   n= 857 
## 
##           CP nsplit rel error    xerror       xstd
## 1 0.47904192      0 1.0000000 1.0000000 0.04274518
## 2 0.03293413      1 0.5209581 0.5419162 0.03577468
## 3 0.01347305      3 0.4550898 0.4700599 0.03390486
## 
## Variable importance
##        LoyalCH      PriceDiff    SalePriceMM        StoreID WeekofPurchase 
##             67              9              5              4              4 
##         DiscMM        PriceMM      PctDiscMM        PriceCH 
##              3              3              3              1 
## 
## Node number 1: 857 observations,    complexity param=0.4790419
##   predicted class=CH  expected loss=0.3897316  P(node) =1
##     class counts:   523   334
##    probabilities: 0.610 0.390 
##   left son=2 (537 obs) right son=3 (320 obs)
##   Primary splits:
##       LoyalCH       &lt; 0.48285   to the right, improve=132.56800, (0 missing)
##       StoreID       &lt; 3.5       to the right, improve= 40.12097, (0 missing)
##       PriceDiff     &lt; 0.015     to the right, improve= 24.26552, (0 missing)
##       ListPriceDiff &lt; 0.255     to the right, improve= 22.79117, (0 missing)
##       SalePriceMM   &lt; 1.84      to the right, improve= 20.16447, (0 missing)
##   Surrogate splits:
##       StoreID        &lt; 3.5       to the right, agree=0.646, adj=0.053, (0 split)
##       PriceMM        &lt; 1.89      to the right, agree=0.638, adj=0.031, (0 split)
##       WeekofPurchase &lt; 229.5     to the right, agree=0.632, adj=0.016, (0 split)
##       DiscMM         &lt; 0.77      to the left,  agree=0.629, adj=0.006, (0 split)
##       SalePriceMM    &lt; 1.385     to the right, agree=0.629, adj=0.006, (0 split)
## 
## Node number 2: 537 observations,    complexity param=0.03293413
##   predicted class=CH  expected loss=0.1750466  P(node) =0.6266044
##     class counts:   443    94
##    probabilities: 0.825 0.175 
##   left son=4 (271 obs) right son=5 (266 obs)
##   Primary splits:
##       LoyalCH       &lt; 0.7648795 to the right, improve=17.669310, (0 missing)
##       PriceDiff     &lt; 0.015     to the right, improve=15.475200, (0 missing)
##       SalePriceMM   &lt; 1.84      to the right, improve=13.951730, (0 missing)
##       ListPriceDiff &lt; 0.255     to the right, improve=11.407560, (0 missing)
##       DiscMM        &lt; 0.15      to the left,  improve= 7.795122, (0 missing)
##   Surrogate splits:
##       WeekofPurchase &lt; 257.5     to the right, agree=0.594, adj=0.180, (0 split)
##       PriceCH        &lt; 1.775     to the right, agree=0.590, adj=0.173, (0 split)
##       StoreID        &lt; 3.5       to the right, agree=0.587, adj=0.165, (0 split)
##       PriceMM        &lt; 2.04      to the right, agree=0.587, adj=0.165, (0 split)
##       SalePriceMM    &lt; 2.04      to the right, agree=0.587, adj=0.165, (0 split)
## 
## Node number 3: 320 observations
##   predicted class=MM  expected loss=0.25  P(node) =0.3733956
##     class counts:    80   240
##    probabilities: 0.250 0.750 
## 
## Node number 4: 271 observations
##   predicted class=CH  expected loss=0.04797048  P(node) =0.3162194
##     class counts:   258    13
##    probabilities: 0.952 0.048 
## 
## Node number 5: 266 observations,    complexity param=0.03293413
##   predicted class=CH  expected loss=0.3045113  P(node) =0.3103851
##     class counts:   185    81
##    probabilities: 0.695 0.305 
##   left son=10 (226 obs) right son=11 (40 obs)
##   Primary splits:
##       PriceDiff     &lt; -0.165    to the right, improve=20.84307, (0 missing)
##       ListPriceDiff &lt; 0.235     to the right, improve=20.82404, (0 missing)
##       SalePriceMM   &lt; 1.84      to the right, improve=16.80587, (0 missing)
##       DiscMM        &lt; 0.15      to the left,  improve=10.05120, (0 missing)
##       PctDiscMM     &lt; 0.0729725 to the left,  improve=10.05120, (0 missing)
##   Surrogate splits:
##       SalePriceMM    &lt; 1.585     to the right, agree=0.906, adj=0.375, (0 split)
##       DiscMM         &lt; 0.57      to the left,  agree=0.895, adj=0.300, (0 split)
##       PctDiscMM      &lt; 0.264375  to the left,  agree=0.895, adj=0.300, (0 split)
##       WeekofPurchase &lt; 274.5     to the left,  agree=0.872, adj=0.150, (0 split)
##       SalePriceCH    &lt; 2.075     to the left,  agree=0.857, adj=0.050, (0 split)
## 
## Node number 10: 226 observations
##   predicted class=CH  expected loss=0.2212389  P(node) =0.2637106
##     class counts:   176    50
##    probabilities: 0.779 0.221 
## 
## Node number 11: 40 observations
##   predicted class=MM  expected loss=0.225  P(node) =0.04667445
##     class counts:     9    31
##    probabilities: 0.225 0.775</code></pre>
<p>The last step is to make predictions on the validation data set. For a classification tree, set argument <code>type = "class"</code>.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="classification-tree.html#cb22-1"></a>oj_model_1b_preds &lt;-<span class="st"> </span><span class="kw">predict</span>(oj_model_1b_pruned, oj_test, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)</span></code></pre></div>
<p>I’ll evaluate the predictions and record the accuracy (correct classification percentage) for comparison to other models. Two ways to evaluate the model are the confusion matrix, and the ROC curve.</p>
<div id="confusion-matrix" class="section level3">
<h3><span class="header-section-number">9.1.1</span> Confusion Matrix</h3>
<p>Print the confusion matrix with <code>caret::confusionMatrix()</code> to see how well does this model performs against the test data set.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="classification-tree.html#cb23-1"></a>oj_model_1b_cm &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(<span class="dt">data =</span> oj_model_1b_preds, <span class="dt">reference =</span> oj_test<span class="op">$</span>Purchase)</span>
<span id="cb23-2"><a href="classification-tree.html#cb23-2"></a>oj_model_1b_cm</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  CH  MM
##         CH 113  13
##         MM  17  70
##                                           
##                Accuracy : 0.8592          
##                  95% CI : (0.8051, 0.9029)
##     No Information Rate : 0.6103          
##     P-Value [Acc &gt; NIR] : 1.265e-15       
##                                           
##                   Kappa : 0.7064          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.5839          
##                                           
##             Sensitivity : 0.8692          
##             Specificity : 0.8434          
##          Pos Pred Value : 0.8968          
##          Neg Pred Value : 0.8046          
##              Prevalence : 0.6103          
##          Detection Rate : 0.5305          
##    Detection Prevalence : 0.5915          
##       Balanced Accuracy : 0.8563          
##                                           
##        &#39;Positive&#39; Class : CH              
## </code></pre>
<p>The confusion matrix is at the top. It also includes a lot of statistics. It’s worth getting familiar with the stats. The model accuracy and 95% CI are calculated from the binomial test.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="classification-tree.html#cb25-1"></a><span class="kw">binom.test</span>(<span class="dt">x =</span> <span class="dv">113</span> <span class="op">+</span><span class="st"> </span><span class="dv">70</span>, <span class="dt">n =</span> <span class="dv">213</span>)</span></code></pre></div>
<pre><code>## 
##  Exact binomial test
## 
## data:  113 + 70 and 213
## number of successes = 183, number of trials = 213, p-value &lt; 2.2e-16
## alternative hypothesis: true probability of success is not equal to 0.5
## 95 percent confidence interval:
##  0.8050785 0.9029123
## sample estimates:
## probability of success 
##              0.8591549</code></pre>
<p>The “No Information Rate” (NIR) statistic is the class rate for the largest class. In this case CH is the largest class, so NIR = 130/213 = 0.6103. “P-Value [Acc &gt; NIR]” is the binomial test that the model accuracy is significantly better than the NIR (i.e., significantly better than just always guessing CH).</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="classification-tree.html#cb27-1"></a><span class="kw">binom.test</span>(<span class="dt">x =</span> <span class="dv">113</span> <span class="op">+</span><span class="st"> </span><span class="dv">70</span>, <span class="dt">n =</span> <span class="dv">213</span>, <span class="dt">p =</span> <span class="dv">130</span><span class="op">/</span><span class="dv">213</span>, <span class="dt">alternative =</span> <span class="st">&quot;greater&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  Exact binomial test
## 
## data:  113 + 70 and 213
## number of successes = 183, number of trials = 213, p-value = 1.265e-15
## alternative hypothesis: true probability of success is greater than 0.6103286
## 95 percent confidence interval:
##  0.8138446 1.0000000
## sample estimates:
## probability of success 
##              0.8591549</code></pre>
<p>The “Accuracy” statistic indicates the model predicts 0.8590 of the observations correctly. That’s good, but less impressive when you consider the prevalence of CH is 0.6103 - you could achieve 61% accuracy just by predicting CH every time. A measure that controls for the prevalence is Cohen’s kappa statistic. The kappa statistic is explained <a href="https://standardwisdom.com/softwarejournal/2011/12/confusion-matrix-another-single-value-metric-kappa-statistic/">here</a>. It compares the accuracy to the accuracy of a “random system”. It is defined as</p>
<p><span class="math display">\[\kappa = \frac{Acc - RA}{1-RA}\]</span></p>
<p>where</p>
<p><span class="math display">\[RA = \frac{ActFalse \times PredFalse + ActTrue \times PredTrue}{Total \times Total}\]</span></p>
<p>is the hypotheical probability of a chance agreement. ActFalse will be the number of “MM” (13 + 70 = 83) and actual true will be the number of “CH” (113 + 17 = 130). The predicted counts are</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="classification-tree.html#cb29-1"></a><span class="kw">table</span>(oj_model_1b_preds)</span></code></pre></div>
<pre><code>## oj_model_1b_preds
##  CH  MM 
## 126  87</code></pre>
<p>So, <span class="math inline">\(RA = (83*87 + 130*126) / 213^2 = 0.5202\)</span> and <span class="math inline">\(\kappa = (0.8592 - 0.5202)/(1 - 0.5202) = 0.7064\)</span>. The kappa statistic varies from 0 to 1 where 0 means accurate predictions occur merely by chance, and 1 means the predictions are in perfect agreement with the observations. In this case, a kappa statistic of 0.7064 is “substantial”. See chart <a href="https://www.statisticshowto.datasciencecentral.com/cohens-kappa-statistic/">here</a>.</p>
<p>The other measures from the <code>confusionMatrix()</code> output are various proportions and you can remind yourself of their definitions in the documentation with <code>?confusionMatrix</code>.</p>
<p>Visuals are almost always helpful. Here is a plot of the confusion matrix.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="classification-tree.html#cb31-1"></a><span class="kw">plot</span>(oj_test<span class="op">$</span>Purchase, oj_model_1b_preds, </span>
<span id="cb31-2"><a href="classification-tree.html#cb31-2"></a>     <span class="dt">main =</span> <span class="st">&quot;Simple Classification: Predicted vs. Actual&quot;</span>,</span>
<span id="cb31-3"><a href="classification-tree.html#cb31-3"></a>     <span class="dt">xlab =</span> <span class="st">&quot;Actual&quot;</span>,</span>
<span id="cb31-4"><a href="classification-tree.html#cb31-4"></a>     <span class="dt">ylab =</span> <span class="st">&quot;Predicted&quot;</span>)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>By the way, how does the validation set accuracy ()</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="classification-tree.html#cb32-1"></a>oj_model_1b_train_preds &lt;-<span class="st"> </span><span class="kw">predict</span>(oj_model_1b_pruned, oj_train, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb32-2"><a href="classification-tree.html#cb32-2"></a>oj_model_1b_train_cm &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(<span class="dt">data =</span> oj_model_1b_train_preds, <span class="dt">reference =</span> oj_train<span class="op">$</span>Purchase)</span>
<span id="cb32-3"><a href="classification-tree.html#cb32-3"></a>oj_model_1b_train_cm<span class="op">$</span>overall</span></code></pre></div>
<pre><code>##       Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull 
##   8.226371e-01   6.323113e-01   7.953840e-01   8.476497e-01   6.102684e-01 
## AccuracyPValue  McnemarPValue 
##   1.859617e-41   4.258396e-02</code></pre>
<p>The accuracy on the training data set was a little lower than on the test data set. I though it would be higher, not lower.</p>
</div>
<div id="roc-curve" class="section level3">
<h3><span class="header-section-number">9.1.2</span> ROC Curve</h3>
<p>Another measure of accuracy is the ROC (receiver operating characteristics) curve <span class="citation">(Fawcett <a href="#ref-Fawcett2005" role="doc-biblioref">2005</a>)</span>. The ROC curve is a plot of the true positive rate (TPR, sensitivity) versus the false positive rate (FPR, 1 - specificity) for a set of thresholds. By default, the threshold for predicting the default classification is 0.50, but it could be any threshold. The ROC curves varies the thresholds. (I’ll use the <code>geom_roc</code> geom from <strong>plotROC</strong>.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="classification-tree.html#cb34-1"></a><span class="kw">data.frame</span>(<span class="dt">M =</span> <span class="kw">predict</span>(oj_model_1b_pruned, oj_test, <span class="st">&quot;prob&quot;</span>)[, <span class="dv">1</span>],</span>
<span id="cb34-2"><a href="classification-tree.html#cb34-2"></a>           <span class="dt">D =</span> <span class="kw">if_else</span>(oj_test<span class="op">$</span>Purchase <span class="op">==</span><span class="st"> &quot;CH&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)) <span class="op">%&gt;%</span></span>
<span id="cb34-3"><a href="classification-tree.html#cb34-3"></a><span class="st">   </span><span class="kw">ggplot</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb34-4"><a href="classification-tree.html#cb34-4"></a><span class="st">   </span><span class="kw">geom_roc</span>(<span class="kw">aes</span>(<span class="dt">m =</span> M, <span class="dt">d =</span> D), <span class="dt">hjust =</span> <span class="fl">-0.4</span>, <span class="dt">vjust =</span> <span class="fl">1.5</span>, <span class="dt">linealpha =</span> <span class="fl">0.6</span>, <span class="dt">labelsize =</span> <span class="dv">3</span>, <span class="dt">n.cuts =</span> <span class="dv">10</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb34-5"><a href="classification-tree.html#cb34-5"></a><span class="st">   </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">slope =</span> <span class="dv">1</span>, <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span></span>
<span id="cb34-6"><a href="classification-tree.html#cb34-6"></a><span class="st">   </span><span class="kw">coord_equal</span>() <span class="op">+</span></span>
<span id="cb34-7"><a href="classification-tree.html#cb34-7"></a><span class="st">   </span><span class="kw">theme_minimal</span>() <span class="op">+</span></span>
<span id="cb34-8"><a href="classification-tree.html#cb34-8"></a><span class="st">   </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;FPR&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;TPR&quot;</span>,</span>
<span id="cb34-9"><a href="classification-tree.html#cb34-9"></a>        <span class="dt">title =</span> <span class="st">&quot;Model 1b ROC Curve&quot;</span>,</span>
<span id="cb34-10"><a href="classification-tree.html#cb34-10"></a>        <span class="dt">subtitle =</span> <span class="st">&quot;Pruned model using rpart&quot;</span>,</span>
<span id="cb34-11"><a href="classification-tree.html#cb34-11"></a>        <span class="dt">caption =</span> <span class="st">&quot;Data: ISLM OJ data set.&quot;</span>)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>You can also use <code>prediction()</code> and <code>plot.prediction()</code> from the <strong>ROCR</strong> package.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="classification-tree.html#cb35-1"></a>pred &lt;-<span class="st"> </span><span class="kw">prediction</span>(<span class="kw">predict</span>(oj_model_1b_pruned, <span class="dt">newdata =</span> oj_test, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>)[, <span class="dv">2</span>], oj_test<span class="op">$</span>Purchase)</span>
<span id="cb35-2"><a href="classification-tree.html#cb35-2"></a><span class="kw">plot</span>(<span class="kw">performance</span>(pred, <span class="st">&quot;tpr&quot;</span>, <span class="st">&quot;fpr&quot;</span>))</span>
<span id="cb35-3"><a href="classification-tree.html#cb35-3"></a><span class="kw">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">lty =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>Hmm, not quite the same…</p>
<p>A few points on the ROC space are helpful for understanding how to use it.</p>
<ul>
<li>The lower left point (0, 0) is the result of <em>always</em> predicting “negative” or in this case “MM” if “CH” is taken as the default class. Sure, your false positive rate is zero, but since you never predict a positive, your true positive rate is also zero.<br />
</li>
<li>The upper right point (1, 1) is the results of <em>always</em> predicting “positive” (or “CH” here). You catch all the positives, but you miss all the negatives.</li>
<li>The upper left point (0, 1) is the result of perfect accuracy. You catch all the positives and all the negatives.</li>
<li>The lower right point (1, 0) is the result of perfect imbecility. You made the exact wrong prediction every time.</li>
<li>The 45 degree diagonal is the result of randomly guessing positive (CH) X percent of the time. If you guess positive 90% of the time and the prevalence is 50%, your TPR will be 90% and your FPR will also be 90%, etc.</li>
</ul>
<p>From the last bullet, it is evident that any point below and to the right of the 45 degree diagonal represents an instance where the model would have been better off just predicting entirely one way or the other. The goal is for all nodes to bunch up in the upper left.</p>
<p>Points to the left of the diagonal with a low TPR can be thought of as “conservative” predicters - they only make positive (CH) predictions with strong evidence. Points to the left of the diagnonal with a high TPR can be thought of as “liberal” predicters - they make positive (CH) predictions with weak evidence.</p>
</div>
<div id="caret-approach" class="section level3">
<h3><span class="header-section-number">9.1.3</span> Caret Approach</h3>
<p>I can also fit the model with <code>caret::train()</code>. There are two ways to tune hyperparameters in <code>train()</code>:</p>
<ul>
<li>set the number of tuning parameter values to consider by setting <code>tuneLength</code>, or</li>
<li>set particular values to consider for each parameter by defining a <code>tuneGrid</code>.</li>
</ul>
<p>I’ll build the model using 10-fold cross-validation to optimize the hyperparameter CP. If you don’t have any idea what the tuning parameter ought to look like, use <code>tuneLength</code> to get close, then fine-tune with <code>tuneGrid</code>. That’s what I’ll do. I’ll create a training control object that I can re-use in other model builds.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="classification-tree.html#cb36-1"></a>oj_trControl =<span class="st"> </span><span class="kw">trainControl</span>(</span>
<span id="cb36-2"><a href="classification-tree.html#cb36-2"></a>   <span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>,  <span class="co"># k-fold cross validation</span></span>
<span id="cb36-3"><a href="classification-tree.html#cb36-3"></a>   <span class="dt">number =</span> <span class="dv">10</span>,  <span class="co"># 10 folds</span></span>
<span id="cb36-4"><a href="classification-tree.html#cb36-4"></a>   <span class="dt">savePredictions =</span> <span class="st">&quot;final&quot;</span>,       <span class="co"># save predictions for the optimal tuning parameter</span></span>
<span id="cb36-5"><a href="classification-tree.html#cb36-5"></a>   <span class="dt">classProbs =</span> <span class="ot">TRUE</span>  <span class="co"># return class probabilities in addition to predicted values</span></span>
<span id="cb36-6"><a href="classification-tree.html#cb36-6"></a><span class="co">#   summaryFunction = twoClassSummary  # computes sensitivity, specificity and the area under the ROC curve.</span></span>
<span id="cb36-7"><a href="classification-tree.html#cb36-7"></a>   )</span></code></pre></div>
<p>Now fit the model.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="classification-tree.html#cb37-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb37-2"><a href="classification-tree.html#cb37-2"></a>oj_model_<span class="dv">2</span> =<span class="st"> </span><span class="kw">train</span>(</span>
<span id="cb37-3"><a href="classification-tree.html#cb37-3"></a>   Purchase <span class="op">~</span><span class="st"> </span>., </span>
<span id="cb37-4"><a href="classification-tree.html#cb37-4"></a>   <span class="dt">data =</span> oj_train, </span>
<span id="cb37-5"><a href="classification-tree.html#cb37-5"></a>   <span class="dt">method =</span> <span class="st">&quot;rpart&quot;</span>,</span>
<span id="cb37-6"><a href="classification-tree.html#cb37-6"></a>   <span class="dt">tuneLength =</span> <span class="dv">5</span>,</span>
<span id="cb37-7"><a href="classification-tree.html#cb37-7"></a>   <span class="dt">metric =</span> <span class="st">&quot;Accuracy&quot;</span>,</span>
<span id="cb37-8"><a href="classification-tree.html#cb37-8"></a>   <span class="dt">trControl =</span> oj_trControl</span>
<span id="cb37-9"><a href="classification-tree.html#cb37-9"></a>   )</span></code></pre></div>
<p><code>caret</code> built a full tree using <code>rpart</code>’s default parameters: gini splitting index, at least 20 observations in a node in order to consider splitting it, and at least 6 observations in each node. Caret then calculated the accuracy for each candidate value of <span class="math inline">\(\alpha\)</span>. Here is the results.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="classification-tree.html#cb38-1"></a><span class="kw">print</span>(oj_model_<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## CART 
## 
## 857 samples
##  17 predictor
##   2 classes: &#39;CH&#39;, &#39;MM&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... 
## Resampling results across tuning parameters:
## 
##   cp           Accuracy   Kappa    
##   0.005988024  0.8085999  0.5931149
##   0.008982036  0.8086267  0.5943277
##   0.013473054  0.8051657  0.5885521
##   0.032934132  0.7841798  0.5371171
##   0.479041916  0.6603904  0.1774773
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was cp = 0.008982036.</code></pre>
<p>The second <code>cp</code> (0.008982036) produced the highest accuracy. I can drill into the best value of <code>cp</code> using a tuning grid. I’ll try that now.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="classification-tree.html#cb40-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb40-2"><a href="classification-tree.html#cb40-2"></a>oj_model_<span class="dv">3</span> =<span class="st"> </span><span class="kw">train</span>(</span>
<span id="cb40-3"><a href="classification-tree.html#cb40-3"></a>   Purchase <span class="op">~</span><span class="st"> </span>., </span>
<span id="cb40-4"><a href="classification-tree.html#cb40-4"></a>   <span class="dt">data =</span> oj_train, </span>
<span id="cb40-5"><a href="classification-tree.html#cb40-5"></a>   <span class="dt">method =</span> <span class="st">&quot;rpart&quot;</span>,</span>
<span id="cb40-6"><a href="classification-tree.html#cb40-6"></a>   <span class="dt">tuneGrid =</span> <span class="kw">expand.grid</span>(<span class="dt">cp =</span> <span class="kw">seq</span>(<span class="dt">from =</span> <span class="fl">0.001</span>, <span class="dt">to =</span> <span class="fl">0.010</span>, <span class="dt">length =</span> <span class="dv">11</span>)),  </span>
<span id="cb40-7"><a href="classification-tree.html#cb40-7"></a>   <span class="dt">metric=</span><span class="st">&#39;Accuracy&#39;</span>,</span>
<span id="cb40-8"><a href="classification-tree.html#cb40-8"></a>   <span class="dt">trControl =</span> oj_trControl</span>
<span id="cb40-9"><a href="classification-tree.html#cb40-9"></a>   )</span>
<span id="cb40-10"><a href="classification-tree.html#cb40-10"></a><span class="kw">print</span>(oj_model_<span class="dv">3</span>)</span></code></pre></div>
<pre><code>## CART 
## 
## 857 samples
##  17 predictor
##   2 classes: &#39;CH&#39;, &#39;MM&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... 
## Resampling results across tuning parameters:
## 
##   cp      Accuracy   Kappa    
##   0.0010  0.8004874  0.5753480
##   0.0019  0.8016502  0.5785232
##   0.0028  0.8039758  0.5845653
##   0.0037  0.8085999  0.5955198
##   0.0046  0.8039351  0.5851273
##   0.0055  0.8085863  0.5937949
##   0.0064  0.8085999  0.5931149
##   0.0073  0.8120883  0.6011446
##   0.0082  0.8120883  0.6011446
##   0.0091  0.8086267  0.5943277
##   0.0100  0.8086540  0.5953150
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was cp = 0.0082.</code></pre>
<p>The beset model is at cp = 0.009. Here are the rules in the final model.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="classification-tree.html#cb42-1"></a>oj_model_<span class="dv">3</span><span class="op">$</span>finalModel</span></code></pre></div>
<pre><code>## n= 857 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##   1) root 857 334 CH (0.61026838 0.38973162)  
##     2) LoyalCH&gt;=0.48285 537  94 CH (0.82495345 0.17504655)  
##       4) LoyalCH&gt;=0.7648795 271  13 CH (0.95202952 0.04797048) *
##       5) LoyalCH&lt; 0.7648795 266  81 CH (0.69548872 0.30451128)  
##        10) PriceDiff&gt;=-0.165 226  50 CH (0.77876106 0.22123894) *
##        11) PriceDiff&lt; -0.165 40   9 MM (0.22500000 0.77500000) *
##     3) LoyalCH&lt; 0.48285 320  80 MM (0.25000000 0.75000000)  
##       6) LoyalCH&gt;=0.2761415 146  58 MM (0.39726027 0.60273973)  
##        12) SalePriceMM&gt;=2.04 71  31 CH (0.56338028 0.43661972)  
##          24) LoyalCH&lt; 0.303104 7   0 CH (1.00000000 0.00000000) *
##          25) LoyalCH&gt;=0.303104 64  31 CH (0.51562500 0.48437500)  
##            50) WeekofPurchase&gt;=246.5 52  22 CH (0.57692308 0.42307692)  
##             100) PriceCH&lt; 1.94 35  11 CH (0.68571429 0.31428571) *
##             101) PriceCH&gt;=1.94 17   6 MM (0.35294118 0.64705882) *
##            51) WeekofPurchase&lt; 246.5 12   3 MM (0.25000000 0.75000000) *
##        13) SalePriceMM&lt; 2.04 75  18 MM (0.24000000 0.76000000) *
##       7) LoyalCH&lt; 0.2761415 174  22 MM (0.12643678 0.87356322) *</code></pre>
<p>Here is the tree.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="classification-tree.html#cb44-1"></a><span class="kw">rpart.plot</span>(oj_model_<span class="dv">3</span><span class="op">$</span>finalModel)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>Here is the ROC curve.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="classification-tree.html#cb45-1"></a><span class="kw">library</span>(plotROC)</span>
<span id="cb45-2"><a href="classification-tree.html#cb45-2"></a><span class="kw">ggplot</span>(oj_model_<span class="dv">3</span><span class="op">$</span>pred) <span class="op">+</span><span class="st"> </span></span>
<span id="cb45-3"><a href="classification-tree.html#cb45-3"></a><span class="st">    </span><span class="kw">geom_roc</span>(</span>
<span id="cb45-4"><a href="classification-tree.html#cb45-4"></a>       <span class="kw">aes</span>(</span>
<span id="cb45-5"><a href="classification-tree.html#cb45-5"></a>          <span class="dt">m =</span> MM, </span>
<span id="cb45-6"><a href="classification-tree.html#cb45-6"></a>          <span class="dt">d =</span> <span class="kw">factor</span>(obs, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;CH&quot;</span>, <span class="st">&quot;MM&quot;</span>))</span>
<span id="cb45-7"><a href="classification-tree.html#cb45-7"></a>       ),</span>
<span id="cb45-8"><a href="classification-tree.html#cb45-8"></a>       <span class="dt">hjust =</span> <span class="fl">-0.4</span>, <span class="dt">vjust =</span> <span class="fl">1.5</span></span>
<span id="cb45-9"><a href="classification-tree.html#cb45-9"></a>    ) <span class="op">+</span></span>
<span id="cb45-10"><a href="classification-tree.html#cb45-10"></a><span class="st">   </span><span class="kw">coord_equal</span>()</span></code></pre></div>
<pre><code>## Warning in verify_d(data$d): D not labeled 0/1, assuming CH = 0 and MM = 1!</code></pre>
<p><img src="data-sci_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>Here are the cross-validated Accuracy for each candidate cp value.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="classification-tree.html#cb47-1"></a><span class="kw">plot</span>(oj_model_<span class="dv">3</span>)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<p>Evaluate the model by making predictions with the test data set.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="classification-tree.html#cb48-1"></a>oj_model_<span class="dv">3</span>_preds &lt;-<span class="st"> </span><span class="kw">predict</span>(oj_model_<span class="dv">3</span>, oj_test, <span class="dt">type =</span> <span class="st">&quot;raw&quot;</span>)</span></code></pre></div>
<p>The confusion matrix shows the true positives and true negatives.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="classification-tree.html#cb49-1"></a>oj_model_<span class="dv">3</span>_cm &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(</span>
<span id="cb49-2"><a href="classification-tree.html#cb49-2"></a>   <span class="dt">data =</span> oj_model_<span class="dv">3</span>_preds, </span>
<span id="cb49-3"><a href="classification-tree.html#cb49-3"></a>   <span class="dt">reference =</span> oj_test<span class="op">$</span>Purchase</span>
<span id="cb49-4"><a href="classification-tree.html#cb49-4"></a>)</span>
<span id="cb49-5"><a href="classification-tree.html#cb49-5"></a>oj_model_<span class="dv">3</span>_cm</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  CH  MM
##         CH 115  18
##         MM  15  65
##                                           
##                Accuracy : 0.8451          
##                  95% CI : (0.7894, 0.8909)
##     No Information Rate : 0.6103          
##     P-Value [Acc &gt; NIR] : 6.311e-14       
##                                           
##                   Kappa : 0.6721          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.7277          
##                                           
##             Sensitivity : 0.8846          
##             Specificity : 0.7831          
##          Pos Pred Value : 0.8647          
##          Neg Pred Value : 0.8125          
##              Prevalence : 0.6103          
##          Detection Rate : 0.5399          
##    Detection Prevalence : 0.6244          
##       Balanced Accuracy : 0.8339          
##                                           
##        &#39;Positive&#39; Class : CH              
## </code></pre>
<p>The accuracy metric is the slightly worse than in my previous model. Here is a graphical representation of the confusion matrix.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="classification-tree.html#cb51-1"></a><span class="kw">plot</span>(oj_test<span class="op">$</span>Purchase, oj_model_<span class="dv">3</span>_preds, </span>
<span id="cb51-2"><a href="classification-tree.html#cb51-2"></a>     <span class="dt">main =</span> <span class="st">&quot;Simple Classification: Predicted vs. Actual&quot;</span>,</span>
<span id="cb51-3"><a href="classification-tree.html#cb51-3"></a>     <span class="dt">xlab =</span> <span class="st">&quot;Actual&quot;</span>,</span>
<span id="cb51-4"><a href="classification-tree.html#cb51-4"></a>     <span class="dt">ylab =</span> <span class="st">&quot;Predicted&quot;</span>)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<p>Finally, here is the variable importance plot.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="classification-tree.html#cb52-1"></a><span class="kw">plot</span>(<span class="kw">varImp</span>(oj_model_<span class="dv">3</span>), <span class="dt">main=</span><span class="st">&quot;Variable Importance with Simple Classication&quot;</span>)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<p>Looks like the manual effort faired best. Here is a summary the accuracy rates of the three models.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="classification-tree.html#cb53-1"></a><span class="kw">rbind</span>(<span class="kw">data.frame</span>(<span class="dt">model =</span> <span class="st">&quot;Manual Class&quot;</span>, <span class="dt">Acc =</span> <span class="kw">round</span>(oj_model_1b_cm<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>], <span class="dv">5</span>)), </span>
<span id="cb53-2"><a href="classification-tree.html#cb53-2"></a>      <span class="kw">data.frame</span>(<span class="dt">model =</span> <span class="st">&quot;Caret w/tuneGrid&quot;</span>, <span class="dt">Acc =</span> <span class="kw">round</span>(oj_model_<span class="dv">3</span>_cm<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>], <span class="dv">5</span>))</span>
<span id="cb53-3"><a href="classification-tree.html#cb53-3"></a>)</span></code></pre></div>
<pre><code>##                      model     Acc
## Accuracy      Manual Class 0.85915
## Accuracy1 Caret w/tuneGrid 0.84507</code></pre>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Fawcett2005">
<p>Fawcett, Tom. 2005. <em>An Introduction to Roc Analysis</em>. ELSEVIER. <a href="https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf">https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="decision-trees.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regression-trees.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["data-sci.pdf", "data-sci.epub"],
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
