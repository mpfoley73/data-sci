<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9.5 Gradient Boosting | My Data Science Notes</title>
  <meta name="description" content="This is a compendium of notes from classes, tutorials, etc. that I reference from time to time." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="9.5 Gradient Boosting | My Data Science Notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a compendium of notes from classes, tutorials, etc. that I reference from time to time." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9.5 Gradient Boosting | My Data Science Notes" />
  
  <meta name="twitter:description" content="This is a compendium of notes from classes, tutorials, etc. that I reference from time to time." />
  

<meta name="author" content="Michael Foley" />


<meta name="date" content="2020-07-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="random-forests.html"/>
<link rel="next" href="summary.html"/>
<script src="assets/jquery-2.2.3/jquery.min.js"></script>
<link href="assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="assets/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="assets/tabwid-1.0.0/tabwid.css" rel="stylesheet" />
<script src="assets/tabwid-1.0.0/tabwid.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">My Data Science Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Intro</a></li>
<li class="chapter" data-level="1" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>1</b> Probability</a><ul>
<li class="chapter" data-level="1.1" data-path="principles.html"><a href="principles.html"><i class="fa fa-check"></i><b>1.1</b> Principles</a></li>
<li class="chapter" data-level="1.2" data-path="disc-dist.html"><a href="disc-dist.html"><i class="fa fa-check"></i><b>1.2</b> Discrete Distributions</a><ul>
<li class="chapter" data-level="1.2.1" data-path="disc-dist.html"><a href="disc-dist.html#bernoulli"><i class="fa fa-check"></i><b>1.2.1</b> Bernoulli</a></li>
<li class="chapter" data-level="1.2.2" data-path="disc-dist.html"><a href="disc-dist.html#binomial"><i class="fa fa-check"></i><b>1.2.2</b> Binomial</a></li>
<li class="chapter" data-level="1.2.3" data-path="disc-dist.html"><a href="disc-dist.html#poission"><i class="fa fa-check"></i><b>1.2.3</b> Poission</a></li>
<li class="chapter" data-level="1.2.4" data-path="disc-dist.html"><a href="disc-dist.html#multinomial"><i class="fa fa-check"></i><b>1.2.4</b> Multinomial</a></li>
<li class="chapter" data-level="1.2.5" data-path="disc-dist.html"><a href="disc-dist.html#negative-binomial"><i class="fa fa-check"></i><b>1.2.5</b> Negative-Binomial</a></li>
<li class="chapter" data-level="1.2.6" data-path="disc-dist.html"><a href="disc-dist.html#geometric"><i class="fa fa-check"></i><b>1.2.6</b> Geometric</a></li>
<li class="chapter" data-level="1.2.7" data-path="disc-dist.html"><a href="disc-dist.html#hypergeometric"><i class="fa fa-check"></i><b>1.2.7</b> Hypergeometric</a></li>
<li class="chapter" data-level="1.2.8" data-path="disc-dist.html"><a href="disc-dist.html#gamma"><i class="fa fa-check"></i><b>1.2.8</b> Gamma</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="cont-dist.html"><a href="cont-dist.html"><i class="fa fa-check"></i><b>1.3</b> Continuous Distributions</a><ul>
<li class="chapter" data-level="1.3.1" data-path="cont-dist.html"><a href="cont-dist.html#normal"><i class="fa fa-check"></i><b>1.3.1</b> Normal</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="join-distributions.html"><a href="join-distributions.html"><i class="fa fa-check"></i><b>1.4</b> Join Distributions</a></li>
<li class="chapter" data-level="1.5" data-path="likelihood.html"><a href="likelihood.html"><i class="fa fa-check"></i><b>1.5</b> Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="discrete-analysis.html"><a href="discrete-analysis.html"><i class="fa fa-check"></i><b>2</b> Categorical Analysis - Nonmodel</a><ul>
<li class="chapter" data-level="2.1" data-path="chi-square-test.html"><a href="chi-square-test.html"><i class="fa fa-check"></i><b>2.1</b> Chi-Square Test</a></li>
<li class="chapter" data-level="2.2" data-path="one-way-tables.html"><a href="one-way-tables.html"><i class="fa fa-check"></i><b>2.2</b> One-Way Tables</a><ul>
<li class="chapter" data-level="2.2.1" data-path="one-way-tables.html"><a href="one-way-tables.html#chi-square-goodness-of-fit-test"><i class="fa fa-check"></i><b>2.2.1</b> Chi-Square Goodness-of-Fit Test</a></li>
<li class="chapter" data-level="2.2.2" data-path="one-way-tables.html"><a href="one-way-tables.html#proportion-test"><i class="fa fa-check"></i><b>2.2.2</b> Proportion Test</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="two-way-tables.html"><a href="two-way-tables.html"><i class="fa fa-check"></i><b>2.3</b> Two-Way Tables</a><ul>
<li class="chapter" data-level="2.3.1" data-path="two-way-tables.html"><a href="two-way-tables.html#chi-square-independence-test"><i class="fa fa-check"></i><b>2.3.1</b> Chi-Square Independence Test</a></li>
<li class="chapter" data-level="2.3.2" data-path="two-way-tables.html"><a href="two-way-tables.html#residuals-analysis"><i class="fa fa-check"></i><b>2.3.2</b> Residuals Analysis</a></li>
<li class="chapter" data-level="2.3.3" data-path="two-way-tables.html"><a href="two-way-tables.html#difference-in-proportions"><i class="fa fa-check"></i><b>2.3.3</b> Difference in Proportions</a></li>
<li class="chapter" data-level="2.3.4" data-path="two-way-tables.html"><a href="two-way-tables.html#relative-risk"><i class="fa fa-check"></i><b>2.3.4</b> Relative Risk</a></li>
<li class="chapter" data-level="2.3.5" data-path="two-way-tables.html"><a href="two-way-tables.html#odds-ratio"><i class="fa fa-check"></i><b>2.3.5</b> Odds Ratio</a></li>
<li class="chapter" data-level="2.3.6" data-path="two-way-tables.html"><a href="two-way-tables.html#partitioning-chi-square"><i class="fa fa-check"></i><b>2.3.6</b> Partitioning Chi-Square</a></li>
<li class="chapter" data-level="2.3.7" data-path="two-way-tables.html"><a href="two-way-tables.html#correlation"><i class="fa fa-check"></i><b>2.3.7</b> Correlation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="k-way-tables.html"><a href="k-way-tables.html"><i class="fa fa-check"></i><b>2.4</b> K-Way Tables</a><ul>
<li class="chapter" data-level="2.4.1" data-path="k-way-tables.html"><a href="k-way-tables.html#odds-ratio-1"><i class="fa fa-check"></i><b>2.4.1</b> Odds Ratio</a></li>
<li class="chapter" data-level="2.4.2" data-path="k-way-tables.html"><a href="k-way-tables.html#chi-square-independence-test-1"><i class="fa fa-check"></i><b>2.4.2</b> Chi-Square Independence Test</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="continuous-analysis.html"><a href="continuous-analysis.html"><i class="fa fa-check"></i><b>3</b> Continuous Variable Analysis</a><ul>
<li class="chapter" data-level="3.0.1" data-path="two-way-tables.html"><a href="two-way-tables.html#correlation"><i class="fa fa-check"></i><b>3.0.1</b> Correlation</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="experiment-design.html"><a href="experiment-design.html"><i class="fa fa-check"></i><b>4</b> Experiment Design</a><ul>
<li class="chapter" data-level="4.1" data-path="single-factor.html"><a href="single-factor.html"><i class="fa fa-check"></i><b>4.1</b> Single Factor</a></li>
<li class="chapter" data-level="4.2" data-path="blocking.html"><a href="blocking.html"><i class="fa fa-check"></i><b>4.2</b> Blocking</a></li>
<li class="chapter" data-level="4.3" data-path="nested.html"><a href="nested.html"><i class="fa fa-check"></i><b>4.3</b> Nested</a></li>
<li class="chapter" data-level="4.4" data-path="split-plot.html"><a href="split-plot.html"><i class="fa fa-check"></i><b>4.4</b> Split Plot</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-2-supervised-machine-learning.html"><a href="part-2-supervised-machine-learning.html"><i class="fa fa-check"></i>PART 2: Supervised Machine Learning</a><ul>
<li class="chapter" data-level="4.5" data-path="linear-regression-model.html"><a href="linear-regression-model.html"><i class="fa fa-check"></i><b>4.5</b> Linear Regression Model</a></li>
<li class="chapter" data-level="4.6" data-path="parameter-estimation.html"><a href="parameter-estimation.html"><i class="fa fa-check"></i><b>4.6</b> Parameter Estimation</a></li>
<li class="chapter" data-level="4.7" data-path="model-assumptions.html"><a href="model-assumptions.html"><i class="fa fa-check"></i><b>4.7</b> Model Assumptions</a><ul>
<li class="chapter" data-level="4.7.1" data-path="model-assumptions.html"><a href="model-assumptions.html#linearity"><i class="fa fa-check"></i><b>4.7.1</b> Linearity</a></li>
<li class="chapter" data-level="4.7.2" data-path="model-assumptions.html"><a href="model-assumptions.html#multicollinearity"><i class="fa fa-check"></i><b>4.7.2</b> Multicollinearity</a></li>
<li class="chapter" data-level="4.7.3" data-path="model-assumptions.html"><a href="model-assumptions.html#normality"><i class="fa fa-check"></i><b>4.7.3</b> Normality</a></li>
<li class="chapter" data-level="4.7.4" data-path="model-assumptions.html"><a href="model-assumptions.html#equal-variances"><i class="fa fa-check"></i><b>4.7.4</b> Equal Variances</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="prediction.html"><a href="prediction.html"><i class="fa fa-check"></i><b>4.8</b> Prediction</a></li>
<li class="chapter" data-level="4.9" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>4.9</b> Inference</a><ul>
<li class="chapter" data-level="4.9.1" data-path="inference.html"><a href="inference.html#t-test"><i class="fa fa-check"></i><b>4.9.1</b> <em>t</em>-Test</a></li>
<li class="chapter" data-level="4.9.2" data-path="inference.html"><a href="inference.html#f-test"><i class="fa fa-check"></i><b>4.9.2</b> <em>F</em>-Test</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="interpretation.html"><a href="interpretation.html"><i class="fa fa-check"></i><b>4.10</b> Interpretation</a></li>
<li class="chapter" data-level="4.11" data-path="model-validation.html"><a href="model-validation.html"><i class="fa fa-check"></i><b>4.11</b> Model Validation</a><ul>
<li class="chapter" data-level="4.11.1" data-path="model-validation.html"><a href="model-validation.html#accuracy-metrics"><i class="fa fa-check"></i><b>4.11.1</b> Accuracy Metrics</a></li>
<li class="chapter" data-level="4.11.2" data-path="model-validation.html"><a href="model-validation.html#cross-validation"><i class="fa fa-check"></i><b>4.11.2</b> Cross-Validation</a></li>
<li class="chapter" data-level="4.11.3" data-path="model-validation.html"><a href="model-validation.html#gain-curve"><i class="fa fa-check"></i><b>4.11.3</b> Gain Curve</a></li>
</ul></li>
<li class="chapter" data-level="4.12" data-path="reference.html"><a href="reference.html"><i class="fa fa-check"></i><b>4.12</b> Reference</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>5</b> Generalized Linear Models</a><ul>
<li class="chapter" data-level="5.1" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>5.1</b> Logistic Regression</a></li>
<li class="chapter" data-level="5.2" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html"><i class="fa fa-check"></i><b>5.2</b> Multinomial Logistic Regression</a></li>
<li class="chapter" data-level="5.3" data-path="ordinal-logistic-regression.html"><a href="ordinal-logistic-regression.html"><i class="fa fa-check"></i><b>5.3</b> Ordinal Logistic Regression</a><ul>
<li class="chapter" data-level="5.3.1" data-path="ordinal-logistic-regression.html"><a href="ordinal-logistic-regression.html#assumptions"><i class="fa fa-check"></i><b>5.3.1</b> Assumptions</a></li>
<li class="chapter" data-level="5.3.2" data-path="ordinal-logistic-regression.html"><a href="ordinal-logistic-regression.html#modeling"><i class="fa fa-check"></i><b>5.3.2</b> Modeling</a></li>
<li class="chapter" data-level="5.3.3" data-path="ordinal-logistic-regression.html"><a href="ordinal-logistic-regression.html#case-study"><i class="fa fa-check"></i><b>5.3.3</b> Case Study</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="poisson-regression.html"><a href="poisson-regression.html"><i class="fa fa-check"></i><b>5.4</b> Poisson Regression</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multivariate-statistical-analysis.html"><a href="multivariate-statistical-analysis.html"><i class="fa fa-check"></i><b>6</b> Multivariate Statistical Analysis</a><ul>
<li class="chapter" data-level="6.1" data-path="background.html"><a href="background.html"><i class="fa fa-check"></i><b>6.1</b> Background</a></li>
<li class="chapter" data-level="6.2" data-path="manova.html"><a href="manova.html"><i class="fa fa-check"></i><b>6.2</b> MANOVA</a></li>
<li class="chapter" data-level="6.3" data-path="repeated-measures.html"><a href="repeated-measures.html"><i class="fa fa-check"></i><b>6.3</b> Repeated Measures</a></li>
<li class="chapter" data-level="6.4" data-path="lda.html"><a href="lda.html"><i class="fa fa-check"></i><b>6.4</b> LDA</a></li>
<li class="chapter" data-level="6.5" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>6.5</b> PCA</a></li>
<li class="chapter" data-level="6.6" data-path="factor-analysis.html"><a href="factor-analysis.html"><i class="fa fa-check"></i><b>6.6</b> Factor Analysis</a></li>
<li class="chapter" data-level="6.7" data-path="canonical-correlation.html"><a href="canonical-correlation.html"><i class="fa fa-check"></i><b>6.7</b> Canonical Correlation</a></li>
<li class="chapter" data-level="6.8" data-path="cluster-analysis.html"><a href="cluster-analysis.html"><i class="fa fa-check"></i><b>6.8</b> Cluster Analysis</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>7</b> Classification</a></li>
<li class="chapter" data-level="8" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>8</b> Regularization</a><ul>
<li class="chapter" data-level="8.1" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>8.1</b> Ridge</a></li>
<li class="chapter" data-level="8.2" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>8.2</b> Lasso</a></li>
<li class="chapter" data-level="8.3" data-path="elastic-net.html"><a href="elastic-net.html"><i class="fa fa-check"></i><b>8.3</b> Elastic Net</a></li>
<li class="chapter" data-level="" data-path="model-summary.html"><a href="model-summary.html"><i class="fa fa-check"></i>Model Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>9</b> Decision Trees</a><ul>
<li class="chapter" data-level="9.1" data-path="classification-tree.html"><a href="classification-tree.html"><i class="fa fa-check"></i><b>9.1</b> Classification Tree</a><ul>
<li class="chapter" data-level="9.1.1" data-path="classification-tree.html"><a href="classification-tree.html#measuring-performance"><i class="fa fa-check"></i><b>9.1.1</b> Measuring Performance</a></li>
<li class="chapter" data-level="9.1.2" data-path="classification-tree.html"><a href="classification-tree.html#training-with-caret"><i class="fa fa-check"></i><b>9.1.2</b> Training with Caret</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="regression-tree.html"><a href="regression-tree.html"><i class="fa fa-check"></i><b>9.2</b> Regression Tree</a><ul>
<li class="chapter" data-level="9.2.1" data-path="regression-tree.html"><a href="regression-tree.html#training-with-caret-1"><i class="fa fa-check"></i><b>9.2.1</b> Training with Caret</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="bagged-trees.html"><a href="bagged-trees.html"><i class="fa fa-check"></i><b>9.3</b> Bagged Trees</a><ul>
<li class="chapter" data-level="9.3.1" data-path="bagged-trees.html"><a href="bagged-trees.html#bagged-classification-tree"><i class="fa fa-check"></i><b>9.3.1</b> Bagged Classification Tree</a></li>
<li class="chapter" data-level="9.3.2" data-path="bagged-trees.html"><a href="bagged-trees.html#bagging-regression-tree"><i class="fa fa-check"></i><b>9.3.2</b> Bagging Regression Tree</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="random-forests.html"><a href="random-forests.html"><i class="fa fa-check"></i><b>9.4</b> Random Forests</a></li>
<li class="chapter" data-level="9.5" data-path="gradient-boosting.html"><a href="gradient-boosting.html"><i class="fa fa-check"></i><b>9.5</b> Gradient Boosting</a></li>
<li class="chapter" data-level="9.6" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>9.6</b> Summary</a><ul>
<li class="chapter" data-level="9.6.1" data-path="summary.html"><a href="summary.html#classification-trees"><i class="fa fa-check"></i><b>9.6.1</b> Classification Trees</a></li>
<li class="chapter" data-level="9.6.2" data-path="summary.html"><a href="summary.html#regression-trees"><i class="fa fa-check"></i><b>9.6.2</b> Regression Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="non-linear-models.html"><a href="non-linear-models.html"><i class="fa fa-check"></i><b>10</b> Non-linear Models</a><ul>
<li class="chapter" data-level="10.1" data-path="splines.html"><a href="splines.html"><i class="fa fa-check"></i><b>10.1</b> Splines</a></li>
<li class="chapter" data-level="10.2" data-path="mars.html"><a href="mars.html"><i class="fa fa-check"></i><b>10.2</b> MARS</a></li>
<li class="chapter" data-level="10.3" data-path="gam.html"><a href="gam.html"><i class="fa fa-check"></i><b>10.3</b> GAM</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>11</b> Support Vector Machines</a><ul>
<li class="chapter" data-level="11.1" data-path="maximal-margin-classifier.html"><a href="maximal-margin-classifier.html"><i class="fa fa-check"></i><b>11.1</b> Maximal Margin Classifier</a></li>
<li class="chapter" data-level="11.2" data-path="support-vector-classifier.html"><a href="support-vector-classifier.html"><i class="fa fa-check"></i><b>11.2</b> Support Vector Classifier</a></li>
<li class="chapter" data-level="11.3" data-path="support-vector-machines-1.html"><a href="support-vector-machines-1.html"><i class="fa fa-check"></i><b>11.3</b> Support Vector Machines</a></li>
<li class="chapter" data-level="11.4" data-path="disc-dist.html"><a href="disc-dist.html#example"><i class="fa fa-check"></i><b>11.4</b> Example</a></li>
<li class="chapter" data-level="11.5" data-path="using-caret.html"><a href="using-caret.html"><i class="fa fa-check"></i><b>11.5</b> Using Caret</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html"><i class="fa fa-check"></i><b>12</b> Principal Components Analysis</a></li>
<li class="chapter" data-level="13" data-path="text-mining.html"><a href="text-mining.html"><i class="fa fa-check"></i><b>13</b> Text Mining</a><ul>
<li class="chapter" data-level="13.1" data-path="n-grams.html"><a href="n-grams.html"><i class="fa fa-check"></i><b>13.1</b> N-Grams</a></li>
<li class="chapter" data-level="13.2" data-path="converting-to-and-from-non-tidy-formats.html"><a href="converting-to-and-from-non-tidy-formats.html"><i class="fa fa-check"></i><b>13.2</b> Converting to and from non-tidy formats</a></li>
<li class="chapter" data-level="13.3" data-path="disc-dist.html"><a href="disc-dist.html#example"><i class="fa fa-check"></i><b>13.3</b> Example</a></li>
<li class="chapter" data-level="13.4" data-path="stringr-package.html"><a href="stringr-package.html"><i class="fa fa-check"></i><b>13.4</b> stringr package</a></li>
<li class="chapter" data-level="13.5" data-path="regular-expressions.html"><a href="regular-expressions.html"><i class="fa fa-check"></i><b>13.5</b> Regular Expressions</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="survival-analysis.html"><a href="survival-analysis.html"><i class="fa fa-check"></i><b>14</b> Survival Analysis</a><ul>
<li class="chapter" data-level="14.1" data-path="basic-concepts.html"><a href="basic-concepts.html"><i class="fa fa-check"></i><b>14.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="14.2" data-path="survival-curve-estimation.html"><a href="survival-curve-estimation.html"><i class="fa fa-check"></i><b>14.2</b> Survival Curve Estimation</a><ul>
<li class="chapter" data-level="14.2.1" data-path="survival-curve-estimation.html"><a href="survival-curve-estimation.html#kaplan-meier"><i class="fa fa-check"></i><b>14.2.1</b> Kaplan-Meier</a></li>
<li class="chapter" data-level="14.2.2" data-path="survival-curve-estimation.html"><a href="survival-curve-estimation.html#weibull"><i class="fa fa-check"></i><b>14.2.2</b> Weibull</a></li>
<li class="chapter" data-level="14.2.3" data-path="survival-curve-estimation.html"><a href="survival-curve-estimation.html#cox"><i class="fa fa-check"></i><b>14.2.3</b> Cox</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="publishing-to-bookdown.html"><a href="publishing-to-bookdown.html"><i class="fa fa-check"></i>Publishing to BookDown</a></li>
<li class="chapter" data-level="" data-path="shiny-apps.html"><a href="shiny-apps.html"><i class="fa fa-check"></i>Shiny Apps</a></li>
<li class="chapter" data-level="" data-path="packages.html"><a href="packages.html"><i class="fa fa-check"></i>Packages</a><ul>
<li class="chapter" data-level="" data-path="packages.html"><a href="packages.html#create-a-package"><i class="fa fa-check"></i>Create a package</a></li>
<li class="chapter" data-level="14.2.4" data-path="packages.html"><a href="packages.html#document-functions-with-roxygen"><i class="fa fa-check"></i><b>14.2.4</b> Document Functions with roxygen</a></li>
<li class="chapter" data-level="" data-path="packages.html"><a href="packages.html#create-data"><i class="fa fa-check"></i>Create Data</a></li>
<li class="chapter" data-level="" data-path="packages.html"><a href="packages.html#create-vignette"><i class="fa fa-check"></i>Create Vignette</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">My Data Science Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="gradient-boosting" class="section level2">
<h2><span class="header-section-number">9.5</span> Gradient Boosting</h2>
<p><strong>Note</strong>: I learned gradient boosting from <a href="https://explained.ai/gradient-boosting/L2-loss.html">explained.ai</a>.</p>
<p>Gradient boosting machine (GBM) is an additive modeling algorithm that gradually builds a composite model by iteratively adding <em>M</em> weak sub-models based on the performance of the prior iteration’s composite,</p>
<p><span class="math display">\[F_M(x) = \sum_m^M f_m(x).\]</span></p>
<p>The idea is to fit a weak model, then replace the response values with the residuals from that model, and fit another model. Adding the residual prediction model to the original response prediction model produces a more accurate model. GBM repeats this process over and over, running new models to predict the residuals of the previous composite models, and adding the results to produce new composites. With each iteration, the model becomes stronger and stronger. The successive trees are usually weighted to slow down the learning rate. “Shrinkage” reduces the influence of each individual tree and leaves space for future trees to improve the model.</p>
<p><span class="math display">\[F_M(x) = f_0 + \eta\sum_{m = 1}^M f_m(x).\]</span></p>
<p>The smaller the learning rate, <span class="math inline">\(\eta\)</span>, the larger the number of trees, <span class="math inline">\(M\)</span>. <span class="math inline">\(\eta\)</span> and <span class="math inline">\(M\)</span> are hyperparameters. Other constraints to the trees are usually applied as additional hyperparameters, including, tree depth, number of nodes, minimum observations per split, and minimum improvement to loss.</p>
<p>The name “gradient boosting” refers to the <em>boosting</em> of a model with a <em>gradient</em>. Each round of training builds a <em>weak learner</em> and uses the residuals to calculate a gradient, the partial derivative of the loss function. Gradient boosting “descends the gradient” to adjust the model parameters to reduce the error in the next round of training.</p>
<p>In the case of classification problems, the loss function is the log-loss; for regression problems, the loss function is mean squared error. GBM continues until it reaches maximum number of trees or an acceptable error level.</p>
<div id="gradient-boosting-classification-tree" class="section level4">
<h4><span class="header-section-number">9.5.0.1</span> Gradient Boosting Classification Tree</h4>
<p>In addition to the gradient boosting machine algorithm, implemented in <strong>caret</strong> with <code>method = gbm</code>, there is a variable called Extreme Gradient Boosting, XGBoost, which frankly I don’t know anything about other than it is supposed to work extremely well. Let’s try them both!</p>
<div id="gbm" class="section level5">
<h5><span class="header-section-number">9.5.0.1.1</span> GBM</h5>
<p>I’ll predict <code>Purchase</code> from the <code>OJ</code> data set again, this time using the GBM method by specifying <code>method = "gbm"</code>. <code>gbm</code> has the following tuneable hyperparameters (see <code>modelLookup("gbm")</code>).</p>
<ul>
<li><code>n.trees</code>: number of boosting iterations, <span class="math inline">\(M\)</span></li>
<li><code>interaction.depth</code>: maximum tree depth</li>
<li><code>shrinkage</code>: shrinkage, <span class="math inline">\(\eta\)</span></li>
<li><code>n.minobsinnode</code>: minimum terminal node size</li>
</ul>
<p>I’ll use <code>tuneLength = 5</code>.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="gradient-boosting.html#cb114-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb114-2"><a href="gradient-boosting.html#cb114-2"></a>garbage &lt;-<span class="st"> </span><span class="kw">capture.output</span>(</span>
<span id="cb114-3"><a href="gradient-boosting.html#cb114-3"></a>oj_mdl_gbm &lt;-<span class="st"> </span><span class="kw">train</span>(</span>
<span id="cb114-4"><a href="gradient-boosting.html#cb114-4"></a>   Purchase <span class="op">~</span><span class="st"> </span>., </span>
<span id="cb114-5"><a href="gradient-boosting.html#cb114-5"></a>   <span class="dt">data =</span> oj_train, </span>
<span id="cb114-6"><a href="gradient-boosting.html#cb114-6"></a>   <span class="dt">method =</span> <span class="st">&quot;gbm&quot;</span>,</span>
<span id="cb114-7"><a href="gradient-boosting.html#cb114-7"></a>   <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>,</span>
<span id="cb114-8"><a href="gradient-boosting.html#cb114-8"></a>   <span class="dt">tuneLength =</span> <span class="dv">5</span>,</span>
<span id="cb114-9"><a href="gradient-boosting.html#cb114-9"></a>   <span class="dt">trControl =</span> oj_trControl</span>
<span id="cb114-10"><a href="gradient-boosting.html#cb114-10"></a>))</span>
<span id="cb114-11"><a href="gradient-boosting.html#cb114-11"></a>oj_mdl_gbm</span></code></pre></div>
<pre><code>## Stochastic Gradient Boosting 
## 
## 857 samples
##  17 predictor
##   2 classes: &#39;CH&#39;, &#39;MM&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... 
## Resampling results across tuning parameters:
## 
##   interaction.depth  n.trees  ROC        Sens       Spec     
##   1                   50      0.8838502  0.8701016  0.7155971
##   1                  100      0.8851784  0.8719521  0.7427807
##   1                  150      0.8843982  0.8738389  0.7548128
##   1                  200      0.8828378  0.8738389  0.7487522
##   1                  250      0.8812937  0.8719884  0.7367201
##   2                   50      0.8843060  0.8718795  0.7546346
##   2                  100      0.8865391  0.8681422  0.7546346
##   2                  150      0.8830249  0.8642961  0.7456328
##   2                  200      0.8822619  0.8642598  0.7515152
##   2                  250      0.8771918  0.8529028  0.7515152
##   3                   50      0.8874290  0.8681422  0.7606061
##   3                  100      0.8828219  0.8605588  0.7726381
##   3                  150      0.8806565  0.8566038  0.7634581
##   3                  200      0.8732572  0.8661829  0.7695187
##   3                  250      0.8711321  0.8604499  0.7604278
##   4                   50      0.8828612  0.8489840  0.7515152
##   4                  100      0.8792110  0.8604862  0.7606061
##   4                  150      0.8723941  0.8527939  0.7695187
##   4                  200      0.8690015  0.8546444  0.7605169
##   4                  250      0.8683316  0.8451016  0.7512478
##   5                   50      0.8893367  0.8604499  0.7636364
##   5                  100      0.8818969  0.8546807  0.7426025
##   5                  150      0.8762509  0.8490203  0.7574866
##   5                  200      0.8739284  0.8470247  0.7426025
##   5                  250      0.8713918  0.8413643  0.7455437
## 
## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1
## 
## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were n.trees = 50, interaction.depth =
##  5, shrinkage = 0.1 and n.minobsinnode = 10.</code></pre>
<p><code>train()</code> tuned <code>n.trees</code> ($M) and <code>interaction.depth</code>, holding <code>shrinkage = 0.1</code> (), and <code>n.minobsinnode = 10</code>. The optimal hyperparameter values were <code>n.trees = 50</code>, and <code>interaction.depth = 5</code>.</p>
<p>You can see from the tuning plot that accuracy is maximized at <span class="math inline">\(M=50\)</span> for tree depth of 5, but <span class="math inline">\(M=50\)</span> with tree depth of 3 worked nearly as well.</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="gradient-boosting.html#cb116-1"></a><span class="kw">plot</span>(oj_mdl_gbm)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-77-1.png" width="672" /></p>
<p>Let’s see how the model performed on the holdout set. The accuracy was 0.8451.</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="gradient-boosting.html#cb117-1"></a>oj_preds_gbm &lt;-<span class="st"> </span><span class="kw">bind_cols</span>(</span>
<span id="cb117-2"><a href="gradient-boosting.html#cb117-2"></a>   <span class="kw">predict</span>(oj_mdl_gbm, <span class="dt">newdata =</span> oj_test, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>),</span>
<span id="cb117-3"><a href="gradient-boosting.html#cb117-3"></a>   <span class="dt">Predicted =</span> <span class="kw">predict</span>(oj_mdl_gbm, <span class="dt">newdata =</span> oj_test, <span class="dt">type =</span> <span class="st">&quot;raw&quot;</span>),</span>
<span id="cb117-4"><a href="gradient-boosting.html#cb117-4"></a>   <span class="dt">Actual =</span> oj_test<span class="op">$</span>Purchase</span>
<span id="cb117-5"><a href="gradient-boosting.html#cb117-5"></a>)</span>
<span id="cb117-6"><a href="gradient-boosting.html#cb117-6"></a></span>
<span id="cb117-7"><a href="gradient-boosting.html#cb117-7"></a>oj_cm_gbm &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(oj_preds_gbm<span class="op">$</span>Predicted, <span class="dt">reference =</span> oj_preds_gbm<span class="op">$</span>Actual)</span>
<span id="cb117-8"><a href="gradient-boosting.html#cb117-8"></a>oj_cm_gbm</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  CH  MM
##         CH 113  16
##         MM  17  67
##                                           
##                Accuracy : 0.8451          
##                  95% CI : (0.7894, 0.8909)
##     No Information Rate : 0.6103          
##     P-Value [Acc &gt; NIR] : 6.311e-14       
##                                           
##                   Kappa : 0.675           
##                                           
##  Mcnemar&#39;s Test P-Value : 1               
##                                           
##             Sensitivity : 0.8692          
##             Specificity : 0.8072          
##          Pos Pred Value : 0.8760          
##          Neg Pred Value : 0.7976          
##              Prevalence : 0.6103          
##          Detection Rate : 0.5305          
##    Detection Prevalence : 0.6056          
##       Balanced Accuracy : 0.8382          
##                                           
##        &#39;Positive&#39; Class : CH              
## </code></pre>
<p>AUC was 0.9386. Here are the ROC and gain curves.</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="gradient-boosting.html#cb119-1"></a>mdl_auc &lt;-<span class="st"> </span>Metrics<span class="op">::</span><span class="kw">auc</span>(<span class="dt">actual =</span> oj_preds_gbm<span class="op">$</span>Actual <span class="op">==</span><span class="st"> &quot;CH&quot;</span>, oj_preds_gbm<span class="op">$</span>CH)</span>
<span id="cb119-2"><a href="gradient-boosting.html#cb119-2"></a>yardstick<span class="op">::</span><span class="kw">roc_curve</span>(oj_preds_gbm, Actual, CH) <span class="op">%&gt;%</span></span>
<span id="cb119-3"><a href="gradient-boosting.html#cb119-3"></a><span class="st">  </span><span class="kw">autoplot</span>() <span class="op">+</span></span>
<span id="cb119-4"><a href="gradient-boosting.html#cb119-4"></a><span class="st">  </span><span class="kw">labs</span>(</span>
<span id="cb119-5"><a href="gradient-boosting.html#cb119-5"></a>    <span class="dt">title =</span> <span class="st">&quot;OJ GBM ROC Curve&quot;</span>,</span>
<span id="cb119-6"><a href="gradient-boosting.html#cb119-6"></a>    <span class="dt">subtitle =</span> <span class="kw">paste0</span>(<span class="st">&quot;AUC = &quot;</span>, <span class="kw">round</span>(mdl_auc, <span class="dv">4</span>))</span>
<span id="cb119-7"><a href="gradient-boosting.html#cb119-7"></a>  )</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-79-1.png" width="672" /></p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="gradient-boosting.html#cb120-1"></a>yardstick<span class="op">::</span><span class="kw">gain_curve</span>(oj_preds_gbm, Actual, CH) <span class="op">%&gt;%</span></span>
<span id="cb120-2"><a href="gradient-boosting.html#cb120-2"></a><span class="st">  </span><span class="kw">autoplot</span>() <span class="op">+</span></span>
<span id="cb120-3"><a href="gradient-boosting.html#cb120-3"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;OJ GBM Gain Curve&quot;</span>)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-79-2.png" width="672" /></p>
<p>Now the variable importance. Just a few variables. <code>LoyalCH</code> is at the top again.</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="gradient-boosting.html#cb121-1"></a><span class="co">#plot(varImp(oj_mdl_gbm), main=&quot;Variable Importance with Gradient Boosting&quot;)</span></span></code></pre></div>
</div>
<div id="xgboost" class="section level5">
<h5><span class="header-section-number">9.5.0.1.2</span> XGBoost</h5>
<p>I’ll predict <code>Purchase</code> from the <code>OJ</code> data set again, this time using the XGBoost method by specifying <code>method = "xgbTree"</code>. <code>xgbTree</code> has the following tuneable hyperparameters (see <code>modelLookup("xgbTree")</code>). The first three are the same as <code>xgb</code>.</p>
<ul>
<li><code>nrounds</code>: number of boosting iterations, <span class="math inline">\(M\)</span></li>
<li><code>max_depth</code>: maximum tree depth</li>
<li><code>eta</code>: shrinkage, <span class="math inline">\(\eta\)</span></li>
<li><code>gamma</code>: minimum loss reduction</li>
<li><code>colsamle_bytree</code>: subsample ratio of columns</li>
<li><code>min_child_weight</code>: minimum size of instance weight</li>
<li><code>substample</code>: subsample percentage</li>
</ul>
<p>I’ll use <code>tuneLength = 5</code> again.</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="gradient-boosting.html#cb122-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb122-2"><a href="gradient-boosting.html#cb122-2"></a>garbage &lt;-<span class="st"> </span><span class="kw">capture.output</span>(</span>
<span id="cb122-3"><a href="gradient-boosting.html#cb122-3"></a>oj_mdl_xgb &lt;-<span class="st"> </span><span class="kw">train</span>(</span>
<span id="cb122-4"><a href="gradient-boosting.html#cb122-4"></a>   Purchase <span class="op">~</span><span class="st"> </span>., </span>
<span id="cb122-5"><a href="gradient-boosting.html#cb122-5"></a>   <span class="dt">data =</span> oj_train, </span>
<span id="cb122-6"><a href="gradient-boosting.html#cb122-6"></a>   <span class="dt">method =</span> <span class="st">&quot;xgbTree&quot;</span>,</span>
<span id="cb122-7"><a href="gradient-boosting.html#cb122-7"></a>   <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>,</span>
<span id="cb122-8"><a href="gradient-boosting.html#cb122-8"></a>   <span class="dt">tuneLength =</span> <span class="dv">5</span>,</span>
<span id="cb122-9"><a href="gradient-boosting.html#cb122-9"></a>   <span class="dt">trControl =</span> oj_trControl</span>
<span id="cb122-10"><a href="gradient-boosting.html#cb122-10"></a>))</span>
<span id="cb122-11"><a href="gradient-boosting.html#cb122-11"></a>oj_mdl_xgb</span></code></pre></div>
<pre><code>## eXtreme Gradient Boosting 
## 
## 857 samples
##  17 predictor
##   2 classes: &#39;CH&#39;, &#39;MM&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... 
## Resampling results across tuning parameters:
## 
##   eta  max_depth  colsample_bytree  subsample  nrounds  ROC        Sens     
##   0.3  1          0.6               0.500       50      0.8840766  0.8834180
##   0.3  1          0.6               0.500      100      0.8790561  0.8662192
##   0.3  1          0.6               0.500      150      0.8766781  0.8701379
##   0.3  1          0.6               0.500      200      0.8776000  0.8624093
##   0.3  1          0.6               0.500      250      0.8757948  0.8624456
##   0.3  1          0.6               0.625       50      0.8859505  0.8834180
##   0.3  1          0.6               0.625      100      0.8835632  0.8719158
##   0.3  1          0.6               0.625      150      0.8795049  0.8623730
##   0.3  1          0.6               0.625      200      0.8779641  0.8623730
##   0.3  1          0.6               0.625      250      0.8750637  0.8604499
##   0.3  1          0.6               0.750       50      0.8888085  0.8796081
##   0.3  1          0.6               0.750      100      0.8821363  0.8701016
##   0.3  1          0.6               0.750      150      0.8788971  0.8758708
##   0.3  1          0.6               0.750      200      0.8795012  0.8682148
##   0.3  1          0.6               0.750      250      0.8776496  0.8643324
##   0.3  1          0.6               0.875       50      0.8873750  0.8699927
##   0.3  1          0.6               0.875      100      0.8842872  0.8738752
##   0.3  1          0.6               0.875      150      0.8836644  0.8643687
##   0.3  1          0.6               0.875      200      0.8815095  0.8643687
##   0.3  1          0.6               0.875      250      0.8810575  0.8585994
##   0.3  1          0.6               1.000       50      0.8901207  0.8814949
##   0.3  1          0.6               1.000      100      0.8873808  0.8814949
##   0.3  1          0.6               1.000      150      0.8850800  0.8777213
##   0.3  1          0.6               1.000      200      0.8839367  0.8777213
##   0.3  1          0.6               1.000      250      0.8830755  0.8701016
##   0.3  1          0.8               0.500       50      0.8813948  0.8661466
##   0.3  1          0.8               0.500      100      0.8758169  0.8605225
##   0.3  1          0.8               0.500      150      0.8775673  0.8719884
##   0.3  1          0.8               0.500      200      0.8705204  0.8452104
##   0.3  1          0.8               0.500      250      0.8708827  0.8623730
##   0.3  1          0.8               0.625       50      0.8837989  0.8680697
##   0.3  1          0.8               0.625      100      0.8798634  0.8642598
##   0.3  1          0.8               0.625      150      0.8771115  0.8681785
##   0.3  1          0.8               0.625      200      0.8767673  0.8643687
##   0.3  1          0.8               0.625      250      0.8757636  0.8662917
##   0.3  1          0.8               0.750       50      0.8873982  0.8776851
##   0.3  1          0.8               0.750      100      0.8845928  0.8663280
##   0.3  1          0.8               0.750      150      0.8812139  0.8796807
##   0.3  1          0.8               0.750      200      0.8810481  0.8662192
##   0.3  1          0.8               0.750      250      0.8791379  0.8605225
##   0.3  1          0.8               0.875       50      0.8860863  0.8699927
##   0.3  1          0.8               0.875      100      0.8824399  0.8757983
##   0.3  1          0.8               0.875      150      0.8800854  0.8720247
##   0.3  1          0.8               0.875      200      0.8781308  0.8643324
##   0.3  1          0.8               0.875      250      0.8760868  0.8623730
##   0.3  1          0.8               1.000       50      0.8885750  0.8738389
##   0.3  1          0.8               1.000      100      0.8864060  0.8738752
##   0.3  1          0.8               1.000      150      0.8846236  0.8738752
##   0.3  1          0.8               1.000      200      0.8834568  0.8777213
##   0.3  1          0.8               1.000      250      0.8828532  0.8701016
##   0.3  2          0.6               0.500       50      0.8827826  0.8604499
##   0.3  2          0.6               0.500      100      0.8738138  0.8623367
##   0.3  2          0.6               0.500      150      0.8712299  0.8509071
##   0.3  2          0.6               0.500      200      0.8628896  0.8489840
##   0.3  2          0.6               0.500      250      0.8588455  0.8432874
##   0.3  2          0.6               0.625       50      0.8852692  0.8566763
##   0.3  2          0.6               0.625      100      0.8763552  0.8432148
##   0.3  2          0.6               0.625      150      0.8707889  0.8355225
##   0.3  2          0.6               0.625      200      0.8673676  0.8317489
##   0.3  2          0.6               0.625      250      0.8648451  0.8393687
##   0.3  2          0.6               0.750       50      0.8820965  0.8662554
##   0.3  2          0.6               0.750      100      0.8763104  0.8623730
##   0.3  2          0.6               0.750      150      0.8705862  0.8623004
##   0.3  2          0.6               0.750      200      0.8665872  0.8566401
##   0.3  2          0.6               0.750      250      0.8646026  0.8528302
##   0.3  2          0.6               0.875       50      0.8854970  0.8604499
##   0.3  2          0.6               0.875      100      0.8756684  0.8546807
##   0.3  2          0.6               0.875      150      0.8718297  0.8546807
##   0.3  2          0.6               0.875      200      0.8662731  0.8489115
##   0.3  2          0.6               0.875      250      0.8657367  0.8432511
##   0.3  2          0.6               1.000       50      0.8851624  0.8662554
##   0.3  2          0.6               1.000      100      0.8790797  0.8547170
##   0.3  2          0.6               1.000      150      0.8764584  0.8547170
##   0.3  2          0.6               1.000      200      0.8731986  0.8585269
##   0.3  2          0.6               1.000      250      0.8691992  0.8470247
##   0.3  2          0.8               0.500       50      0.8795484  0.8546807
##   0.3  2          0.8               0.500      100      0.8725858  0.8585269
##   0.3  2          0.8               0.500      150      0.8668489  0.8490929
##   0.3  2          0.8               0.500      200      0.8627999  0.8317852
##   0.3  2          0.8               0.500      250      0.8604951  0.8260160
##   0.3  2          0.8               0.625       50      0.8798383  0.8585994
##   0.3  2          0.8               0.625      100      0.8731001  0.8546807
##   0.3  2          0.8               0.625      150      0.8727083  0.8547170
##   0.3  2          0.8               0.625      200      0.8705551  0.8489478
##   0.3  2          0.8               0.625      250      0.8648865  0.8432511
##   0.3  2          0.8               0.750       50      0.8807836  0.8528302
##   0.3  2          0.8               0.750      100      0.8778557  0.8413280
##   0.3  2          0.8               0.750      150      0.8710876  0.8489840
##   0.3  2          0.8               0.750      200      0.8675137  0.8527213
##   0.3  2          0.8               0.750      250      0.8669419  0.8432511
##   0.3  2          0.8               0.875       50      0.8830350  0.8585994
##   0.3  2          0.8               0.875      100      0.8768979  0.8528302
##   0.3  2          0.8               0.875      150      0.8703421  0.8585631
##   0.3  2          0.8               0.875      200      0.8675309  0.8547170
##   0.3  2          0.8               0.875      250      0.8631085  0.8470972
##   0.3  2          0.8               1.000       50      0.8816738  0.8623730
##   0.3  2          0.8               1.000      100      0.8754326  0.8546444
##   0.3  2          0.8               1.000      150      0.8727592  0.8508345
##   0.3  2          0.8               1.000      200      0.8716418  0.8470247
##   0.3  2          0.8               1.000      250      0.8671499  0.8451379
##   0.3  3          0.6               0.500       50      0.8784097  0.8585269
##   0.3  3          0.6               0.500      100      0.8657922  0.8393324
##   0.3  3          0.6               0.500      150      0.8546087  0.8412554
##   0.3  3          0.6               0.500      200      0.8508247  0.8392235
##   0.3  3          0.6               0.500      250      0.8466136  0.8374093
##   0.3  3          0.6               0.625       50      0.8769267  0.8566038
##   0.3  3          0.6               0.625      100      0.8669206  0.8393687
##   0.3  3          0.6               0.625      150      0.8619305  0.8393324
##   0.3  3          0.6               0.625      200      0.8574598  0.8451379
##   0.3  3          0.6               0.625      250      0.8540881  0.8470247
##   0.3  3          0.6               0.750       50      0.8757081  0.8528302
##   0.3  3          0.6               0.750      100      0.8631662  0.8394049
##   0.3  3          0.6               0.750      150      0.8589792  0.8413643
##   0.3  3          0.6               0.750      200      0.8583321  0.8413280
##   0.3  3          0.6               0.750      250      0.8534161  0.8394049
##   0.3  3          0.6               0.875       50      0.8728172  0.8566763
##   0.3  3          0.6               0.875      100      0.8656370  0.8508345
##   0.3  3          0.6               0.875      150      0.8615151  0.8490566
##   0.3  3          0.6               0.875      200      0.8578709  0.8432511
##   0.3  3          0.6               0.875      250      0.8577331  0.8451379
##   0.3  3          0.6               1.000       50      0.8776702  0.8605225
##   0.3  3          0.6               1.000      100      0.8731200  0.8586357
##   0.3  3          0.6               1.000      150      0.8680546  0.8566401
##   0.3  3          0.6               1.000      200      0.8652595  0.8432511
##   0.3  3          0.6               1.000      250      0.8626786  0.8451742
##   0.3  3          0.8               0.500       50      0.8704410  0.8431785
##   0.3  3          0.8               0.500      100      0.8611050  0.8470247
##   0.3  3          0.8               0.500      150      0.8574252  0.8432511
##   0.3  3          0.8               0.500      200      0.8478274  0.8279390
##   0.3  3          0.8               0.500      250      0.8415372  0.8260160
##   0.3  3          0.8               0.625       50      0.8752167  0.8661829
##   0.3  3          0.8               0.625      100      0.8665248  0.8508345
##   0.3  3          0.8               0.625      150      0.8670495  0.8547170
##   0.3  3          0.8               0.625      200      0.8552427  0.8431785
##   0.3  3          0.8               0.625      250      0.8534494  0.8355951
##   0.3  3          0.8               0.750       50      0.8763169  0.8585269
##   0.3  3          0.8               0.750      100      0.8684903  0.8470610
##   0.3  3          0.8               0.750      150      0.8634290  0.8354862
##   0.3  3          0.8               0.750      200      0.8598103  0.8393687
##   0.3  3          0.8               0.750      250      0.8560082  0.8354499
##   0.3  3          0.8               0.875       50      0.8756224  0.8585631
##   0.3  3          0.8               0.875      100      0.8691344  0.8508708
##   0.3  3          0.8               0.875      150      0.8613913  0.8489115
##   0.3  3          0.8               0.875      200      0.8589936  0.8414006
##   0.3  3          0.8               0.875      250      0.8583912  0.8395501
##   0.3  3          0.8               1.000       50      0.8785875  0.8546444
##   0.3  3          0.8               1.000      100      0.8697357  0.8584906
##   0.3  3          0.8               1.000      150      0.8663404  0.8470247
##   0.3  3          0.8               1.000      200      0.8647051  0.8489840
##   0.3  3          0.8               1.000      250      0.8613289  0.8412917
##   0.3  4          0.6               0.500       50      0.8684512  0.8469884
##   0.3  4          0.6               0.500      100      0.8548516  0.8374456
##   0.3  4          0.6               0.500      150      0.8564383  0.8355225
##   0.3  4          0.6               0.500      200      0.8531487  0.8374819
##   0.3  4          0.6               0.500      250      0.8503835  0.8546807
##   0.3  4          0.6               0.625       50      0.8725837  0.8528302
##   0.3  4          0.6               0.625      100      0.8650128  0.8490566
##   0.3  4          0.6               0.625      150      0.8594804  0.8318215
##   0.3  4          0.6               0.625      200      0.8570231  0.8317852
##   0.3  4          0.6               0.625      250      0.8551338  0.8432511
##   0.3  4          0.6               0.750       50      0.8726290  0.8547170
##   0.3  4          0.6               0.750      100      0.8622128  0.8451016
##   0.3  4          0.6               0.750      150      0.8628721  0.8412917
##   0.3  4          0.6               0.750      200      0.8565407  0.8297533
##   0.3  4          0.6               0.750      250      0.8516140  0.8336357
##   0.3  4          0.6               0.875       50      0.8690627  0.8585269
##   0.3  4          0.6               0.875      100      0.8598471  0.8510160
##   0.3  4          0.6               0.875      150      0.8531149  0.8451742
##   0.3  4          0.6               0.875      200      0.8522017  0.8355951
##   0.3  4          0.6               0.875      250      0.8490782  0.8394775
##   0.3  4          0.6               1.000       50      0.8757086  0.8585269
##   0.3  4          0.6               1.000      100      0.8648118  0.8469884
##   0.3  4          0.6               1.000      150      0.8605393  0.8450653
##   0.3  4          0.6               1.000      200      0.8559972  0.8317126
##   0.3  4          0.6               1.000      250      0.8533540  0.8241292
##   0.3  4          0.8               0.500       50      0.8599347  0.8507983
##   0.3  4          0.8               0.500      100      0.8520126  0.8451379
##   0.3  4          0.8               0.500      150      0.8495768  0.8450653
##   0.3  4          0.8               0.500      200      0.8497165  0.8374456
##   0.3  4          0.8               0.500      250      0.8446128  0.8336357
##   0.3  4          0.8               0.625       50      0.8729510  0.8490566
##   0.3  4          0.8               0.625      100      0.8644594  0.8490203
##   0.3  4          0.8               0.625      150      0.8562968  0.8393687
##   0.3  4          0.8               0.625      200      0.8513927  0.8336357
##   0.3  4          0.8               0.625      250      0.8466296  0.8337446
##   0.3  4          0.8               0.750       50      0.8670178  0.8431785
##   0.3  4          0.8               0.750      100      0.8608809  0.8432511
##   0.3  4          0.8               0.750      150      0.8557375  0.8394775
##   0.3  4          0.8               0.750      200      0.8545252  0.8375181
##   0.3  4          0.8               0.750      250      0.8516413  0.8338171
##   0.3  4          0.8               0.875       50      0.8665294  0.8374819
##   0.3  4          0.8               0.875      100      0.8624626  0.8374456
##   0.3  4          0.8               0.875      150      0.8600795  0.8337446
##   0.3  4          0.8               0.875      200      0.8542276  0.8298984
##   0.3  4          0.8               0.875      250      0.8509514  0.8261248
##   0.3  4          0.8               1.000       50      0.8712199  0.8546807
##   0.3  4          0.8               1.000      100      0.8622010  0.8566401
##   0.3  4          0.8               1.000      150      0.8616357  0.8527939
##   0.3  4          0.8               1.000      200      0.8570352  0.8414006
##   0.3  4          0.8               1.000      250      0.8555653  0.8432511
##   0.3  5          0.6               0.500       50      0.8568017  0.8526488
##   0.3  5          0.6               0.500      100      0.8473738  0.8354862
##   0.3  5          0.6               0.500      150      0.8491211  0.8277939
##   0.3  5          0.6               0.500      200      0.8471236  0.8240929
##   0.3  5          0.6               0.500      250      0.8459383  0.8145864
##   0.3  5          0.6               0.625       50      0.8589184  0.8432148
##   0.3  5          0.6               0.625      100      0.8553874  0.8412917
##   0.3  5          0.6               0.625      150      0.8517198  0.8393687
##   0.3  5          0.6               0.625      200      0.8527430  0.8469884
##   0.3  5          0.6               0.625      250      0.8462826  0.8354862
##   0.3  5          0.6               0.750       50      0.8633934  0.8412554
##   0.3  5          0.6               0.750      100      0.8575012  0.8298258
##   0.3  5          0.6               0.750      150      0.8516705  0.8222061
##   0.3  5          0.6               0.750      200      0.8499948  0.8240566
##   0.3  5          0.6               0.750      250      0.8484495  0.8297170
##   0.3  5          0.6               0.875       50      0.8642104  0.8450653
##   0.3  5          0.6               0.875      100      0.8571687  0.8355588
##   0.3  5          0.6               0.875      150      0.8535070  0.8375181
##   0.3  5          0.6               0.875      200      0.8498702  0.8337083
##   0.3  5          0.6               0.875      250      0.8503518  0.8242017
##   0.3  5          0.6               1.000       50      0.8654518  0.8450653
##   0.3  5          0.6               1.000      100      0.8606506  0.8375181
##   0.3  5          0.6               1.000      150      0.8579745  0.8375907
##   0.3  5          0.6               1.000      200      0.8539322  0.8299347
##   0.3  5          0.6               1.000      250      0.8525294  0.8280116
##   0.3  5          0.8               0.500       50      0.8611009  0.8373367
##   0.3  5          0.8               0.500      100      0.8525752  0.8412554
##   0.3  5          0.8               0.500      150      0.8529361  0.8373367
##   0.3  5          0.8               0.500      200      0.8490953  0.8354862
##   0.3  5          0.8               0.500      250      0.8459399  0.8279390
##   0.3  5          0.8               0.625       50      0.8616727  0.8546444
##   0.3  5          0.8               0.625      100      0.8583265  0.8432874
##   0.3  5          0.8               0.625      150      0.8550993  0.8279753
##   0.3  5          0.8               0.625      200      0.8523346  0.8318940
##   0.3  5          0.8               0.625      250      0.8526050  0.8298984
##   0.3  5          0.8               0.750       50      0.8615628  0.8451379
##   0.3  5          0.8               0.750      100      0.8543987  0.8353774
##   0.3  5          0.8               0.750      150      0.8495081  0.8240203
##   0.3  5          0.8               0.750      200      0.8433454  0.8334906
##   0.3  5          0.8               0.750      250      0.8398512  0.8182874
##   0.3  5          0.8               0.875       50      0.8695798  0.8470610
##   0.3  5          0.8               0.875      100      0.8576054  0.8413280
##   0.3  5          0.8               0.875      150      0.8529901  0.8355225
##   0.3  5          0.8               0.875      200      0.8505461  0.8240929
##   0.3  5          0.8               0.875      250      0.8486020  0.8260160
##   0.3  5          0.8               1.000       50      0.8683535  0.8452104
##   0.3  5          0.8               1.000      100      0.8608524  0.8452830
##   0.3  5          0.8               1.000      150      0.8576962  0.8433962
##   0.3  5          0.8               1.000      200      0.8535975  0.8415094
##   0.3  5          0.8               1.000      250      0.8513341  0.8415094
##   0.4  1          0.6               0.500       50      0.8819773  0.8624093
##   0.4  1          0.6               0.500      100      0.8781782  0.8585631
##   0.4  1          0.6               0.500      150      0.8732982  0.8529028
##   0.4  1          0.6               0.500      200      0.8717570  0.8604862
##   0.4  1          0.6               0.500      250      0.8731235  0.8509434
##   0.4  1          0.6               0.625       50      0.8850621  0.8661103
##   0.4  1          0.6               0.625      100      0.8838074  0.8624456
##   0.4  1          0.6               0.625      150      0.8764260  0.8528665
##   0.4  1          0.6               0.625      200      0.8740510  0.8490929
##   0.4  1          0.6               0.625      250      0.8769566  0.8605588
##   0.4  1          0.6               0.750       50      0.8816833  0.8738389
##   0.4  1          0.6               0.750      100      0.8794957  0.8700290
##   0.4  1          0.6               0.750      150      0.8800637  0.8662192
##   0.4  1          0.6               0.750      200      0.8782589  0.8661829
##   0.4  1          0.6               0.750      250      0.8765927  0.8585631
##   0.4  1          0.6               0.875       50      0.8859525  0.8815312
##   0.4  1          0.6               0.875      100      0.8805408  0.8815312
##   0.4  1          0.6               0.875      150      0.8770713  0.8720247
##   0.4  1          0.6               0.875      200      0.8755091  0.8681422
##   0.4  1          0.6               0.875      250      0.8736719  0.8624819
##   0.4  1          0.6               1.000       50      0.8892252  0.8719521
##   0.4  1          0.6               1.000      100      0.8862227  0.8700653
##   0.4  1          0.6               1.000      150      0.8834724  0.8681785
##   0.4  1          0.6               1.000      200      0.8827532  0.8681422
##   0.4  1          0.6               1.000      250      0.8817585  0.8681422
##   0.4  1          0.8               0.500       50      0.8819568  0.8796081
##   0.4  1          0.8               0.500      100      0.8771383  0.8682148
##   0.4  1          0.8               0.500      150      0.8704260  0.8604862
##   0.4  1          0.8               0.500      200      0.8702556  0.8566401
##   0.4  1          0.8               0.500      250      0.8672987  0.8510160
##   0.4  1          0.8               0.625       50      0.8827328  0.8777213
##   0.4  1          0.8               0.625      100      0.8796493  0.8700653
##   0.4  1          0.8               0.625      150      0.8781007  0.8604862
##   0.4  1          0.8               0.625      200      0.8704065  0.8527939
##   0.4  1          0.8               0.625      250      0.8726450  0.8547896
##   0.4  1          0.8               0.750       50      0.8850542  0.8680334
##   0.4  1          0.8               0.750      100      0.8813316  0.8624093
##   0.4  1          0.8               0.750      150      0.8808412  0.8566401
##   0.4  1          0.8               0.750      200      0.8787724  0.8642961
##   0.4  1          0.8               0.750      250      0.8749581  0.8547533
##   0.4  1          0.8               0.875       50      0.8874193  0.8758345
##   0.4  1          0.8               0.875      100      0.8836916  0.8719521
##   0.4  1          0.8               0.875      150      0.8809541  0.8642961
##   0.4  1          0.8               0.875      200      0.8762188  0.8681785
##   0.4  1          0.8               0.875      250      0.8747890  0.8603774
##   0.4  1          0.8               1.000       50      0.8878571  0.8680697
##   0.4  1          0.8               1.000      100      0.8842316  0.8777213
##   0.4  1          0.8               1.000      150      0.8821720  0.8757983
##   0.4  1          0.8               1.000      200      0.8810133  0.8662554
##   0.4  1          0.8               1.000      250      0.8800336  0.8681422
##   0.4  2          0.6               0.500       50      0.8766361  0.8566401
##   0.4  2          0.6               0.500      100      0.8741177  0.8508708
##   0.4  2          0.6               0.500      150      0.8709445  0.8470972
##   0.4  2          0.6               0.500      200      0.8626755  0.8509071
##   0.4  2          0.6               0.500      250      0.8578641  0.8469884
##   0.4  2          0.6               0.625       50      0.8751713  0.8527939
##   0.4  2          0.6               0.625      100      0.8693670  0.8509434
##   0.4  2          0.6               0.625      150      0.8659128  0.8450653
##   0.4  2          0.6               0.625      200      0.8592171  0.8373730
##   0.4  2          0.6               0.625      250      0.8580717  0.8259071
##   0.4  2          0.6               0.750       50      0.8719918  0.8566038
##   0.4  2          0.6               0.750      100      0.8693202  0.8604862
##   0.4  2          0.6               0.750      150      0.8668895  0.8431422
##   0.4  2          0.6               0.750      200      0.8619742  0.8450290
##   0.4  2          0.6               0.750      250      0.8609928  0.8527213
##   0.4  2          0.6               0.875       50      0.8797380  0.8490203
##   0.4  2          0.6               0.875      100      0.8725045  0.8451379
##   0.4  2          0.6               0.875      150      0.8652448  0.8489115
##   0.4  2          0.6               0.875      200      0.8670949  0.8412554
##   0.4  2          0.6               0.875      250      0.8633280  0.8507620
##   0.4  2          0.6               1.000       50      0.8831165  0.8604136
##   0.4  2          0.6               1.000      100      0.8728979  0.8489115
##   0.4  2          0.6               1.000      150      0.8677760  0.8393324
##   0.4  2          0.6               1.000      200      0.8659265  0.8393324
##   0.4  2          0.6               1.000      250      0.8646603  0.8451016
##   0.4  2          0.8               0.500       50      0.8748110  0.8528665
##   0.4  2          0.8               0.500      100      0.8683823  0.8489115
##   0.4  2          0.8               0.500      150      0.8618886  0.8565675
##   0.4  2          0.8               0.500      200      0.8546334  0.8431785
##   0.4  2          0.8               0.500      250      0.8560853  0.8354862
##   0.4  2          0.8               0.625       50      0.8716244  0.8547533
##   0.4  2          0.8               0.625      100      0.8660106  0.8470972
##   0.4  2          0.8               0.625      150      0.8583216  0.8432874
##   0.4  2          0.8               0.625      200      0.8545955  0.8413280
##   0.4  2          0.8               0.625      250      0.8523242  0.8394049
##   0.4  2          0.8               0.750       50      0.8747039  0.8528665
##   0.4  2          0.8               0.750      100      0.8720468  0.8470972
##   0.4  2          0.8               0.750      150      0.8632754  0.8412917
##   0.4  2          0.8               0.750      200      0.8601299  0.8488752
##   0.4  2          0.8               0.750      250      0.8551371  0.8317126
##   0.4  2          0.8               0.875       50      0.8808220  0.8681422
##   0.4  2          0.8               0.875      100      0.8722082  0.8470610
##   0.4  2          0.8               0.875      150      0.8654225  0.8470247
##   0.4  2          0.8               0.875      200      0.8665308  0.8547896
##   0.4  2          0.8               0.875      250      0.8624247  0.8509071
##   0.4  2          0.8               1.000       50      0.8767032  0.8546444
##   0.4  2          0.8               1.000      100      0.8713492  0.8489115
##   0.4  2          0.8               1.000      150      0.8683053  0.8470610
##   0.4  2          0.8               1.000      200      0.8666212  0.8470610
##   0.4  2          0.8               1.000      250      0.8633723  0.8451742
##   0.4  3          0.6               0.500       50      0.8640470  0.8489840
##   0.4  3          0.6               0.500      100      0.8623627  0.8395138
##   0.4  3          0.6               0.500      150      0.8600232  0.8450290
##   0.4  3          0.6               0.500      200      0.8545515  0.8278665
##   0.4  3          0.6               0.500      250      0.8486454  0.8412917
##   0.4  3          0.6               0.625       50      0.8761459  0.8585631
##   0.4  3          0.6               0.625      100      0.8622783  0.8469884
##   0.4  3          0.6               0.625      150      0.8568207  0.8336357
##   0.4  3          0.6               0.625      200      0.8552490  0.8373730
##   0.4  3          0.6               0.625      250      0.8535823  0.8411829
##   0.4  3          0.6               0.750       50      0.8718950  0.8489478
##   0.4  3          0.6               0.750      100      0.8651224  0.8450290
##   0.4  3          0.6               0.750      150      0.8638079  0.8317126
##   0.4  3          0.6               0.750      200      0.8565601  0.8469521
##   0.4  3          0.6               0.750      250      0.8571380  0.8355588
##   0.4  3          0.6               0.875       50      0.8746308  0.8431060
##   0.4  3          0.6               0.875      100      0.8665481  0.8566038
##   0.4  3          0.6               0.875      150      0.8606995  0.8507620
##   0.4  3          0.6               0.875      200      0.8578503  0.8374093
##   0.4  3          0.6               0.875      250      0.8524245  0.8412554
##   0.4  3          0.6               1.000       50      0.8754352  0.8604499
##   0.4  3          0.6               1.000      100      0.8665944  0.8489840
##   0.4  3          0.6               1.000      150      0.8630463  0.8509071
##   0.4  3          0.6               1.000      200      0.8594737  0.8394775
##   0.4  3          0.6               1.000      250      0.8553279  0.8355951
##   0.4  3          0.8               0.500       50      0.8651490  0.8432511
##   0.4  3          0.8               0.500      100      0.8569925  0.8529390
##   0.4  3          0.8               0.500      150      0.8477096  0.8375181
##   0.4  3          0.8               0.500      200      0.8457458  0.8279028
##   0.4  3          0.8               0.500      250      0.8426879  0.8318940
##   0.4  3          0.8               0.625       50      0.8658222  0.8451742
##   0.4  3          0.8               0.625      100      0.8572253  0.8297896
##   0.4  3          0.8               0.625      150      0.8534702  0.8279390
##   0.4  3          0.8               0.625      200      0.8520551  0.8221335
##   0.4  3          0.8               0.625      250      0.8469493  0.8317852
##   0.4  3          0.8               0.750       50      0.8699383  0.8566763
##   0.4  3          0.8               0.750      100      0.8621005  0.8433237
##   0.4  3          0.8               0.750      150      0.8543705  0.8451016
##   0.4  3          0.8               0.750      200      0.8552381  0.8355951
##   0.4  3          0.8               0.750      250      0.8539085  0.8451016
##   0.4  3          0.8               0.875       50      0.8688165  0.8585269
##   0.4  3          0.8               0.875      100      0.8602130  0.8470972
##   0.4  3          0.8               0.875      150      0.8547113  0.8336720
##   0.4  3          0.8               0.875      200      0.8528785  0.8414006
##   0.4  3          0.8               0.875      250      0.8518811  0.8337083
##   0.4  3          0.8               1.000       50      0.8709665  0.8489115
##   0.4  3          0.8               1.000      100      0.8653586  0.8412554
##   0.4  3          0.8               1.000      150      0.8624485  0.8337446
##   0.4  3          0.8               1.000      200      0.8594047  0.8355951
##   0.4  3          0.8               1.000      250      0.8559894  0.8317852
##   0.4  4          0.6               0.500       50      0.8582166  0.8413280
##   0.4  4          0.6               0.500      100      0.8477981  0.8318215
##   0.4  4          0.6               0.500      150      0.8487107  0.8279753
##   0.4  4          0.6               0.500      200      0.8442558  0.8357402
##   0.4  4          0.6               0.500      250      0.8408989  0.8413280
##   0.4  4          0.6               0.625       50      0.8586802  0.8413280
##   0.4  4          0.6               0.625      100      0.8515141  0.8355225
##   0.4  4          0.6               0.625      150      0.8489226  0.8317126
##   0.4  4          0.6               0.625      200      0.8401825  0.8201742
##   0.4  4          0.6               0.625      250      0.8382434  0.8279390
##   0.4  4          0.6               0.750       50      0.8631389  0.8412554
##   0.4  4          0.6               0.750      100      0.8550658  0.8356676
##   0.4  4          0.6               0.750      150      0.8514552  0.8279028
##   0.4  4          0.6               0.750      200      0.8511109  0.8317852
##   0.4  4          0.6               0.750      250      0.8472358  0.8299347
##   0.4  4          0.6               0.875       50      0.8712762  0.8508708
##   0.4  4          0.6               0.875      100      0.8631429  0.8413280
##   0.4  4          0.6               0.875      150      0.8551864  0.8355588
##   0.4  4          0.6               0.875      200      0.8514942  0.8337083
##   0.4  4          0.6               0.875      250      0.8505960  0.8336357
##   0.4  4          0.6               1.000       50      0.8682160  0.8412917
##   0.4  4          0.6               1.000      100      0.8611073  0.8317852
##   0.4  4          0.6               1.000      150      0.8565338  0.8260160
##   0.4  4          0.6               1.000      200      0.8536485  0.8202830
##   0.4  4          0.6               1.000      250      0.8517281  0.8126270
##   0.4  4          0.8               0.500       50      0.8630090  0.8528665
##   0.4  4          0.8               0.500      100      0.8568123  0.8355588
##   0.4  4          0.8               0.500      150      0.8511253  0.8355225
##   0.4  4          0.8               0.500      200      0.8483375  0.8298621
##   0.4  4          0.8               0.500      250      0.8452968  0.8335994
##   0.4  4          0.8               0.625       50      0.8582869  0.8431422
##   0.4  4          0.8               0.625      100      0.8525613  0.8335631
##   0.4  4          0.8               0.625      150      0.8488468  0.8335631
##   0.4  4          0.8               0.625      200      0.8445724  0.8222424
##   0.4  4          0.8               0.625      250      0.8473872  0.8184688
##   0.4  4          0.8               0.750       50      0.8655905  0.8452104
##   0.4  4          0.8               0.750      100      0.8547722  0.8451016
##   0.4  4          0.8               0.750      150      0.8490490  0.8260522
##   0.4  4          0.8               0.750      200      0.8457003  0.8298258
##   0.4  4          0.8               0.750      250      0.8460344  0.8221698
##   0.4  4          0.8               0.875       50      0.8636225  0.8469884
##   0.4  4          0.8               0.875      100      0.8538044  0.8432874
##   0.4  4          0.8               0.875      150      0.8503236  0.8374456
##   0.4  4          0.8               0.875      200      0.8494653  0.8222061
##   0.4  4          0.8               0.875      250      0.8481847  0.8298258
##   0.4  4          0.8               1.000       50      0.8682368  0.8507983
##   0.4  4          0.8               1.000      100      0.8628753  0.8508708
##   0.4  4          0.8               1.000      150      0.8578768  0.8471698
##   0.4  4          0.8               1.000      200      0.8554612  0.8356313
##   0.4  4          0.8               1.000      250      0.8525602  0.8394775
##   0.4  5          0.6               0.500       50      0.8559203  0.8412554
##   0.4  5          0.6               0.500      100      0.8451355  0.8375907
##   0.4  5          0.6               0.500      150      0.8441891  0.8337083
##   0.4  5          0.6               0.500      200      0.8413874  0.8394049
##   0.4  5          0.6               0.500      250      0.8393260  0.8433237
##   0.4  5          0.6               0.625       50      0.8598324  0.8337808
##   0.4  5          0.6               0.625      100      0.8536449  0.8336720
##   0.4  5          0.6               0.625      150      0.8495237  0.8355588
##   0.4  5          0.6               0.625      200      0.8479709  0.8374456
##   0.4  5          0.6               0.625      250      0.8438965  0.8374819
##   0.4  5          0.6               0.750       50      0.8574996  0.8411829
##   0.4  5          0.6               0.750      100      0.8486574  0.8469158
##   0.4  5          0.6               0.750      150      0.8449496  0.8279390
##   0.4  5          0.6               0.750      200      0.8449407  0.8374456
##   0.4  5          0.6               0.750      250      0.8446452  0.8279390
##   0.4  5          0.6               0.875       50      0.8635692  0.8470610
##   0.4  5          0.6               0.875      100      0.8574420  0.8414369
##   0.4  5          0.6               0.875      150      0.8545774  0.8337808
##   0.4  5          0.6               0.875      200      0.8512760  0.8357039
##   0.4  5          0.6               0.875      250      0.8476474  0.8185051
##   0.4  5          0.6               1.000       50      0.8628838  0.8412554
##   0.4  5          0.6               1.000      100      0.8549096  0.8317489
##   0.4  5          0.6               1.000      150      0.8515179  0.8164731
##   0.4  5          0.6               1.000      200      0.8466769  0.8222424
##   0.4  5          0.6               1.000      250      0.8449488  0.8241292
##   0.4  5          0.8               0.500       50      0.8664443  0.8451016
##   0.4  5          0.8               0.500      100      0.8564837  0.8451379
##   0.4  5          0.8               0.500      150      0.8505925  0.8355588
##   0.4  5          0.8               0.500      200      0.8472447  0.8355951
##   0.4  5          0.8               0.500      250      0.8430956  0.8317126
##   0.4  5          0.8               0.625       50      0.8583130  0.8508345
##   0.4  5          0.8               0.625      100      0.8554109  0.8356676
##   0.4  5          0.8               0.625      150      0.8512384  0.8394049
##   0.4  5          0.8               0.625      200      0.8506076  0.8298984
##   0.4  5          0.8               0.625      250      0.8482805  0.8336720
##   0.4  5          0.8               0.750       50      0.8609450  0.8527576
##   0.4  5          0.8               0.750      100      0.8500522  0.8374819
##   0.4  5          0.8               0.750      150      0.8457927  0.8299347
##   0.4  5          0.8               0.750      200      0.8441071  0.8203193
##   0.4  5          0.8               0.750      250      0.8414376  0.8223149
##   0.4  5          0.8               0.875       50      0.8625080  0.8527939
##   0.4  5          0.8               0.875      100      0.8535023  0.8336720
##   0.4  5          0.8               0.875      150      0.8487509  0.8317852
##   0.4  5          0.8               0.875      200      0.8464541  0.8318578
##   0.4  5          0.8               0.875      250      0.8457981  0.8298984
##   0.4  5          0.8               1.000       50      0.8697430  0.8509071
##   0.4  5          0.8               1.000      100      0.8605089  0.8451742
##   0.4  5          0.8               1.000      150      0.8562087  0.8414006
##   0.4  5          0.8               1.000      200      0.8521614  0.8414006
##   0.4  5          0.8               1.000      250      0.8500181  0.8376270
##   Spec     
##   0.7456328
##   0.7518717
##   0.7337790
##   0.7308378
##   0.7397504
##   0.7545455
##   0.7576649
##   0.7456328
##   0.7365419
##   0.7336007
##   0.7486631
##   0.7455437
##   0.7245989
##   0.7336898
##   0.7395722
##   0.7456328
##   0.7426025
##   0.7484848
##   0.7395722
##   0.7455437
##   0.7457219
##   0.7366310
##   0.7425134
##   0.7515152
##   0.7485740
##   0.7485740
##   0.7365419
##   0.7394831
##   0.7364528
##   0.7395722
##   0.7364528
##   0.7635472
##   0.7426025
##   0.7395722
##   0.7305704
##   0.7425134
##   0.7426916
##   0.7367201
##   0.7454545
##   0.7363636
##   0.7398396
##   0.7365419
##   0.7396613
##   0.7516043
##   0.7336898
##   0.7426025
##   0.7426025
##   0.7515152
##   0.7457219
##   0.7516043
##   0.7604278
##   0.7515152
##   0.7517825
##   0.7396613
##   0.7275401
##   0.7396613
##   0.7574866
##   0.7606952
##   0.7606952
##   0.7426916
##   0.7726381
##   0.7545455
##   0.7396613
##   0.7335116
##   0.7394831
##   0.7694296
##   0.7397504
##   0.7575758
##   0.7546346
##   0.7455437
##   0.7635472
##   0.7573975
##   0.7514260
##   0.7485740
##   0.7487522
##   0.7394831
##   0.7486631
##   0.7366310
##   0.7425134
##   0.7486631
##   0.7605169
##   0.7573084
##   0.7574866
##   0.7573084
##   0.7394831
##   0.7604278
##   0.7426025
##   0.7456328
##   0.7487522
##   0.7427807
##   0.7456328
##   0.7516043
##   0.7515152
##   0.7456328
##   0.7425134
##   0.7635472
##   0.7542781
##   0.7484848
##   0.7455437
##   0.7367201
##   0.7576649
##   0.7303922
##   0.7154189
##   0.7155971
##   0.7275401
##   0.7637255
##   0.7394831
##   0.7277184
##   0.7335116
##   0.7364528
##   0.7543672
##   0.7422460
##   0.7333333
##   0.7363636
##   0.7395722
##   0.7696078
##   0.7545455
##   0.7546346
##   0.7457219
##   0.7486631
##   0.7574866
##   0.7484848
##   0.7366310
##   0.7485740
##   0.7308378
##   0.7545455
##   0.7334225
##   0.7098039
##   0.7364528
##   0.7126560
##   0.7515152
##   0.7304813
##   0.7393048
##   0.7426916
##   0.7246881
##   0.7453654
##   0.7245989
##   0.7336007
##   0.7276292
##   0.7247772
##   0.7426025
##   0.7394831
##   0.7455437
##   0.7485740
##   0.7397504
##   0.7544563
##   0.7515152
##   0.7394831
##   0.7453654
##   0.7454545
##   0.7393048
##   0.7420677
##   0.7304813
##   0.7154189
##   0.7153298
##   0.7514260
##   0.7362745
##   0.7483957
##   0.7512478
##   0.7424242
##   0.7364528
##   0.7184492
##   0.7483957
##   0.7275401
##   0.7243316
##   0.7458111
##   0.7395722
##   0.7306595
##   0.7278075
##   0.7218360
##   0.7574866
##   0.7335116
##   0.7306595
##   0.7306595
##   0.7217469
##   0.7186275
##   0.7187166
##   0.7278966
##   0.7217469
##   0.7304813
##   0.7602496
##   0.7393939
##   0.7124777
##   0.7245098
##   0.7094474
##   0.7483066
##   0.7395722
##   0.7275401
##   0.7187166
##   0.7185383
##   0.7542781
##   0.7365419
##   0.7336007
##   0.7456328
##   0.7395722
##   0.7425134
##   0.7304813
##   0.7277184
##   0.7306595
##   0.7217469
##   0.7304813
##   0.7064171
##   0.7155080
##   0.7272727
##   0.7184492
##   0.7333333
##   0.7213012
##   0.7272727
##   0.7213904
##   0.7034759
##   0.7483066
##   0.7275401
##   0.7277184
##   0.7126560
##   0.7097148
##   0.7452763
##   0.7424242
##   0.7273619
##   0.7186275
##   0.7307487
##   0.7393048
##   0.7212121
##   0.7274510
##   0.7274510
##   0.7334225
##   0.7426025
##   0.7303922
##   0.7305704
##   0.7246881
##   0.7097148
##   0.7241533
##   0.7390374
##   0.7245098
##   0.7274510
##   0.7215686
##   0.7484848
##   0.7304813
##   0.7245989
##   0.7187166
##   0.7066845
##   0.7455437
##   0.7364528
##   0.7247772
##   0.7218360
##   0.7308378
##   0.7333333
##   0.7484848
##   0.7364528
##   0.7249554
##   0.7159537
##   0.7424242
##   0.7187166
##   0.7397504
##   0.7336007
##   0.7426025
##   0.7306595
##   0.7457219
##   0.7485740
##   0.7456328
##   0.7395722
##   0.7399287
##   0.7459002
##   0.7577540
##   0.7337790
##   0.7427807
##   0.7426025
##   0.7485740
##   0.7396613
##   0.7307487
##   0.7397504
##   0.7426025
##   0.7365419
##   0.7455437
##   0.7456328
##   0.7426025
##   0.7453654
##   0.7394831
##   0.7245989
##   0.7336007
##   0.7277184
##   0.7485740
##   0.7425134
##   0.7665775
##   0.7277184
##   0.7486631
##   0.7545455
##   0.7365419
##   0.7334225
##   0.7336898
##   0.7336007
##   0.7365419
##   0.7484848
##   0.7426025
##   0.7457219
##   0.7426916
##   0.7487522
##   0.7454545
##   0.7366310
##   0.7425134
##   0.7456328
##   0.7546346
##   0.7603387
##   0.7514260
##   0.7122103
##   0.7155080
##   0.7574866
##   0.7488414
##   0.7606952
##   0.7396613
##   0.7336898
##   0.7663993
##   0.7304813
##   0.7575758
##   0.7548128
##   0.7336007
##   0.7486631
##   0.7663102
##   0.7366310
##   0.7368093
##   0.7396613
##   0.7694296
##   0.7397504
##   0.7305704
##   0.7394831
##   0.7394831
##   0.7364528
##   0.7216578
##   0.7278075
##   0.7067736
##   0.7005348
##   0.7545455
##   0.7517825
##   0.7547237
##   0.7276292
##   0.7277184
##   0.7515152
##   0.7457219
##   0.7456328
##   0.7277184
##   0.7393939
##   0.7484848
##   0.7395722
##   0.7334225
##   0.7364528
##   0.7395722
##   0.7483957
##   0.7365419
##   0.7394831
##   0.7455437
##   0.7484848
##   0.7425134
##   0.7514260
##   0.7276292
##   0.7365419
##   0.7216578
##   0.7575758
##   0.7277184
##   0.7182709
##   0.7219251
##   0.7123886
##   0.7394831
##   0.7335116
##   0.7186275
##   0.7125668
##   0.7213904
##   0.7516043
##   0.7485740
##   0.7334225
##   0.7336898
##   0.7246881
##   0.7574866
##   0.7336898
##   0.7397504
##   0.7368093
##   0.7160428
##   0.7398396
##   0.7362745
##   0.7302139
##   0.7245098
##   0.7158645
##   0.7426025
##   0.7336898
##   0.7247772
##   0.7216578
##   0.7156863
##   0.7276292
##   0.7335116
##   0.7304813
##   0.7246881
##   0.7187166
##   0.7394831
##   0.7512478
##   0.7424242
##   0.7543672
##   0.7275401
##   0.7306595
##   0.7393048
##   0.7424242
##   0.7367201
##   0.7276292
##   0.7515152
##   0.7513369
##   0.7304813
##   0.7336007
##   0.7426025
##   0.7216578
##   0.7337790
##   0.7274510
##   0.7186275
##   0.7125668
##   0.7458111
##   0.7396613
##   0.7397504
##   0.7338681
##   0.7219251
##   0.7332442
##   0.7543672
##   0.7247772
##   0.7336007
##   0.7245989
##   0.7304813
##   0.7274510
##   0.7276292
##   0.7187166
##   0.7156863
##   0.7275401
##   0.7215686
##   0.7126560
##   0.7065954
##   0.7065954
##   0.7664884
##   0.7271836
##   0.7242424
##   0.7333333
##   0.7274510
##   0.7220143
##   0.7215686
##   0.7244207
##   0.7123886
##   0.7245989
##   0.7394831
##   0.7395722
##   0.7393048
##   0.7333333
##   0.7333333
##   0.7546346
##   0.7308378
##   0.7099822
##   0.7191622
##   0.7042781
##   0.7484848
##   0.7242424
##   0.7242424
##   0.7123886
##   0.7214795
##   0.7213904
##   0.7336007
##   0.7394831
##   0.7245989
##   0.7245098
##   0.7182709
##   0.7094474
##   0.7036542
##   0.7125668
##   0.7064171
##   0.7335116
##   0.7336007
##   0.7336898
##   0.7278075
##   0.7246881
##   0.7423351
##   0.7395722
##   0.7337790
##   0.7216578
##   0.7188057
##   0.7127451
##   0.7127451
##   0.7158645
##   0.7275401
##   0.7008021
##   0.7274510
##   0.7277184
##   0.7363636
##   0.7303030
##   0.7273619
##   0.7364528
##   0.7244207
##   0.7184492
##   0.7216578
##   0.7155971
##   0.7335116
##   0.7365419
##   0.7336898
##   0.7309269
##   0.7279857
##   0.7216578
##   0.7098039
##   0.7247772
##   0.7249554
##   0.7190731
## 
## Tuning parameter &#39;gamma&#39; was held constant at a value of 0
## Tuning
##  parameter &#39;min_child_weight&#39; was held constant at a value of 1
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were nrounds = 50, max_depth = 1, eta
##  = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample
##  = 1.</code></pre>
<p><code>train()</code> tuned <code>eta</code> (<span class="math inline">\(\eta\)</span>), <code>max_depth</code>, <code>colsample_bytree</code>, <code>subsample</code>, and <code>nrounds</code>, holding <code>gamma = 0</code>, and <code>min_child_weight = 1</code>. The optimal hyperparameter values were <code>eta = 0.3</code>, <code>max_depth - 1</code>, <code>colsample_bytree = 0.6</code>, <code>subsample = 1</code>, and <code>nrounds = 50</code>.</p>
<p>With so many hyperparameters, the tuning plot is nearly unreadable.</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="gradient-boosting.html#cb124-1"></a><span class="kw">plot</span>(oj_mdl_xgb)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-81-1.png" width="672" /><img src="data-sci_files/figure-html/unnamed-chunk-81-2.png" width="672" /></p>
<p>Let’s see how the model performed on the holdout set. The accuracy was 0.8732 - much better than the 0.8451 from regular gradient boosting.</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="gradient-boosting.html#cb125-1"></a>oj_preds_xgb &lt;-<span class="st"> </span><span class="kw">bind_cols</span>(</span>
<span id="cb125-2"><a href="gradient-boosting.html#cb125-2"></a>   <span class="kw">predict</span>(oj_mdl_xgb, <span class="dt">newdata =</span> oj_test, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>),</span>
<span id="cb125-3"><a href="gradient-boosting.html#cb125-3"></a>   <span class="dt">Predicted =</span> <span class="kw">predict</span>(oj_mdl_xgb, <span class="dt">newdata =</span> oj_test, <span class="dt">type =</span> <span class="st">&quot;raw&quot;</span>),</span>
<span id="cb125-4"><a href="gradient-boosting.html#cb125-4"></a>   <span class="dt">Actual =</span> oj_test<span class="op">$</span>Purchase</span>
<span id="cb125-5"><a href="gradient-boosting.html#cb125-5"></a>)</span>
<span id="cb125-6"><a href="gradient-boosting.html#cb125-6"></a></span>
<span id="cb125-7"><a href="gradient-boosting.html#cb125-7"></a>oj_cm_xgb &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(oj_preds_xgb<span class="op">$</span>Predicted, <span class="dt">reference =</span> oj_preds_xgb<span class="op">$</span>Actual)</span>
<span id="cb125-8"><a href="gradient-boosting.html#cb125-8"></a>oj_cm_xgb</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  CH  MM
##         CH 120  17
##         MM  10  66
##                                          
##                Accuracy : 0.8732         
##                  95% CI : (0.821, 0.9148)
##     No Information Rate : 0.6103         
##     P-Value [Acc &gt; NIR] : &lt;2e-16         
##                                          
##                   Kappa : 0.7294         
##                                          
##  Mcnemar&#39;s Test P-Value : 0.2482         
##                                          
##             Sensitivity : 0.9231         
##             Specificity : 0.7952         
##          Pos Pred Value : 0.8759         
##          Neg Pred Value : 0.8684         
##              Prevalence : 0.6103         
##          Detection Rate : 0.5634         
##    Detection Prevalence : 0.6432         
##       Balanced Accuracy : 0.8591         
##                                          
##        &#39;Positive&#39; Class : CH             
## </code></pre>
<p>AUC was 0.9386 for gradient boosting, and here it is 0.9323. Here are the ROC and gain curves.</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="gradient-boosting.html#cb127-1"></a>mdl_auc &lt;-<span class="st"> </span>Metrics<span class="op">::</span><span class="kw">auc</span>(<span class="dt">actual =</span> oj_preds_xgb<span class="op">$</span>Actual <span class="op">==</span><span class="st"> &quot;CH&quot;</span>, oj_preds_xgb<span class="op">$</span>CH)</span>
<span id="cb127-2"><a href="gradient-boosting.html#cb127-2"></a>yardstick<span class="op">::</span><span class="kw">roc_curve</span>(oj_preds_xgb, Actual, CH) <span class="op">%&gt;%</span></span>
<span id="cb127-3"><a href="gradient-boosting.html#cb127-3"></a><span class="st">  </span><span class="kw">autoplot</span>() <span class="op">+</span></span>
<span id="cb127-4"><a href="gradient-boosting.html#cb127-4"></a><span class="st">  </span><span class="kw">labs</span>(</span>
<span id="cb127-5"><a href="gradient-boosting.html#cb127-5"></a>    <span class="dt">title =</span> <span class="st">&quot;OJ XGBoost ROC Curve&quot;</span>,</span>
<span id="cb127-6"><a href="gradient-boosting.html#cb127-6"></a>    <span class="dt">subtitle =</span> <span class="kw">paste0</span>(<span class="st">&quot;AUC = &quot;</span>, <span class="kw">round</span>(mdl_auc, <span class="dv">4</span>))</span>
<span id="cb127-7"><a href="gradient-boosting.html#cb127-7"></a>  )</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-83-1.png" width="672" /></p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="gradient-boosting.html#cb128-1"></a>yardstick<span class="op">::</span><span class="kw">gain_curve</span>(oj_preds_xgb, Actual, CH) <span class="op">%&gt;%</span></span>
<span id="cb128-2"><a href="gradient-boosting.html#cb128-2"></a><span class="st">  </span><span class="kw">autoplot</span>() <span class="op">+</span></span>
<span id="cb128-3"><a href="gradient-boosting.html#cb128-3"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;OJ XGBoost Gain Curve&quot;</span>)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-83-2.png" width="672" /></p>
<p>Now the variable importance. Nothing jumps out at me here. It’s the same top variables as regular gradient boosting.</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="gradient-boosting.html#cb129-1"></a><span class="kw">plot</span>(<span class="kw">varImp</span>(oj_mdl_xgb), <span class="dt">main=</span><span class="st">&quot;Variable Importance with XGBoost&quot;</span>)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-84-1.png" width="672" /></p>
<p>Okay, let’s check in with the leader board. Wow, XGBoost <em>is</em> extreme.</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="gradient-boosting.html#cb130-1"></a>oj_scoreboard &lt;-<span class="st"> </span><span class="kw">rbind</span>(oj_scoreboard,</span>
<span id="cb130-2"><a href="gradient-boosting.html#cb130-2"></a>   <span class="kw">data.frame</span>(<span class="dt">Model =</span> <span class="st">&quot;Gradient Boosting&quot;</span>, <span class="dt">Accuracy =</span> oj_cm_gbm<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>])) <span class="op">%&gt;%</span></span>
<span id="cb130-3"><a href="gradient-boosting.html#cb130-3"></a><span class="st">   </span><span class="kw">rbind</span>(<span class="kw">data.frame</span>(<span class="dt">Model =</span> <span class="st">&quot;XGBoost&quot;</span>, <span class="dt">Accuracy =</span> oj_cm_xgb<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>])) <span class="op">%&gt;%</span></span>
<span id="cb130-4"><a href="gradient-boosting.html#cb130-4"></a><span class="kw">arrange</span>(<span class="kw">desc</span>(Accuracy))</span>
<span id="cb130-5"><a href="gradient-boosting.html#cb130-5"></a><span class="kw">scoreboard</span>(oj_scoreboard)</span></code></pre></div>
<div class="tabwid"><table style='border-collapse:collapse;'><thead><tr style="overflow-wrap:break-word;"><td style="width:112px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 2.00px solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">Model</span></p></td><td style="width:66px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 2.00px solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">Accuracy</span></p></td></tr></thead><tbody><tr style="overflow-wrap:break-word;"><td style="width:112px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">XGBoost</span></p></td><td style="width:66px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">0.8732</span></p></td></tr><tr style="overflow-wrap:break-word;"><td style="width:112px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">Single Tree</span></p></td><td style="width:66px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">0.8592</span></p></td></tr><tr style="overflow-wrap:break-word;"><td style="width:112px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">Single Tree (caret)</span></p></td><td style="width:66px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">0.8545</span></p></td></tr><tr style="overflow-wrap:break-word;"><td style="width:112px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">Bagging</span></p></td><td style="width:66px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">0.8451</span></p></td></tr><tr style="overflow-wrap:break-word;"><td style="width:112px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">Gradient Boosting</span></p></td><td style="width:66px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">0.8451</span></p></td></tr><tr style="overflow-wrap:break-word;"><td style="width:112px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">Random Forest</span></p></td><td style="width:66px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">0.8310</span></p></td></tr></tbody></table></div>
</div>
</div>
<div id="gradient-boosting-regression-tree" class="section level4">
<h4><span class="header-section-number">9.5.0.2</span> Gradient Boosting Regression Tree</h4>
<div id="gbm-1" class="section level5">
<h5><span class="header-section-number">9.5.0.2.1</span> GBM</h5>
<p>I’ll predict <code>Sales</code> from the <code>Carseats</code> data set again, this time using the bagging method by specifying <code>method = "gbm"</code></p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="gradient-boosting.html#cb131-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb131-2"><a href="gradient-boosting.html#cb131-2"></a>garbage &lt;-<span class="st"> </span><span class="kw">capture.output</span>(</span>
<span id="cb131-3"><a href="gradient-boosting.html#cb131-3"></a>cs_mdl_gbm &lt;-<span class="st"> </span><span class="kw">train</span>(</span>
<span id="cb131-4"><a href="gradient-boosting.html#cb131-4"></a>   Sales <span class="op">~</span><span class="st"> </span>., </span>
<span id="cb131-5"><a href="gradient-boosting.html#cb131-5"></a>   <span class="dt">data =</span> cs_train, </span>
<span id="cb131-6"><a href="gradient-boosting.html#cb131-6"></a>   <span class="dt">method =</span> <span class="st">&quot;gbm&quot;</span>,</span>
<span id="cb131-7"><a href="gradient-boosting.html#cb131-7"></a>   <span class="dt">tuneLength =</span> <span class="dv">5</span>,</span>
<span id="cb131-8"><a href="gradient-boosting.html#cb131-8"></a>   <span class="dt">trControl =</span> cs_trControl</span>
<span id="cb131-9"><a href="gradient-boosting.html#cb131-9"></a>))</span>
<span id="cb131-10"><a href="gradient-boosting.html#cb131-10"></a>cs_mdl_gbm</span></code></pre></div>
<pre><code>## Stochastic Gradient Boosting 
## 
## 321 samples
##  10 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... 
## Resampling results across tuning parameters:
## 
##   interaction.depth  n.trees  RMSE      Rsquared   MAE      
##   1                   50      1.842468  0.6718370  1.4969754
##   1                  100      1.516967  0.7823612  1.2407807
##   1                  150      1.309295  0.8277888  1.0639501
##   1                  200      1.216079  0.8429002  0.9866820
##   1                  250      1.161540  0.8488463  0.9384418
##   2                   50      1.527454  0.7801995  1.2207991
##   2                  100      1.240990  0.8381156  1.0063802
##   2                  150      1.187603  0.8415216  0.9616681
##   2                  200      1.174303  0.8425011  0.9527720
##   2                  250      1.172116  0.8403490  0.9500902
##   3                   50      1.390969  0.8071393  1.1316570
##   3                  100      1.227525  0.8321632  0.9888203
##   3                  150      1.201264  0.8345775  0.9694065
##   3                  200      1.214462  0.8282833  0.9761625
##   3                  250      1.232145  0.8221405  0.9882254
##   4                   50      1.341893  0.8128778  1.0949502
##   4                  100      1.252282  0.8230712  0.9907410
##   4                  150      1.243045  0.8229433  0.9860813
##   4                  200      1.258093  0.8162033  0.9947218
##   4                  250      1.271058  0.8114156  1.0144873
##   5                   50      1.318251  0.8128033  1.0552929
##   5                  100      1.250053  0.8226441  0.9958713
##   5                  150      1.248402  0.8214824  0.9888330
##   5                  200      1.263445  0.8158033  1.0106345
##   5                  250      1.273024  0.8124672  1.0213099
## 
## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1
## 
## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were n.trees = 250, interaction.depth =
##  1, shrinkage = 0.1 and n.minobsinnode = 10.</code></pre>
<p>The optimal tuning parameters were at <span class="math inline">\(M = 250\)</span> and <code>interation.depth = 1</code>.</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="gradient-boosting.html#cb133-1"></a><span class="kw">plot</span>(cs_mdl_gbm)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-87-1.png" width="672" /></p>
<p>Here is the holdout set performance.</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="gradient-boosting.html#cb134-1"></a>cs_preds_gbm &lt;-<span class="st"> </span><span class="kw">bind_cols</span>(</span>
<span id="cb134-2"><a href="gradient-boosting.html#cb134-2"></a>   <span class="dt">Predicted =</span> <span class="kw">predict</span>(cs_mdl_gbm, <span class="dt">newdata =</span> cs_test),</span>
<span id="cb134-3"><a href="gradient-boosting.html#cb134-3"></a>   <span class="dt">Actual =</span> cs_test<span class="op">$</span>Sales</span>
<span id="cb134-4"><a href="gradient-boosting.html#cb134-4"></a>)</span>
<span id="cb134-5"><a href="gradient-boosting.html#cb134-5"></a></span>
<span id="cb134-6"><a href="gradient-boosting.html#cb134-6"></a><span class="co"># Model over-predicts at low end of Sales and under-predicts at high end</span></span>
<span id="cb134-7"><a href="gradient-boosting.html#cb134-7"></a>cs_preds_gbm <span class="op">%&gt;%</span></span>
<span id="cb134-8"><a href="gradient-boosting.html#cb134-8"></a><span class="st">   </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Actual, <span class="dt">y =</span> Predicted)) <span class="op">+</span></span>
<span id="cb134-9"><a href="gradient-boosting.html#cb134-9"></a><span class="st">   </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.6</span>, <span class="dt">color =</span> <span class="st">&quot;cadetblue&quot;</span>) <span class="op">+</span></span>
<span id="cb134-10"><a href="gradient-boosting.html#cb134-10"></a><span class="st">   </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;loess&quot;</span>, <span class="dt">formula =</span> <span class="st">&quot;y ~ x&quot;</span>) <span class="op">+</span></span>
<span id="cb134-11"><a href="gradient-boosting.html#cb134-11"></a><span class="st">   </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">slope =</span> <span class="dv">1</span>, <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span></span>
<span id="cb134-12"><a href="gradient-boosting.html#cb134-12"></a><span class="st">   </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Carseats GBM, Predicted vs Actual&quot;</span>)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-88-1.png" width="672" /></p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="gradient-boosting.html#cb135-1"></a><span class="co">#plot(varImp(cs_mdl_gbm), main=&quot;Variable Importance with GBM&quot;)</span></span></code></pre></div>
<p>The RMSE is 1.438 - the best of the bunch.</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="gradient-boosting.html#cb136-1"></a>cs_rmse_gbm &lt;-<span class="st"> </span><span class="kw">RMSE</span>(<span class="dt">pred =</span> cs_preds_gbm<span class="op">$</span>Predicted, <span class="dt">obs =</span> cs_preds_gbm<span class="op">$</span>Actual)</span>
<span id="cb136-2"><a href="gradient-boosting.html#cb136-2"></a>cs_scoreboard &lt;-<span class="st"> </span><span class="kw">rbind</span>(cs_scoreboard,</span>
<span id="cb136-3"><a href="gradient-boosting.html#cb136-3"></a>   <span class="kw">data.frame</span>(<span class="dt">Model =</span> <span class="st">&quot;GBM&quot;</span>, <span class="dt">RMSE =</span> cs_rmse_gbm)</span>
<span id="cb136-4"><a href="gradient-boosting.html#cb136-4"></a>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">arrange</span>(RMSE)</span>
<span id="cb136-5"><a href="gradient-boosting.html#cb136-5"></a><span class="kw">scoreboard</span>(cs_scoreboard)</span></code></pre></div>
<div class="tabwid"><table style='border-collapse:collapse;'><thead><tr style="overflow-wrap:break-word;"><td style="width:112px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 2.00px solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">Model</span></p></td><td style="width:54px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 2.00px solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">RMSE</span></p></td></tr></thead><tbody><tr style="overflow-wrap:break-word;"><td style="width:112px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">GBM</span></p></td><td style="width:54px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">1.4381</span></p></td></tr><tr style="overflow-wrap:break-word;"><td style="width:112px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">Random Forest</span></p></td><td style="width:54px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">1.7184</span></p></td></tr><tr style="overflow-wrap:break-word;"><td style="width:112px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">Bagging</span></p></td><td style="width:54px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">1.9185</span></p></td></tr><tr style="overflow-wrap:break-word;"><td style="width:112px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">Single Tree (caret)</span></p></td><td style="width:54px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">2.2983</span></p></td></tr><tr style="overflow-wrap:break-word;"><td style="width:112px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">Single Tree</span></p></td><td style="width:54px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">2.3632</span></p></td></tr></tbody></table></div>
</div>
<div id="xgboost-1" class="section level5">
<h5><span class="header-section-number">9.5.0.2.2</span> XGBoost</h5>
<p>I’ll predict <code>Sales</code> from the <code>Carseats</code> data set again, this time using the bagging method by specifying <code>method = "xgb"</code></p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="gradient-boosting.html#cb137-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb137-2"><a href="gradient-boosting.html#cb137-2"></a>garbage &lt;-<span class="st"> </span><span class="kw">capture.output</span>(</span>
<span id="cb137-3"><a href="gradient-boosting.html#cb137-3"></a>cs_mdl_xgb &lt;-<span class="st"> </span><span class="kw">train</span>(</span>
<span id="cb137-4"><a href="gradient-boosting.html#cb137-4"></a>   Sales <span class="op">~</span><span class="st"> </span>., </span>
<span id="cb137-5"><a href="gradient-boosting.html#cb137-5"></a>   <span class="dt">data =</span> cs_train, </span>
<span id="cb137-6"><a href="gradient-boosting.html#cb137-6"></a>   <span class="dt">method =</span> <span class="st">&quot;xgbTree&quot;</span>,</span>
<span id="cb137-7"><a href="gradient-boosting.html#cb137-7"></a>   <span class="dt">tuneLength =</span> <span class="dv">5</span>,</span>
<span id="cb137-8"><a href="gradient-boosting.html#cb137-8"></a>   <span class="dt">trControl =</span> cs_trControl</span>
<span id="cb137-9"><a href="gradient-boosting.html#cb137-9"></a>))</span>
<span id="cb137-10"><a href="gradient-boosting.html#cb137-10"></a>cs_mdl_xgb</span></code></pre></div>
<pre><code>## eXtreme Gradient Boosting 
## 
## 321 samples
##  10 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... 
## Resampling results across tuning parameters:
## 
##   eta  max_depth  colsample_bytree  subsample  nrounds  RMSE      Rsquared 
##   0.3  1          0.6               0.500       50      1.389281  0.8016720
##   0.3  1          0.6               0.500      100      1.170297  0.8433733
##   0.3  1          0.6               0.500      150      1.148507  0.8459571
##   0.3  1          0.6               0.500      200      1.132626  0.8503489
##   0.3  1          0.6               0.500      250      1.137985  0.8480528
##   0.3  1          0.6               0.625       50      1.380563  0.8058802
##   0.3  1          0.6               0.625      100      1.175902  0.8451644
##   0.3  1          0.6               0.625      150      1.134595  0.8519167
##   0.3  1          0.6               0.625      200      1.140584  0.8494364
##   0.3  1          0.6               0.625      250      1.132677  0.8506928
##   0.3  1          0.6               0.750       50      1.385163  0.8075073
##   0.3  1          0.6               0.750      100      1.187364  0.8431270
##   0.3  1          0.6               0.750      150      1.138854  0.8496572
##   0.3  1          0.6               0.750      200      1.142356  0.8476610
##   0.3  1          0.6               0.750      250      1.143148  0.8464808
##   0.3  1          0.6               0.875       50      1.400731  0.8084364
##   0.3  1          0.6               0.875      100      1.173469  0.8511850
##   0.3  1          0.6               0.875      150      1.118825  0.8588693
##   0.3  1          0.6               0.875      200      1.114795  0.8577995
##   0.3  1          0.6               0.875      250      1.115795  0.8561051
##   0.3  1          0.6               1.000       50      1.416728  0.8051186
##   0.3  1          0.6               1.000      100      1.183031  0.8523200
##   0.3  1          0.6               1.000      150      1.116098  0.8609886
##   0.3  1          0.6               1.000      200      1.095355  0.8630307
##   0.3  1          0.6               1.000      250      1.092266  0.8623048
##   0.3  1          0.8               0.500       50      1.397701  0.7954989
##   0.3  1          0.8               0.500      100      1.197568  0.8360239
##   0.3  1          0.8               0.500      150      1.196533  0.8353681
##   0.3  1          0.8               0.500      200      1.173332  0.8379905
##   0.3  1          0.8               0.500      250      1.168466  0.8387034
##   0.3  1          0.8               0.625       50      1.362801  0.8117822
##   0.3  1          0.8               0.625      100      1.182354  0.8452858
##   0.3  1          0.8               0.625      150      1.146062  0.8496697
##   0.3  1          0.8               0.625      200      1.134709  0.8519799
##   0.3  1          0.8               0.625      250      1.153457  0.8449150
##   0.3  1          0.8               0.750       50      1.383044  0.8082881
##   0.3  1          0.8               0.750      100      1.173819  0.8472920
##   0.3  1          0.8               0.750      150      1.126809  0.8561366
##   0.3  1          0.8               0.750      200      1.129222  0.8533769
##   0.3  1          0.8               0.750      250      1.126619  0.8531121
##   0.3  1          0.8               0.875       50      1.413715  0.8041976
##   0.3  1          0.8               0.875      100      1.201149  0.8438495
##   0.3  1          0.8               0.875      150      1.144407  0.8518164
##   0.3  1          0.8               0.875      200      1.142756  0.8500162
##   0.3  1          0.8               0.875      250      1.136565  0.8510828
##   0.3  1          0.8               1.000       50      1.419007  0.8073251
##   0.3  1          0.8               1.000      100      1.189885  0.8509704
##   0.3  1          0.8               1.000      150      1.124194  0.8600180
##   0.3  1          0.8               1.000      200      1.101172  0.8620828
##   0.3  1          0.8               1.000      250      1.099683  0.8608583
##   0.3  2          0.6               0.500       50      1.293173  0.8098524
##   0.3  2          0.6               0.500      100      1.305913  0.7994695
##   0.3  2          0.6               0.500      150      1.334434  0.7910862
##   0.3  2          0.6               0.500      200      1.351337  0.7889760
##   0.3  2          0.6               0.500      250      1.399503  0.7733491
##   0.3  2          0.6               0.625       50      1.292586  0.8063764
##   0.3  2          0.6               0.625      100      1.263678  0.8152226
##   0.3  2          0.6               0.625      150      1.293964  0.8062317
##   0.3  2          0.6               0.625      200      1.315392  0.7997793
##   0.3  2          0.6               0.625      250      1.334901  0.7937768
##   0.3  2          0.6               0.750       50      1.231750  0.8307135
##   0.3  2          0.6               0.750      100      1.226186  0.8265136
##   0.3  2          0.6               0.750      150      1.255043  0.8162509
##   0.3  2          0.6               0.750      200      1.283946  0.8074367
##   0.3  2          0.6               0.750      250      1.303713  0.8010811
##   0.3  2          0.6               0.875       50      1.258933  0.8251014
##   0.3  2          0.6               0.875      100      1.241878  0.8212522
##   0.3  2          0.6               0.875      150      1.251549  0.8181272
##   0.3  2          0.6               0.875      200      1.274938  0.8100110
##   0.3  2          0.6               0.875      250      1.303342  0.8005471
##   0.3  2          0.6               1.000       50      1.256803  0.8262635
##   0.3  2          0.6               1.000      100      1.238776  0.8266329
##   0.3  2          0.6               1.000      150      1.265238  0.8165886
##   0.3  2          0.6               1.000      200      1.304089  0.8038062
##   0.3  2          0.6               1.000      250      1.313278  0.7993977
##   0.3  2          0.8               0.500       50      1.250846  0.8239134
##   0.3  2          0.8               0.500      100      1.283386  0.8104486
##   0.3  2          0.8               0.500      150      1.321724  0.7960073
##   0.3  2          0.8               0.500      200      1.318019  0.7962637
##   0.3  2          0.8               0.500      250      1.359167  0.7854175
##   0.3  2          0.8               0.625       50      1.274641  0.8145409
##   0.3  2          0.8               0.625      100      1.296240  0.8030477
##   0.3  2          0.8               0.625      150      1.331523  0.7916075
##   0.3  2          0.8               0.625      200      1.348491  0.7869900
##   0.3  2          0.8               0.625      250      1.370067  0.7812792
##   0.3  2          0.8               0.750       50      1.281282  0.8138115
##   0.3  2          0.8               0.750      100      1.322842  0.7977326
##   0.3  2          0.8               0.750      150      1.361090  0.7835828
##   0.3  2          0.8               0.750      200      1.390268  0.7741471
##   0.3  2          0.8               0.750      250      1.417393  0.7651649
##   0.3  2          0.8               0.875       50      1.244801  0.8257320
##   0.3  2          0.8               0.875      100      1.247462  0.8192641
##   0.3  2          0.8               0.875      150      1.281509  0.8065895
##   0.3  2          0.8               0.875      200      1.314502  0.7961421
##   0.3  2          0.8               0.875      250      1.328593  0.7903572
##   0.3  2          0.8               1.000       50      1.246988  0.8235627
##   0.3  2          0.8               1.000      100      1.254742  0.8156077
##   0.3  2          0.8               1.000      150      1.272987  0.8089693
##   0.3  2          0.8               1.000      200      1.299067  0.7995842
##   0.3  2          0.8               1.000      250      1.314005  0.7939138
##   0.3  3          0.6               0.500       50      1.453313  0.7482614
##   0.3  3          0.6               0.500      100      1.443955  0.7490196
##   0.3  3          0.6               0.500      150      1.460086  0.7444962
##   0.3  3          0.6               0.500      200      1.459021  0.7453958
##   0.3  3          0.6               0.500      250      1.461777  0.7433961
##   0.3  3          0.6               0.625       50      1.429503  0.7548261
##   0.3  3          0.6               0.625      100      1.461512  0.7435044
##   0.3  3          0.6               0.625      150      1.481449  0.7365953
##   0.3  3          0.6               0.625      200      1.500053  0.7299786
##   0.3  3          0.6               0.625      250      1.501585  0.7290869
##   0.3  3          0.6               0.750       50      1.421454  0.7630552
##   0.3  3          0.6               0.750      100      1.433825  0.7563111
##   0.3  3          0.6               0.750      150      1.457657  0.7462889
##   0.3  3          0.6               0.750      200      1.469307  0.7416679
##   0.3  3          0.6               0.750      250      1.472062  0.7403402
##   0.3  3          0.6               0.875       50      1.356252  0.7851032
##   0.3  3          0.6               0.875      100      1.364665  0.7796411
##   0.3  3          0.6               0.875      150      1.375881  0.7742679
##   0.3  3          0.6               0.875      200      1.389415  0.7695483
##   0.3  3          0.6               0.875      250      1.391074  0.7688236
##   0.3  3          0.6               1.000       50      1.384287  0.7794592
##   0.3  3          0.6               1.000      100      1.381510  0.7782446
##   0.3  3          0.6               1.000      150      1.391995  0.7738100
##   0.3  3          0.6               1.000      200      1.401096  0.7709089
##   0.3  3          0.6               1.000      250      1.408173  0.7684265
##   0.3  3          0.8               0.500       50      1.466576  0.7443588
##   0.3  3          0.8               0.500      100      1.470374  0.7459076
##   0.3  3          0.8               0.500      150      1.496380  0.7370927
##   0.3  3          0.8               0.500      200      1.499561  0.7367047
##   0.3  3          0.8               0.500      250      1.505484  0.7347325
##   0.3  3          0.8               0.625       50      1.375329  0.7744059
##   0.3  3          0.8               0.625      100      1.428242  0.7555375
##   0.3  3          0.8               0.625      150      1.457635  0.7477745
##   0.3  3          0.8               0.625      200      1.467690  0.7445208
##   0.3  3          0.8               0.625      250      1.472929  0.7429085
##   0.3  3          0.8               0.750       50      1.312568  0.7989913
##   0.3  3          0.8               0.750      100      1.354787  0.7842244
##   0.3  3          0.8               0.750      150      1.374144  0.7780788
##   0.3  3          0.8               0.750      200      1.387497  0.7738912
##   0.3  3          0.8               0.750      250      1.394445  0.7713237
##   0.3  3          0.8               0.875       50      1.356074  0.7840423
##   0.3  3          0.8               0.875      100      1.383648  0.7740281
##   0.3  3          0.8               0.875      150      1.397579  0.7695579
##   0.3  3          0.8               0.875      200      1.409785  0.7656689
##   0.3  3          0.8               0.875      250      1.416674  0.7632181
##   0.3  3          0.8               1.000       50      1.412655  0.7687839
##   0.3  3          0.8               1.000      100      1.424012  0.7620757
##   0.3  3          0.8               1.000      150      1.439480  0.7566077
##   0.3  3          0.8               1.000      200      1.451549  0.7523296
##   0.3  3          0.8               1.000      250      1.458618  0.7497903
##   0.3  4          0.6               0.500       50      1.569280  0.7094530
##   0.3  4          0.6               0.500      100      1.580197  0.7042346
##   0.3  4          0.6               0.500      150      1.592367  0.7008648
##   0.3  4          0.6               0.500      200      1.597278  0.6995339
##   0.3  4          0.6               0.500      250      1.597937  0.6994128
##   0.3  4          0.6               0.625       50      1.530974  0.7297968
##   0.3  4          0.6               0.625      100      1.535797  0.7254394
##   0.3  4          0.6               0.625      150      1.538343  0.7241281
##   0.3  4          0.6               0.625      200      1.537639  0.7242822
##   0.3  4          0.6               0.625      250      1.538275  0.7241181
##   0.3  4          0.6               0.750       50      1.547119  0.7180914
##   0.3  4          0.6               0.750      100      1.556740  0.7129409
##   0.3  4          0.6               0.750      150      1.561608  0.7106459
##   0.3  4          0.6               0.750      200      1.564089  0.7096162
##   0.3  4          0.6               0.750      250      1.564431  0.7094984
##   0.3  4          0.6               0.875       50      1.471640  0.7500857
##   0.3  4          0.6               0.875      100      1.479754  0.7467273
##   0.3  4          0.6               0.875      150      1.485605  0.7443475
##   0.3  4          0.6               0.875      200      1.487136  0.7437163
##   0.3  4          0.6               0.875      250      1.487416  0.7435871
##   0.3  4          0.6               1.000       50      1.510171  0.7346207
##   0.3  4          0.6               1.000      100      1.515788  0.7313073
##   0.3  4          0.6               1.000      150      1.520840  0.7293296
##   0.3  4          0.6               1.000      200      1.523764  0.7280906
##   0.3  4          0.6               1.000      250      1.523985  0.7279482
##   0.3  4          0.8               0.500       50      1.585758  0.6955732
##   0.3  4          0.8               0.500      100      1.585807  0.6973966
##   0.3  4          0.8               0.500      150      1.585594  0.6976265
##   0.3  4          0.8               0.500      200      1.585817  0.6977293
##   0.3  4          0.8               0.500      250      1.586836  0.6972025
##   0.3  4          0.8               0.625       50      1.464957  0.7408598
##   0.3  4          0.8               0.625      100      1.472085  0.7365818
##   0.3  4          0.8               0.625      150      1.474420  0.7359573
##   0.3  4          0.8               0.625      200      1.475394  0.7355419
##   0.3  4          0.8               0.625      250      1.475935  0.7352571
##   0.3  4          0.8               0.750       50      1.479284  0.7447946
##   0.3  4          0.8               0.750      100      1.480732  0.7436399
##   0.3  4          0.8               0.750      150      1.483649  0.7426801
##   0.3  4          0.8               0.750      200      1.484418  0.7425073
##   0.3  4          0.8               0.750      250      1.484841  0.7423580
##   0.3  4          0.8               0.875       50      1.458562  0.7546509
##   0.3  4          0.8               0.875      100      1.472183  0.7483962
##   0.3  4          0.8               0.875      150      1.477248  0.7464561
##   0.3  4          0.8               0.875      200      1.479051  0.7457234
##   0.3  4          0.8               0.875      250      1.479591  0.7455318
##   0.3  4          0.8               1.000       50      1.457976  0.7535251
##   0.3  4          0.8               1.000      100      1.467507  0.7493579
##   0.3  4          0.8               1.000      150      1.470959  0.7482184
##   0.3  4          0.8               1.000      200      1.471814  0.7479341
##   0.3  4          0.8               1.000      250      1.472068  0.7477930
##   0.3  5          0.6               0.500       50      1.700753  0.6592397
##   0.3  5          0.6               0.500      100      1.692083  0.6615261
##   0.3  5          0.6               0.500      150      1.693283  0.6606029
##   0.3  5          0.6               0.500      200      1.693456  0.6605135
##   0.3  5          0.6               0.500      250      1.693228  0.6605998
##   0.3  5          0.6               0.625       50      1.568624  0.7057694
##   0.3  5          0.6               0.625      100      1.575393  0.7021118
##   0.3  5          0.6               0.625      150      1.575092  0.7020729
##   0.3  5          0.6               0.625      200      1.574565  0.7023054
##   0.3  5          0.6               0.625      250      1.574676  0.7022545
##   0.3  5          0.6               0.750       50      1.610183  0.6850206
##   0.3  5          0.6               0.750      100      1.608108  0.6858285
##   0.3  5          0.6               0.750      150      1.608374  0.6857578
##   0.3  5          0.6               0.750      200      1.608359  0.6857570
##   0.3  5          0.6               0.750      250      1.608364  0.6857550
##   0.3  5          0.6               0.875       50      1.594728  0.7085554
##   0.3  5          0.6               0.875      100      1.596690  0.7075460
##   0.3  5          0.6               0.875      150      1.597451  0.7071851
##   0.3  5          0.6               0.875      200      1.597613  0.7071015
##   0.3  5          0.6               0.875      250      1.597619  0.7071010
##   0.3  5          0.6               1.000       50      1.587431  0.7090162
##   0.3  5          0.6               1.000      100      1.585906  0.7091964
##   0.3  5          0.6               1.000      150      1.585726  0.7091545
##   0.3  5          0.6               1.000      200      1.585890  0.7090935
##   0.3  5          0.6               1.000      250      1.585890  0.7090936
##   0.3  5          0.8               0.500       50      1.453342  0.7480100
##   0.3  5          0.8               0.500      100      1.465671  0.7438649
##   0.3  5          0.8               0.500      150      1.465946  0.7439473
##   0.3  5          0.8               0.500      200      1.466345  0.7437855
##   0.3  5          0.8               0.500      250      1.466461  0.7437409
##   0.3  5          0.8               0.625       50      1.483848  0.7411989
##   0.3  5          0.8               0.625      100      1.491508  0.7387221
##   0.3  5          0.8               0.625      150      1.492587  0.7381294
##   0.3  5          0.8               0.625      200      1.492728  0.7380536
##   0.3  5          0.8               0.625      250      1.492720  0.7380556
##   0.3  5          0.8               0.750       50      1.499538  0.7367183
##   0.3  5          0.8               0.750      100      1.500264  0.7355592
##   0.3  5          0.8               0.750      150      1.501034  0.7352748
##   0.3  5          0.8               0.750      200      1.501054  0.7352572
##   0.3  5          0.8               0.750      250      1.501037  0.7352629
##   0.3  5          0.8               0.875       50      1.552811  0.7203387
##   0.3  5          0.8               0.875      100      1.555675  0.7192573
##   0.3  5          0.8               0.875      150      1.556166  0.7190813
##   0.3  5          0.8               0.875      200      1.556231  0.7190557
##   0.3  5          0.8               0.875      250      1.556231  0.7190558
##   0.3  5          0.8               1.000       50      1.520495  0.7326256
##   0.3  5          0.8               1.000      100      1.523499  0.7309716
##   0.3  5          0.8               1.000      150      1.523813  0.7308277
##   0.3  5          0.8               1.000      200      1.523831  0.7308251
##   0.3  5          0.8               1.000      250      1.523831  0.7308251
##   0.4  1          0.6               0.500       50      1.305965  0.8084091
##   0.4  1          0.6               0.500      100      1.178563  0.8377337
##   0.4  1          0.6               0.500      150      1.199335  0.8295587
##   0.4  1          0.6               0.500      200      1.196994  0.8320221
##   0.4  1          0.6               0.500      250      1.202046  0.8291259
##   0.4  1          0.6               0.625       50      1.265496  0.8248607
##   0.4  1          0.6               0.625      100      1.149913  0.8479978
##   0.4  1          0.6               0.625      150      1.147030  0.8459638
##   0.4  1          0.6               0.625      200      1.171456  0.8383996
##   0.4  1          0.6               0.625      250      1.182452  0.8350858
##   0.4  1          0.6               0.750       50      1.287414  0.8269098
##   0.4  1          0.6               0.750      100      1.136249  0.8538553
##   0.4  1          0.6               0.750      150      1.127024  0.8538154
##   0.4  1          0.6               0.750      200      1.123760  0.8536378
##   0.4  1          0.6               0.750      250      1.137872  0.8493174
##   0.4  1          0.6               0.875       50      1.315689  0.8209103
##   0.4  1          0.6               0.875      100      1.171732  0.8443010
##   0.4  1          0.6               0.875      150      1.142831  0.8488703
##   0.4  1          0.6               0.875      200      1.146450  0.8481316
##   0.4  1          0.6               0.875      250      1.145700  0.8487642
##   0.4  1          0.6               1.000       50      1.292197  0.8314804
##   0.4  1          0.6               1.000      100      1.132869  0.8579057
##   0.4  1          0.6               1.000      150      1.099252  0.8619553
##   0.4  1          0.6               1.000      200      1.097694  0.8605919
##   0.4  1          0.6               1.000      250      1.098242  0.8594757
##   0.4  1          0.8               0.500       50      1.251767  0.8274638
##   0.4  1          0.8               0.500      100      1.188002  0.8355258
##   0.4  1          0.8               0.500      150      1.165220  0.8431987
##   0.4  1          0.8               0.500      200      1.202049  0.8323099
##   0.4  1          0.8               0.500      250      1.195515  0.8340015
##   0.4  1          0.8               0.625       50      1.255466  0.8291206
##   0.4  1          0.8               0.625      100      1.201771  0.8318582
##   0.4  1          0.8               0.625      150      1.198833  0.8323405
##   0.4  1          0.8               0.625      200      1.210606  0.8296303
##   0.4  1          0.8               0.625      250      1.215722  0.8282653
##   0.4  1          0.8               0.750       50      1.264434  0.8282491
##   0.4  1          0.8               0.750      100      1.149053  0.8492607
##   0.4  1          0.8               0.750      150      1.158742  0.8448717
##   0.4  1          0.8               0.750      200      1.157622  0.8437963
##   0.4  1          0.8               0.750      250      1.170919  0.8413468
##   0.4  1          0.8               0.875       50      1.284380  0.8279937
##   0.4  1          0.8               0.875      100      1.156331  0.8476284
##   0.4  1          0.8               0.875      150      1.148054  0.8476783
##   0.4  1          0.8               0.875      200      1.147374  0.8474017
##   0.4  1          0.8               0.875      250      1.155491  0.8437466
##   0.4  1          0.8               1.000       50      1.257657  0.8427955
##   0.4  1          0.8               1.000      100      1.114307  0.8632200
##   0.4  1          0.8               1.000      150      1.089528  0.8646780
##   0.4  1          0.8               1.000      200      1.092569  0.8621329
##   0.4  1          0.8               1.000      250      1.090890  0.8618759
##   0.4  2          0.6               0.500       50      1.382846  0.7732921
##   0.4  2          0.6               0.500      100      1.380633  0.7748210
##   0.4  2          0.6               0.500      150      1.427312  0.7656735
##   0.4  2          0.6               0.500      200      1.472135  0.7498858
##   0.4  2          0.6               0.500      250      1.483114  0.7450315
##   0.4  2          0.6               0.625       50      1.297364  0.8041826
##   0.4  2          0.6               0.625      100      1.325380  0.7940477
##   0.4  2          0.6               0.625      150      1.342430  0.7908894
##   0.4  2          0.6               0.625      200      1.359175  0.7852144
##   0.4  2          0.6               0.625      250      1.393512  0.7742468
##   0.4  2          0.6               0.750       50      1.296944  0.7991756
##   0.4  2          0.6               0.750      100      1.337103  0.7838766
##   0.4  2          0.6               0.750      150      1.366138  0.7730353
##   0.4  2          0.6               0.750      200      1.407433  0.7601699
##   0.4  2          0.6               0.750      250      1.415998  0.7579753
##   0.4  2          0.6               0.875       50      1.281847  0.8123070
##   0.4  2          0.6               0.875      100      1.274670  0.8103222
##   0.4  2          0.6               0.875      150      1.314215  0.7964751
##   0.4  2          0.6               0.875      200      1.332842  0.7898352
##   0.4  2          0.6               0.875      250      1.348344  0.7847896
##   0.4  2          0.6               1.000       50      1.249770  0.8241132
##   0.4  2          0.6               1.000      100      1.273366  0.8129779
##   0.4  2          0.6               1.000      150      1.291021  0.8062827
##   0.4  2          0.6               1.000      200      1.319222  0.7968751
##   0.4  2          0.6               1.000      250      1.336921  0.7909965
##   0.4  2          0.8               0.500       50      1.375613  0.7758602
##   0.4  2          0.8               0.500      100      1.435903  0.7624594
##   0.4  2          0.8               0.500      150      1.466381  0.7509387
##   0.4  2          0.8               0.500      200      1.524180  0.7337150
##   0.4  2          0.8               0.500      250      1.560392  0.7221041
##   0.4  2          0.8               0.625       50      1.306547  0.8010212
##   0.4  2          0.8               0.625      100      1.311037  0.7949561
##   0.4  2          0.8               0.625      150      1.347385  0.7829079
##   0.4  2          0.8               0.625      200      1.365435  0.7772369
##   0.4  2          0.8               0.625      250      1.365801  0.7752091
##   0.4  2          0.8               0.750       50      1.330693  0.7941363
##   0.4  2          0.8               0.750      100      1.359145  0.7830225
##   0.4  2          0.8               0.750      150      1.410331  0.7680722
##   0.4  2          0.8               0.750      200      1.451177  0.7547243
##   0.4  2          0.8               0.750      250      1.449425  0.7560458
##   0.4  2          0.8               0.875       50      1.300123  0.7999244
##   0.4  2          0.8               0.875      100      1.339805  0.7834315
##   0.4  2          0.8               0.875      150      1.368530  0.7747646
##   0.4  2          0.8               0.875      200      1.407303  0.7623228
##   0.4  2          0.8               0.875      250      1.435079  0.7528887
##   0.4  2          0.8               1.000       50      1.260959  0.8130258
##   0.4  2          0.8               1.000      100      1.286146  0.8047402
##   0.4  2          0.8               1.000      150      1.330098  0.7909329
##   0.4  2          0.8               1.000      200      1.364343  0.7802868
##   0.4  2          0.8               1.000      250      1.390332  0.7713007
##   0.4  3          0.6               0.500       50      1.459147  0.7540519
##   0.4  3          0.6               0.500      100      1.489147  0.7448597
##   0.4  3          0.6               0.500      150      1.511626  0.7343949
##   0.4  3          0.6               0.500      200      1.517107  0.7318241
##   0.4  3          0.6               0.500      250      1.518259  0.7316891
##   0.4  3          0.6               0.625       50      1.426459  0.7524073
##   0.4  3          0.6               0.625      100      1.477395  0.7344770
##   0.4  3          0.6               0.625      150      1.497116  0.7286593
##   0.4  3          0.6               0.625      200      1.501331  0.7274881
##   0.4  3          0.6               0.625      250      1.503481  0.7272506
##   0.4  3          0.6               0.750       50      1.407118  0.7688564
##   0.4  3          0.6               0.750      100      1.455873  0.7531430
##   0.4  3          0.6               0.750      150      1.471028  0.7468242
##   0.4  3          0.6               0.750      200      1.478464  0.7438432
##   0.4  3          0.6               0.750      250      1.480622  0.7430396
##   0.4  3          0.6               0.875       50      1.389217  0.7670554
##   0.4  3          0.6               0.875      100      1.437512  0.7503580
##   0.4  3          0.6               0.875      150      1.457816  0.7429962
##   0.4  3          0.6               0.875      200      1.464680  0.7407345
##   0.4  3          0.6               0.875      250      1.469581  0.7391227
##   0.4  3          0.6               1.000       50      1.404112  0.7752231
##   0.4  3          0.6               1.000      100      1.413625  0.7703729
##   0.4  3          0.6               1.000      150      1.419060  0.7666844
##   0.4  3          0.6               1.000      200      1.423849  0.7645119
##   0.4  3          0.6               1.000      250      1.427611  0.7631356
##   0.4  3          0.8               0.500       50      1.560611  0.7128458
##   0.4  3          0.8               0.500      100      1.613221  0.6920922
##   0.4  3          0.8               0.500      150      1.624152  0.6892353
##   0.4  3          0.8               0.500      200      1.633569  0.6859070
##   0.4  3          0.8               0.500      250      1.634972  0.6857252
##   0.4  3          0.8               0.625       50      1.430450  0.7632358
##   0.4  3          0.8               0.625      100      1.473931  0.7513469
##   0.4  3          0.8               0.625      150      1.496736  0.7441334
##   0.4  3          0.8               0.625      200      1.504713  0.7414591
##   0.4  3          0.8               0.625      250      1.507800  0.7395685
##   0.4  3          0.8               0.750       50      1.470519  0.7450424
##   0.4  3          0.8               0.750      100      1.543300  0.7205907
##   0.4  3          0.8               0.750      150      1.567029  0.7129916
##   0.4  3          0.8               0.750      200      1.575830  0.7102100
##   0.4  3          0.8               0.750      250      1.579058  0.7089863
##   0.4  3          0.8               0.875       50      1.380954  0.7786713
##   0.4  3          0.8               0.875      100      1.378215  0.7799626
##   0.4  3          0.8               0.875      150      1.395653  0.7733847
##   0.4  3          0.8               0.875      200      1.404123  0.7698943
##   0.4  3          0.8               0.875      250      1.404418  0.7695173
##   0.4  3          0.8               1.000       50      1.328849  0.7937076
##   0.4  3          0.8               1.000      100      1.366506  0.7819128
##   0.4  3          0.8               1.000      150      1.382197  0.7768419
##   0.4  3          0.8               1.000      200      1.387384  0.7748091
##   0.4  3          0.8               1.000      250      1.389670  0.7740772
##   0.4  4          0.6               0.500       50      1.575142  0.7056286
##   0.4  4          0.6               0.500      100      1.579966  0.7049851
##   0.4  4          0.6               0.500      150      1.585896  0.7032350
##   0.4  4          0.6               0.500      200      1.587251  0.7024304
##   0.4  4          0.6               0.500      250      1.588390  0.7020538
##   0.4  4          0.6               0.625       50      1.572469  0.6969080
##   0.4  4          0.6               0.625      100      1.580685  0.6950103
##   0.4  4          0.6               0.625      150      1.584498  0.6933922
##   0.4  4          0.6               0.625      200      1.585625  0.6929754
##   0.4  4          0.6               0.625      250      1.585385  0.6930482
##   0.4  4          0.6               0.750       50      1.577369  0.7047530
##   0.4  4          0.6               0.750      100      1.589654  0.7007150
##   0.4  4          0.6               0.750      150      1.590692  0.7000641
##   0.4  4          0.6               0.750      200      1.590921  0.6998228
##   0.4  4          0.6               0.750      250      1.591109  0.6997410
##   0.4  4          0.6               0.875       50      1.566603  0.7227653
##   0.4  4          0.6               0.875      100      1.585162  0.7157663
##   0.4  4          0.6               0.875      150      1.587427  0.7151324
##   0.4  4          0.6               0.875      200      1.588393  0.7147788
##   0.4  4          0.6               0.875      250      1.588491  0.7147416
##   0.4  4          0.6               1.000       50      1.575535  0.7063201
##   0.4  4          0.6               1.000      100      1.588724  0.6991124
##   0.4  4          0.6               1.000      150      1.591885  0.6977843
##   0.4  4          0.6               1.000      200      1.592292  0.6975742
##   0.4  4          0.6               1.000      250      1.592368  0.6975165
##   0.4  4          0.8               0.500       50      1.704180  0.6554198
##   0.4  4          0.8               0.500      100      1.742700  0.6421642
##   0.4  4          0.8               0.500      150      1.749431  0.6394268
##   0.4  4          0.8               0.500      200      1.751546  0.6384905
##   0.4  4          0.8               0.500      250      1.751423  0.6385508
##   0.4  4          0.8               0.625       50      1.596006  0.6900303
##   0.4  4          0.8               0.625      100      1.611045  0.6831381
##   0.4  4          0.8               0.625      150      1.612709  0.6827259
##   0.4  4          0.8               0.625      200      1.613116  0.6826293
##   0.4  4          0.8               0.625      250      1.613229  0.6825674
##   0.4  4          0.8               0.750       50      1.562654  0.7217163
##   0.4  4          0.8               0.750      100      1.576654  0.7154935
##   0.4  4          0.8               0.750      150      1.578169  0.7149824
##   0.4  4          0.8               0.750      200      1.578445  0.7148381
##   0.4  4          0.8               0.750      250      1.578533  0.7148008
##   0.4  4          0.8               0.875       50      1.556560  0.7109702
##   0.4  4          0.8               0.875      100      1.563499  0.7078702
##   0.4  4          0.8               0.875      150      1.566406  0.7067509
##   0.4  4          0.8               0.875      200      1.566620  0.7066865
##   0.4  4          0.8               0.875      250      1.566623  0.7066945
##   0.4  4          0.8               1.000       50      1.570160  0.7094464
##   0.4  4          0.8               1.000      100      1.581473  0.7054040
##   0.4  4          0.8               1.000      150      1.584587  0.7043297
##   0.4  4          0.8               1.000      200      1.585064  0.7041741
##   0.4  4          0.8               1.000      250      1.585156  0.7041437
##   0.4  5          0.6               0.500       50      1.898603  0.5890062
##   0.4  5          0.6               0.500      100      1.909160  0.5864830
##   0.4  5          0.6               0.500      150      1.909618  0.5862835
##   0.4  5          0.6               0.500      200      1.909594  0.5862958
##   0.4  5          0.6               0.500      250      1.909600  0.5862944
##   0.4  5          0.6               0.625       50      1.679796  0.6716745
##   0.4  5          0.6               0.625      100      1.689155  0.6683290
##   0.4  5          0.6               0.625      150      1.689804  0.6681863
##   0.4  5          0.6               0.625      200      1.689905  0.6681524
##   0.4  5          0.6               0.625      250      1.689917  0.6681456
##   0.4  5          0.6               0.750       50      1.668344  0.6622040
##   0.4  5          0.6               0.750      100      1.673505  0.6599669
##   0.4  5          0.6               0.750      150      1.673395  0.6599716
##   0.4  5          0.6               0.750      200      1.673394  0.6599679
##   0.4  5          0.6               0.750      250      1.673396  0.6599668
##   0.4  5          0.6               0.875       50      1.719785  0.6472494
##   0.4  5          0.6               0.875      100      1.724025  0.6453718
##   0.4  5          0.6               0.875      150      1.724172  0.6452741
##   0.4  5          0.6               0.875      200      1.724178  0.6452710
##   0.4  5          0.6               0.875      250      1.724182  0.6452697
##   0.4  5          0.6               1.000       50      1.626033  0.6812134
##   0.4  5          0.6               1.000      100      1.625980  0.6813039
##   0.4  5          0.6               1.000      150      1.626088  0.6812283
##   0.4  5          0.6               1.000      200      1.626088  0.6812283
##   0.4  5          0.6               1.000      250      1.626088  0.6812283
##   0.4  5          0.8               0.500       50      1.643016  0.6892097
##   0.4  5          0.8               0.500      100      1.642470  0.6911773
##   0.4  5          0.8               0.500      150      1.643028  0.6910391
##   0.4  5          0.8               0.500      200      1.643019  0.6910702
##   0.4  5          0.8               0.500      250      1.643060  0.6910534
##   0.4  5          0.8               0.625       50      1.582925  0.6894787
##   0.4  5          0.8               0.625      100      1.589245  0.6876139
##   0.4  5          0.8               0.625      150      1.589230  0.6876971
##   0.4  5          0.8               0.625      200      1.589220  0.6877053
##   0.4  5          0.8               0.625      250      1.589219  0.6877055
##   0.4  5          0.8               0.750       50      1.651746  0.6920814
##   0.4  5          0.8               0.750      100      1.654738  0.6912944
##   0.4  5          0.8               0.750      150      1.654761  0.6913242
##   0.4  5          0.8               0.750      200      1.654754  0.6913270
##   0.4  5          0.8               0.750      250      1.654762  0.6913237
##   0.4  5          0.8               0.875       50      1.567923  0.7103048
##   0.4  5          0.8               0.875      100      1.570973  0.7095698
##   0.4  5          0.8               0.875      150      1.570945  0.7095782
##   0.4  5          0.8               0.875      200      1.570942  0.7095796
##   0.4  5          0.8               0.875      250      1.570943  0.7095789
##   0.4  5          0.8               1.000       50      1.616367  0.6908239
##   0.4  5          0.8               1.000      100      1.620053  0.6895021
##   0.4  5          0.8               1.000      150      1.620265  0.6894187
##   0.4  5          0.8               1.000      200      1.620265  0.6894187
##   0.4  5          0.8               1.000      250      1.620264  0.6894187
##   MAE      
##   1.1099093
##   0.9245568
##   0.9057354
##   0.9006311
##   0.9015552
##   1.1286382
##   0.9572945
##   0.9132658
##   0.9165967
##   0.9051691
##   1.1384399
##   0.9673060
##   0.9222308
##   0.9184808
##   0.9176063
##   1.1252645
##   0.9495337
##   0.8948260
##   0.8857623
##   0.8895031
##   1.1516519
##   0.9596417
##   0.9041220
##   0.8832723
##   0.8767988
##   1.1376807
##   0.9555981
##   0.9498462
##   0.9381644
##   0.9299479
##   1.1119231
##   0.9512825
##   0.9129568
##   0.8984064
##   0.9179026
##   1.1151193
##   0.9515504
##   0.9118135
##   0.9031139
##   0.9021886
##   1.1526421
##   0.9645016
##   0.9148278
##   0.9116130
##   0.9037894
##   1.1540039
##   0.9646394
##   0.9148862
##   0.8898829
##   0.8862980
##   1.0383184
##   1.0434680
##   1.0556690
##   1.0522643
##   1.0938810
##   1.0221279
##   0.9938015
##   1.0194798
##   1.0427383
##   1.0538486
##   0.9766370
##   0.9584203
##   0.9760733
##   0.9976643
##   1.0183018
##   1.0169939
##   0.9964501
##   0.9902019
##   1.0033305
##   1.0271237
##   1.0213601
##   1.0052177
##   1.0120138
##   1.0415740
##   1.0429271
##   0.9958238
##   1.0269492
##   1.0597647
##   1.0522737
##   1.0815983
##   1.0170641
##   1.0154035
##   1.0495999
##   1.0640600
##   1.0843004
##   1.0037579
##   1.0221311
##   1.0589776
##   1.0849648
##   1.1073361
##   0.9974183
##   0.9899921
##   1.0068759
##   1.0310381
##   1.0401662
##   1.0094043
##   1.0109650
##   1.0196010
##   1.0406793
##   1.0528244
##   1.1600688
##   1.1550582
##   1.1642242
##   1.1647047
##   1.1654880
##   1.1180830
##   1.1492295
##   1.1686543
##   1.1775218
##   1.1774666
##   1.1318122
##   1.1317377
##   1.1467042
##   1.1571832
##   1.1571273
##   1.0870838
##   1.0863043
##   1.0880827
##   1.0985640
##   1.0982434
##   1.0604554
##   1.0502054
##   1.0649334
##   1.0720000
##   1.0786415
##   1.1531216
##   1.1781170
##   1.1895009
##   1.1949643
##   1.1928002
##   1.0831387
##   1.1216112
##   1.1423253
##   1.1455744
##   1.1495571
##   1.0508423
##   1.0928080
##   1.1134834
##   1.1209379
##   1.1263917
##   1.0839660
##   1.1043719
##   1.1181375
##   1.1286702
##   1.1348693
##   1.1231899
##   1.1332556
##   1.1440118
##   1.1532248
##   1.1589587
##   1.2329020
##   1.2546853
##   1.2665031
##   1.2722047
##   1.2732579
##   1.1870184
##   1.1862673
##   1.1890759
##   1.1900053
##   1.1905862
##   1.2216723
##   1.2273658
##   1.2314961
##   1.2330389
##   1.2332627
##   1.1645394
##   1.1662378
##   1.1719584
##   1.1729361
##   1.1728676
##   1.1981096
##   1.2005869
##   1.2054637
##   1.2069327
##   1.2072018
##   1.2296038
##   1.2428201
##   1.2455444
##   1.2476926
##   1.2481663
##   1.1865587
##   1.1959458
##   1.1976634
##   1.1989303
##   1.1994840
##   1.2033866
##   1.2095660
##   1.2095742
##   1.2099509
##   1.2101849
##   1.1314935
##   1.1479772
##   1.1505859
##   1.1515812
##   1.1520882
##   1.1588330
##   1.1635980
##   1.1673839
##   1.1681091
##   1.1684457
##   1.3196433
##   1.3213751
##   1.3209509
##   1.3213929
##   1.3212479
##   1.2464763
##   1.2549353
##   1.2541473
##   1.2538593
##   1.2539536
##   1.2745960
##   1.2701805
##   1.2706173
##   1.2705911
##   1.2705874
##   1.2265959
##   1.2292165
##   1.2304741
##   1.2305427
##   1.2305490
##   1.2379645
##   1.2409164
##   1.2409592
##   1.2411755
##   1.2411754
##   1.1800773
##   1.1922281
##   1.1921384
##   1.1923750
##   1.1924936
##   1.1526669
##   1.1590584
##   1.1593376
##   1.1593077
##   1.1592939
##   1.2044166
##   1.2043238
##   1.2052008
##   1.2052033
##   1.2051871
##   1.2367109
##   1.2391400
##   1.2394809
##   1.2395398
##   1.2395384
##   1.1737210
##   1.1752563
##   1.1751172
##   1.1751439
##   1.1751438
##   1.0425901
##   0.9485953
##   0.9543804
##   0.9636615
##   0.9611338
##   1.0181250
##   0.9083674
##   0.9102625
##   0.9310841
##   0.9377568
##   1.0440465
##   0.9150213
##   0.8970604
##   0.8981324
##   0.9034927
##   1.0859945
##   0.9524925
##   0.9247410
##   0.9302486
##   0.9250009
##   1.0405790
##   0.9154058
##   0.8873516
##   0.8861441
##   0.8868640
##   1.0069704
##   0.9614287
##   0.9448432
##   0.9625521
##   0.9482392
##   1.0157217
##   0.9679074
##   0.9624925
##   0.9666940
##   0.9673617
##   1.0284018
##   0.9200545
##   0.9226820
##   0.9315137
##   0.9354304
##   1.0394872
##   0.9224045
##   0.9150610
##   0.9204423
##   0.9292643
##   1.0275615
##   0.9086050
##   0.8827242
##   0.8835900
##   0.8801720
##   1.1206259
##   1.1239517
##   1.1574683
##   1.1906528
##   1.1961089
##   1.0290172
##   1.0590727
##   1.0598536
##   1.0669337
##   1.0989375
##   1.0292234
##   1.0631206
##   1.0814106
##   1.1179271
##   1.1210284
##   1.0240165
##   1.0116957
##   1.0339817
##   1.0479882
##   1.0574304
##   1.0056519
##   1.0265503
##   1.0477291
##   1.0716691
##   1.0805169
##   1.0870670
##   1.1339180
##   1.1576224
##   1.1821356
##   1.2156836
##   1.0610358
##   1.0469251
##   1.0770922
##   1.0846617
##   1.0878711
##   1.0435288
##   1.0654351
##   1.1044403
##   1.1306805
##   1.1273219
##   1.0514433
##   1.0789669
##   1.0893829
##   1.1166413
##   1.1366951
##   1.0156490
##   1.0304713
##   1.0695845
##   1.0970000
##   1.1166431
##   1.1808794
##   1.2019858
##   1.2127666
##   1.2206289
##   1.2185911
##   1.1391185
##   1.1780853
##   1.1898540
##   1.1888891
##   1.1894796
##   1.1182877
##   1.1590121
##   1.1741198
##   1.1823603
##   1.1834222
##   1.0701272
##   1.1100680
##   1.1229392
##   1.1260755
##   1.1299649
##   1.1070961
##   1.1132061
##   1.1187244
##   1.1222461
##   1.1260774
##   1.2496636
##   1.2905972
##   1.3011817
##   1.3091366
##   1.3084400
##   1.1283285
##   1.1556825
##   1.1763817
##   1.1836317
##   1.1845570
##   1.1804628
##   1.2231375
##   1.2379052
##   1.2462196
##   1.2481401
##   1.0979053
##   1.1041015
##   1.1175120
##   1.1246294
##   1.1247143
##   1.0663160
##   1.0997386
##   1.1117570
##   1.1165800
##   1.1180344
##   1.2521192
##   1.2639940
##   1.2654973
##   1.2653661
##   1.2669386
##   1.2600470
##   1.2692798
##   1.2719551
##   1.2720295
##   1.2717641
##   1.2671093
##   1.2752579
##   1.2764972
##   1.2770010
##   1.2772723
##   1.2549843
##   1.2712944
##   1.2740373
##   1.2749655
##   1.2750516
##   1.2320308
##   1.2421190
##   1.2429721
##   1.2430057
##   1.2429580
##   1.3749277
##   1.4210402
##   1.4246910
##   1.4257667
##   1.4254044
##   1.2398914
##   1.2516430
##   1.2535347
##   1.2539251
##   1.2538263
##   1.2088494
##   1.2224489
##   1.2239979
##   1.2240627
##   1.2241538
##   1.2226143
##   1.2289839
##   1.2318367
##   1.2319648
##   1.2319454
##   1.2440424
##   1.2516369
##   1.2536160
##   1.2536640
##   1.2537735
##   1.5166461
##   1.5268671
##   1.5267662
##   1.5269792
##   1.5269728
##   1.3461198
##   1.3549654
##   1.3552708
##   1.3553762
##   1.3553749
##   1.2971210
##   1.3002670
##   1.3000096
##   1.2999752
##   1.2999876
##   1.3658947
##   1.3689984
##   1.3691216
##   1.3691293
##   1.3691316
##   1.3019971
##   1.2992930
##   1.2993411
##   1.2993408
##   1.2993410
##   1.2721764
##   1.2710665
##   1.2716481
##   1.2717112
##   1.2717652
##   1.2646808
##   1.2674157
##   1.2673933
##   1.2673961
##   1.2673972
##   1.2926777
##   1.2970153
##   1.2970959
##   1.2970874
##   1.2970958
##   1.2374779
##   1.2387754
##   1.2387794
##   1.2387732
##   1.2387769
##   1.2654542
##   1.2674219
##   1.2675774
##   1.2675775
##   1.2675770
## 
## Tuning parameter &#39;gamma&#39; was held constant at a value of 0
## Tuning
##  parameter &#39;min_child_weight&#39; was held constant at a value of 1
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were nrounds = 150, max_depth = 1, eta
##  = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample
##  = 1.</code></pre>
<p>The optimal tuning parameters were at <span class="math inline">\(M = 250\)</span> and <code>interation.depth = 1</code>.
<code>train()</code> tuned <code>eta</code> (<span class="math inline">\(\eta\)</span>), <code>max_depth</code>, <code>gamma</code>, <code>colsample_bytree</code>, <code>subsample</code>, and <code>nrounds</code>, holding <code>gamma = 0</code>, and <code>min_child_weight = 1</code>. The optimal hyperparameter values were <code>eta = 0.4</code>, <code>max_depth = 1</code>, <code>gamma = 0</code>, <code>colsample_bytree = 0.8</code>, <code>subsample = 1</code>, and <code>nrounds = 150</code>.</p>
<p>With so many hyperparameters, the tuning plot is nearly unreadable.</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="gradient-boosting.html#cb139-1"></a><span class="kw">plot</span>(cs_mdl_xgb)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-91-1.png" width="672" /><img src="data-sci_files/figure-html/unnamed-chunk-91-2.png" width="672" /></p>
<p>Here is the holdout set performance.</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="gradient-boosting.html#cb140-1"></a>cs_preds_xgb &lt;-<span class="st"> </span><span class="kw">bind_cols</span>(</span>
<span id="cb140-2"><a href="gradient-boosting.html#cb140-2"></a>   <span class="dt">Predicted =</span> <span class="kw">predict</span>(cs_mdl_xgb, <span class="dt">newdata =</span> cs_test),</span>
<span id="cb140-3"><a href="gradient-boosting.html#cb140-3"></a>   <span class="dt">Actual =</span> cs_test<span class="op">$</span>Sales</span>
<span id="cb140-4"><a href="gradient-boosting.html#cb140-4"></a>)</span>
<span id="cb140-5"><a href="gradient-boosting.html#cb140-5"></a></span>
<span id="cb140-6"><a href="gradient-boosting.html#cb140-6"></a><span class="co"># Model over-predicts at low end of Sales and under-predicts at high end</span></span>
<span id="cb140-7"><a href="gradient-boosting.html#cb140-7"></a>cs_preds_xgb <span class="op">%&gt;%</span></span>
<span id="cb140-8"><a href="gradient-boosting.html#cb140-8"></a><span class="st">   </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Actual, <span class="dt">y =</span> Predicted)) <span class="op">+</span></span>
<span id="cb140-9"><a href="gradient-boosting.html#cb140-9"></a><span class="st">   </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.6</span>, <span class="dt">color =</span> <span class="st">&quot;cadetblue&quot;</span>) <span class="op">+</span></span>
<span id="cb140-10"><a href="gradient-boosting.html#cb140-10"></a><span class="st">   </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;loess&quot;</span>, <span class="dt">formula =</span> <span class="st">&quot;y ~ x&quot;</span>) <span class="op">+</span></span>
<span id="cb140-11"><a href="gradient-boosting.html#cb140-11"></a><span class="st">   </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">slope =</span> <span class="dv">1</span>, <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span></span>
<span id="cb140-12"><a href="gradient-boosting.html#cb140-12"></a><span class="st">   </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Carseats XGBoost, Predicted vs Actual&quot;</span>)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-92-1.png" width="672" /></p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="gradient-boosting.html#cb141-1"></a><span class="kw">plot</span>(<span class="kw">varImp</span>(cs_mdl_xgb), <span class="dt">main=</span><span class="st">&quot;Variable Importance with XGBoost&quot;</span>)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-93-1.png" width="672" /></p>
<p>The RMSE is 1.438 - the best of the bunch.</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="gradient-boosting.html#cb142-1"></a>cs_rmse_xgb &lt;-<span class="st"> </span><span class="kw">RMSE</span>(<span class="dt">pred =</span> cs_preds_xgb<span class="op">$</span>Predicted, <span class="dt">obs =</span> cs_preds_xgb<span class="op">$</span>Actual)</span>
<span id="cb142-2"><a href="gradient-boosting.html#cb142-2"></a>cs_scoreboard &lt;-<span class="st"> </span><span class="kw">rbind</span>(cs_scoreboard,</span>
<span id="cb142-3"><a href="gradient-boosting.html#cb142-3"></a>   <span class="kw">data.frame</span>(<span class="dt">Model =</span> <span class="st">&quot;XGBoost&quot;</span>, <span class="dt">RMSE =</span> cs_rmse_xgb)</span>
<span id="cb142-4"><a href="gradient-boosting.html#cb142-4"></a>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">arrange</span>(RMSE)</span>
<span id="cb142-5"><a href="gradient-boosting.html#cb142-5"></a><span class="kw">scoreboard</span>(cs_scoreboard)</span></code></pre></div>
<div class="tabwid"><table style='border-collapse:collapse;'><thead><tr style="overflow-wrap:break-word;"><td style="width:112px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 2.00px solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">Model</span></p></td><td style="width:54px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 2.00px solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">RMSE</span></p></td></tr></thead><tbody><tr style="overflow-wrap:break-word;"><td style="width:112px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">XGBoost</span></p></td><td style="width:54px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">1.3952</span></p></td></tr><tr style="overflow-wrap:break-word;"><td style="width:112px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">GBM</span></p></td><td style="width:54px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">1.4381</span></p></td></tr><tr style="overflow-wrap:break-word;"><td style="width:112px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">Random Forest</span></p></td><td style="width:54px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">1.7184</span></p></td></tr><tr style="overflow-wrap:break-word;"><td style="width:112px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">Bagging</span></p></td><td style="width:54px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">1.9185</span></p></td></tr><tr style="overflow-wrap:break-word;"><td style="width:112px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">Single Tree (caret)</span></p></td><td style="width:54px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">2.2983</span></p></td></tr><tr style="overflow-wrap:break-word;"><td style="width:112px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">Single Tree</span></p></td><td style="width:54px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;"><p style="margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;background-color:transparent;"><span style="font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;">2.3632</span></p></td></tr></tbody></table></div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="random-forests.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="summary.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["data-sci.pdf", "data-sci.epub"],
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
