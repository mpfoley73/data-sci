<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.7 Model Validation | My Data Science Notes</title>
  <meta name="description" content="This is a compendium of notes from classes, tutorials, etc. that I reference from time to time." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="4.7 Model Validation | My Data Science Notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a compendium of notes from classes, tutorials, etc. that I reference from time to time." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.7 Model Validation | My Data Science Notes" />
  
  <meta name="twitter:description" content="This is a compendium of notes from classes, tutorials, etc. that I reference from time to time." />
  

<meta name="author" content="Michael Foley" />


<meta name="date" content="2020-07-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="interpretation.html"/>
<link rel="next" href="ols-reference.html"/>
<script src="assets/jquery-2.2.3/jquery.min.js"></script>
<link href="assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="assets/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="assets/tabwid-1.0.0/tabwid.css" rel="stylesheet" />
<script src="assets/tabwid-1.0.0/tabwid.js"></script>
<script src="assets/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="assets/plotly-binding-4.9.2.1/plotly.js"></script>
<script src="assets/typedarray-0.1/typedarray.min.js"></script>
<link href="assets/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="assets/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<link href="assets/plotly-htmlwidgets-css-1.52.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="assets/plotly-main-1.52.2/plotly-latest.min.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">My Data Science Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Intro</a></li>
<li class="chapter" data-level="1" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>1</b> Probability</a><ul>
<li class="chapter" data-level="1.1" data-path="principles.html"><a href="principles.html"><i class="fa fa-check"></i><b>1.1</b> Principles</a></li>
<li class="chapter" data-level="1.2" data-path="disc-dist.html"><a href="disc-dist.html"><i class="fa fa-check"></i><b>1.2</b> Discrete Distributions</a><ul>
<li class="chapter" data-level="1.2.1" data-path="disc-dist.html"><a href="disc-dist.html#bernoulli"><i class="fa fa-check"></i><b>1.2.1</b> Bernoulli</a></li>
<li class="chapter" data-level="1.2.2" data-path="disc-dist.html"><a href="disc-dist.html#binomial"><i class="fa fa-check"></i><b>1.2.2</b> Binomial</a></li>
<li class="chapter" data-level="1.2.3" data-path="disc-dist.html"><a href="disc-dist.html#poission"><i class="fa fa-check"></i><b>1.2.3</b> Poission</a></li>
<li class="chapter" data-level="1.2.4" data-path="disc-dist.html"><a href="disc-dist.html#multinomial"><i class="fa fa-check"></i><b>1.2.4</b> Multinomial</a></li>
<li class="chapter" data-level="1.2.5" data-path="disc-dist.html"><a href="disc-dist.html#negative-binomial"><i class="fa fa-check"></i><b>1.2.5</b> Negative-Binomial</a></li>
<li class="chapter" data-level="1.2.6" data-path="disc-dist.html"><a href="disc-dist.html#geometric"><i class="fa fa-check"></i><b>1.2.6</b> Geometric</a></li>
<li class="chapter" data-level="1.2.7" data-path="disc-dist.html"><a href="disc-dist.html#hypergeometric"><i class="fa fa-check"></i><b>1.2.7</b> Hypergeometric</a></li>
<li class="chapter" data-level="1.2.8" data-path="disc-dist.html"><a href="disc-dist.html#gamma"><i class="fa fa-check"></i><b>1.2.8</b> Gamma</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="cont-dist.html"><a href="cont-dist.html"><i class="fa fa-check"></i><b>1.3</b> Continuous Distributions</a><ul>
<li class="chapter" data-level="1.3.1" data-path="cont-dist.html"><a href="cont-dist.html#normal"><i class="fa fa-check"></i><b>1.3.1</b> Normal</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="join-distributions.html"><a href="join-distributions.html"><i class="fa fa-check"></i><b>1.4</b> Join Distributions</a></li>
<li class="chapter" data-level="1.5" data-path="likelihood.html"><a href="likelihood.html"><i class="fa fa-check"></i><b>1.5</b> Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-tests.html"><a href="statistical-tests.html"><i class="fa fa-check"></i><b>2</b> Statistical Tests</a><ul>
<li class="chapter" data-level="2.1" data-path="chi-square-test.html"><a href="chi-square-test.html"><i class="fa fa-check"></i><b>2.1</b> Chi-Square Test</a></li>
<li class="chapter" data-level="2.2" data-path="one-way-tables.html"><a href="one-way-tables.html"><i class="fa fa-check"></i><b>2.2</b> One-Way Tables</a><ul>
<li class="chapter" data-level="2.2.1" data-path="one-way-tables.html"><a href="one-way-tables.html#chi-square-goodness-of-fit-test"><i class="fa fa-check"></i><b>2.2.1</b> Chi-Square Goodness-of-Fit Test</a></li>
<li class="chapter" data-level="2.2.2" data-path="one-way-tables.html"><a href="one-way-tables.html#proportion-test"><i class="fa fa-check"></i><b>2.2.2</b> Proportion Test</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="two-way-tables.html"><a href="two-way-tables.html"><i class="fa fa-check"></i><b>2.3</b> Two-Way Tables</a><ul>
<li class="chapter" data-level="2.3.1" data-path="two-way-tables.html"><a href="two-way-tables.html#chi-square-independence-test"><i class="fa fa-check"></i><b>2.3.1</b> Chi-Square Independence Test</a></li>
<li class="chapter" data-level="2.3.2" data-path="two-way-tables.html"><a href="two-way-tables.html#residuals-analysis"><i class="fa fa-check"></i><b>2.3.2</b> Residuals Analysis</a></li>
<li class="chapter" data-level="2.3.3" data-path="two-way-tables.html"><a href="two-way-tables.html#difference-in-proportions"><i class="fa fa-check"></i><b>2.3.3</b> Difference in Proportions</a></li>
<li class="chapter" data-level="2.3.4" data-path="two-way-tables.html"><a href="two-way-tables.html#relative-risk"><i class="fa fa-check"></i><b>2.3.4</b> Relative Risk</a></li>
<li class="chapter" data-level="2.3.5" data-path="two-way-tables.html"><a href="two-way-tables.html#odds-ratio"><i class="fa fa-check"></i><b>2.3.5</b> Odds Ratio</a></li>
<li class="chapter" data-level="2.3.6" data-path="two-way-tables.html"><a href="two-way-tables.html#partitioning-chi-square"><i class="fa fa-check"></i><b>2.3.6</b> Partitioning Chi-Square</a></li>
<li class="chapter" data-level="2.3.7" data-path="two-way-tables.html"><a href="two-way-tables.html#correlation"><i class="fa fa-check"></i><b>2.3.7</b> Correlation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="k-way-tables.html"><a href="k-way-tables.html"><i class="fa fa-check"></i><b>2.4</b> K-Way Tables</a><ul>
<li class="chapter" data-level="2.4.1" data-path="k-way-tables.html"><a href="k-way-tables.html#odds-ratio-1"><i class="fa fa-check"></i><b>2.4.1</b> Odds Ratio</a></li>
<li class="chapter" data-level="2.4.2" data-path="k-way-tables.html"><a href="k-way-tables.html#chi-square-independence-test-1"><i class="fa fa-check"></i><b>2.4.2</b> Chi-Square Independence Test</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="continuous-analysis.html"><a href="continuous-analysis.html"><i class="fa fa-check"></i><b>2.5</b> Continuous Variable Analysis</a><ul>
<li class="chapter" data-level="2.5.1" data-path="continuous-analysis.html"><a href="continuous-analysis.html#correlation-1"><i class="fa fa-check"></i><b>2.5.1</b> Correlation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="experiment-design.html"><a href="experiment-design.html"><i class="fa fa-check"></i><b>3</b> Experiment Design</a><ul>
<li class="chapter" data-level="3.1" data-path="single-factor.html"><a href="single-factor.html"><i class="fa fa-check"></i><b>3.1</b> Single Factor</a></li>
<li class="chapter" data-level="3.2" data-path="blocking.html"><a href="blocking.html"><i class="fa fa-check"></i><b>3.2</b> Blocking</a></li>
<li class="chapter" data-level="3.3" data-path="nested.html"><a href="nested.html"><i class="fa fa-check"></i><b>3.3</b> Nested</a></li>
<li class="chapter" data-level="3.4" data-path="split-plot.html"><a href="split-plot.html"><i class="fa fa-check"></i><b>3.4</b> Split Plot</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-2-supervised-machine-learning.html"><a href="part-2-supervised-machine-learning.html"><i class="fa fa-check"></i>PART 2: Supervised Machine Learning</a></li>
<li class="chapter" data-level="4" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html"><i class="fa fa-check"></i><b>4</b> Ordinary Least Squares</a><ul>
<li class="chapter" data-level="4.1" data-path="linear-regression-model.html"><a href="linear-regression-model.html"><i class="fa fa-check"></i><b>4.1</b> Linear Regression Model</a></li>
<li class="chapter" data-level="4.2" data-path="parameter-estimation.html"><a href="parameter-estimation.html"><i class="fa fa-check"></i><b>4.2</b> Parameter Estimation</a></li>
<li class="chapter" data-level="4.3" data-path="model-assumptions.html"><a href="model-assumptions.html"><i class="fa fa-check"></i><b>4.3</b> Model Assumptions</a><ul>
<li class="chapter" data-level="4.3.1" data-path="model-assumptions.html"><a href="model-assumptions.html#linearity"><i class="fa fa-check"></i><b>4.3.1</b> Linearity</a></li>
<li class="chapter" data-level="4.3.2" data-path="model-assumptions.html"><a href="model-assumptions.html#multicollinearity"><i class="fa fa-check"></i><b>4.3.2</b> Multicollinearity</a></li>
<li class="chapter" data-level="4.3.3" data-path="model-assumptions.html"><a href="model-assumptions.html#normality"><i class="fa fa-check"></i><b>4.3.3</b> Normality</a></li>
<li class="chapter" data-level="4.3.4" data-path="model-assumptions.html"><a href="model-assumptions.html#equal-variances"><i class="fa fa-check"></i><b>4.3.4</b> Equal Variances</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="prediction.html"><a href="prediction.html"><i class="fa fa-check"></i><b>4.4</b> Prediction</a></li>
<li class="chapter" data-level="4.5" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>4.5</b> Inference</a><ul>
<li class="chapter" data-level="4.5.1" data-path="inference.html"><a href="inference.html#t-test"><i class="fa fa-check"></i><b>4.5.1</b> <em>t</em>-Test</a></li>
<li class="chapter" data-level="4.5.2" data-path="inference.html"><a href="inference.html#f-test"><i class="fa fa-check"></i><b>4.5.2</b> <em>F</em>-Test</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="interpretation.html"><a href="interpretation.html"><i class="fa fa-check"></i><b>4.6</b> Interpretation</a></li>
<li class="chapter" data-level="4.7" data-path="model-validation.html"><a href="model-validation.html"><i class="fa fa-check"></i><b>4.7</b> Model Validation</a><ul>
<li class="chapter" data-level="4.7.1" data-path="model-validation.html"><a href="model-validation.html#accuracy-metrics"><i class="fa fa-check"></i><b>4.7.1</b> Accuracy Metrics</a></li>
<li class="chapter" data-level="4.7.2" data-path="model-validation.html"><a href="model-validation.html#cross-validation"><i class="fa fa-check"></i><b>4.7.2</b> Cross-Validation</a></li>
<li class="chapter" data-level="4.7.3" data-path="model-validation.html"><a href="model-validation.html#gain-curve"><i class="fa fa-check"></i><b>4.7.3</b> Gain Curve</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="ols-reference.html"><a href="ols-reference.html"><i class="fa fa-check"></i><b>4.8</b> OLS Reference</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>5</b> Generalized Linear Models</a><ul>
<li class="chapter" data-level="5.1" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>5.1</b> Logistic Regression</a></li>
<li class="chapter" data-level="5.2" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html"><i class="fa fa-check"></i><b>5.2</b> Multinomial Logistic Regression</a></li>
<li class="chapter" data-level="5.3" data-path="ordinal-logistic-regression.html"><a href="ordinal-logistic-regression.html"><i class="fa fa-check"></i><b>5.3</b> Ordinal Logistic Regression</a><ul>
<li class="chapter" data-level="5.3.1" data-path="ordinal-logistic-regression.html"><a href="ordinal-logistic-regression.html#assumptions"><i class="fa fa-check"></i><b>5.3.1</b> Assumptions</a></li>
<li class="chapter" data-level="5.3.2" data-path="ordinal-logistic-regression.html"><a href="ordinal-logistic-regression.html#modeling"><i class="fa fa-check"></i><b>5.3.2</b> Modeling</a></li>
<li class="chapter" data-level="5.3.3" data-path="ordinal-logistic-regression.html"><a href="ordinal-logistic-regression.html#case-study"><i class="fa fa-check"></i><b>5.3.3</b> Case Study</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="poisson-regression.html"><a href="poisson-regression.html"><i class="fa fa-check"></i><b>5.4</b> Poisson Regression</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multivariate-statistical-analysis.html"><a href="multivariate-statistical-analysis.html"><i class="fa fa-check"></i><b>6</b> Multivariate Statistical Analysis</a><ul>
<li class="chapter" data-level="6.1" data-path="background.html"><a href="background.html"><i class="fa fa-check"></i><b>6.1</b> Background</a></li>
<li class="chapter" data-level="6.2" data-path="manova.html"><a href="manova.html"><i class="fa fa-check"></i><b>6.2</b> MANOVA</a></li>
<li class="chapter" data-level="6.3" data-path="repeated-measures.html"><a href="repeated-measures.html"><i class="fa fa-check"></i><b>6.3</b> Repeated Measures</a></li>
<li class="chapter" data-level="6.4" data-path="lda.html"><a href="lda.html"><i class="fa fa-check"></i><b>6.4</b> LDA</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>7</b> Regularization</a><ul>
<li class="chapter" data-level="7.1" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>7.1</b> Ridge</a></li>
<li class="chapter" data-level="7.2" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>7.2</b> Lasso</a></li>
<li class="chapter" data-level="7.3" data-path="elastic-net.html"><a href="elastic-net.html"><i class="fa fa-check"></i><b>7.3</b> Elastic Net</a></li>
<li class="chapter" data-level="" data-path="model-summary.html"><a href="model-summary.html"><i class="fa fa-check"></i>Model Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>8</b> Decision Trees</a><ul>
<li class="chapter" data-level="8.1" data-path="classification-tree.html"><a href="classification-tree.html"><i class="fa fa-check"></i><b>8.1</b> Classification Tree</a><ul>
<li class="chapter" data-level="8.1.1" data-path="classification-tree.html"><a href="classification-tree.html#measuring-performance"><i class="fa fa-check"></i><b>8.1.1</b> Measuring Performance</a></li>
<li class="chapter" data-level="8.1.2" data-path="classification-tree.html"><a href="classification-tree.html#training-with-caret"><i class="fa fa-check"></i><b>8.1.2</b> Training with Caret</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="regression-tree.html"><a href="regression-tree.html"><i class="fa fa-check"></i><b>8.2</b> Regression Tree</a><ul>
<li class="chapter" data-level="8.2.1" data-path="regression-tree.html"><a href="regression-tree.html#training-with-caret-1"><i class="fa fa-check"></i><b>8.2.1</b> Training with Caret</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="bagged-trees.html"><a href="bagged-trees.html"><i class="fa fa-check"></i><b>8.3</b> Bagged Trees</a><ul>
<li class="chapter" data-level="8.3.1" data-path="bagged-trees.html"><a href="bagged-trees.html#bagged-classification-tree"><i class="fa fa-check"></i><b>8.3.1</b> Bagged Classification Tree</a></li>
<li class="chapter" data-level="8.3.2" data-path="bagged-trees.html"><a href="bagged-trees.html#bagging-regression-tree"><i class="fa fa-check"></i><b>8.3.2</b> Bagging Regression Tree</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="random-forests.html"><a href="random-forests.html"><i class="fa fa-check"></i><b>8.4</b> Random Forests</a></li>
<li class="chapter" data-level="8.5" data-path="gradient-boosting.html"><a href="gradient-boosting.html"><i class="fa fa-check"></i><b>8.5</b> Gradient Boosting</a></li>
<li class="chapter" data-level="8.6" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>8.6</b> Summary</a><ul>
<li class="chapter" data-level="8.6.1" data-path="summary.html"><a href="summary.html#classification-trees"><i class="fa fa-check"></i><b>8.6.1</b> Classification Trees</a></li>
<li class="chapter" data-level="8.6.2" data-path="summary.html"><a href="summary.html#regression-trees"><i class="fa fa-check"></i><b>8.6.2</b> Regression Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="non-linear-models.html"><a href="non-linear-models.html"><i class="fa fa-check"></i><b>9</b> Non-linear Models</a><ul>
<li class="chapter" data-level="9.1" data-path="splines.html"><a href="splines.html"><i class="fa fa-check"></i><b>9.1</b> Splines</a></li>
<li class="chapter" data-level="9.2" data-path="mars.html"><a href="mars.html"><i class="fa fa-check"></i><b>9.2</b> MARS</a></li>
<li class="chapter" data-level="9.3" data-path="gam.html"><a href="gam.html"><i class="fa fa-check"></i><b>9.3</b> GAM</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>10</b> Support Vector Machines</a><ul>
<li class="chapter" data-level="10.1" data-path="maximal-margin-classifier.html"><a href="maximal-margin-classifier.html"><i class="fa fa-check"></i><b>10.1</b> Maximal Margin Classifier</a></li>
<li class="chapter" data-level="10.2" data-path="support-vector-classifier.html"><a href="support-vector-classifier.html"><i class="fa fa-check"></i><b>10.2</b> Support Vector Classifier</a></li>
<li class="chapter" data-level="10.3" data-path="support-vector-machines-1.html"><a href="support-vector-machines-1.html"><i class="fa fa-check"></i><b>10.3</b> Support Vector Machines</a></li>
<li class="chapter" data-level="10.4" data-path="my-svm-example.html"><a href="my-svm-example.html"><i class="fa fa-check"></i><b>10.4</b> My svm Example</a></li>
<li class="chapter" data-level="10.5" data-path="using-caret.html"><a href="using-caret.html"><i class="fa fa-check"></i><b>10.5</b> Using Caret</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-3-unupervised-machine-learning.html"><a href="part-3-unupervised-machine-learning.html"><i class="fa fa-check"></i>PART 3: Unupervised Machine Learning</a><ul>
<li class="chapter" data-level="10.6" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>10.6</b> PCA</a></li>
<li class="chapter" data-level="10.7" data-path="t-sne.html"><a href="t-sne.html"><i class="fa fa-check"></i><b>10.7</b> t-SNE</a></li>
<li class="chapter" data-level="10.8" data-path="svd.html"><a href="svd.html"><i class="fa fa-check"></i><b>10.8</b> SVD</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="cluster-analysis.html"><a href="cluster-analysis.html"><i class="fa fa-check"></i><b>11</b> Cluster Analysis</a><ul>
<li class="chapter" data-level="11.1" data-path="k-means.html"><a href="k-means.html"><i class="fa fa-check"></i><b>11.1</b> K-Means</a></li>
<li class="chapter" data-level="11.2" data-path="hca.html"><a href="hca.html"><i class="fa fa-check"></i><b>11.2</b> HCA</a></li>
<li class="chapter" data-level="11.3" data-path="k-means-vs-hca.html"><a href="k-means-vs-hca.html"><i class="fa fa-check"></i><b>11.3</b> K-Means vs HCA</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="text-mining.html"><a href="text-mining.html"><i class="fa fa-check"></i><b>12</b> Text Mining</a><ul>
<li class="chapter" data-level="12.1" data-path="tidy-text.html"><a href="tidy-text.html"><i class="fa fa-check"></i><b>12.1</b> Tidy Text</a></li>
<li class="chapter" data-level="12.2" data-path="bag-of-words.html"><a href="bag-of-words.html"><i class="fa fa-check"></i><b>12.2</b> Bag of Words</a></li>
<li class="chapter" data-level="12.3" data-path="sentiment-analysis.html"><a href="sentiment-analysis.html"><i class="fa fa-check"></i><b>12.3</b> Sentiment Analysis</a><ul>
<li class="chapter" data-level="12.3.1" data-path="sentiment-analysis.html"><a href="sentiment-analysis.html#n-grams"><i class="fa fa-check"></i><b>12.3.1</b> N-Grams</a></li>
<li class="chapter" data-level="12.3.2" data-path="sentiment-analysis.html"><a href="sentiment-analysis.html#converting-to-and-from-non-tidy-formats"><i class="fa fa-check"></i><b>12.3.2</b> Converting to and from non-tidy formats</a></li>
<li class="chapter" data-level="12.3.3" data-path="disc-dist.html"><a href="disc-dist.html#example"><i class="fa fa-check"></i><b>12.3.3</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="topic-modeling.html"><a href="topic-modeling.html"><i class="fa fa-check"></i><b>12.4</b> Topic Modeling</a></li>
<li class="chapter" data-level="12.5" data-path="appendix-string-manipulation.html"><a href="appendix-string-manipulation.html"><i class="fa fa-check"></i><b>12.5</b> Appendix: String Manipulation</a><ul>
<li class="chapter" data-level="12.5.1" data-path="appendix-string-manipulation.html"><a href="appendix-string-manipulation.html#stringr-package"><i class="fa fa-check"></i><b>12.5.1</b> stringr package</a></li>
<li class="chapter" data-level="12.5.2" data-path="appendix-string-manipulation.html"><a href="appendix-string-manipulation.html#regular-expressions"><i class="fa fa-check"></i><b>12.5.2</b> Regular Expressions</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="reference-links.html"><a href="reference-links.html"><i class="fa fa-check"></i><b>12.6</b> Reference Links</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="survival-analysis.html"><a href="survival-analysis.html"><i class="fa fa-check"></i><b>13</b> Survival Analysis</a><ul>
<li class="chapter" data-level="13.1" data-path="basic-concepts.html"><a href="basic-concepts.html"><i class="fa fa-check"></i><b>13.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="13.2" data-path="survival-curve-estimation.html"><a href="survival-curve-estimation.html"><i class="fa fa-check"></i><b>13.2</b> Survival Curve Estimation</a><ul>
<li class="chapter" data-level="13.2.1" data-path="survival-curve-estimation.html"><a href="survival-curve-estimation.html#kaplan-meier"><i class="fa fa-check"></i><b>13.2.1</b> Kaplan-Meier</a></li>
<li class="chapter" data-level="13.2.2" data-path="survival-curve-estimation.html"><a href="survival-curve-estimation.html#weibull"><i class="fa fa-check"></i><b>13.2.2</b> Weibull</a></li>
<li class="chapter" data-level="13.2.3" data-path="survival-curve-estimation.html"><a href="survival-curve-estimation.html#cox"><i class="fa fa-check"></i><b>13.2.3</b> Cox</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="publishing-to-bookdown.html"><a href="publishing-to-bookdown.html"><i class="fa fa-check"></i>Publishing to BookDown</a></li>
<li class="chapter" data-level="" data-path="shiny-apps.html"><a href="shiny-apps.html"><i class="fa fa-check"></i>Shiny Apps</a></li>
<li class="chapter" data-level="" data-path="packages.html"><a href="packages.html"><i class="fa fa-check"></i>Packages</a><ul>
<li class="chapter" data-level="" data-path="packages.html"><a href="packages.html#create-a-package"><i class="fa fa-check"></i>Create a package</a></li>
<li class="chapter" data-level="13.2.4" data-path="packages.html"><a href="packages.html#document-functions-with-roxygen"><i class="fa fa-check"></i><b>13.2.4</b> Document Functions with roxygen</a></li>
<li class="chapter" data-level="" data-path="packages.html"><a href="packages.html#create-data"><i class="fa fa-check"></i>Create Data</a></li>
<li class="chapter" data-level="" data-path="packages.html"><a href="packages.html#create-vignette"><i class="fa fa-check"></i>Create Vignette</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">My Data Science Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="model-validation" class="section level2">
<h2><span class="header-section-number">4.7</span> Model Validation</h2>
<p>Evaluate predictive accuracy by training the model on a training data set and testing on a test data set.</p>
<div id="accuracy-metrics" class="section level3">
<h3><span class="header-section-number">4.7.1</span> Accuracy Metrics</h3>
<p>The most common measures of model fit are R-squared, RMSE, RSE, MAE, Adjusted R-squared, AIC, AICc, BIC, and Mallow’s Cp.</p>
<div id="r-squared" class="section level4">
<h4><span class="header-section-number">4.7.1.1</span> R-Squared</h4>
<p>The coefficient of determination (<strong>R-squared</strong>) is the percent of total variation in the response variable that is explained by the regression line.</p>
<p><span class="math display">\[R^2 = 1 - \frac{SSE}{SST}\]</span></p>
<p>where <span class="math inline">\(SSE = \sum_{i=1}^n{(y_i - \hat{y}_i)^2}\)</span> is the sum squared differences between the predicted and observed value, <span class="math inline">\(SST = \sum_{i = 1}^n{(y_i - \bar{y})^2}\)</span> is the sum of squared differences between the observed and overall mean value, and <span class="math inline">\(RSS = \sum_{i=1}^n{(\hat{y}_i - \bar{y})^2}\)</span> is the sum of squared differences between the predicted and overall mean “no-relationship line” value. At the extremes, <span class="math inline">\(R^2 = 1\)</span> means all data points fall perfectly on the regression line - the predictors account for <em>all</em> variation in the response; <span class="math inline">\(R^2 = 0\)</span> means the regression line is horizontal at <span class="math inline">\(\bar{y}\)</span> - the predictors account for <em>none</em> of the variation in the response. In the simple case of a single predictor variable, <span class="math inline">\(R^2\)</span> equals the squared correlation between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, <span class="math inline">\(r = Cor(x,y)\)</span>.</p>
<div class="sourceCode" id="cb443"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb443-1"><a href="model-validation.html#cb443-1"></a>ssr &lt;-<span class="st"> </span><span class="kw">sum</span>((m<span class="op">$</span>fitted.values <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(d<span class="op">$</span>mpg))<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb443-2"><a href="model-validation.html#cb443-2"></a>sse &lt;-<span class="st"> </span><span class="kw">sum</span>(m<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb443-3"><a href="model-validation.html#cb443-3"></a>sst &lt;-<span class="st"> </span><span class="kw">sum</span>((d<span class="op">$</span>mpg <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(d<span class="op">$</span>mpg))<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb443-4"><a href="model-validation.html#cb443-4"></a>(r2 &lt;-<span class="st"> </span>ssr <span class="op">/</span><span class="st"> </span>sst)</span></code></pre></div>
<pre><code>## [1] 0.88</code></pre>
<div class="sourceCode" id="cb445"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb445-1"><a href="model-validation.html#cb445-1"></a>(r2 &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>sse <span class="op">/</span><span class="st"> </span>sst)</span></code></pre></div>
<pre><code>## [1] 0.88</code></pre>
<div class="sourceCode" id="cb447"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb447-1"><a href="model-validation.html#cb447-1"></a><span class="kw">summary</span>(m)<span class="op">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.88</code></pre>
<p><span class="math inline">\(R^2\)</span> is also equal to the correlation between the fitted value and observed values, <span class="math inline">\(R^2 = Cor(Y, \hat{Y})^2\)</span>.</p>
<div class="sourceCode" id="cb449"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb449-1"><a href="model-validation.html#cb449-1"></a><span class="kw">cor</span>(m<span class="op">$</span>fitted.values, d<span class="op">$</span>mpg)<span class="op">^</span><span class="dv">2</span></span></code></pre></div>
<pre><code>## [1] 0.88</code></pre>
<p>R-squared is proportional to the the variance in the response, <em>SST</em>. Given a constant percentage error in predictions, a test set with relatively low variation in the reponse will have a lower R-squared. Conversely, test sets with large variation, e.g., housing data with home sale ranging from $60K to $2M may have a large R-squared despite average prediction errors of &gt;$10K.</p>
<p>A close variant of R-squared is the non-parametric Spearman’s rank correlation. This statistic is the correlation of the <em>ranks</em> of the response and the predicted values. It is used when the model goal is ranking.</p>
</div>
<div id="rmse" class="section level4">
<h4><span class="header-section-number">4.7.1.2</span> RMSE</h4>
<p>The root mean squared error (<strong>RMSE</strong>) is the average prediction error (square root of mean squared error).</p>
<p><span class="math display">\[RMSE = \sqrt{\frac{\sum_{i=1}^n{(y_i - \hat{y}_i)^2}}{n}}\]</span></p>
<div class="sourceCode" id="cb451"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb451-1"><a href="model-validation.html#cb451-1"></a><span class="kw">sqrt</span>(<span class="kw">mean</span>((d<span class="op">$</span>mpg <span class="op">-</span><span class="st"> </span>m<span class="op">$</span>fitted.values)<span class="op">^</span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 2.1</code></pre>
<p>The <code>rmse()</code> function from the <code>Metrics</code> package, and the <code>postResample()</code> function in <code>caret</code> calculate RMSE.</p>
<div class="sourceCode" id="cb453"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb453-1"><a href="model-validation.html#cb453-1"></a><span class="kw">rmse</span>(<span class="dt">actual =</span> d<span class="op">$</span>mpg, <span class="dt">predicted =</span> m<span class="op">$</span>fitted.values)</span></code></pre></div>
<pre><code>## [1] 2.1</code></pre>
<div class="sourceCode" id="cb455"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb455-1"><a href="model-validation.html#cb455-1"></a><span class="kw">postResample</span>(<span class="dt">pred =</span> m<span class="op">$</span>fitted.values, <span class="dt">obs =</span> d<span class="op">$</span>mpg)[<span class="dv">1</span>]</span></code></pre></div>
<pre><code>## RMSE 
##  2.1</code></pre>
<p>The mean squared error of a model with theoretical residual of mean zero and constant variance <span class="math inline">\(\sigma^2\)</span> can be decomposed into the model’s bias and the model’s variance:</p>
<p><span class="math display">\[E[MSE] = \sigma^2 + Bias^2 + Var.\]</span></p>
<p>A model that predicts the response closely will have low bias, but be relatively sensitive to the training data and thus have high variance. A model that predicts the response conservatively (e.g., a simple mean) will have large bias, but be relatively insensitive to nuances in the training data. Here is an example of a simulated sine wave. A model predicting the mean value at the upper and lower levels has low variance, but high bias, and a model of an actual sine wave has low bias and high variance. This is referred to as the variance-bias trade-off.</p>
<p><img src="data-sci_files/figure-html/unnamed-chunk-197-1.png" width="672" /></p>
</div>
<div id="rse" class="section level4">
<h4><span class="header-section-number">4.7.1.3</span> RSE</h4>
<p>The residual standard error (<strong>RSE</strong>, or model sigma <span class="math inline">\(\hat{\sigma}\)</span>) is an estimate of the standard deviation of <span class="math inline">\(\epsilon\)</span>. It is roughly the average amount the response deviates from the true regression line.</p>
<p><span class="math display">\[\sigma = \sqrt{\frac{\sum_{i=1}^n{(y_i - \hat{y}_i)^2}}{n-k-1}}\]</span></p>
<div class="sourceCode" id="cb457"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb457-1"><a href="model-validation.html#cb457-1"></a><span class="kw">sqrt</span>(<span class="kw">sum</span>((d<span class="op">$</span>mpg <span class="op">-</span><span class="st"> </span>m<span class="op">$</span>fitted.values)<span class="op">^</span><span class="dv">2</span>) <span class="op">/</span><span class="st"> </span>(n <span class="op">-</span><span class="st"> </span>k <span class="op">-</span><span class="st"> </span><span class="dv">1</span>))</span></code></pre></div>
<pre><code>## [1] 2.5</code></pre>
<div class="sourceCode" id="cb459"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb459-1"><a href="model-validation.html#cb459-1"></a><span class="co"># sd is sqrt(sse / (n-1)), sigma = sqrt(sse / (n - k - 1))</span></span>
<span id="cb459-2"><a href="model-validation.html#cb459-2"></a><span class="kw">sd</span>(m<span class="op">$</span>residuals) <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>((n <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span>(n <span class="op">-</span><span class="st"> </span>k <span class="op">-</span><span class="st"> </span><span class="dv">1</span>))  </span></code></pre></div>
<pre><code>## [1] 2.5</code></pre>
<div class="sourceCode" id="cb461"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb461-1"><a href="model-validation.html#cb461-1"></a><span class="kw">summary</span>(m)<span class="op">$</span>sigma </span></code></pre></div>
<pre><code>## [1] 2.5</code></pre>
<div class="sourceCode" id="cb463"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb463-1"><a href="model-validation.html#cb463-1"></a><span class="kw">sigma</span>(m)</span></code></pre></div>
<pre><code>## [1] 2.5</code></pre>
</div>
<div id="mae" class="section level4">
<h4><span class="header-section-number">4.7.1.4</span> MAE</h4>
<p>The mean absolute error (<strong>MAE</strong>) is the average absolute prediction arror. It is less sensitive to outliers.</p>
<p><span class="math display">\[MAE = \frac{\sum_{i=1}^n{|y_i - \hat{y}_i|}}{n}\]</span></p>
<div class="sourceCode" id="cb465"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb465-1"><a href="model-validation.html#cb465-1"></a><span class="kw">sum</span>(<span class="kw">abs</span>(d<span class="op">$</span>mpg <span class="op">-</span><span class="st"> </span>m<span class="op">$</span>fitted.values)) <span class="op">/</span><span class="st"> </span>n</span></code></pre></div>
<pre><code>## [1] 1.7</code></pre>
<p>The <code>postResample()</code> function in <code>caret</code> conveniently calculates all three.</p>
<div class="sourceCode" id="cb467"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb467-1"><a href="model-validation.html#cb467-1"></a><span class="kw">postResample</span>(<span class="dt">pred =</span> m<span class="op">$</span>fitted.values, <span class="dt">obs =</span> d<span class="op">$</span>mpg)</span></code></pre></div>
<pre><code>##     RMSE Rsquared      MAE 
##     2.08     0.88     1.70</code></pre>
<div class="sourceCode" id="cb469"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb469-1"><a href="model-validation.html#cb469-1"></a><span class="kw">defaultSummary</span>(<span class="dt">data =</span> <span class="kw">data.frame</span>(<span class="dt">obs =</span> d<span class="op">$</span>mpg, <span class="dt">pred =</span> m<span class="op">$</span>fitted.values), <span class="dt">model =</span> m)</span></code></pre></div>
<pre><code>##     RMSE Rsquared      MAE 
##     2.08     0.88     1.70</code></pre>
<div class="sourceCode" id="cb471"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb471-1"><a href="model-validation.html#cb471-1"></a><span class="kw">apply</span>(<span class="kw">as.matrix</span>(m<span class="op">$</span>fitted.values), <span class="dv">2</span>, postResample, <span class="dt">obs =</span> d<span class="op">$</span>mpg)</span></code></pre></div>
<pre><code>##          [,1]
## RMSE     2.08
## Rsquared 0.88
## MAE      1.70</code></pre>
<p>These metrics are good for evaluating a model, but less useful for comparing models. The problem is that they tend to improve with additional variables added to the model, even if the improvement is not significant. The following metrics aid model comparison by penalizing added variables.</p>
</div>
<div id="adjusted-r-squared" class="section level4">
<h4><span class="header-section-number">4.7.1.5</span> Adjusted R-squared</h4>
<p>The adjusted R-squared (<span class="math inline">\(\bar{R}^2\)</span>) penalizes the R-squared metric for increasing number of predictors.</p>
<p><span class="math display">\[\bar{R}^2 = 1 - \frac{SSE}{SST} \cdot \frac{n-1}{n-k-1}\]</span></p>
<div class="sourceCode" id="cb473"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb473-1"><a href="model-validation.html#cb473-1"></a>(adj_r2 &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>sse<span class="op">/</span>sst <span class="op">*</span><span class="st"> </span>(n <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span>(n <span class="op">-</span><span class="st"> </span>k <span class="op">-</span><span class="st"> </span><span class="dv">1</span>))</span></code></pre></div>
<pre><code>## [1] 0.83</code></pre>
<div class="sourceCode" id="cb475"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb475-1"><a href="model-validation.html#cb475-1"></a><span class="kw">summary</span>(m)<span class="op">$</span>adj.r.squared</span></code></pre></div>
<pre><code>## [1] 0.83</code></pre>
</div>
<div id="aic" class="section level4">
<h4><span class="header-section-number">4.7.1.6</span> AIC</h4>
<p>Akaike’s Information Criteria (<strong>AIC</strong>) is a penalization metric. The lower the AIC, the better the model.</p>
<div class="sourceCode" id="cb477"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb477-1"><a href="model-validation.html#cb477-1"></a><span class="kw">AIC</span>(m)</span></code></pre></div>
<pre><code>## [1] 160</code></pre>
</div>
<div id="aicc" class="section level4">
<h4><span class="header-section-number">4.7.1.7</span> AICc</h4>
<p><strong>AICc</strong> corrects AIC for small sample sizes.</p>
<div class="sourceCode" id="cb479"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb479-1"><a href="model-validation.html#cb479-1"></a><span class="kw">AIC</span>(m) <span class="op">+</span><span class="st"> </span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>k <span class="op">*</span><span class="st"> </span>(k <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)) <span class="op">/</span><span class="st"> </span>(n <span class="op">-</span><span class="st"> </span>k <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 168</code></pre>
</div>
<div id="bic" class="section level4">
<h4><span class="header-section-number">4.7.1.8</span> BIC</h4>
<p>The Basiean information criteria (<strong>BIC</strong>) is like AIC, but with a stronger penalty for additional variables.</p>
<div class="sourceCode" id="cb481"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb481-1"><a href="model-validation.html#cb481-1"></a><span class="kw">BIC</span>(m)</span></code></pre></div>
<pre><code>## [1] 176</code></pre>
</div>
<div id="mallows-cp" class="section level4">
<h4><span class="header-section-number">4.7.1.9</span> Mallows Cp</h4>
<p>Mallows Cp is a variant of AIC.</p>
<div id="example-7" class="section level5">
<h5><span class="header-section-number">4.7.1.9.1</span> Example</h5>
<p>Compare the full model to a model without <code>cyl</code>.</p>
<p>The <code>glance()</code> function from the <code>broom</code> package calculates many validation metrics. Here are the validation stats for the full model and then the reduced model.</p>
<div class="sourceCode" id="cb483"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb483-1"><a href="model-validation.html#cb483-1"></a><span class="kw">library</span>(broom)</span>
<span id="cb483-2"><a href="model-validation.html#cb483-2"></a></span>
<span id="cb483-3"><a href="model-validation.html#cb483-3"></a><span class="kw">glance</span>(m) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(adj.r.squared, sigma, AIC, BIC, p.value)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 5
##   adj.r.squared sigma   AIC   BIC      p.value
##           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;
## 1         0.826  2.51  160.  176. 0.0000000481</code></pre>
<div class="sourceCode" id="cb485"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb485-1"><a href="model-validation.html#cb485-1"></a><span class="kw">glance</span>(<span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span>. <span class="op">-</span><span class="st"> </span>cyl, d[, <span class="dv">1</span><span class="op">:</span><span class="dv">9</span>])) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(adj.r.squared, sigma, AIC, BIC, p.value)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 5
##   adj.r.squared sigma   AIC   BIC       p.value
##           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;
## 1         0.829  2.50  158.  171. 0.00000000453</code></pre>
<p>The ajusted R2 increased and AIC and BIC decreased, meaning the full model is less efficient at explaining the variability in the response value. The residual standard error <code>sigma</code> is smaller for the reduced model. Finally, the <em>F</em> statistic p-value is smaller for the reduced model, meaning the reduced model is statistically more significant.</p>
<p>Note that these regression metrics are all internal measures, that is they have been computed on the training dataset, not the test dataset.</p>
</div>
</div>
</div>
<div id="cross-validation" class="section level3">
<h3><span class="header-section-number">4.7.2</span> Cross-Validation</h3>
<p>Cross-validation is a set of methods for measuring the performance of a predictive model on a test dataset. The main measures of prediction performance are R2, RMSE and MAE.</p>
<div id="validation-set" class="section level4">
<h4><span class="header-section-number">4.7.2.1</span> Validation Set</h4>
<p>To perform validation set cross validation, randomly split the data into a training data set and a test data set. Fit models to the training data set, then predict values with the validation set. The model that produces the best prediction performance is the preferred model.</p>
<p>The <code>caret</code> package provides useful methods for cross-validation.</p>
<div id="example-8" class="section level5">
<h5><span class="header-section-number">4.7.2.1.1</span> Example</h5>
<div class="sourceCode" id="cb487"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb487-1"><a href="model-validation.html#cb487-1"></a><span class="kw">library</span>(caret)</span>
<span id="cb487-2"><a href="model-validation.html#cb487-2"></a></span>
<span id="cb487-3"><a href="model-validation.html#cb487-3"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb487-4"><a href="model-validation.html#cb487-4"></a>train_idx &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(<span class="dt">y =</span> d<span class="op">$</span>mpg, <span class="dt">p =</span> <span class="fl">0.80</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)</span>
<span id="cb487-5"><a href="model-validation.html#cb487-5"></a>d.train &lt;-<span class="st"> </span>d[train_idx, ]</span>
<span id="cb487-6"><a href="model-validation.html#cb487-6"></a>d.test &lt;-<span class="st"> </span>d[<span class="op">-</span>train_idx, ]</span></code></pre></div>
<p>Build the model using <code>d.train</code>, make predictions, then calculate the R2, RMSE, and MAE. Use the <code>train()</code> function from the <code>caret</code> package. Use <code>method = "none"</code> to simply fit the model to the entire data set.</p>
<div class="sourceCode" id="cb488"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb488-1"><a href="model-validation.html#cb488-1"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb488-2"><a href="model-validation.html#cb488-2"></a>m1 &lt;-<span class="st"> </span><span class="kw">train</span>(mpg <span class="op">~</span><span class="st"> </span>., </span>
<span id="cb488-3"><a href="model-validation.html#cb488-3"></a>            <span class="dt">data =</span> d.train[, <span class="dv">1</span><span class="op">:</span><span class="dv">9</span>],</span>
<span id="cb488-4"><a href="model-validation.html#cb488-4"></a>            <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb488-5"><a href="model-validation.html#cb488-5"></a>            <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;none&quot;</span>))</span>
<span id="cb488-6"><a href="model-validation.html#cb488-6"></a><span class="kw">print</span>(m1)</span></code></pre></div>
<pre><code>## Linear Regression 
## 
## 28 samples
##  8 predictor
## 
## No pre-processing
## Resampling: None</code></pre>
<div class="sourceCode" id="cb490"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb490-1"><a href="model-validation.html#cb490-1"></a><span class="kw">postResample</span>(<span class="dt">pred =</span> <span class="kw">predict</span>(m1, <span class="dt">newdata =</span> d.test), </span>
<span id="cb490-2"><a href="model-validation.html#cb490-2"></a>             <span class="dt">obs =</span> d.test<span class="op">$</span>mpg)</span></code></pre></div>
<pre><code>##     RMSE Rsquared      MAE 
##     3.10     0.96     2.45</code></pre>
<p>The validation set method is only useful when you have a large data set to partition. A second disadvantage is that building a model on a fraction of the data leaves out information. The test error will vary with which observations are included in the training set.</p>
</div>
</div>
<div id="loocv" class="section level4">
<h4><span class="header-section-number">4.7.2.2</span> LOOCV</h4>
<p>Leave one out cross validation (LOOCV) works by successively modeling with training sets leaving out one data point, then averaging the prediction errors.</p>
<div class="sourceCode" id="cb492"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb492-1"><a href="model-validation.html#cb492-1"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb492-2"><a href="model-validation.html#cb492-2"></a>m2 &lt;-<span class="st"> </span><span class="kw">train</span>(mpg <span class="op">~</span><span class="st"> </span>., </span>
<span id="cb492-3"><a href="model-validation.html#cb492-3"></a>            <span class="dt">data =</span> d.train[, <span class="dv">1</span><span class="op">:</span><span class="dv">9</span>],</span>
<span id="cb492-4"><a href="model-validation.html#cb492-4"></a>            <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb492-5"><a href="model-validation.html#cb492-5"></a>            <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;LOOCV&quot;</span>))</span>
<span id="cb492-6"><a href="model-validation.html#cb492-6"></a><span class="kw">print</span>(m2)</span></code></pre></div>
<pre><code>## Linear Regression 
## 
## 28 samples
##  8 predictor
## 
## No pre-processing
## Resampling: Leave-One-Out Cross-Validation 
## Summary of sample sizes: 27, 27, 27, 27, 27, 27, ... 
## Resampling results:
## 
##   RMSE  Rsquared  MAE
##   2.8   0.76      2.3
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<div class="sourceCode" id="cb494"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb494-1"><a href="model-validation.html#cb494-1"></a><span class="kw">postResample</span>(<span class="dt">pred =</span> <span class="kw">predict</span>(m2, <span class="dt">newdata =</span> d.test), </span>
<span id="cb494-2"><a href="model-validation.html#cb494-2"></a>             <span class="dt">obs =</span> d.test<span class="op">$</span>mpg)</span></code></pre></div>
<pre><code>##     RMSE Rsquared      MAE 
##     3.10     0.96     2.45</code></pre>
<p>This method isn’t perfect either. It repeats as many times as there are data points, so the execution time may be long. LOOCV is also sensitive to outliers.</p>
</div>
<div id="k-fold-cross-validation" class="section level4">
<h4><span class="header-section-number">4.7.2.3</span> K-fold Cross-Validation</h4>
<p>K-fold cross-validation splits the dataset into <em>k</em> folds (subsets), then uses <em>k-1</em> of the folds for a training set and the remaining fold for a test set, then repeats for all permutations of k taken k-1 at a time. E.g., 3-fold cross-validation will partition the data into sets A, B, and C, then create train/test splits of [AB, C], [AC, B], and [BC, A].</p>
<p>K-fold cross-validation is less computationally expensive than LOOCV, and often yields more accurate test error rate estimates. What is the right value of k? The lower is <em>k</em> the more biased the estimates; the higher is <em>k</em> the larger the estimate variability. At the extremes <em>k</em> = 2 is the validation set method, and <em>k = n</em> is the LOOCV method. In practice, one typically performs k-fold cross-validation using k = 5 or k = 10 because these values have been empirically shown to balence bias and variance.</p>
<div class="sourceCode" id="cb496"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb496-1"><a href="model-validation.html#cb496-1"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb496-2"><a href="model-validation.html#cb496-2"></a>m3 &lt;-<span class="st"> </span><span class="kw">train</span>(mpg <span class="op">~</span><span class="st"> </span>., </span>
<span id="cb496-3"><a href="model-validation.html#cb496-3"></a>            <span class="dt">data =</span> d.train[, <span class="dv">1</span><span class="op">:</span><span class="dv">9</span>],</span>
<span id="cb496-4"><a href="model-validation.html#cb496-4"></a>            <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb496-5"><a href="model-validation.html#cb496-5"></a>            <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>,</span>
<span id="cb496-6"><a href="model-validation.html#cb496-6"></a>                                     <span class="dt">number =</span> <span class="dv">5</span>))</span>
<span id="cb496-7"><a href="model-validation.html#cb496-7"></a><span class="kw">print</span>(m3)</span></code></pre></div>
<pre><code>## Linear Regression 
## 
## 28 samples
##  8 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 22, 22, 23, 22, 23 
## Resampling results:
## 
##   RMSE  Rsquared  MAE
##   3     0.85      2.6
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<div class="sourceCode" id="cb498"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb498-1"><a href="model-validation.html#cb498-1"></a><span class="kw">postResample</span>(<span class="dt">pred =</span> <span class="kw">predict</span>(m3, <span class="dt">newdata =</span> d.test), </span>
<span id="cb498-2"><a href="model-validation.html#cb498-2"></a>             <span class="dt">obs =</span> d.test<span class="op">$</span>mpg)</span></code></pre></div>
<pre><code>##     RMSE Rsquared      MAE 
##     3.10     0.96     2.45</code></pre>
</div>
<div id="repeated-k-fold-cv" class="section level4">
<h4><span class="header-section-number">4.7.2.4</span> Repeated K-fold CV</h4>
<p>You can also perform k-fold cross-validation multiple times and average the results. Specify <code>method = "repeatedcv"</code> and <code>repeats = 3</code> in the <code>trainControl</code> object for three repeats.</p>
<div class="sourceCode" id="cb500"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb500-1"><a href="model-validation.html#cb500-1"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb500-2"><a href="model-validation.html#cb500-2"></a>m4 &lt;-<span class="st"> </span><span class="kw">train</span>(mpg <span class="op">~</span><span class="st"> </span>., </span>
<span id="cb500-3"><a href="model-validation.html#cb500-3"></a>            <span class="dt">data =</span> d.train[, <span class="dv">1</span><span class="op">:</span><span class="dv">9</span>],</span>
<span id="cb500-4"><a href="model-validation.html#cb500-4"></a>            <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb500-5"><a href="model-validation.html#cb500-5"></a>            <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;repeatedcv&quot;</span>,</span>
<span id="cb500-6"><a href="model-validation.html#cb500-6"></a>                                     <span class="dt">number =</span> <span class="dv">5</span>,</span>
<span id="cb500-7"><a href="model-validation.html#cb500-7"></a>                                     <span class="dt">repeats =</span> <span class="dv">3</span>))</span>
<span id="cb500-8"><a href="model-validation.html#cb500-8"></a><span class="kw">print</span>(m4)</span></code></pre></div>
<pre><code>## Linear Regression 
## 
## 28 samples
##  8 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold, repeated 3 times) 
## Summary of sample sizes: 22, 22, 23, 22, 23, 23, ... 
## Resampling results:
## 
##   RMSE  Rsquared  MAE
##   3.1   0.81      2.7
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<div class="sourceCode" id="cb502"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb502-1"><a href="model-validation.html#cb502-1"></a><span class="kw">postResample</span>(<span class="dt">pred =</span> <span class="kw">predict</span>(m4, <span class="dt">newdata =</span> d.test), </span>
<span id="cb502-2"><a href="model-validation.html#cb502-2"></a>             <span class="dt">obs =</span> d.test<span class="op">$</span>mpg)</span></code></pre></div>
<pre><code>##     RMSE Rsquared      MAE 
##     3.10     0.96     2.45</code></pre>
</div>
<div id="bootstrapping" class="section level4">
<h4><span class="header-section-number">4.7.2.5</span> Bootstrapping</h4>
<p>Bootstrapping randomly selects a sample of n observations with replacement from the original dataset to evaluate the model. The procedure is repeated many times.</p>
<p>Specify <code>method = "boot"</code> and <code>number = 100</code> to perform 100 bootstrap samples.</p>
<div class="sourceCode" id="cb504"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb504-1"><a href="model-validation.html#cb504-1"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb504-2"><a href="model-validation.html#cb504-2"></a>m5 &lt;-<span class="st"> </span><span class="kw">train</span>(mpg <span class="op">~</span><span class="st"> </span>., </span>
<span id="cb504-3"><a href="model-validation.html#cb504-3"></a>            <span class="dt">data =</span> d.train[, <span class="dv">1</span><span class="op">:</span><span class="dv">9</span>],</span>
<span id="cb504-4"><a href="model-validation.html#cb504-4"></a>            <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb504-5"><a href="model-validation.html#cb504-5"></a>            <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;boot&quot;</span>,</span>
<span id="cb504-6"><a href="model-validation.html#cb504-6"></a>                                     <span class="dt">number =</span> <span class="dv">100</span>))</span></code></pre></div>
<pre><code>## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit
## may be misleading

## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit
## may be misleading

## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit
## may be misleading</code></pre>
<div class="sourceCode" id="cb506"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb506-1"><a href="model-validation.html#cb506-1"></a><span class="kw">print</span>(m5)</span></code></pre></div>
<pre><code>## Linear Regression 
## 
## 28 samples
##  8 predictor
## 
## No pre-processing
## Resampling: Bootstrapped (100 reps) 
## Summary of sample sizes: 28, 28, 28, 28, 28, 28, ... 
## Resampling results:
## 
##   RMSE  Rsquared  MAE
##   3.9   0.64      3.2
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<div class="sourceCode" id="cb508"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb508-1"><a href="model-validation.html#cb508-1"></a><span class="kw">postResample</span>(<span class="dt">pred =</span> <span class="kw">predict</span>(m5, <span class="dt">newdata =</span> d.test), </span>
<span id="cb508-2"><a href="model-validation.html#cb508-2"></a>             <span class="dt">obs =</span> d.test<span class="op">$</span>mpg)</span></code></pre></div>
<pre><code>##     RMSE Rsquared      MAE 
##     3.10     0.96     2.45</code></pre>
</div>
</div>
<div id="gain-curve" class="section level3">
<h3><span class="header-section-number">4.7.3</span> Gain Curve</h3>
<p>For supervised learning purposes, a visual way to evaluate a regression model is with the gain curve. This visualization compares a predictive model score to an actual outcome (either binary (0/1) or continuous). The gain curve plot measures how well the model score sorts the data compared to the true outcome value. The x-axis is the fraction of items seen when sorted by score, and the y-axis is the cumulative summed true outcome when sorted by score. For comparison, GainCurvePlot also plots the “wizard curve”: the gain curve when the data is sorted according to its true outcome. A relative Gini score close to 1 means the model sorts responses well.</p>
<div class="sourceCode" id="cb510"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb510-1"><a href="model-validation.html#cb510-1"></a><span class="kw">library</span>(WVPlots)</span>
<span id="cb510-2"><a href="model-validation.html#cb510-2"></a>d<span class="op">$</span>fitted &lt;-<span class="st"> </span>m<span class="op">$</span>fitted.values</span>
<span id="cb510-3"><a href="model-validation.html#cb510-3"></a><span class="kw">GainCurvePlot</span>(d, <span class="dt">xvar =</span> <span class="st">&quot;fitted&quot;</span>, <span class="dt">truthVar =</span> <span class="st">&quot;mpg&quot;</span>, <span class="dt">title =</span> <span class="st">&quot;Model Gain Curve&quot;</span>)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-212-1.png" width="672" /></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="interpretation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ols-reference.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["data-sci.pdf", "data-sci.epub"],
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
