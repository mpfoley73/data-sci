<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8.2 Regression Trees | My Data Science Notes</title>
  <meta name="description" content="This is a compendium of notes from classes, tutorials, etc. that I reference from time to time." />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="8.2 Regression Trees | My Data Science Notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a compendium of notes from classes, tutorials, etc. that I reference from time to time." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8.2 Regression Trees | My Data Science Notes" />
  
  <meta name="twitter:description" content="This is a compendium of notes from classes, tutorials, etc. that I reference from time to time." />
  

<meta name="author" content="Michael Foley" />


<meta name="date" content="2020-02-05" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classification-tree.html"/>
<link rel="next" href="bagging.html"/>
<script src="assets/jquery-2.2.3/jquery.min.js"></script>
<link href="assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Intro</a></li>
<li class="chapter" data-level="1" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>1</b> Probability</a><ul>
<li class="chapter" data-level="1.1" data-path="principles.html"><a href="principles.html"><i class="fa fa-check"></i><b>1.1</b> Principles</a></li>
<li class="chapter" data-level="1.2" data-path="discrete-distributions.html"><a href="discrete-distributions.html"><i class="fa fa-check"></i><b>1.2</b> Discrete Distributions</a><ul>
<li class="chapter" data-level="1.2.1" data-path="discrete-distributions.html"><a href="discrete-distributions.html#binomial"><i class="fa fa-check"></i><b>1.2.1</b> Binomial</a></li>
<li class="chapter" data-level="1.2.2" data-path="discrete-distributions.html"><a href="discrete-distributions.html#negative-binomial"><i class="fa fa-check"></i><b>1.2.2</b> Negative-Binomial</a></li>
<li class="chapter" data-level="1.2.3" data-path="discrete-distributions.html"><a href="discrete-distributions.html#geometric"><i class="fa fa-check"></i><b>1.2.3</b> Geometric</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="continuous-distributions.html"><a href="continuous-distributions.html"><i class="fa fa-check"></i><b>1.3</b> Continuous Distributions</a><ul>
<li class="chapter" data-level="1.3.1" data-path="continuous-distributions.html"><a href="continuous-distributions.html#normal"><i class="fa fa-check"></i><b>1.3.1</b> Normal</a></li>
<li class="chapter" data-level="1.3.2" data-path="continuous-distributions.html"><a href="continuous-distributions.html#example-1"><i class="fa fa-check"></i><b>1.3.2</b> Example</a></li>
<li class="chapter" data-level="1.3.3" data-path="continuous-distributions.html"><a href="continuous-distributions.html#example-2"><i class="fa fa-check"></i><b>1.3.3</b> Example</a></li>
<li class="chapter" data-level="1.3.4" data-path="continuous-distributions.html"><a href="continuous-distributions.html#example-3"><i class="fa fa-check"></i><b>1.3.4</b> Example</a></li>
<li class="chapter" data-level="1.3.5" data-path="continuous-distributions.html"><a href="continuous-distributions.html#normal-approximation-to-binomial"><i class="fa fa-check"></i><b>1.3.5</b> Normal Approximation to Binomial</a></li>
<li class="chapter" data-level="1.3.6" data-path="continuous-distributions.html"><a href="continuous-distributions.html#example-4"><i class="fa fa-check"></i><b>1.3.6</b> Example</a></li>
<li class="chapter" data-level="1.3.7" data-path="continuous-distributions.html"><a href="continuous-distributions.html#example-5"><i class="fa fa-check"></i><b>1.3.7</b> Example</a></li>
<li class="chapter" data-level="1.3.8" data-path="continuous-distributions.html"><a href="continuous-distributions.html#from-sample-to-population"><i class="fa fa-check"></i><b>1.3.8</b> From Sample to Population</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>2</b> Inference</a></li>
<li class="chapter" data-level="3" data-path="experiments.html"><a href="experiments.html"><i class="fa fa-check"></i><b>3</b> Experiments</a><ul>
<li class="chapter" data-level="3.1" data-path="example-one.html"><a href="example-one.html"><i class="fa fa-check"></i><b>3.1</b> Example one</a></li>
<li class="chapter" data-level="3.2" data-path="example-two.html"><a href="example-two.html"><i class="fa fa-check"></i><b>3.2</b> Example two</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>4</b> Regression</a></li>
<li class="chapter" data-level="5" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>5</b> Classification</a></li>
<li class="chapter" data-level="6" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>6</b> Regularization</a></li>
<li class="chapter" data-level="7" data-path="non-linear-models.html"><a href="non-linear-models.html"><i class="fa fa-check"></i><b>7</b> Non-linear Models</a><ul>
<li class="chapter" data-level="7.1" data-path="splines.html"><a href="splines.html"><i class="fa fa-check"></i><b>7.1</b> Splines</a></li>
<li class="chapter" data-level="7.2" data-path="mars.html"><a href="mars.html"><i class="fa fa-check"></i><b>7.2</b> MARS</a></li>
<li class="chapter" data-level="7.3" data-path="gam.html"><a href="gam.html"><i class="fa fa-check"></i><b>7.3</b> GAM</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>8</b> Decision Trees</a><ul>
<li class="chapter" data-level="8.1" data-path="classification-tree.html"><a href="classification-tree.html"><i class="fa fa-check"></i><b>8.1</b> Classification Tree</a><ul>
<li class="chapter" data-level="8.1.1" data-path="classification-tree.html"><a href="classification-tree.html#confusion-matrix"><i class="fa fa-check"></i><b>8.1.1</b> Confusion Matrix</a></li>
<li class="chapter" data-level="8.1.2" data-path="classification-tree.html"><a href="classification-tree.html#roc-curve"><i class="fa fa-check"></i><b>8.1.2</b> ROC Curve</a></li>
<li class="chapter" data-level="8.1.3" data-path="classification-tree.html"><a href="classification-tree.html#caret-approach"><i class="fa fa-check"></i><b>8.1.3</b> Caret Approach</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="regression-trees.html"><a href="regression-trees.html"><i class="fa fa-check"></i><b>8.2</b> Regression Trees</a><ul>
<li class="chapter" data-level="8.2.1" data-path="regression-trees.html"><a href="regression-trees.html#caret-approach-1"><i class="fa fa-check"></i><b>8.2.1</b> Caret Approach</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="bagging.html"><a href="bagging.html"><i class="fa fa-check"></i><b>8.3</b> Bagging</a></li>
<li class="chapter" data-level="8.4" data-path="random-forests.html"><a href="random-forests.html"><i class="fa fa-check"></i><b>8.4</b> Random Forests</a></li>
<li class="chapter" data-level="8.5" data-path="gradient-boosting.html"><a href="gradient-boosting.html"><i class="fa fa-check"></i><b>8.5</b> Gradient Boosting</a></li>
<li class="chapter" data-level="8.6" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>8.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="reference.html"><a href="reference.html"><i class="fa fa-check"></i><b>9</b> Reference</a></li>
<li class="chapter" data-level="10" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>10</b> Support Vector Machines</a><ul>
<li class="chapter" data-level="10.1" data-path="maximal-margin-classifier.html"><a href="maximal-margin-classifier.html"><i class="fa fa-check"></i><b>10.1</b> Maximal Margin Classifier</a></li>
<li class="chapter" data-level="10.2" data-path="support-vector-classifier.html"><a href="support-vector-classifier.html"><i class="fa fa-check"></i><b>10.2</b> Support Vector Classifier</a></li>
<li class="chapter" data-level="10.3" data-path="support-vector-machines-1.html"><a href="support-vector-machines-1.html"><i class="fa fa-check"></i><b>10.3</b> Support Vector Machines</a></li>
<li class="chapter" data-level="10.4" data-path="example-6.html"><a href="example-6.html"><i class="fa fa-check"></i><b>10.4</b> Example</a></li>
<li class="chapter" data-level="10.5" data-path="using-caret.html"><a href="using-caret.html"><i class="fa fa-check"></i><b>10.5</b> Using Caret</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html"><i class="fa fa-check"></i><b>11</b> Principal Components Analysis</a></li>
<li class="chapter" data-level="12" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>12</b> Clustering</a></li>
<li class="chapter" data-level="13" data-path="text-mining.html"><a href="text-mining.html"><i class="fa fa-check"></i><b>13</b> Text Mining</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a></li>
<li class="chapter" data-level="" data-path="publishing-to-bookdown.html"><a href="publishing-to-bookdown.html"><i class="fa fa-check"></i>Publishing to BookDown</a></li>
<li class="chapter" data-level="" data-path="shiny-apps.html"><a href="shiny-apps.html"><i class="fa fa-check"></i>Shiny Apps</a></li>
<li class="chapter" data-level="14" data-path="rstudioconf.html"><a href="rstudioconf.html"><i class="fa fa-check"></i><b>14</b> rstudio::conf</a><ul>
<li class="chapter" data-level="14.1" data-path="open-source-software-for-data-science.html"><a href="open-source-software-for-data-science.html"><i class="fa fa-check"></i><b>14.1</b> Open Source Software for Data Science</a></li>
<li class="chapter" data-level="14.2" data-path="data-visualization-and-designing-with-ai.html"><a href="data-visualization-and-designing-with-ai.html"><i class="fa fa-check"></i><b>14.2</b> Data, visualization, and designing with AI</a></li>
<li class="chapter" data-level="14.3" data-path="deploying-end-to-end-data-science-with-shiny-plumber-and-pins.html"><a href="deploying-end-to-end-data-science-with-shiny-plumber-and-pins.html"><i class="fa fa-check"></i><b>14.3</b> Deploying End-to-End Data Science with Shiny, Plumber, and Pins</a></li>
<li class="chapter" data-level="14.4" data-path="health-connected-siloed-data-sources-and-streamlined-reporting-using-r.html"><a href="health-connected-siloed-data-sources-and-streamlined-reporting-using-r.html"><i class="fa fa-check"></i><b>14.4</b> Health Connected Siloed Data Sources and Streamlined Reporting Using R</a></li>
<li class="chapter" data-level="14.5" data-path="builiding-a-new-data-science-pipeline-for-teh-ft-with-rstudio-connect.html"><a href="builiding-a-new-data-science-pipeline-for-teh-ft-with-rstudio-connect.html"><i class="fa fa-check"></i><b>14.5</b> Builiding a new data science pipeline for teh FT with RStudio Connect</a></li>
<li class="chapter" data-level="14.6" data-path="how-to-win-an-ai-hackathon-without-using-ai.html"><a href="how-to-win-an-ai-hackathon-without-using-ai.html"><i class="fa fa-check"></i><b>14.6</b> How to win an AI Hackathon without using AI</a></li>
<li class="chapter" data-level="14.7" data-path="production-grade-shiny-apps-with-golem.html"><a href="production-grade-shiny-apps-with-golem.html"><i class="fa fa-check"></i><b>14.7</b> Production-grade Shiny Apps with golem</a></li>
<li class="chapter" data-level="14.8" data-path="making-the-shiny-contest.html"><a href="making-the-shiny-contest.html"><i class="fa fa-check"></i><b>14.8</b> Making the Shiny Contest</a></li>
<li class="chapter" data-level="14.9" data-path="styling-shiny-apps-with-sass-and-bootstrap-4.html"><a href="styling-shiny-apps-with-sass-and-bootstrap-4.html"><i class="fa fa-check"></i><b>14.9</b> Styling Shiny apps with Sass and Bootstrap 4</a></li>
<li class="chapter" data-level="14.10" data-path="r-then-and-now.html"><a href="r-then-and-now.html"><i class="fa fa-check"></i><b>14.10</b> R: Then and Now</a></li>
<li class="chapter" data-level="14.11" data-path="journalism-with-rstudio-r-and-teh-tidyverse.html"><a href="journalism-with-rstudio-r-and-teh-tidyverse.html"><i class="fa fa-check"></i><b>14.11</b> Journalism with RStudio, R, and teh Tidyverse</a></li>
<li class="chapter" data-level="14.12" data-path="learning-r-with-humorous-side-projects-or-technical-debt-is-a-social-problem.html"><a href="learning-r-with-humorous-side-projects-or-technical-debt-is-a-social-problem.html"><i class="fa fa-check"></i><b>14.12</b> Learning R with humorous side projects - or - Technical debt is a social problem</a></li>
<li class="chapter" data-level="14.13" data-path="flatironkitchen-how-we-overhauled-a-frankensteinian-sql-workflow-with-the-tidyverse.html"><a href="flatironkitchen-how-we-overhauled-a-frankensteinian-sql-workflow-with-the-tidyverse.html"><i class="fa fa-check"></i><b>14.13</b> FlatironKitchen: How we overhauled a Frankensteinian SQL workflow with the tidyverse</a></li>
<li class="chapter" data-level="14.14" data-path="making-better-spaghetti-plots-exploring-longitudinal-data-with-brolgar-package.html"><a href="making-better-spaghetti-plots-exploring-longitudinal-data-with-brolgar-package.html"><i class="fa fa-check"></i><b>14.14</b> Making better spaghetti (plots): Exploring longitudinal data with brolgar package</a></li>
<li class="chapter" data-level="14.15" data-path="object-of-type-closure-is-not-subsettable.html"><a href="object-of-type-closure-is-not-subsettable.html"><i class="fa fa-check"></i><b>14.15</b> Object of type ‘closure’ is not subsettable</a></li>
<li class="chapter" data-level="14.16" data-path="branding-and-packaging-reports-with-r-markdown.html"><a href="branding-and-packaging-reports-with-r-markdown.html"><i class="fa fa-check"></i><b>14.16</b> Branding and Packaging Reports with R Markdown</a></li>
<li class="chapter" data-level="14.17" data-path="the-glamour-of-graphics.html"><a href="the-glamour-of-graphics.html"><i class="fa fa-check"></i><b>14.17</b> The Glamour of Graphics</a></li>
<li class="chapter" data-level="14.18" data-path="dont-repeat-yourself-talk-to-yourself-repeated-reporting-in-the-r-universe.html"><a href="dont-repeat-yourself-talk-to-yourself-repeated-reporting-in-the-r-universe.html"><i class="fa fa-check"></i><b>14.18</b> Don’t repeat yourself, talk to yourself! Repeated reporting in the R universe</a></li>
<li class="chapter" data-level="14.19" data-path="rstudio-1-3-sneak-preview.html"><a href="rstudio-1-3-sneak-preview.html"><i class="fa fa-check"></i><b>14.19</b> RStudio 1.3 Sneak Preview</a></li>
<li class="chapter" data-level="14.20" data-path="one-r-markdown-document-fourteen-demos-or-tidyverse-2019-20.html"><a href="one-r-markdown-document-fourteen-demos-or-tidyverse-2019-20.html"><i class="fa fa-check"></i><b>14.20</b> One R Markdown Document, Fourteen Demos - or - Tidyverse 2019-20</a></li>
<li class="chapter" data-level="14.21" data-path="best-practices-for-programing-with-ggplot2.html"><a href="best-practices-for-programing-with-ggplot2.html"><i class="fa fa-check"></i><b>14.21</b> Best Practices for programing with ggplot2</a></li>
<li class="chapter" data-level="14.22" data-path="spruce-up-your-ggplot2-visualization-with-formatted-text.html"><a href="spruce-up-your-ggplot2-visualization-with-formatted-text.html"><i class="fa fa-check"></i><b>14.22</b> Spruce up your ggplot2 visualization with formatted text</a></li>
<li class="chapter" data-level="14.23" data-path="the-little-package-that-could-taking-visualization-to-the-next-level-with-the-scales-package.html"><a href="the-little-package-that-could-taking-visualization-to-the-next-level-with-the-scales-package.html"><i class="fa fa-check"></i><b>14.23</b> The little package that could: taking visualization to the next level with the scales package</a></li>
<li class="chapter" data-level="14.24" data-path="advances-in-tidyeval.html"><a href="advances-in-tidyeval.html"><i class="fa fa-check"></i><b>14.24</b> Advances in tidyeval</a></li>
<li class="chapter" data-level="14.25" data-path="section.html"><a href="section.html"><i class="fa fa-check"></i><b>14.25</b> </a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">My Data Science Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression-trees" class="section level2">
<h2><span class="header-section-number">8.2</span> Regression Trees</h2>
<p>A simple regression tree is built in a manner similar to a simple classificatioon tree, and like the simple classification tree, it is rarely invoked on its own; the bagged, random forest, and gradient boosting methods build on this logic. I’ll learn by example again. Using the <code>ISLR::Carseats</code> data set, I will predict <code>Sales</code> using from the 10 feature variables. Load the data.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="regression-trees.html#cb114-1"></a>carseats_dat &lt;-<span class="st"> </span>Carseats</span>
<span id="cb114-2"><a href="regression-trees.html#cb114-2"></a><span class="kw">skim_with</span>(<span class="dt">numeric =</span> <span class="kw">list</span>(<span class="dt">p0 =</span> <span class="ot">NULL</span>, <span class="dt">p25 =</span> <span class="ot">NULL</span>, <span class="dt">p50 =</span> <span class="ot">NULL</span>, <span class="dt">p75 =</span> <span class="ot">NULL</span>, </span>
<span id="cb114-3"><a href="regression-trees.html#cb114-3"></a>                                <span class="dt">p100 =</span> <span class="ot">NULL</span>, <span class="dt">hist =</span> <span class="ot">NULL</span>))</span></code></pre></div>
<pre><code>## Creating new skimming functions for the following classes: p0, p25, p50, p75, p100, hist.
## They did not have recognized defaults. Call get_default_skimmers() for more information.</code></pre>
<pre><code>## function (data, ...) 
## {
##     data_name &lt;- rlang::expr_label(substitute(data))
##     if (!is.data.frame(data)) {
##         data &lt;- as.data.frame(data)
##     }
##     stopifnot(is.data.frame(data))
##     .vars &lt;- rlang::quos(...)
##     cols &lt;- names(data)
##     if (length(.vars) == 0) {
##         selected &lt;- cols
##     }
##     else {
##         selected &lt;- tidyselect::vars_select(cols, !!!.vars)
##     }
##     grps &lt;- dplyr::groups(data)
##     if (length(grps) &gt; 0) {
##         group_variables &lt;- selected %in% as.character(grps)
##         selected &lt;- selected[!group_variables]
##     }
##     skimmers &lt;- purrr::map(selected, get_final_skimmers, data, 
##         local_skimmers, append)
##     types &lt;- purrr::map_chr(skimmers, &quot;skim_type&quot;)
##     unique_skimmers &lt;- reduce_skimmers(skimmers, types)
##     combined_skimmers &lt;- purrr::map(unique_skimmers, join_with_base, 
##         base)
##     ready_to_skim &lt;- tibble::tibble(skim_type = unique(types), 
##         skimmers = purrr::map(combined_skimmers, mangle_names, 
##             names(base$funs)), skim_variable = split(selected, 
##             types)[unique(types)])
##     grouped &lt;- dplyr::group_by(ready_to_skim, .data$skim_type)
##     nested &lt;- dplyr::summarize(grouped, skimmed = purrr::map2(.data$skimmers, 
##         .data$skim_variable, skim_by_type, data))
##     structure(tidyr::unnest(nested, .data$skimmed), class = c(&quot;skim_df&quot;, 
##         &quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;), data_rows = nrow(data), 
##         data_cols = ncol(data), df_name = data_name, groups = dplyr::groups(data), 
##         base_skimmers = names(base$funs), skimmers_used = get_skimmers_used(unique_skimmers))
## }
## &lt;bytecode: 0x00000000226532f0&gt;
## &lt;environment: 0x00000000291f4240&gt;</code></pre>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="regression-trees.html#cb117-1"></a><span class="kw">skim</span>(carseats_dat)</span></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-59">Table 8.2: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">carseats_dat</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">400</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">11</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">factor</td>
<td align="left">3</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">8</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: factor</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="left">ordered</th>
<th align="right">n_unique</th>
<th align="left">top_counts</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">ShelveLoc</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">3</td>
<td align="left">Med: 219, Bad: 96, Goo: 85</td>
</tr>
<tr class="even">
<td align="left">Urban</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">2</td>
<td align="left">Yes: 282, No: 118</td>
</tr>
<tr class="odd">
<td align="left">US</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">2</td>
<td align="left">Yes: 258, No: 142</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Sales</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">7.50</td>
<td align="right">2.82</td>
<td align="right">0</td>
<td align="right">5.39</td>
<td align="right">7.49</td>
<td align="right">9.32</td>
<td align="right">16.27</td>
<td align="left">▁▆▇▃▁</td>
</tr>
<tr class="even">
<td align="left">CompPrice</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">124.97</td>
<td align="right">15.33</td>
<td align="right">77</td>
<td align="right">115.00</td>
<td align="right">125.00</td>
<td align="right">135.00</td>
<td align="right">175.00</td>
<td align="left">▁▅▇▃▁</td>
</tr>
<tr class="odd">
<td align="left">Income</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">68.66</td>
<td align="right">27.99</td>
<td align="right">21</td>
<td align="right">42.75</td>
<td align="right">69.00</td>
<td align="right">91.00</td>
<td align="right">120.00</td>
<td align="left">▇▆▇▆▅</td>
</tr>
<tr class="even">
<td align="left">Advertising</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">6.63</td>
<td align="right">6.65</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">5.00</td>
<td align="right">12.00</td>
<td align="right">29.00</td>
<td align="left">▇▃▃▁▁</td>
</tr>
<tr class="odd">
<td align="left">Population</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">264.84</td>
<td align="right">147.38</td>
<td align="right">10</td>
<td align="right">139.00</td>
<td align="right">272.00</td>
<td align="right">398.50</td>
<td align="right">509.00</td>
<td align="left">▇▇▇▇▇</td>
</tr>
<tr class="even">
<td align="left">Price</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">115.80</td>
<td align="right">23.68</td>
<td align="right">24</td>
<td align="right">100.00</td>
<td align="right">117.00</td>
<td align="right">131.00</td>
<td align="right">191.00</td>
<td align="left">▁▂▇▆▁</td>
</tr>
<tr class="odd">
<td align="left">Age</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">53.32</td>
<td align="right">16.20</td>
<td align="right">25</td>
<td align="right">39.75</td>
<td align="right">54.50</td>
<td align="right">66.00</td>
<td align="right">80.00</td>
<td align="left">▇▆▇▇▇</td>
</tr>
<tr class="even">
<td align="left">Education</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">13.90</td>
<td align="right">2.62</td>
<td align="right">10</td>
<td align="right">12.00</td>
<td align="right">14.00</td>
<td align="right">16.00</td>
<td align="right">18.00</td>
<td align="left">▇▇▃▇▇</td>
</tr>
</tbody>
</table>
<p>I’ll split <code>careseats_dat</code> (n = 400) into <code>carseats_train</code> (80%, n = 321) and <code>carseats_test</code> (20%, n = 79). I’ll fit a simple decision tree with <code>carseats_train</code>, then later a bagged tree, a random forest, and a gradient boosting tree. I’ll compare their predictive performance with <code>carseats_test</code>.</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="regression-trees.html#cb118-1"></a><span class="kw">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb118-2"><a href="regression-trees.html#cb118-2"></a>partition &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(<span class="dt">y =</span> carseats_dat<span class="op">$</span>Sales, <span class="dt">p =</span> <span class="fl">0.8</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)</span>
<span id="cb118-3"><a href="regression-trees.html#cb118-3"></a>carseats_train &lt;-<span class="st"> </span>carseats_dat[partition, ]</span>
<span id="cb118-4"><a href="regression-trees.html#cb118-4"></a>carseats_test &lt;-<span class="st"> </span>carseats_dat[<span class="op">-</span>partition, ]</span></code></pre></div>
<p>The first step is to build a full tree, then perform k-fold cross-validation to help select the optimal cost complexity (cp). The only difference here is the <code>rpart()</code> parameter <code>method = "anova"</code> to produce a regression tree.</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="regression-trees.html#cb119-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb119-2"><a href="regression-trees.html#cb119-2"></a>carseats_model_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">rpart</span>(</span>
<span id="cb119-3"><a href="regression-trees.html#cb119-3"></a>   <span class="dt">formula =</span> Sales <span class="op">~</span><span class="st"> </span>.,</span>
<span id="cb119-4"><a href="regression-trees.html#cb119-4"></a>   <span class="dt">data =</span> carseats_train,</span>
<span id="cb119-5"><a href="regression-trees.html#cb119-5"></a>   <span class="dt">method =</span> <span class="st">&quot;anova&quot;</span>, </span>
<span id="cb119-6"><a href="regression-trees.html#cb119-6"></a>   <span class="dt">xval =</span> <span class="dv">10</span>,</span>
<span id="cb119-7"><a href="regression-trees.html#cb119-7"></a>   <span class="dt">model =</span> <span class="ot">TRUE</span>  <span class="co"># to plot splits with factor variables.</span></span>
<span id="cb119-8"><a href="regression-trees.html#cb119-8"></a>)</span>
<span id="cb119-9"><a href="regression-trees.html#cb119-9"></a><span class="kw">print</span>(carseats_model_<span class="dv">1</span>)</span></code></pre></div>
<pre><code>## n= 321 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 321 2567.76800  7.535950  
##    2) ShelveLoc=Bad,Medium 251 1474.14100  6.770359  
##      4) Price&gt;=105.5 168  719.70630  5.987024  
##        8) ShelveLoc=Bad 50  165.70160  4.693600  
##         16) Population&lt; 201.5 20   48.35505  3.646500 *
##         17) Population&gt;=201.5 30   80.79922  5.391667 *
##        9) ShelveLoc=Medium 118  434.91370  6.535085  
##         18) Advertising&lt; 11.5 88  290.05490  6.113068  
##           36) CompPrice&lt; 142 69  193.86340  5.769420  
##             72) Price&gt;=132.5 16   50.75440  4.455000 *
##             73) Price&lt; 132.5 53  107.12060  6.166226 *
##           37) CompPrice&gt;=142 19   58.45118  7.361053 *
##         19) Advertising&gt;=11.5 30   83.21323  7.773000 *
##      5) Price&lt; 105.5 83  442.68920  8.355904  
##       10) Age&gt;=63.5 32  153.42300  6.922500  
##         20) Price&gt;=85 25   66.89398  6.160800  
##           40) ShelveLoc=Bad 9   18.39396  4.772222 *
##           41) ShelveLoc=Medium 16   21.38544  6.941875 *
##         21) Price&lt; 85 7   20.22194  9.642857 *
##       11) Age&lt; 63.5 51  182.26350  9.255294  
##         22) Income&lt; 57.5 12   28.03042  7.707500 *
##         23) Income&gt;=57.5 39  116.63950  9.731538  
##           46) Age&gt;=50.5 14   21.32597  8.451429 *
##           47) Age&lt; 50.5 25   59.52474 10.448400 *
##    3) ShelveLoc=Good 70  418.98290 10.281140  
##      6) Price&gt;=107.5 49  242.58730  9.441633  
##       12) Advertising&lt; 13.5 41  162.47820  8.926098  
##         24) Age&gt;=61 17   53.37051  7.757647 *
##         25) Age&lt; 61 24   69.45776  9.753750 *
##       13) Advertising&gt;=13.5 8   13.36599 12.083750 *
##      7) Price&lt; 107.5 21   61.28200 12.240000 *</code></pre>
<p>The output starts with the root node. The predicted <code>Sales</code> at the root is the mean <code>Sales</code> for the training data set, 7.535950 (values are $000s). The deviance at the root is the SSE, 2567.768. The child nodes of node “x” are labeled 2x) and 2x+1), so the child nodes of 1) are 2) and 3), and the child nodes of 2) are 4) and 5). Terminal nodes are labeled with an asterisk (*).</p>
<p>The first split is at <code>ShelveLoc</code> = [Bad, Medium] vs Good. Here is what the full (unpruned) tree looks like.</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="regression-trees.html#cb121-1"></a><span class="kw">rpart.plot</span>(carseats_model_<span class="dv">1</span>, <span class="dt">yesno =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-62-1.png" width="672" /></p>
<p>The boxes show the node predicted value (mean) and the proportion of observations that are in the node (or child nodes).</p>
<p><code>rpart()</code> not only grew the full tree, it also used cross-validation to test the performance of the possible complexity hyperparameters. <code>printcp()</code> displays the candidate cp values. You can use this table to decide how to prune the tree.</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="regression-trees.html#cb122-1"></a><span class="kw">printcp</span>(carseats_model_<span class="dv">1</span>)</span></code></pre></div>
<pre><code>## 
## Regression tree:
## rpart(formula = Sales ~ ., data = carseats_train, method = &quot;anova&quot;, 
##     model = TRUE, xval = 10)
## 
## Variables actually used in tree construction:
## [1] Advertising Age         CompPrice   Income      Population  Price      
## [7] ShelveLoc  
## 
## Root node error: 2567.8/321 = 7.9993
## 
## n= 321 
## 
##          CP nsplit rel error  xerror     xstd
## 1  0.262736      0   1.00000 1.00635 0.076664
## 2  0.121407      1   0.73726 0.74888 0.058981
## 3  0.046379      2   0.61586 0.65278 0.050839
## 4  0.044830      3   0.56948 0.67245 0.051638
## 5  0.041671      4   0.52465 0.66230 0.051065
## 6  0.025993      5   0.48298 0.62345 0.049368
## 7  0.025823      6   0.45698 0.61980 0.048026
## 8  0.024007      7   0.43116 0.62058 0.048213
## 9  0.015441      8   0.40715 0.58061 0.041738
## 10 0.014698      9   0.39171 0.56413 0.041368
## 11 0.014641     10   0.37701 0.56277 0.041271
## 12 0.014233     11   0.36237 0.56081 0.041097
## 13 0.014015     12   0.34814 0.55647 0.038308
## 14 0.013938     13   0.33413 0.55647 0.038308
## 15 0.010560     14   0.32019 0.57110 0.038872
## 16 0.010000     15   0.30963 0.56676 0.038090</code></pre>
<p>There are 16 possible cp values in this model. The model with the smallest complexity parameter allows the most splits (<code>nsplit</code>). The highest complexity parameter corresponds to a tree with just a root node. <code>rel error</code> is the SSE relative to the root node. The root node SSE is 2567.76800, so its <code>rel error</code> is 2567.76800/2567.76800 = 1.0. That means the absolute error of the full tree (at CP = 0.01) is 0.30963 * 2567.76800 = 795.058. You can verify that by calculating the SSE of the model predicted values:</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="regression-trees.html#cb124-1"></a><span class="kw">data.frame</span>(<span class="dt">pred =</span> <span class="kw">predict</span>(carseats_model_<span class="dv">1</span>, <span class="dt">newdata =</span> carseats_train)) <span class="op">%&gt;%</span></span>
<span id="cb124-2"><a href="regression-trees.html#cb124-2"></a><span class="st">   </span><span class="kw">mutate</span>(<span class="dt">obs =</span> carseats_train<span class="op">$</span>Sales,</span>
<span id="cb124-3"><a href="regression-trees.html#cb124-3"></a>          <span class="dt">sq_err =</span> (obs <span class="op">-</span><span class="st"> </span>pred)<span class="op">^</span><span class="dv">2</span>) <span class="op">%&gt;%</span></span>
<span id="cb124-4"><a href="regression-trees.html#cb124-4"></a><span class="st">   </span><span class="kw">summarize</span>(<span class="dt">sse =</span> <span class="kw">sum</span>(sq_err))</span></code></pre></div>
<pre><code>##        sse
## 1 795.0525</code></pre>
<p>Finishing the CP table tour, <code>xerror</code> is the cross-validated SSE and <code>xstd</code> is its standard error. If you want the lowest possible error, then prune to the tree with the smallest relative SSE (<code>xerror</code>). If you want to balance predictive power with simplicity, prune to the smallest tree within 1 SE of the one with the smallest relative SSE. The CP table is not super-helpful for finding that tree. I’ll add a column to find it.</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="regression-trees.html#cb126-1"></a>carseats_model_<span class="dv">1</span><span class="op">$</span>cptable <span class="op">%&gt;%</span></span>
<span id="cb126-2"><a href="regression-trees.html#cb126-2"></a><span class="st">   </span><span class="kw">data.frame</span>() <span class="op">%&gt;%</span></span>
<span id="cb126-3"><a href="regression-trees.html#cb126-3"></a><span class="st">   </span><span class="kw">mutate</span>(<span class="dt">min_xerror_idx =</span> <span class="kw">which.min</span>(carseats_model_<span class="dv">1</span><span class="op">$</span>cptable[, <span class="st">&quot;xerror&quot;</span>]),</span>
<span id="cb126-4"><a href="regression-trees.html#cb126-4"></a>          <span class="dt">rownum =</span> <span class="kw">row_number</span>(),</span>
<span id="cb126-5"><a href="regression-trees.html#cb126-5"></a>          <span class="dt">xerror_cap =</span> carseats_model_<span class="dv">1</span><span class="op">$</span>cptable[min_xerror_idx, <span class="st">&quot;xerror&quot;</span>] <span class="op">+</span><span class="st"> </span></span>
<span id="cb126-6"><a href="regression-trees.html#cb126-6"></a><span class="st">             </span>carseats_model_<span class="dv">1</span><span class="op">$</span>cptable[min_xerror_idx, <span class="st">&quot;xstd&quot;</span>],</span>
<span id="cb126-7"><a href="regression-trees.html#cb126-7"></a>          <span class="dt">eval =</span> <span class="kw">case_when</span>(rownum <span class="op">==</span><span class="st"> </span>min_xerror_idx <span class="op">~</span><span class="st"> &quot;min xerror&quot;</span>,</span>
<span id="cb126-8"><a href="regression-trees.html#cb126-8"></a>                           xerror <span class="op">&lt;</span><span class="st"> </span>xerror_cap <span class="op">~</span><span class="st"> &quot;under cap&quot;</span>,</span>
<span id="cb126-9"><a href="regression-trees.html#cb126-9"></a>                           <span class="ot">TRUE</span> <span class="op">~</span><span class="st"> &quot;&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb126-10"><a href="regression-trees.html#cb126-10"></a><span class="st">   </span><span class="kw">select</span>(<span class="op">-</span>rownum, <span class="op">-</span>min_xerror_idx) </span></code></pre></div>
<pre><code>##            CP nsplit rel.error    xerror       xstd xerror_cap       eval
## 1  0.26273578      0 1.0000000 1.0063530 0.07666355  0.5947744           
## 2  0.12140705      1 0.7372642 0.7488767 0.05898146  0.5947744           
## 3  0.04637919      2 0.6158572 0.6527823 0.05083938  0.5947744           
## 4  0.04483023      3 0.5694780 0.6724529 0.05163819  0.5947744           
## 5  0.04167149      4 0.5246478 0.6623028 0.05106530  0.5947744           
## 6  0.02599265      5 0.4829763 0.6234457 0.04936799  0.5947744           
## 7  0.02582284      6 0.4569836 0.6198034 0.04802643  0.5947744           
## 8  0.02400748      7 0.4311608 0.6205756 0.04821332  0.5947744           
## 9  0.01544139      8 0.4071533 0.5806072 0.04173785  0.5947744  under cap
## 10 0.01469771      9 0.3917119 0.5641331 0.04136793  0.5947744  under cap
## 11 0.01464055     10 0.3770142 0.5627713 0.04127139  0.5947744  under cap
## 12 0.01423309     11 0.3623736 0.5608073 0.04109662  0.5947744  under cap
## 13 0.01401541     12 0.3481405 0.5564663 0.03830810  0.5947744 min xerror
## 14 0.01393771     13 0.3341251 0.5564663 0.03830810  0.5947744  under cap
## 15 0.01055959     14 0.3201874 0.5710951 0.03887227  0.5947744  under cap
## 16 0.01000000     15 0.3096278 0.5667561 0.03808991  0.5947744  under cap</code></pre>
<p>Okay, so the simplest tree is the one with CP = 0.01544139 (8 splits). Fortunately, <code>plotcp()</code> presents a nice graphical representation of the relationship between <code>xerror</code> and <code>cp</code>.</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="regression-trees.html#cb128-1"></a><span class="kw">plotcp</span>(carseats_model_<span class="dv">1</span>, <span class="dt">upper =</span> <span class="st">&quot;splits&quot;</span>)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-66-1.png" width="672" /></p>
<p>The dashed line is set at the minimum <code>xerror</code> + <code>xstd</code>. The top axis shows the number of splits in the tree. I’m not sure why the CP values are not the same as in the table (they are close, but not the same). The smallest relative error is at 0.0140154, but the maximum CP below the dashed line (one standard deviation above the mimimum error) is at CP = .019 (8 splits). Use the <code>prune()</code> function to prune the tree by specifying the associated cost-complexity <code>cp</code>.</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="regression-trees.html#cb129-1"></a>carseats_model_<span class="dv">1</span>_pruned &lt;-<span class="st"> </span><span class="kw">prune</span>(</span>
<span id="cb129-2"><a href="regression-trees.html#cb129-2"></a>   carseats_model_<span class="dv">1</span>,</span>
<span id="cb129-3"><a href="regression-trees.html#cb129-3"></a>   <span class="dt">cp =</span> carseats_model_<span class="dv">1</span><span class="op">$</span>cptable[carseats_model_<span class="dv">1</span><span class="op">$</span>cptable[, <span class="dv">2</span>] <span class="op">==</span><span class="st"> </span><span class="dv">8</span>, <span class="st">&quot;CP&quot;</span>]</span>
<span id="cb129-4"><a href="regression-trees.html#cb129-4"></a>)</span>
<span id="cb129-5"><a href="regression-trees.html#cb129-5"></a><span class="kw">rpart.plot</span>(carseats_model_<span class="dv">1</span>_pruned, <span class="dt">yesno =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-67-1.png" width="672" /></p>
<p>The most “important” indicator of <code>Sales</code> is <code>ShelveLoc</code>. Here are the importance values from the model.</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="regression-trees.html#cb130-1"></a>carseats_model_<span class="dv">1</span>_pruned<span class="op">$</span>variable.importance <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb130-2"><a href="regression-trees.html#cb130-2"></a><span class="st">   </span><span class="kw">data.frame</span>() <span class="op">%&gt;%</span></span>
<span id="cb130-3"><a href="regression-trees.html#cb130-3"></a><span class="st">   </span><span class="kw">rownames_to_column</span>(<span class="dt">var =</span> <span class="st">&quot;Feature&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb130-4"><a href="regression-trees.html#cb130-4"></a><span class="st">   </span><span class="kw">rename</span>(<span class="dt">Overall =</span> <span class="st">&#39;.&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb130-5"><a href="regression-trees.html#cb130-5"></a><span class="st">   </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">fct_reorder</span>(Feature, Overall), <span class="dt">y =</span> Overall)) <span class="op">+</span></span>
<span id="cb130-6"><a href="regression-trees.html#cb130-6"></a><span class="st">   </span><span class="kw">geom_pointrange</span>(<span class="kw">aes</span>(<span class="dt">ymin =</span> <span class="dv">0</span>, <span class="dt">ymax =</span> Overall), <span class="dt">color =</span> <span class="st">&quot;cadetblue&quot;</span>, <span class="dt">size =</span> <span class="fl">.3</span>) <span class="op">+</span></span>
<span id="cb130-7"><a href="regression-trees.html#cb130-7"></a><span class="st">   </span><span class="kw">theme_minimal</span>() <span class="op">+</span></span>
<span id="cb130-8"><a href="regression-trees.html#cb130-8"></a><span class="st">   </span><span class="kw">coord_flip</span>() <span class="op">+</span></span>
<span id="cb130-9"><a href="regression-trees.html#cb130-9"></a><span class="st">   </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;&quot;</span>, <span class="dt">title =</span> <span class="st">&quot;Variable Importance with Simple Regression&quot;</span>)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-68-1.png" width="672" /></p>
<p>The most important indicator of <code>Sales</code> is <code>ShelveLoc</code>, then <code>Price</code>, then <code>Age</code>, all of which appear in the final model. <code>CompPrice</code> was also important.</p>
<p>The last step is to make predictions on the validation data set. The root mean squared error (<span class="math inline">\(RMSE = \sqrt{(1/2) \sum{(actual - pred)^2}})\)</span> and mean absolute error (<span class="math inline">\(MAE = (1/n) \sum{|actual - pred|}\)</span>) are the two most common measures of predictive accuracy. The key difference is that RMSE punishes large errors more harshly. For a regression tree, set argument <code>type = "vector"</code> (or do not specify at all).</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="regression-trees.html#cb131-1"></a>carseats_model_<span class="dv">1</span>_preds &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb131-2"><a href="regression-trees.html#cb131-2"></a>   carseats_model_<span class="dv">1</span>_pruned, </span>
<span id="cb131-3"><a href="regression-trees.html#cb131-3"></a>   carseats_test, </span>
<span id="cb131-4"><a href="regression-trees.html#cb131-4"></a>   <span class="dt">type =</span> <span class="st">&quot;vector&quot;</span></span>
<span id="cb131-5"><a href="regression-trees.html#cb131-5"></a>)</span>
<span id="cb131-6"><a href="regression-trees.html#cb131-6"></a></span>
<span id="cb131-7"><a href="regression-trees.html#cb131-7"></a>carseats_model_<span class="dv">1</span>_pruned_rmse &lt;-<span class="st"> </span><span class="kw">RMSE</span>(</span>
<span id="cb131-8"><a href="regression-trees.html#cb131-8"></a>   <span class="dt">pred =</span> carseats_model_<span class="dv">1</span>_preds,</span>
<span id="cb131-9"><a href="regression-trees.html#cb131-9"></a>   <span class="dt">obs =</span> carseats_test<span class="op">$</span>Sales</span>
<span id="cb131-10"><a href="regression-trees.html#cb131-10"></a>)</span>
<span id="cb131-11"><a href="regression-trees.html#cb131-11"></a>carseats_model_<span class="dv">1</span>_pruned_rmse</span></code></pre></div>
<pre><code>## [1] 2.388059</code></pre>
<p>The pruning process leads to an average prediction error of 2.388 in the test data set. Not too bad considering the standard deviation of <code>Sales</code> is 2.801. Here is a predicted vs actual plot.</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="regression-trees.html#cb133-1"></a><span class="kw">plot</span>(carseats_test<span class="op">$</span>Sales, carseats_model_<span class="dv">1</span>_preds, </span>
<span id="cb133-2"><a href="regression-trees.html#cb133-2"></a>     <span class="dt">main =</span> <span class="st">&quot;Simple Regression: Predicted vs. Actual&quot;</span>,</span>
<span id="cb133-3"><a href="regression-trees.html#cb133-3"></a>     <span class="dt">xlab =</span> <span class="st">&quot;Actual&quot;</span>,</span>
<span id="cb133-4"><a href="regression-trees.html#cb133-4"></a>     <span class="dt">ylab =</span> <span class="st">&quot;Predicted&quot;</span>)</span>
<span id="cb133-5"><a href="regression-trees.html#cb133-5"></a><span class="kw">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-70-1.png" width="672" /></p>
<p>The 6 possible predicted values do a decent job of binning the observations.</p>
<div id="caret-approach-1" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Caret Approach</h3>
<p>I can also fit the model with <code>caret::train()</code>, specifying <code>method = "rpart"</code>.</p>
<p>I’ll build the model using 10-fold cross-validation to optimize the hyperparameter CP.</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="regression-trees.html#cb134-1"></a>carseats_trControl =<span class="st"> </span><span class="kw">trainControl</span>(</span>
<span id="cb134-2"><a href="regression-trees.html#cb134-2"></a>   <span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>,  <span class="co"># k-fold cross validation</span></span>
<span id="cb134-3"><a href="regression-trees.html#cb134-3"></a>   <span class="dt">number =</span> <span class="dv">10</span>,  <span class="co"># 10 folds</span></span>
<span id="cb134-4"><a href="regression-trees.html#cb134-4"></a>   <span class="dt">savePredictions =</span> <span class="st">&quot;final&quot;</span>       <span class="co"># save predictions for the optimal tuning parameter</span></span>
<span id="cb134-5"><a href="regression-trees.html#cb134-5"></a>)</span></code></pre></div>
<p>I’ll let the model look for the best CP tuning parameter with <code>tuneLength</code> to get close, then fine-tune with <code>tuneGrid</code>.</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="regression-trees.html#cb135-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb135-2"><a href="regression-trees.html#cb135-2"></a>carseats_model_<span class="dv">2</span> =<span class="st"> </span><span class="kw">train</span>(</span>
<span id="cb135-3"><a href="regression-trees.html#cb135-3"></a>   Sales <span class="op">~</span><span class="st"> </span>., </span>
<span id="cb135-4"><a href="regression-trees.html#cb135-4"></a>   <span class="dt">data =</span> carseats_train, </span>
<span id="cb135-5"><a href="regression-trees.html#cb135-5"></a>   <span class="dt">method =</span> <span class="st">&quot;rpart&quot;</span>,  <span class="co"># for classification tree</span></span>
<span id="cb135-6"><a href="regression-trees.html#cb135-6"></a>   <span class="dt">tuneLength =</span> <span class="dv">5</span>,  <span class="co"># choose up to 5 combinations of tuning parameters (cp)</span></span>
<span id="cb135-7"><a href="regression-trees.html#cb135-7"></a>   <span class="dt">metric =</span> <span class="st">&quot;RMSE&quot;</span>,  <span class="co"># evaluate hyperparamter combinations with RMSE</span></span>
<span id="cb135-8"><a href="regression-trees.html#cb135-8"></a>   <span class="dt">trControl =</span> carseats_trControl</span>
<span id="cb135-9"><a href="regression-trees.html#cb135-9"></a>)</span></code></pre></div>
<pre><code>## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :
## There were missing values in resampled performance measures.</code></pre>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="regression-trees.html#cb137-1"></a><span class="kw">print</span>(carseats_model_<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## CART 
## 
## 321 samples
##  10 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... 
## Resampling results across tuning parameters:
## 
##   cp          RMSE      Rsquared   MAE     
##   0.04167149  2.209383  0.4065251  1.778797
##   0.04483023  2.243618  0.3849728  1.805027
##   0.04637919  2.275563  0.3684309  1.808814
##   0.12140705  2.400455  0.2942663  1.936927
##   0.26273578  2.692867  0.1898998  2.192774
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was cp = 0.04167149.</code></pre>
<p>The first <code>cp</code> (0.04167149) produced the smallest RMSE. I can drill into the best value of <code>cp</code> using a tuning grid. I’ll try that now.</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="regression-trees.html#cb139-1"></a>myGrid &lt;-<span class="st">  </span><span class="kw">expand.grid</span>(<span class="dt">cp =</span> <span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">0</span>, <span class="dt">to =</span> <span class="fl">0.1</span>, <span class="dt">by =</span> <span class="fl">0.01</span>))</span>
<span id="cb139-2"><a href="regression-trees.html#cb139-2"></a>carseats_model_<span class="dv">3</span> =<span class="st"> </span><span class="kw">train</span>(</span>
<span id="cb139-3"><a href="regression-trees.html#cb139-3"></a>   Sales <span class="op">~</span><span class="st"> </span>., </span>
<span id="cb139-4"><a href="regression-trees.html#cb139-4"></a>   <span class="dt">data =</span> carseats_train, </span>
<span id="cb139-5"><a href="regression-trees.html#cb139-5"></a>   <span class="dt">method =</span> <span class="st">&quot;rpart&quot;</span>,  <span class="co"># for classification tree</span></span>
<span id="cb139-6"><a href="regression-trees.html#cb139-6"></a>   <span class="dt">tuneGrid =</span> myGrid,  <span class="co"># choose up to 5 combinations of tuning parameters (cp)</span></span>
<span id="cb139-7"><a href="regression-trees.html#cb139-7"></a>   <span class="dt">metric =</span> <span class="st">&quot;RMSE&quot;</span>,  <span class="co"># evaluate hyperparamter combinations with RMSE</span></span>
<span id="cb139-8"><a href="regression-trees.html#cb139-8"></a>   <span class="dt">trControl =</span> carseats_trControl</span>
<span id="cb139-9"><a href="regression-trees.html#cb139-9"></a>)</span>
<span id="cb139-10"><a href="regression-trees.html#cb139-10"></a><span class="kw">print</span>(carseats_model_<span class="dv">3</span>)</span></code></pre></div>
<pre><code>## CART 
## 
## 321 samples
##  10 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 289, 289, 289, 289, 288, 289, ... 
## Resampling results across tuning parameters:
## 
##   cp    RMSE      Rsquared   MAE     
##   0.00  2.131814  0.4578761  1.725960
##   0.01  2.203111  0.4294647  1.790050
##   0.02  2.240209  0.3948080  1.834786
##   0.03  2.206168  0.4139717  1.762170
##   0.04  2.274313  0.3686176  1.795154
##   0.05  2.309746  0.3405228  1.830556
##   0.06  2.246757  0.3703977  1.780266
##   0.07  2.253725  0.3679986  1.794485
##   0.08  2.253725  0.3679986  1.794485
##   0.09  2.253725  0.3679986  1.794485
##   0.10  2.253725  0.3679986  1.794485
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was cp = 0.</code></pre>
<p>It looks like the best performing tree is the unpruned one.</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="regression-trees.html#cb141-1"></a><span class="kw">plot</span>(carseats_model_<span class="dv">3</span>)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-74-1.png" width="672" /></p>
<p>Lets’s see the final model.</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="regression-trees.html#cb142-1"></a><span class="kw">rpart.plot</span>(carseats_model_<span class="dv">3</span><span class="op">$</span>finalModel)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-75-1.png" width="672" /></p>
<p>What were the most important variables?</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="regression-trees.html#cb143-1"></a><span class="kw">plot</span>(<span class="kw">varImp</span>(carseats_model_<span class="dv">3</span>), <span class="dt">main=</span><span class="st">&quot;Variable Importance with Simple Regression&quot;</span>)</span></code></pre></div>
<p><img src="data-sci_files/figure-html/unnamed-chunk-76-1.png" width="672" /></p>
<p>Evaluate the model by making predictions with the test data set.</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="regression-trees.html#cb144-1"></a>carseats_model_<span class="dv">3</span>_preds &lt;-<span class="st"> </span><span class="kw">predict</span>(carseats_model_<span class="dv">3</span>, carseats_test, <span class="dt">type =</span> <span class="st">&quot;raw&quot;</span>)</span>
<span id="cb144-2"><a href="regression-trees.html#cb144-2"></a><span class="kw">data.frame</span>(<span class="dt">Actual =</span> carseats_test<span class="op">$</span>Sales, <span class="dt">Predicted =</span> carseats_model_<span class="dv">3</span>_preds) <span class="op">%&gt;%</span></span>
<span id="cb144-3"><a href="regression-trees.html#cb144-3"></a><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Actual, <span class="dt">y =</span> Predicted)) <span class="op">+</span></span>
<span id="cb144-4"><a href="regression-trees.html#cb144-4"></a><span class="st">   </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb144-5"><a href="regression-trees.html#cb144-5"></a><span class="st">   </span><span class="kw">geom_smooth</span>() <span class="op">+</span></span>
<span id="cb144-6"><a href="regression-trees.html#cb144-6"></a><span class="st">   </span><span class="kw">geom_abline</span>(<span class="dt">slope =</span> <span class="dv">1</span>, <span class="dt">intercept =</span> <span class="dv">0</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb144-7"><a href="regression-trees.html#cb144-7"></a><span class="st">   </span><span class="kw">scale_y_continuous</span>(<span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">15</span>)) <span class="op">+</span></span>
<span id="cb144-8"><a href="regression-trees.html#cb144-8"></a><span class="st">   </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Simple Regression: Predicted vs. Actual&quot;</span>)</span></code></pre></div>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="data-sci_files/figure-html/unnamed-chunk-77-1.png" width="672" /></p>
<p>Looks like the model over-estimates at the low end and undestimates at the high end. Calculate the test data set RMSE.</p>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="regression-trees.html#cb146-1"></a>carseats_model_<span class="dv">3</span>_pruned_rmse &lt;-<span class="st"> </span><span class="kw">RMSE</span>(</span>
<span id="cb146-2"><a href="regression-trees.html#cb146-2"></a>   <span class="dt">pred =</span> carseats_model_<span class="dv">3</span>_preds,</span>
<span id="cb146-3"><a href="regression-trees.html#cb146-3"></a>   <span class="dt">obs =</span> carseats_test<span class="op">$</span>Sales</span>
<span id="cb146-4"><a href="regression-trees.html#cb146-4"></a>)</span>
<span id="cb146-5"><a href="regression-trees.html#cb146-5"></a>carseats_model_<span class="dv">3</span>_pruned_rmse</span></code></pre></div>
<pre><code>## [1] 2.298331</code></pre>
<p>Caret faired better in this model. Here is a summary the RMSE values of the two models.</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="regression-trees.html#cb148-1"></a><span class="kw">rbind</span>(<span class="kw">data.frame</span>(<span class="dt">model =</span> <span class="st">&quot;Manual ANOVA&quot;</span>, </span>
<span id="cb148-2"><a href="regression-trees.html#cb148-2"></a>                 <span class="dt">RMSE =</span> <span class="kw">round</span>(carseats_model_<span class="dv">1</span>_pruned_rmse, <span class="dv">5</span>)), </span>
<span id="cb148-3"><a href="regression-trees.html#cb148-3"></a>      <span class="kw">data.frame</span>(<span class="dt">model =</span> <span class="st">&quot;Caret&quot;</span>, </span>
<span id="cb148-4"><a href="regression-trees.html#cb148-4"></a>                 <span class="dt">RMSE =</span> <span class="kw">round</span>(carseats_model_<span class="dv">3</span>_pruned_rmse, <span class="dv">5</span>))</span>
<span id="cb148-5"><a href="regression-trees.html#cb148-5"></a>)</span></code></pre></div>
<pre><code>##          model    RMSE
## 1 Manual ANOVA 2.38806
## 2        Caret 2.29833</code></pre>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classification-tree.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bagging.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["data-sci.pdf", "data-sci.epub"],
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
