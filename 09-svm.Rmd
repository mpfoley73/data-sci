# Support Vector Machines

These notes rely on [@James2013], [@Hastie2017], and [@Kuhn2016]. I also reviewed the material in PSU's Applied Data Mining and Statistical Learning ([STAT 508](https://online.stat.psu.edu/stat508/)).

The SVM algorithm searches for a linearly separable hyperplane separating members of one class from the other. If such a hyperplane does not exist, SVM uses a nonlinear mapping to transform the training data into a higher dimension. With an appropriate nonlinear mapping to a sufficiently high dimension, data from two classes can always be separated by a hyperplane. The SVM algorithm finds this hyperplane using support vectors and margins. SVM has relatively high accuracy, and is less prone to overfitting.

The support vector machine (SVM) is an extension of the *support vector classifier* which in turn is a generalization of the simple and intuitive *maximal margin classifier*. 


## Maximal Margin Classifier

The maximal margin classifier is elegant and simple, but rarely applicable to data sets since it requires that the classes be separable by a linear boundary.  However it is the foundation of SVM and therefore a good path to understanding.

Given an $n \times p$ data matrix **X** with binary response variable $y \in [-1, 1]$ it *may* be possible to define a *p*-dimensional hyperplane

$$\beta_0 + \beta_1X_1 + \beta_2X_2 \dots + \beta_pX_p = 0$$

such that all observations of each class fall on opposite sides of the hyperplane. This "separating hyperplane" has the property that 

$$y_i (\beta_0 + \beta_1X_1 + \beta_2X_2 \dots + \beta_pX_p = 0) \ge M$$

where $M$ is some positive margin.  If the $\beta$ values are constrained so that $\sum\beta^2 = 1$, then (it can be shown that) $M$ is the perpendicular distance from observation *i* and the hyperplane.  The maximal margin classifier *maximizes* $M$.  The maximization of $M$ is a straight-forward task for your computer ([@James2013]).

The illustration below shows two classes of observations with measurements on two variables and a separating hyperplane.  The dashed lines show the maximal margin separating the classes. Three observations lie along the maximal margin lines. These three observations are *support vectors*.  They “support” the maximal margin hyperplane in the sense that if these points were moved the maximal margin hyperplane would move as well. The maximal margin hyperplane depends on the support vectors, but not on the other observations.

```{r echo=FALSE, fig.width=5}
df <- data.frame(
  x = c(-1.5, -1.2, -.9, -.8, -.7, -.2, -.4, -.3, -.1, .6, .4, .4, .5, 1, 1.1, 1.3, 1.4, 1.5, 1.9, 2.0, 2.7, 3.1, 3.2),
  y = c(1.8, 1.5, 1, 3, 3, 1.8, 2.9, .3, 2, 0, -.8, -1.4, -1, -.9, 0, .4, .5, .2, .8, 0, -0.8, -.2, -1.1),
  grp = factor(c(rep(1, 10), rep(2, 13)))
)
ggplot(df, aes(x = x, y = y, color = grp)) +
  geom_point() +
  geom_abline(intercept = -1, slope = 1.25) +
  geom_abline(intercept = -0.8, slope = 1.25, linetype = 3) +
  geom_abline(intercept = -1.2, slope = 1.25, linetype = 3) +
  theme(legend.position = "top") +
  labs(color = "class", x = "", y = "")
```

It should be obvious that in most cases a separating hyperplane does not exist.  In fact, even if a separating hyperplane does exist, it may not be desirable, such as when the resulting margin is extremely narrow. 

## Support Vector Classifier

The maximal margin classifier can be generalized to non-separable cases using a so-called "soft margin".  The generalization is called the *support vector classifier*.  The soft margin allows some misclassification in the interest of greater robustness to individual observations.

The support vector classifier optimizes 

$$y_i (\beta_0 + \beta_1X_1 + \beta_2X_2 \dots + \beta_pX_p = 0) \ge M(1 - \epsilon_i)$$

where the $\epsilon_i$ are *slack variables* which sum to some constant tuning parameter $\sum{\epsilon_i} = C$.  The slack variable values indicate where the observation lies.  $\epsilon_i = 0$ means the observation is on the correct side of the margin.  $\epsilon_i > 0$ means the observation is on the wrong side of the margin.  $\epsilon_i > 1$ means the observation is on the wrong side of the hyperplane.  $C$ sets the tolerance for margin violation.  If $C = 0$, then all observations must reside on the correct side of the margin, as in the maximal margin classifier.  Notice that $C$ is a also an upper bound on the number of observations that violate the hyperplane.  $C$ controls the bias-variance trade-off.  As $C$ increases, the margin widens allows more violations.  The classifier bias increases but its variance decreases. 

The optimization problem has an interesting property in that only observations that either lie on the margin or that violate the margin affect the hyperplane.  These observations are known as support vectors.  As $C$ increases, the number of violating observations increase, and thus the number of support vectors increases.  This property makes the algorith robust to the extreme observations far away from the hyperplane.

The only shortcoming with the algorithm is that 

The Support Vector Machine (SVM) algorithm finds the optimal hyperplane that classifies the data points.  The optimal hyperplane is the one which maximizes the margin betweeen the data points and the hyperplane.  Support vectors are data points close to the hyperplane and influence the position and orientation of the hyperplane.

Maximize the margin with the hinge loss function:

$$c(x, y, f(x)) = \begin{cases} 
0, & \mbox{if } y \cdot f(x)>1 \\ 
1-y \cdot f(x), & \mbox{else} 
\end{cases}$$

dfd

$$f(n) = \begin{cases} n/2, & \mbox{if } n\mbox{ is even} \\ 3n+1, & \mbox{if } n\mbox{ is odd} \end{cases}$$

A **separable** dataset is one in which the classes do not overlap, so the classes can be separated by a decision boundary. The **maximal margin separator** is the decision boundary that is furthest from both classes. It is located at the mean of the relevant extreme points from each class. 

The **kernal** is the type of decision boundary (linear, polynomial, etc.).

```{r}
library(tidyverse)
df <- data.frame(
  samp = 1:25,
  sugar = c(10.9, 10.9, 10.6, 10, 8, 
            8.2, 8.6, 10.9, 10.7, 8,
            7.7, 7.8, 8.4, 11.5, 11.2,
            8.9, 8.7, 7.4, 10.9, 10, 
            11.4, 10.8, 8.5, 8.2, 10.6)
)

ggplot(df, aes(x = sugar)) +
  geom_point(aes(y = 0))
```

The **e1071** library implements the SVM algorithm.  The following function builds a linear SVM classifier.

```{r}
library(e1071)

svm_model <- svm(samp ~ .,
                 df,
                 type = "C-classification",
                 kernel = "linear",
                 scale = FALSE)

print(svm_model)
```

Build the weight vector from the coeficients and sv elements in the model.

J
```{r}
w <- t(svm_model$coefs) %*% svm_model$SV
slope_1 <- w[1]/w[2]
intercept_1 <- svm_model$rho/w[2]
```
