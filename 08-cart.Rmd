# Decision Trees

Decision trees, also known as classification and regression tree (CART) models, are tree-based methods for supervised machine learning.  Simple *classification trees* and *regression trees* are easy to use and interpret, but are not competitive with the best machine learning methods. However, they form the foundation for **bagged trees**, **random forests**, and **boosted trees** models, which although less interpretable, are very accurate.

CART models segment the predictor space into $K$ non-overlapping terminal nodes (leaves), $A_1, A_2, \dots, A_K$.  Each node is described by a set of rules which can be used to predict new responses. The predicted value $\hat{y}$ for each node is the mode (classification), or mean (regression).

CART models define the nodes through a *top-down greedy* process called *recursive binary splitting*. The process is *top-down* because it begins at the top of the tree with all observations in a single region and successively splits the predictor space. It is *greedy* because at each splitting step, the best split is made at that particular step without consideration to subsequent splits.

The best split is the predictor variable and cutpoint that minimizes a cost function. For a regression tree, the most common cost function is the sum of squared residuals, 

$$RSS = \sum_{k=1}^K\sum_{i \in A_k}{\left(y_i - \hat{y}_{A_k} \right)^2}.$$

For a classification tree, the most common cost functions are the Gini index, 

$$G = \sum_{c=1}^C{\hat{p}_{kc}(1 - \hat{p}_{kc})},$$

or the entropy 

$$D = - \sum_{c=1}^C{\hat{p}_{kc} \log \hat{p}_{kc}}$$

where $\hat{p}_{kc}$ is the proportion of training observations in node $k$ node that are class $c$.  A completely *pure* node in a binary tree will have $\hat{p} \in [0, 1]$ and $G = D = 0$. A completely impure node in a binary tree will have $\hat{p} = 0.5$ and $G = 0.5^2 \cdot 2 = 0.25$ and $D = -(0.5 \log(0.5)) \cdot 2 = 0.69$.

CART repeats the splitting process for each of the child nodes until a *stopping criterion* is satisfied, usually when no node size surpasses a predefined maximum, or continued splitting does not improve the model significantly.  CART may also impose a minimum number of observations in each node.

The resulting tree likely over-fits the training data and therefore does not generalize well to test data, so CART *prunes* the tree, minimizing the cross-validated prediction error. Rather than cross-validating every possible subtree to find the one with minimum error, CART uses *cost-complexity pruning*. Cost-complexity is the tradeoff between error (cost) and tree size (complexity) where the tradeoff is quantified with cost-complexity parameter $\alpha$.  In the equation below, the cost complexity of the tree $R_\alpha(T)$ is the sum of its risk (error) plus a factor $\alpha$ of the tree size $|T|$.  

$$R_\alpha(T) = R(T) + \alpha|T|$$

The factor is the cost complexity parameter (CP). There are a unique set of CP values that minimize $R_\alpha(T)$ [@James2013] [@Therneau2019].  *Read more on pages 12-13 of the [rpart vignette](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf) [@Therneau2019]*.  Cost-complexity prunity minimizes the cross-validated error over the range of complexity parameters.


## Classification Tree

A simple classification tree is rarely performed on its own; the bagged, random forest, and gradient boosting methods build on this logic.  However, it is good to start here to build understanding. I'll learn by example. Using the `ISLR::OJ` data set, I will predict which brand of orange juice, Citrus Hill (CH) or Minute Maid = (MM), customers `Purchase` using from the 17 feature variables.  Load the libraries and data.

```{r warning=FALSE, message=FALSE}
library(ISLR)  # OJ dataset
library(rpart)  # classification and regression trees 
library(caret)  # modeling workflow
library(rpart.plot)  # better formatted plots than the ones in rpart
library(plotROC)  # ROC curves
library(tidyverse)
library(skimr)  # neat alternative to glance & summary

oj_dat <- OJ
skim_with(numeric = list(p0 = NULL, p25 = NULL, p50 = NULL, p75 = NULL, 
                                p100 = NULL, hist = NULL))
skim(oj_dat)
```

I'll split `oj_dat` (n = 1,070) into `oj_train` (80%, n = 857) and `oj_test` (20%, n = 213).  I'll fit a simple decision tree with `oj_train`, then later a bagged tree, a random forest, and a gradient boosting tree.  I'll compare their predictive performance with `oj_test`. 

```{r}
set.seed(12345)
partition <- createDataPartition(y = oj_dat$Purchase, p = 0.8, list = FALSE)
oj_train <- oj_dat[partition, ]
oj_test <- oj_dat[-partition, ]
```

Function `rpart::rpart()` builds a full tree, minimizing the Gini index $G$ by default (`parms = list(split = "gini")`), until the stopping criterion is satisfied.  The default stopping criterion is 

* only attempt a split if the current node as at least `minsplit = 20` observations,
* only accept a split if each of the two resulting nodes have at least `minbucket = round(minsplit/3)` observations, and 
* only accept a split if the resulting overall fit improves by `cp = 0.01` (i.e., $\Delta G <= 0.01$).

```{r}
set.seed(123)
oj_model_1 <- rpart(
   formula = Purchase ~ .,
   data = oj_train,
   method = "class"  # "class" for classification, "anova" for regression
   )
print(oj_model_1)
```

The output starts with the root node.  The predicted class at the root is `CH` and this prediction produces 334 errors on the 857 observations for a success rate of 0.61026838 and an error rate of 0.38973162.  The child nodes of node "x" are labeled 2x) and 2x+1), so the child nodes of 1) are 2) and 3), and the child nodes of 2) are 4) and 5).  Terminal nodes are labeled with an asterisk (*).  

Surprisingly, only 3 of the 17 features were used the in full tree: `LoyalCH` (Customer brand loyalty for CH), `PriceDiff` (relative price of MM over CH), and `SalePriceMM` (absolute price of MM).  The first split is at `LoyalCH` = 0.48285.  Here is what the full (unpruned) tree looks like.

```{r}
rpart.plot(oj_model_1, yesno = TRUE)
```

The boxes show the node classification (based on mode), the proportion of observations that are *not* `CH`, and the proportion of observations included in the node. 

`rpart()` not only grew the full tree, it also used cross-validation to test the performance of the possible complexity hyperparameters. `printcp()` displays the candidate $\alpha$ values. You can use this table to decide how to prune the tree.

```{r}
printcp(oj_model_1)
```

There are only 4 possible $\alpha$ values in this model.  The model with the smallest complexity parameter allows the most splits (`nsplit`).  The highest complexity parameter corresponds to a tree with just a root node.  `rel error` is the error rate relative to the root node.  The root node absolute error is 0.38973162 (the proportion of MM), so its `rel error` is 0.38973162/0.38973162 = 1.0.  That means the absolute error of the full tree (at CP = 0.01) is 0.42814 * 0.38973162 = 0.1669.  You can vefify that by evaluating the predicted values:

```{r}
data.frame(pred = predict(oj_model_1, newdata = oj_train, type = "class")) %>%
   mutate(obs = oj_train$Purchase,
          err = if_else(pred != obs, 1, 0)) %>%
   summarize(mean_err = mean(err))
```

Finishing the CP table tour, `xerror` is the cross-validated error rate and `xstd` is its standard error. If you want the lowest possible error, then prune to the tree with the smallest relative error (`xerror`).  If you want to balance predictive power with simplicity, prune to the smallest tree within 1 SE of the one with the smallest relative error.  The CP table is not super-helpful for finding that tree. I'll add a column to find it.

```{r}
oj_model_1$cptable %>%
   data.frame() %>%
   mutate(min_xerror_idx = which.min(oj_model_1$cptable[, "xerror"]),
          rownum = row_number(),
          xerror_cap = oj_model_1$cptable[min_xerror_idx, "xerror"] + 
             oj_model_1$cptable[min_xerror_idx, "xstd"],
          eval = case_when(rownum == min_xerror_idx ~ "min xerror",
                           xerror < xerror_cap ~ "under cap",
                           TRUE ~ "")) %>%
   select(-rownum, -min_xerror_idx) 
```

Okay, so the simplest tree is the one with CP = 0.01347305.  Fortunately, `plotcp()` presents a nice graphical representation of the relationship between `xerror` and `cp`.

```{r}
plotcp(oj_model_1, upper = "splits")
```

The dashed line is set at the minimum `xerror` + `xstd`.  The top axis shows the number of splits in the tree.  I'm not sure why the CP values are not the same as in the table (they are close, but not the same).  The figure suggests I should prune to CP = 0.012 or CP = 0.021.  I see this curve never really hits a minimum - it is still decreasing at CP = 0.012.  The default parameter value `cp = 0.01` is probably too small, so I'll set it to `cp = 0.001` and start over.

```{r}
set.seed(123)
oj_model_1b <- rpart(
   formula = Purchase ~ .,
   data = oj_train,
   method = "class",
   cp = 0.001
   )
print(oj_model_1b)
```

This is a much larger tree.  Did I find a `cp` value that produces a local min?

```{r}
plotcp(oj_model_1b, upper = "splits")
```

Yes, the min is at CP = 0.011 with 5 splits.  The min + 1 SE is at CP = 0.021 with 3 splits.  I'll prune the tree to 3 splits.

```{r}
oj_model_1b_pruned <- prune(
   oj_model_1b,
   cp = oj_model_1b$cptable[oj_model_1b$cptable[, 2] == 3, "CP"]
)
rpart.plot(oj_model_1b_pruned, yesno = TRUE)
```

The most "important" indicator of `Purchase` is `LoyalCH`.  From the **rpart** [vignette](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf) (page 12), 

> "An overall measure of variable importance is the sum of the goodness of split measures for each split for which it was the primary variable, plus goodness * (adjusted agreement) for all splits in which it was a surrogate."

I don't know what that means, but here are the importance values from my model anyway. There is no plot for this in `rpart` that I'm aware of, so I'll make my own.

```{r}
oj_model_1b_pruned$variable.importance %>% 
   data.frame() %>%
   rownames_to_column(var = "Feature") %>%
   rename(Overall = '.') %>%
   ggplot(aes(x = fct_reorder(Feature, Overall), y = Overall)) +
   geom_pointrange(aes(ymin = 0, ymax = Overall), color = "cadetblue", size = .3) +
   theme_minimal() +
   coord_flip() +
   labs(x = "", y = "", title = "Variable Importance with Simple Classication")
```

There are a lot of variables with importance > 0 that are not in the pruned model. In fact, they are not even the same variables as in the unpruned model.  I am confused.  

The last step is to make predictions on the validation data set.  For a classification tree, set argument `type = "class"`. 

```{r message=FALSE, warning=FALSE}
oj_model_1b_preds <- predict(oj_model_1b_pruned, oj_test, type = "class")
```

I'll evaluate the predictions and record the accuracy (correct classification percentage) for comparison to other models. Two ways to evaluate the model are the confusion matrix, and the ROC curve. 


### Confusion Matrix

Print the confusion matrix with `caret::confusionMatrix()` to see how well does this model performs against the test data set.

```{r}
oj_model_1b_cm <- confusionMatrix(data = oj_model_1b_preds, reference = oj_test$Purchase)
oj_model_1b_cm
```

The confusion matrix is at the top.  It also includes a lot of statistics.  It's worth getting familiar with the stats.  The model accuracy and 95% CI are calculated from the binomial test.

```{r}
binom.test(x = 113 + 70, n = 213)
```

The "No Information Rate" (NIR) statistic is the class rate for the largest class.  In this case CH is the largest class, so NIR = 130/213 = 0.6103.  "P-Value [Acc > NIR]" is the binomial test that the model accuracy is significantly better than the NIR (i.e., significantly better than just always guessing CH).

```{r}
binom.test(x = 113 + 70, n = 213, p = 130/213, alternative = "greater")
```

The "Accuracy" statistic indicates the model predicts 0.8590 of the observations correctly.  That's good, but less impressive when you consider the prevalence of CH is 0.6103 - you could achieve 61% accuracy just by predicting CH every time. A measure that controls for the prevalence is Cohen's kappa statistic. The kappa statistic is explained [here](https://standardwisdom.com/softwarejournal/2011/12/confusion-matrix-another-single-value-metric-kappa-statistic/). It compares the accuracy to the accuracy of a "random system".  It is defined as

$$\kappa = \frac{Acc - RA}{1-RA}$$

where 

$$RA = \frac{ActFalse \times PredFalse + ActTrue \times PredTrue}{Total \times Total}$$

is the hypotheical probability of a chance agreement.  ActFalse will be the number of "MM" (13 + 70 = 83) and actual true will be the number of "CH" (113 + 17 = 130).  The predicted counts are

```{r}
table(oj_model_1b_preds)
```

So, $RA = (83*87 + 130*126) / 213^2 = 0.5202$ and $\kappa = (0.8592 - 0.5202)/(1 - 0.5202) = 0.7064$.  The kappa statistic varies from 0 to 1 where 0 means accurate predictions occur merely by chance, and 1 means the predictions are in perfect agreement with the observations.  In this case, a kappa statistic of 0.7064 is "substantial".  See chart [here](https://www.statisticshowto.datasciencecentral.com/cohens-kappa-statistic/).

The other measures from the `confusionMatrix()` output are various proportions and you can remind yourself of their definitions in the documentation with `?confusionMatrix`.

Visuals are almost always helpful.  Here is a plot of the confusion matrix.

```{r}
plot(oj_test$Purchase, oj_model_1b_preds, 
     main = "Simple Classification: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")
```

By the way, how does the validation set accuracy ()

```{r}
oj_model_1b_train_preds <- predict(oj_model_1b_pruned, oj_train, type = "class")
oj_model_1b_train_cm <- confusionMatrix(data = oj_model_1b_train_preds, reference = oj_train$Purchase)
oj_model_1b_train_cm$overall
```

The accuracy on the training data set was a little lower than on the test data set.  I though it would be higher, not lower.


### ROC Curve

Another measure of accuracy is the ROC (receiver operating characteristics) curve [@Fawcett2005].  The ROC curve is a plot of the true positive rate (TPR, sensitivity) versus the false positive rate (FPR, 1 - specificity) for a set of thresholds. By default, the threshold for predicting the default classification is 0.50, but it could be any threshold.  The ROC curves varies the thresholds.  (I'll use the `geom_roc` geom from **plotROC**.

```{r}
data.frame(M = predict(oj_model_1b_pruned, oj_test, "prob")[, 1],
           D = if_else(oj_test$Purchase == "CH", 1, 0)) %>%
   ggplot() + 
   geom_roc(aes(m = M, d = D), hjust = -0.4, vjust = 1.5, linealpha = 0.6, labelsize = 3, n.cuts = 10) + 
   geom_abline(intercept = 0, slope = 1, linetype = 2) +
   coord_equal() +
   theme_minimal() +
   labs(x = "FPR", y = "TPR",
        title = "Model 1b ROC Curve",
        subtitle = "Pruned model using rpart",
        caption = "Data: ISLM OJ data set.")

```

You can also use `prediction()` and `plot.prediction()` from the **ROCR** package. 

```{r}
pred <- prediction(predict(oj_model_1b_pruned, newdata = oj_test, type = "prob")[, 2], oj_test$Purchase)
plot(performance(pred, "tpr", "fpr"))
abline(0, 1, lty = 2)
```

Hmm, not quite the same...

A few points on the ROC space are helpful for understanding how to use it.  

* The lower left point (0, 0) is the result of *always* predicting "negative" or in this case "MM" if "CH" is taken as the default class.  Sure, your false positive rate is zero, but since you never predict a positive, your true positive rate is also zero.  
* The upper right point (1, 1) is the results of *always* predicting "positive" (or "CH" here).  You catch all the positives, but you miss all the negatives.
* The upper left point (0, 1) is the result of perfect accuracy.  You catch all the positives and all the negatives.
* The lower right point (1, 0) is the result of perfect imbecility.  You made the exact wrong prediction every time. 
* The 45 degree diagonal is the result of randomly guessing positive (CH) X percent of the time.  If you guess positive 90% of the time and the prevalence is 50%, your TPR will be 90% and your FPR will also be 90%, etc.

From the last bullet, it is evident that any point below and to the right of the 45 degree diagonal represents an instance where the model would have been better off just predicting entirely one way or the other.  The goal is for all nodes to bunch up in the upper left.

Points to the left of the diagonal with a low TPR can be thought of as "conservative" predicters - they only make positive (CH) predictions with strong evidence.  Points to the left of the diagnonal with a high TPR can be thought of as "liberal" predicters - they make positive (CH) predictions with weak evidence.  

### Caret Approach

I can also fit the model with `caret::train()`.  There are two ways to tune hyperparameters in `train()`: 

* set the number of tuning parameter values to consider by setting `tuneLength`, or
* set particular values to consider for each parameter by defining a `tuneGrid`.

If you don't have any idea what the tuning parameter ought to look like, use `tuneLength` to get close, then fine-tune with `tuneGrid`.  That's what I'll do.  I'll build the model using 10-fold cross-validation to optimize the hyperparameter, $\alpha$. I'll create a training control object that I can re-use in other model builds.  

```{r}
oj_trControl = trainControl(
   method = "cv",  # k-fold cross validation
   number = 10,  # 10 folds
   savePredictions = "final",       # save predictions for the optimal tuning parameter
   classProbs = TRUE  # return class probabilities in addition to predicted values
#   summaryFunction = twoClassSummary  # computes sensitivity, specificity and the area under the ROC curve.
   )
```


```{r}
oj_model_2 = train(
   Purchase ~ ., 
   data = oj_train, 
   method = "rpart",
   tuneLength = 5,  # choose up to 5 combinations of tuning parameters (cp)
   metric = "Accuracy",  # evaluate hyperparamter combinations with ROC
   trControl = oj_trControl
   )
```

`caret` built a full tree using `rpart`'s default parameters: gini splitting index, at least 20 observations in a node in order to consider splitting it, and at least 6 observations in each node.  Caret then calculated the area under the ROC curve for each candidate value of $\alpha$.  Here is the results.

```{r}
print(oj_model_2)
```

So the smallest value of `cp` produced the highest area under the ROC curve.  Here are the rules in the final model.  

```{r}
oj_model_2$finalModel
```

Here is the tree.

```{r}
rpart.plot(oj_model_2$finalModel)
```

Here is the ROC curve.

```{r}
library(plotROC)
ggplot(oj_model_2$pred) + 
    geom_roc(
       aes(
          m = MM, 
          d = factor(obs, levels = c("CH", "MM"))
       ),
       hjust = -0.4, vjust = 1.5
    ) +
   coord_equal()
```

Here are the AUCs for each all candidate $\alpha$ values.

```{r}
plot(oj_model_2)
```
 
It sure looks like the best value of $\alpha$ was near zero.  Evaluate the model by making predictions with the test data set.  

```{r}
oj_model_2_preds <- predict(oj_model_2, oj_test, type = "raw")
```

The confusion matrix shows the true positives and true negatives.

```{r}
oj_model_2_cm <- confusionMatrix(
   data = oj_model_2_preds, 
   reference = oj_test$Purchase
)
oj_model_2_cm
```

The accuracy metric is the slightly worse than in my previous model.  Here is a graphical representation of the confusion matrix.

```{r}
plot(oj_test$Purchase, oj_model_2_preds, 
     main = "Simple Classification: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")
```

Finally, here is the variable importance plot.

```{r}
plot(varImp(oj_model_2), main="Variable Importance with Simple Classication")
```

I could have drilled into the best value of $\alpha$ using a tuning grid.  I'll try that now.

```{r}
set.seed(1234)
oj_model_3 = train(
   Purchase ~ ., 
   data = oj_train, 
   method = "rpart",
   tuneGrid = expand.grid(cp = seq(from = 0.001, to = 0.010, length = 10)),  
   metric='ROC',  # evaluate hyperparamter combinations with ROC
   trControl = oj_trControl
   )
```

I tweaked the grid searching for the min.
```{r}
plot(oj_model_3)
```

The ROC curve is maximized at 0.005.  You can see that in the model summary.

```{r}
print(oj_model_3)
```

Now make predictions.

```{r}
oj_model_3_preds <- predict(oj_model_3, oj_test, type = "raw")
oj_model_3_cm <- confusionMatrix(data = oj_model_3_preds, reference = oj_test$Purchase)
oj_model_3_cm$overall["Accuracy"]
```

The final model is 

```{r}
rpart.plot(oj_model_3$finalModel)
```

```{r}
plot(varImp(oj_model_3), main="Variable Importance with Simple Classication")
```

Looks like the manual effort faired best.  Here is a summary the accuracy rates of the three models.

```{r}
rbind(data.frame(model = "Manual Class", Acc = round(oj_model_1b_cm$overall["Accuracy"], 5)), 
      data.frame(model = "Caret w/tuneLength", Acc = round(oj_model_2_cm$overall["Accuracy"], 5)),
      data.frame(model = "Caret w/tuneGrid", Acc = round(oj_model_3_cm$overall["Accuracy"], 5))
)
```


## Regression Trees

A simple regression tree is built the same way as a simple classificatioon tree.  Like the simple classification tree, it is rarely invoked on its own; the bagged, random forest, and gradient boosting methods build on this logic.

#### Example

Using the `Carseats` data set from `ISLR`, I'll predict `Sales` using the available feature variables.  

```{r}
carseats_dat <- Carseats
skim_to_wide(carseats_dat)
partition <- createDataPartition(y = carseats_dat$Sales, p = 0.8, list = FALSE)
carseats.train <- carseats_dat[partition, ]
carseats.test <- carseats_dat[-partition, ]
rm(partition)
```

The first step is to build a full tree, then perform k-fold cross-validation to help select the optimal cost complexity ($cp$).  The only difference here is the `rpart()` parameter `method = "anova"` to produce a regression tree.

```{r}
set.seed(1234)
# Specify model = TRUE to handle plotting splits with factor variables.
carseats.full_anova <- rpart(formula = Sales ~ .,
                             data = carseats.train,
                             method = "anova", 
                             xval = 10,
                             model = TRUE)
rpart.plot(carseats.full_anova, yesno = TRUE)
printcp(carseats.full_anova)
```

The algorithm included `r length(carseats.full_anova$variable.importance)` variables in the full tree.  

The second step is to prune the tree to avoid overfitting.  The CP table shows the relavent statistics to choose the appropriate pruning paramter.  The CP table is included in the model summary.  The `rel error` column is the error rate / root node error produced when pruning the tree using complexity parameter `CP` to `nsplits` splits. The `xerror` column shows the error rate.  A plot of `xerror` vs `cp` shows the relationship.

```{r}
plotcp(carseats.full_anova)
```

In this case, the smallest relative error is at `r carseats.full_anova$cptable[which.min(carseats.full_anova$cptable[, "xerror"]), "CP"]`, but the maximum CP below the dashed line (one standard deviation above the mimimum error) is at `cp ~ .039`.Use the `prune()` function to prune the tree by specifying the associated cost-complexity `cp`.  

```{r}
carseats.anova <- prune(carseats.full_anova, 
                        cp = 0.039)
rpart.plot(carseats.anova, yesno = TRUE)
rm(carseats.full_anova)
```

The pruned tree has `r length(carseats.anova$variable.importance)` variables.  The most important indicator of `Sales` is shelving location.  

The third and last step is to make predictions on the validation data set and record the root mean squared error (RMSE) for comparison to other models.  The root mean squared error ($RMSE = \sqrt{(1/2) \sum{(actual - pred)^2}})$ and mean absolute error ($MAE = (1/n) \sum{|actual - pred|}$) are the two most common measures of predictive accuracy.  The key difference is that RMSE punishes large errors more harshly. For a regression tree, set argument `type = "vector"` (or do not specify at all). 

```{r message=FALSE, warning=FALSE}
carseats.anova.pred <- predict(carseats.anova, carseats.test, type = "vector")
plot(carseats.test$Sales, carseats.anova.pred, 
     main = "Simple Regression: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")
abline(0, 1)

(carseats.anova.rmse <- RMSE(pred = carseats.anova.pred,
                            obs = carseats.test$Sales))
rm(carseats.anova.pred)
```

The pruning process leads to an average prediction error of `r round(carseats.anova.rmse, 3)` in the test data set.  Not too bad considering the standard deviation of `Sales` is `r round(sd(carseats.test$Sales), 3)`.

All of this can happen more or less automatically with the `caret::train()` function, specifying `method = "rpart"` and specifying either `tuneLength` or `tuneGrid`.

I'll do this with `tuneLength` first.

```{r}
carseats.anova2 = train(Sales ~ ., 
                    data = carseats.train, 
                    method = "rpart",  # for classification tree
                    tuneLength = 5,  # choose up to 5 combinations of tuning parameters (cp)
                    metric = "RMSE",  # evaluate hyperparamter combinations with RMSE
                    trControl = trainControl(
                      method = "cv",  # k-fold cross validation
                      number = 10,  # 10 folds
                      savePredictions = "final"       # save predictions for the optimal tuning parameter
                      )
                    )
carseats.anova2
plot(carseats.anova2)
carseats.anova.pred <- predict(carseats.anova2, carseats.test, type = "raw")
plot(carseats.test$Sales, carseats.anova.pred, 
     main = "Simple Regression: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")

(carseats.anova.rmse2 <- RMSE(pred = carseats.anova.pred,
                             obs = carseats.test$Sales))
rm(oj.class.pred)
rpart.plot(carseats.anova2$finalModel)
plot(varImp(carseats.anova2), main="Variable Importance with Simple Regression")
```

Now with `tuneGrid`.

```{r}
myGrid <-  expand.grid(cp = (0:2)/10)
carseats.anova3 = train(Sales ~ ., 
                    data = carseats.train, 
                    method = "rpart",  # for classification tree
                    tuneGrid = myGrid,  # choose up to 5 combinations of tuning parameters (cp)
                    metric = "RMSE",  # evaluate hyperparamter combinations with RMSE
                    trControl = trainControl(
                      method = "cv",  # k-fold cross validation
                      number = 10,  # 10 folds
                      savePredictions = "final"       # save predictions for the optimal tuning parameter
                      )
                    )
carseats.anova3
plot(carseats.anova3)
carseats.anova.pred <- predict(carseats.anova3, carseats.test, type = "raw")
plot(carseats.test$Sales, carseats.anova.pred, 
     main = "Simple Regression: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")

(carseats.anova.rmse3 <- RMSE(pred = carseats.anova.pred,
                             obs = carseats.test$Sales))
rm(carseats.anova.pred)
rpart.plot(carseats.anova3$finalModel)
plot(varImp(carseats.anova3), main="Variable Importance with Simple Regression")
```

Looks like the manual effort faired best again.  Here is a summary the RMSE values of the three models.

```{r}
rbind(data.frame(model = "Manual ANOVA", 
                 RMSE = round(carseats.anova.rmse, 5)), 
      data.frame(model = "Caret w/tuneLength", 
                 RMSE = round(carseats.anova.rmse2, 5)),
      data.frame(model = "Caret w.tuneGrid", 
                 RMSE = round(carseats.anova.rmse3, 5))
)
```


## Bagging

Bootstrap aggregation, or *bagging*, is a general-purpose procedure for reducing the variance of a statistical learning method.  The algorithm constructs *B* regression trees using *B* bootstrapped training sets, and averages the resulting predictions. These trees are grown deep, and are not pruned. Hence each individual tree has high variance, but low bias. Averaging these *B* trees reduces the variance.  For classification trees, bagging takes the "majority vote" for the prediction.  Use a value of *B* sufficiently large that the error has settled down.

To test the model accuracy, the out-of-bag observations are predicted from the models that do not use them.  If B/3 of observations are in-bag, there are *B/3* predictions per observation.  These predictions are averaged for the test prediction.  Again, for classification trees, a majority vote is taken.

The downside to bagging is that it improves accuracy at the expense of interpretability.  There is no longer a single tree to interpret, so it is no longer clear which variables are more important than others.  

Bagged trees are a special case of random forests, so see the next section for an example.


## Random Forests

Random forests improve bagged trees by way of a small tweak that de-correlates the trees.  As in bagging, the algorithm builds a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of *mtry* predictors is chosen as split candidates from the full set of *p* predictors.  A fresh sample of *mtry* predictors is taken at each split.  Typically $mtry \sim \sqrt{b}$.  Bagged trees are thus a special case of random forests where *mtry = p*.


#### Bagging Classification Example

Again using the `OJ` data set to predict `Purchase`, this time I'll use the bagging method by specifying `method = "treebag"`.  I'll use `tuneLength = 5` and not worry about `tuneGrid` anymore.  Caret has no hyperparameters to tune with this model. 

```{r}
oj.bag = train(Purchase ~ ., 
               data = oj_train, 
               method = "treebag",  # for bagging
               tuneLength = 5,  # choose up to 5 combinations of tuning parameters
               metric = "ROC",  # evaluate hyperparamter combinations with ROC
               trControl = trainControl(
                 method = "cv",  # k-fold cross validation
                 number = 10,  # k=10 folds
                 savePredictions = "final",       # save predictions for the optimal tuning parameters
                      classProbs = TRUE,  # return class probabilities in addition to predicted values
                      summaryFunction = twoClassSummary  # for binary response variable
                      )
                    )
oj.bag
#plot(oj.bag$)
oj.pred <- predict(oj.bag, oj_test, type = "raw")
plot(oj_test$Purchase, oj.pred, 
     main = "Bagging Classification: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")

(oj.conf <- confusionMatrix(data = oj.pred, 
                            reference = oj_test$Purchase))
oj.bag.acc <- as.numeric(oj.conf$overall[1])
rm(oj.pred)
rm(oj.conf)
#plot(oj.bag$, oj.bag$finalModel$y)
plot(varImp(oj.bag), main="Variable Importance with Simple Classication")
```

#### Random Forest Classification Example

Now I'll try it with the random forest method by specifying `method = "ranger"`.  I'll stick with `tuneLength = 5`.  Caret tunes three hyperparameters: 

* `mtry`: number of randomly selected predictors.  Default is sqrt(p).
* `splitrule`: splitting rule.  For classification, options are "gini" (default) and "extratrees".
* `min.node.size`: minimal node size.  Default is 1 for classification.

```{r}
oj.frst = train(Purchase ~ ., 
               data = oj_train, 
               method = "ranger",  # for random forest
               tuneLength = 5,  # choose up to 5 combinations of tuning parameters
               metric = "ROC",  # evaluate hyperparamter combinations with ROC
               trControl = trainControl(
                 method = "cv",  # k-fold cross validation
                 number = 10,  # 10 folds
                 savePredictions = "final",       # save predictions for the optimal tuning parameter1
                 classProbs = TRUE,  # return class probabilities in addition to predicted values
                 summaryFunction = twoClassSummary  # for binary response variable
                 )
               )
oj.frst
plot(oj.frst)
oj.pred <- predict(oj.frst, oj_test, type = "raw")
plot(oj_test$Purchase, oj.pred, 
     main = "Random Forest Classification: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")

(oj.conf <- confusionMatrix(data = oj.pred, 
                            reference = oj_test$Purchase))
oj.frst.acc <- as.numeric(oj.conf$overall[1])
rm(oj.pred)
rm(oj.conf)
#plot(oj.bag$, oj.bag$finalModel$y)
#plot(varImp(oj.frst), main="Variable Importance with Simple Classication")
```

The model algorithm explains *"ROC was used to select the optimal model using the largest value. The final values used for the model were mtry = 9, splitrule = extratrees and min.node.size = 1."*  You can see the results of tuning grid combinations in the associated plot of ROC AUC vs mtry grouped by splitting rule.

The bagging (accuracy = 0.80751) and random forest (accuracy = 0.81690) models faired pretty well, but the manual classification tree is still in first place.  There's still gradient boosting to investigate!

```{r}
rbind(data.frame(model = "Manual Class", Accuracy = round(oj_model_1b_cm$overall["Accuracy"], 5)), 
      data.frame(model = "Class w/tuneLength", Accuracy = round(oj_model_2_cm$overall["Accuracy"], 5)),
      data.frame(model = "Class w.tuneGrid", Accuracy = round(oj_model_3_cm$overall["Accuracy"], 5)),
      data.frame(model = "Bagging", Accuracy = round(oj.bag.acc, 5)),
      data.frame(model = "Random Forest", Accuracy = round(oj.frst.acc, 5))
) %>% arrange(desc(Accuracy))
```

#### Bagging Regression Example

Again using the `Carseats` data set to predict `Sales`, this time I'll use the bagging method by specifying `method = "treebag"`.  I'll use `tuneLength = 5` and not worry about `tuneGrid` anymore.  Caret has no hyperparameters to tune with this model. 

```{r}
carseats.bag = train(Sales ~ ., 
               data = carseats.train, 
               method = "treebag",  # for bagging
               tuneLength = 5,  # choose up to 5 combinations of tuning parameters
               metric = "RMSE",  # evaluate hyperparamter combinations with RMSE
               trControl = trainControl(
                 method = "cv",  # k-fold cross validation
                 number = 10,  # 10 folds
                 savePredictions = "final"       # save predictions for the optimal tuning parameter1
                 )
               )
carseats.bag
#plot(carseats.bag$finalModel)
carseats.pred <- predict(carseats.bag, carseats.test, type = "raw")
plot(carseats.test$Sales, carseats.pred, 
     main = "Bagging Regression: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")
abline(0, 1)

(carseats.bag.rmse <- RMSE(pred = carseats.pred,
                           obs = carseats.test$Sales))
rm(carseats.pred)
plot(varImp(carseats.bag), main="Variable Importance with Regression Bagging")
```


#### Random Forest Regression Example

Now I'll try it with the random forest method by specifying `method = "ranger"`.  I'll stick with `tuneLength = 5`.  Caret tunes three hyperparameters: 

* `mtry`: number of randomly selected predictors
* `splitrule`: splitting rule.  For regression, options are "variance" (default), "extratrees", and "maxstat". 
* `min.node.size`: minimal node size

```{r}
carseats.frst = train(Sales ~ ., 
               data = carseats.train, 
               method = "ranger",  # for random forest
               tuneLength = 5,  # choose up to 5 combinations of tuning parameters
               metric = "RMSE",  # evaluate hyperparamter combinations with RMSE
               trControl = trainControl(
                 method = "cv",  # k-fold cross validation
                 number = 10,  # 10 folds
                 savePredictions = "final"       # save predictions for the optimal tuning parameter1
                 )
               )
carseats.frst
plot(carseats.frst)
carseats.pred <- predict(carseats.frst, carseats.test, type = "raw")
plot(carseats.test$Sales, carseats.pred, 
     main = "Random Forest Regression: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")
abline(0, 1)
(carseats.frst.rmse <- RMSE(pred = carseats.pred,
                           obs = carseats.test$Sales))
rm(carseats.pred)
#plot(varImp(carseats.frst), main="Variable Importance with Regression Random Forest")
```

The model algorithm explains *"RMSE was used to select the optimal model using the smallest value. The final values used for the model were mtry = 11, splitrule = variance and min.node.size = 5."*  You can see the results of tuning grid combinations in the associated plot of ROC AUC vs mtry grouped by splitting rule.

The bagging and random forest models faired very well - they took over the first and second place!

```{r}
rbind(data.frame(model = "Manual ANOVA", RMSE = round(carseats.anova.rmse, 5)), 
      data.frame(model = "ANOVA w/tuneLength", RMSE = round(carseats.anova.rmse2, 5)),
      data.frame(model = "ANOVA w.tuneGrid", RMSE = round(carseats.anova.rmse3, 5)),
      data.frame(model = "Bagging", RMSE = round(carseats.bag.rmse, 5)),
      data.frame(model = "Random Forest", RMSE = round(carseats.frst.rmse, 5))
) %>% arrange(RMSE)
```


## Gradient Boosting

Boosting is a method to improve (boost) the week learners sequentially and increase the model accuracy with a combined model. There are several boosting algorithms.  One of the earliest was AdaBoost (adaptive boost).  A more recent innovation is gradient boosting.

Adaboost creates a single split tree (decision stump) then weights the observations by how well the initial tree performed, putting more weight on the difficult observations.  It then creates a second tree using the weights so that it focuses on the difficult observations.  Observations that are difficult to classify receive increasing larger weights until the algorithm identifies a model that correctly classifies them.  The final model returns predictions that are a majority vode. (*I think Adaboost applies only to classification problems, not regressions*). 
 
Gradient boosting generalizes the AdaBoost method, so that the object is to minimize a loss function.  In the case of classification problems, the loss function is the log-loss; for regression problems, the loss function is mean squared error.  The regression trees are addative, so that the successive models can be added together to correct the residuals in the earlier models.  Gradient boosting constructs its trees in a "greedy" manner, meaning it chooses the best splits based on purity scores like Gini or minimizing the loss.  It is common to constrain the weak learners by setting maximum tree size parameters.  Gradient boosting continues until it reaches maximum number of trees or an acceptible error level.  This can result in overfitting, so it is common to employ regularization methods that penalize aspects of the model.

**Tree Constraints**.  In general the more constrained the tree, the more trees need to be grown.  Parameters to optimize include number of trees, tree depth, number of nodes, minimmum observations per split, and minimum improvement to loss.

**Learning Rate**.  Each successive tree can be weighted to slow down the learning rate.  Decreasing the learning rate increases the number of required trees.  Common growth rates are 0.1 to 0.3.

The gradient boosting algorithm fits a shallow tree $T_1$ to the data, $M_1 = T_1$.  Then it fits a tree $T_2$ to the residuals and adds a weighted sum of the tree to the original tree as $M_2 = M_1 + \gamma T_2$.  For regularized boosting, include a learning rate factor $\eta \in (0..1)$, $M_2 = M_1 + \eta \gamma T_2$.  A larger $\eta$ produces faster learning, but risks overfitting.  The process repeats until the residuals are small enough, or until it reaches the maximum iterations.  Because overfitting is a risk, use cross-validation to select the appropriate number of trees (the number of trees producing the lowest RMSE).


#### Gradient Boosting Classification Example

Again using the `OJ` data set to predict `Purchase`, this time I'll use the gradient boosting method by specifying `method = "gbm"`.  I'll use `tuneLength = 5` and not worry about `tuneGrid` anymore.  Caret tunes the following  hyperparameters (see `modelLookup("gbm")`). 

* `n.trees`: number of boosting iterations
* `interaction.depth`: maximum tree depth
* `shrinkage`: shrinkage
* `n.minobsinnode`: mimimum terminal node size

```{r}
oj.gbm <- train(Purchase ~ ., 
               data = oj_train, 
               method = "gbm",  # for bagged tree
               tuneLength = 5,  # choose up to 5 combinations of tuning parameters
               metric = "ROC",  # evaluate hyperparamter combinations with ROC
               trControl = trainControl(
                 method = "cv",  # k-fold cross validation
                 number = 10,  # 10 folds
                 savePredictions = "final",       # save predictions for the optimal tuning parameter1
                      classProbs = TRUE,  # return class probabilities in addition to predicted values
                      summaryFunction = twoClassSummary  # for binary response variable
                      )
                    )
oj.gbm
plot(oj.gbm)
oj.pred <- predict(oj.gbm, oj_test, type = "raw")
plot(oj_test$Purchase, oj.pred, 
     main = "Gradient Boosing Classification: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")

(oj.conf <- confusionMatrix(data = oj.pred, 
                            reference = oj_test$Purchase))
oj.gbm.acc <- as.numeric(oj.conf$overall[1])
rm(oj.pred)
rm(oj.conf)
#plot(oj.bag$, oj.bag$finalModel$y)
#plot(varImp(oj.gbm), main="Variable Importance with Gradient Boosting")
```


#### Gradient Boosting Regression Example

Again using the `Carseats` data set to predict `Sales`, this time I'll use the gradient boosting method by specifying `method = "gbm"`.  I'll use `tuneLength = 5` and not worry about `tuneGrid` anymore.  Caret tunes the following  hyperparameters. 

* `n.trees`: number of boosting iterations (increasing `n.trees` reduces the error on training set, but may lead to over-fitting)
* `interaction.depth`: maximum tree depth (the default six - node tree appears to do an excellent job)
* `shrinkage`: learning rate (reduces the impact of each additional fitted base-learner (tree) by reducing the size of incremental steps and thus penalizes the importance of each consecutive iteration.  The intuition is that it is better to improve a model by taking many small steps than by taking fewer large steps. If one of the boosting iterations turns out to be erroneous, its negative impact can be easily corrected in subsequent steps.)
* `n.minobsinnode`: mimimum terminal node size

```{r}
carseats.gbm <- train(Sales ~ ., 
                      data = carseats.train, 
                      method = "gbm",  # for bagged tree
                      tuneLength = 5,  # choose up to 5 combinations of tuning parameters
                      metric = "RMSE",  # evaluate hyperparamter combinations with ROC
                      trControl = trainControl(
                        method = "cv",  # k-fold cross validation
                        number = 10,  # 10 folds
                        savePredictions = "final",       # save predictions for the optimal tuning parameter1
                        verboseIter = FALSE,
                        returnData = FALSE
                        )
                      )
carseats.gbm
plot(carseats.gbm)
carseats.pred <- predict(carseats.gbm, carseats.test, type = "raw")
plot(carseats.test$Sales, carseats.pred, 
     main = "Gradient Boosing Regression: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")
abline(0,1)

(carseats.gbm.rmse <- RMSE(pred = carseats.pred,
                           obs = carseats.test$Sales))
rm(carseats.pred)

#plot(varImp(carseats.gbm), main="Variable Importance with Gradient Boosting")
```

## Summary

Okay, I'm going to tally up the results!  For the classification division, the winner is the manual classification tree!  Gradient boosting made a valiant run at it, but came up just a little short.

```{r}
rbind(data.frame(model = "Manual Class", Acc = round(oj_model_1b_cm$overall["Accuracy"], 5)), 
      data.frame(model = "Class w/tuneLength", Acc = round(oj_model_2_cm$overall["Accuracy"], 5)),
      data.frame(model = "Class w.tuneGrid", Acc = round(oj_model_3_cm$overall["Accuracy"], 5)),
      data.frame(model = "Bagging", Acc = round(oj.bag.acc, 5)),
      data.frame(model = "Random Forest", Acc = round(oj.frst.acc, 5)),
      data.frame(model = "Gradient Boosting", Acc = round(oj.gbm.acc, 5))
) %>% arrange(desc(Acc))
```

And now for the regression division, the winnner is... gradient boosting!

```{r}
rbind(data.frame(model = "Manual ANOVA", RMSE = round(carseats.anova.rmse, 5)), 
      data.frame(model = "ANOVA w/tuneLength", RMSE = round(carseats.anova.rmse2, 5)),
      data.frame(model = "ANOVA w.tuneGrid", RMSE = round(carseats.anova.rmse3, 5)),
      data.frame(model = "Bagging", RMSE = round(carseats.bag.rmse, 5)),
      data.frame(model = "Random Forest", RMSE = round(carseats.frst.rmse, 5)),
      data.frame(model = "Gradient Boosting", RMSE = round(carseats.gbm.rmse, 5))
) %>% arrange(RMSE)
```


Here are plots of the ROC curves for all the models (one from each chapter) on the same graph. The ROCR package provides the prediction() and performance() functions which generate the data required for plotting the ROC curve, given a set of predictions and actual (true) values. 
The more "up and to the left" the ROC curve of a model is, the better the model. The AUC performance metric is literally the "Area Under the ROC Curve", so the greater the area under this curve, the higher the AUC, and the better-performing the model is.

```{r}
library(ROCR)
# List of predictions
oj.class.pred <- predict(oj_model_3, oj_test, type = "prob")[,2]
oj.bag.pred <- predict(oj.bag, oj_test, type = "prob")[,2]
oj.frst.pred <- predict(oj.frst, oj_test, type = "prob")[,2]
oj.gbm.pred <- predict(oj.gbm, oj_test, type = "prob")[,2]

preds_list <- list(oj.class.pred, oj.bag.pred, oj.frst.pred, oj.gbm.pred)
#preds_list <- list(oj.class.pred)

# List of actual values (same for all)
m <- length(preds_list)
actuals_list <- rep(list(oj_test$Purchase), m)

# Plot the ROC curves
pred <- prediction(preds_list, actuals_list)
#pred <- prediction(oj.class.pred[,2], oj_test$Purchase)
rocs <- performance(pred, "tpr", "fpr")
plot(rocs, col = as.list(1:m), main = "Test Set ROC Curves")
legend(x = "bottomright", 
       legend = c("Decision Tree", "Bagged Trees", "Random Forest", "GBM"),
       fill = 1:m)
```


# Reference

Penn State University, STAT 508: Applied Data Mining and Statistical Learning, "Lesson 11: Tree-based Methods". [https://newonlinecourses.science.psu.edu/stat508/lesson/11](https://newonlinecourses.science.psu.edu/stat508/lesson/11).

Brownlee, Jason. "Classification And Regression Trees for Machine Learning", Machine Learning Mastery.  [https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/](https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/). 

Brownlee, Jason. "A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning", Machine Learning Mastery. [https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/).

[DataCamp: Machine Learning with Tree-Based Models in R](https://campus.datacamp.com/courses/machine-learning-with-tree-based-models-in-r)

[An Introduction to Statistical Learning](http://faculty.marshall.usc.edu/gareth-james/ISL/) by Gareth James, et al.

[SAS Documentation](http://support.sas.com/documentation/cdl/en/stathpug/68163/HTML/default/viewer.htm#stathpug_hpsplit_details01.htm)

[StatMethods: Tree-Based Models](https://www.statmethods.net/advstats/cart.html)

[Machine Learning Plus](https://www.machinelearningplus.com/machine-learning/caret-package/)

[GBM (Boosted Models) Tuning Parameters](https://www.listendata.com/2015/07/gbm-boosted-models-tuning-parameters.html)  from Listen Data

[Harry Southworth](https://github.com/harrysouthworth/gbm/blob/master/demo/bernoulli.R) on GitHub

[Gradient Boosting Classification with GBM in R](https://www.datatechnotes.com/2018/03/classification-with-gradient-boosting.html) in DataTechNotes

Molnar, Christoph. "Interpretable machine learning. A Guide for Making Black Box Models Explainable", 2019. [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/).

