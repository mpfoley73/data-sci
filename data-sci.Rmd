--- 
title: "My Data Science Notes"
author: "Michael Foley"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output:
  bookdown::gitbook:
    lib_dir: assets
    split_by: section
    config:
      toolbar:
        position: static
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "This is a compendium of notes from classes, tutorials, etc. that I reference from time to time."
---
--- 
title: "My Data Science Notes"
author: "Michael Foley"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output:
  bookdown::gitbook:
    lib_dir: assets
    split_by: section
    config:
      toolbar:
        position: static
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "This is a compendium of notes from classes, tutorials, etc. that I reference from time to time."
---

# Intro {-}

These notes are pulled from various classes, tutorials, books, etc. and are intended for my own consumption.  If you are finding this on the internet, I hope it is useful to you, but you should know that I am just a student and there's a good chance whatever you're reading here is mistaken.  In fact, that should probably be your null hypothesis... or your prior.  Whatever.

<!--chapter:end:index.Rmd-->


# Probability {#probability}

Placeholder


## Principles

<!--chapter:end:01-probability.Rmd-->


# Discrete Distributions {#disc_dist}

Placeholder


## Bernoulli
## Binomial
## Poission
## Multinomial
## Negative-Binomial
#### Examples {-}
## Geometric
#### Examples {-}
## Hypergeometric
#### Example {-}
## Gamma
#### Examples {-}

<!--chapter:end:02-disc_dist.Rmd-->


# Continuous Distributions {#cont_dist}

Placeholder


## Normal
#### Example {-}
### Example
### Example
### Example
### Normal Approximation to Binomial
### Example
### Example
### From Sample to Population
## Join Distributions
## Likelihood

<!--chapter:end:03-cont_dist.Rmd-->


# Discrete Analysis {#discrete_analysis}

Placeholder


## Chi-Square Test
## One-Way Tables
### Chi-Square Goodness-of-Fit Test
### Proportion Test
## Two-Way Tables
### Chi-Square Independence Test
### Residuals Analysis
### Difference in Proportions
### Relative Risk
### Odds Ratio
### Partitioning Chi-Square
### Correlation
## Example of Chi-Square Test of Homogeneity

<!--chapter:end:04-disc_anal.Rmd-->

```{r include=FALSE}
library(tidyverse)
library(mfstylr)
```

# Continuous Variable Analysis {#continuous_analysis}


### Correlation

Correlation measures the strength and direction of association between two variables.  There are three common correlation tests: the Pearson product moment (Pearson's r), Spearman's rank-order (Spearman's rho), and Kendall's tau (Kendall's tau). 

Use the **Pearson's r** if both variables are quantitative (interval or ratio), normally distributed, and the relationship is linear with homoscedastic residuals.

The **Spearman's rho** and **Kendal's tao** correlations are [non-parametric](https://www.statisticshowto.datasciencecentral.com/parametric-and-non-parametric-data/) measures, so they are valid for both quantitative and ordinal variables and do not carry the normality and homoscedasticity conditions.  However, non-parametric tests have less statistical power than parametric tests, so only use these correlations if Pearson does not apply.


#### Pearson's r

Pearson's $r$ 

$$r = \frac{\sum{(X_i - \bar{X})(Y_i - \bar{Y})}}{\sqrt{\sum{(X_i - \bar{X})^2 \sum{(Y_i - \bar{Y})^2}}}} = \frac{cov(X,Y)}{s_X s_Y}$$

estimates the population correlation $\rho$.  Pearson's $r$ ranges from $-1$ (perfect negative linear relationship) to $+1$ (perfect positive linear relationship, and $r = 0$ when there is no linear relationship.  A correlation in the range $(.1, .3)$ is condidered small, $(.3, .5)$ medium, and $(.5, 1.0)$ large.

Pearson's $r$ only applies if the variables are interval or ratio, normally distributed, linearly related, there are minimal outliers, and the residuals are homoscedastic.

Test $H_0: \rho = 0$ with test statistic 

$$T = r \sqrt{\frac{n-2}{1-r^2}}.$$


The `nascard` data set consists of $n = 898$ races from 1975 - 2003.  

```{r message=FALSE, warning=FALSE, cache=TRUE}
nascard <- read.fwf(
  file = url("http://jse.amstat.org/datasets/nascard.dat.txt"),
  widths = c(5, 6, 4, 4, 4, 5, 9, 4, 11, 30),
  col.names = c('series_race', 'year', 'race_year', 'finish_pos', 'start_pos',
                'laps_comp', 'winnings', 'num_cars','car_make', 'driver')
)
nascard_sr1 <- nascard[nascard$series_race == 1,]
glimpse(nascard_sr1)
```

In race 1 of 1975 $(n = 35)$, what was the correlation between the driver's finishing position and prize?

Explore the relationship with a scatterplot.  As expected, there is a negative relationship between finish position and winnings, but it is non-linear.  However, 1/winnings may be linearly related to finish position. 

```{r message=FALSE, warning=FALSE, fig.height=3.5, fig.width=6.5}
p1 <- ggplot(data = nascard_sr1, aes(x = finish_pos, y = winnings)) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  theme_mf() +
  labs(title = "Pearson's Rho", subtitle = "")
p2 <- ggplot(data = nascard_sr1, aes(x = finish_pos, y = 1/winnings)) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  theme_mf() +
  labs(title = "Pearson's Rho", subtitle = "y = 1 / winnings")
gridExtra::grid.arrange(p1, p2, nrow = 1)
```

(*I tried several [variable tranformations](https://stattrek.com/regression/linear-transformation.aspx) and chose the one producing the highest $R^2$ that also had a normal distribution in each variable.)

```{r}
#summary(lm(winnings ~ finish_pos, data = nascard_sr1)) # r2 = .5474
#summary(lm(log(winnings) ~ finish_pos, data = nascard_sr1)) # r2 = .9015
nascard_lm <- lm(1/winnings ~ finish_pos, data = nascard_sr1) # r2 = .9509
#summary(lm(winnings ~ log(finish_pos), data = nascard_sr1)) # r2 = .883
#summary(lm(log(winnings) ~ log(finish_pos), data = nascard_sr1)) # r2 = .9766
#summary(lm(winnings ~ log(finish_pos), data = nascard_sr1)) # r2 = .883
coef(nascard_lm)
```

Finish position and prize are ratio variables, so the Pearson's r applies.  Check whether each variable is normally distributed.  

```{r warning=FALSE, message=FALSE, fig.height=3.5, fig.width=6.5}
p1 <- ggplot(nascard_sr1, aes(sample = finish_pos)) +
  stat_qq() +
  stat_qq_line() +
  theme_mf() +
  labs(title = "Q-Q Plots", subtitle = "Finish Position")
p2 <- ggplot(nascard_sr1, aes(sample = winnings)) +
  stat_qq() +
  stat_qq_line() +
  theme_mf() +
  labs(title = "", subtitle = "Winnings")
gridExtra::grid.arrange(p1, p2, nrow = 1)
```

The normal distribution plots look good.  You can also use the Anderson-Darling statistical test.
```{r}
nortest::ad.test(nascard_sr1$finish_pos)
nortest::ad.test(1/nascard_sr1$winnings)
```

Both fail to reject the normality null hypothesis.

Pearson's $r$ is 

```{r}
x <- nascard_sr1$finish_pos
y <- 1/nascard_sr1$winnings
(r = sum((x - mean(x)) * (y - mean(y))) /
   sqrt(sum((x - mean(x))^2) * sum((y - mean(y))^2)))
```

Test $H_0: \rho = 0$ with test statistic $T$.

```{r}
n <- nrow(nascard_sr1)
(t = r * sqrt((n - 2) / (1 - r^2)))
```

```{r}
pt(q = t, df = n - 2, lower.tail = FALSE) * 2
```

$P(T>.9752) < .0001$, so reject $H_0$ that the correlation is zero.  `cor.test()` performs these calculations.  Specify `method = "pearson"`.

```{r}
cor.test(
  x = nascard_sr1$finish_pos,
  y = 1/nascard_sr1$winnings,
  alternative = "two.sided",
  method = "pearson"
)
```


#### Spearman's Rho

Spearman's $\rho$ is the Pearson's r applied to the sample variable *ranks*.  Let $(X_i, Y_i)$ be the ranks of the $n$ sample pairs with mean ranks $\bar{X} = \bar{Y} = (n+1)/2$.  Spearman's rho is   

$$\hat{\rho} = \frac{\sum{(X_i - \bar{X})(Y_i - \bar{Y})}}{\sqrt{\sum{(X_i - \bar{X})^2 \sum{(Y_i - \bar{Y})^2}}}}$$

Spearman's rho is a non-parametric test, so there is no associated confidence interval.

From the `nascard` study, what was the correlation between the driver's starting position `start_pos` and finishing position `finish_pos`?  From the scatterplot, there does not appear to be much of a relationship.

```{r fig.height=3.5, fig.width=3.75}
nascard_sr1 %>%
  ggplot(aes(x = start_pos, y = finish_pos)) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  theme_mf() +
  labs(title = "Spearman's Rho")
```

Both `star_pos` and `finish_pos` are ordinal variables, so use Spearman's rho instead of Pearson. Normally, you replace the variable values with their ranks, but in this case, the values *are* their ranks.  After that, Spearman's $\rho$ is the same as Pearson's r.

```{r}
x <- rank(nascard_sr1$start_pos)
y <- rank(nascard_sr1$finish_pos)

(rho = sum((x - mean(x)) * (y - mean(y))) /
   sqrt(sum((x - mean(x))^2) * sum((y - mean(y))^2)))
```

Test $H_0: \rho = 0$ with test statistic $T$.

```{r}
n <- nrow(nascard_sr1)
(t = rho * sqrt((n - 2) / (1 - rho^2)))
```

```{r}
pt(q = abs(t), df = n - 2, lower.tail = FALSE) * 2
```

$P(T>.2206) = .8268$, so *do not* reject $H_0$ that the correlation is zero.  `cor.test()` performs these calculations.  Specify `method = "spearman"`.

```{r}
cor.test(
  x = nascard_sr1$start_pos,
  y = nascard_sr1$finish_pos,
  alternative = "two.sided",
  method = "spearman"
)
```


#### Kendall's Tau

The Kendall Rank correlation coefficient (Kendall's Tau) measures the relationship between ranked variables.^[See article at [Statistics How To](https://www.statisticshowto.datasciencecentral.com/kendalls-tau/)]   Let $(X_i, Y_i)$ be the ranks of the $n$ sample pairs.  For each $X_i$, count $Y > X_i$. The total count is $k$.   Kendall's tau is

$\hat{\tau} = \frac{4k}{n(n-1)} -1$

Note that if $X$ and $Y$ have the same rank orders, $\hat{\tau} = 1$, and if they have opposite rank orders $\hat{\tau} = -1$.  Test $H_0: \tau = 0$ with test statistic $Z = \hat{\tau} \sqrt{\frac{9n(n - 1)}{2(2n + 5)}}$.  As a non-parametric test, Kendall's $\tau$ does not produce a confidence interval.

## Example Using Kendall's Tau
From the `nascard` data set above, in race 1 of 1975, what was the correlation between the driver's starting position `start_pos` and finishing position `finish_pos`?

Again, because both `start_pos` and `finish_pos` are ordinal variables, we use a non-parametric test.  This time use Kendall's tau.

For each observation, count the number of other observations where both `start_pos` and  `finish_pos` is larger.  The sum is $k = 293$.  Then calculate Kendall's $\tau$ is $\hat{\tau} = \frac{4 * 293}{35(35-1)} -1 = -.0151$.

```{r}
n <- nrow(nascard_sr1)
k <- 0
for(i in 1:n) {
  start_pos_i = nascard_sr1[i, 'start_pos']
  finish_pos_i = nascard_sr1[i, 'finish_pos']
  for (j in 1:n) {
    start_pos_j = nascard_sr1[j, 'start_pos']
    finish_pos_j = nascard_sr1[j, 'finish_pos']
    if(finish_pos_j > finish_pos_i && 
        start_pos_j > start_pos_i) {
      k <- k + 1
    }
  }
}
(k)
(tau = 4.0*k / (n * (n - 1)) - 1)

```

The test statistic is $Z = \hat{\tau} \sqrt{\frac{9n(n - 1)}{2(2n + 5)}} = -0.0151 \sqrt{\frac{9(35)(35 - 1)}{2(2(35) + 5)}} = -0.1278$. $P(Z < -.1278) = .8991$, so do *not* reject $H_0$ that the correlation is zero.
```{r}
(z = tau * sqrt(9 * n * (n-1) / (2 * (2*n + 5))))
pt(q = abs(z), df = n-2, lower.tail = FALSE) * 2
```

R function `cor.test` performs these calculations.  Specify `method = "kendall"`.  

```{r}
cor.test(x = nascard_sr1$start_pos,
         y = nascard_sr1$finish_pos,
         alternative = "two.sided",
         method = "kendall")
```

## Tie it all together!
Calculate both Spearman's rho and Kendall's tau for all 898 races in the `nascard` data set to compare the results.
```{r message=FALSE, warning=FALSE}
series_race <- nascard %>% 
  distinct(nascard$series_race, nascard$year)
spearman <- sapply(series_race[,1], function(series_race) {
  cor.test(x = nascard[nascard$series_race == series_race, 'start_pos'],
         y = nascard[nascard$series_race == series_race, 'finish_pos'],
         alternative = "two.sided",
         method = "spearman")$estimate
})
spearman.p <- sapply(series_race[,1], function(series_race) {
  cor.test(x = nascard[nascard$series_race == series_race, 'start_pos'],
         y = nascard[nascard$series_race == series_race, 'finish_pos'],
         alternative = "two.sided",
         method = "spearman")$p.value
})
kendall <- sapply(series_race[,1], function(series_race) {
  cor.test(x = nascard[nascard$series_race == series_race, 'start_pos'],
         y = nascard[nascard$series_race == series_race, 'finish_pos'],
         alternative = "two.sided",
         method = "kendall")$estimate
})
kendall.p <- sapply(series_race[,1], function(series_race) {
  cor.test(x = nascard[nascard$series_race == series_race, 'start_pos'],
         y = nascard[nascard$series_race == series_race, 'finish_pos'],
         alternative = "two.sided",
         method = "kendall")$p.value
})
nascard_smry <- data.frame(series_race = series_race$`nascard$series_race`, 
                           year = series_race$`nascard$year`,
                           spearman, spearman.p,
                           kendall, kendall.p)
```

```{r message=FALSE, warning=FALSE}
library(tidyr)
nascard_smry2 <- nascard_smry %>% 
  group_by(year) %>%
  summarize(spearman_avg = mean(spearman),
            kendall_avg = mean(kendall)) %>%
  gather(key = method, value = estimate, c(spearman_avg, kendall_avg))
ggplot(data = nascard_smry2, aes(x = year, color = method)) +
  geom_line(aes(y = estimate)) +
  geom_line(aes(y = estimate)) +
  expand_limits(y = 0) +
  labs(title = "Average Rank Correlations vs Year")
```

Summarize the results in a contingency table.  The two tests usually return the same results in terms of significance.  
```{r}
nascard_smry$spearman_assoc <- 
  ifelse(nascard_smry$spearman.p < .05, "sig", "insig")
nascard_smry$kendall_assoc <- 
  ifelse(nascard_smry$kendall.p < .05, "sig", "insig")
table(nascard_smry$spearman_assoc, nascard_smry$kendall_assoc)
```

We can calculate the Pearson coefficient to measure the relationship between Spearman's rho and Kendall's tau.  
```{r}
ggplot(data = nascard_smry, aes(x = kendall, y = spearman)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  labs(title = "Spearman's rho vs Kendall's tau")
```

The scatterplot with fitted line has r-squared equal to the Pearson coefficient squared.

```{r}
cor.test(x = nascard_smry$spearman,
         y = nascard_smry$kendall,
         alternative = "two.sided",
         method = "pearson")

summary(lm(kendall~spearman, data = nascard_smry))
```




```{r}
pears.cor = function(table, rscore, cscore) { 
  dim=dim(table) 
  rbar=sum(margin.table(table,1)*rscore)/sum(table) 
  rdif=rscore-rbar 
  cbar=sum(margin.table(table,2)*cscore)/sum(table) 
  cdif=cscore-cbar 
  ssr=sum(margin.table(table,1)*(rdif^2)) 
  ssc=sum(margin.table(table,2)*(cdif^2)) 
  ssrc=sum(t(table*rdif)*cdif) 
  pcor=ssrc/(sqrt(ssr*ssc)) 
  pcor 
  M2=(sum(table)-1)*pcor^2 
  M2 
  result=c(pcor, M2) result
} 
```

<!--chapter:end:05-cont_anal.Rmd-->

# Regression

<!--chapter:end:06-ols.Rmd-->


# Generalized Linear Models

Placeholder


## Logistic Regression
### Example {-}
## Multinomial Logistic Regression
## Ordinal Logistic Regression
## Poisson Regression
### Example {-}

<!--chapter:end:07-glm.Rmd-->

# Classification

<!--chapter:end:08-chisq.Rmd-->

# Classification

<!--chapter:end:09-class.Rmd-->


# Decision Trees

Placeholder


## Classification Tree
### Confusion Matrix
### ROC Curve
### Caret Approach
## Regression Trees
### Caret Approach
## Bagging
## Random Forests
#### Bagging Classification Example
#### Random Forest Classification Example
#### Bagging Regression Example
#### Random Forest Regression Example
## Gradient Boosting
#### Gradient Boosting Classification Example
#### Gradient Boosting Regression Example
## Summary
## Reference

<!--chapter:end:10-cart.Rmd-->

# Regularization

<!--chapter:end:11-regularization.Rmd-->


# Non-linear Models

Placeholder


## Splines
## MARS
## GAM

<!--chapter:end:12-nonlin.Rmd-->


# Support Vector Machines

Placeholder


## Maximal Margin Classifier
## Support Vector Classifier
## Support Vector Machines
## Example
## Using Caret

<!--chapter:end:13-svm.Rmd-->

# Principal Components Analysis

<!--chapter:end:14-pca.Rmd-->

# Clustering

<!--chapter:end:15-cluster.Rmd-->

# Text Mining

<!--chapter:end:16-text-mining.Rmd-->


# Appendix {-}

Placeholder


## Publishing to BookDown {-}
## Shiny Apps {-}
## Packages {-}
### Create a package {-}
### Document Functions with roxygen
### Create Data {-}
### Create Vignette {-}
#### Step 2: Create an R Markdown template {-}

<!--chapter:end:17-appendix.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:18-references.Rmd-->

