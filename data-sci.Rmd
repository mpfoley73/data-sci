--- 
title: "My Data Science Notes"
author: "Michael Foley"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output:
  bookdown::gitbook:
    lib_dir: assets
    split_by: section
    config:
      toolbar:
        position: static
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "This is a compendium of notes from classes, tutorials, etc. that I reference from time to time."
---
--- 
title: "My Data Science Notes"
author: "Michael Foley"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output:
  bookdown::gitbook:
    lib_dir: assets
    split_by: section
    config:
      toolbar:
        position: static
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "This is a compendium of notes from classes, tutorials, etc. that I reference from time to time."
---

# Intro {-}

These notes are pulled from various classes, tutorials, books, etc. and are intended for my own consumption.  If you are finding this on the internet, I hope it is useful to you, but you should know that I am just a student and there's a good chance whatever you're reading here is mistaken.  In fact, that should probably be your null hypothesis... or your prior.  Whatever.

<!--chapter:end:index.Rmd-->


# Probability {#probability}

Placeholder


## Principles

<!--chapter:end:01-probability.Rmd-->


# Discrete Distributions {#disc_dist}

Placeholder


## Bernoulli
## Binomial
## Poission
## Multinomial
## Negative-Binomial
#### Examples {-}
## Geometric
#### Examples {-}
## Hypergeometric
#### Example {-}
## Gamma
#### Examples {-}

<!--chapter:end:02-disc_dist.Rmd-->


# Continuous Distributions {#cont_dist}

Placeholder


## Normal
#### Example {-}
### Example
### Example
### Example
### Normal Approximation to Binomial
### Example
### Example
### From Sample to Population
## Join Distributions
## Likelihood

<!--chapter:end:03-cont_dist.Rmd-->


# Categorical Analysis - Nonmodel {#discrete_analysis}

Placeholder


## Chi-Square Test
## One-Way Tables
### Chi-Square Goodness-of-Fit Test
### Proportion Test
## Two-Way Tables
### Chi-Square Independence Test
### Residuals Analysis
### Difference in Proportions
### Relative Risk
### Odds Ratio
### Partitioning Chi-Square
### Correlation
## K-Way Tables
### Odds Ratio
### Chi-Square Independence Test
#### Mutual Independence
#### Joint Independence
#### Marginal Independence
#### Conditional Independence
#### Homogenous Associations

<!--chapter:end:04-disc_anal.Rmd-->


# Continuous Variable Analysis {#continuous_analysis}

Placeholder


### Correlation
#### Pearson's r
#### Spearman's Rho
#### Kendall's Tau

<!--chapter:end:05-cont_anal.Rmd-->


# Regression

Placeholder


## Linear Regression Model
## Parameter Estimation
#### Example
## Model Assumptions
### Linearity
### Multicollinearity
#### Example
#### Example
### Normality
### Equal Variances
## Prediction
#### Example
#### Example 
## Inference
### *t*-Test
#### Example
### *F*-Test
#### Example
## Interpretation
## Model Validation
### Accuracy Metrics
#### R-Squared
#### RMSE
#### RSE
#### MAE
#### Adjusted R-squared
#### AIC
#### AICc
#### BIC
#### Mallows Cp
##### Example
### Cross-Validation
#### Validation Set
##### Example
#### LOOCV
#### K-fold Cross-Validation
#### Repeated K-fold CV
#### Bootstrapping
### Gain Curve
## Reference

<!--chapter:end:06-ols.Rmd-->

# Generalized Linear Models

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(broom)  # for augment()
library(WVPlots)  # for GainCurvePlot()
library(caret)  # for RMSE()
library(readr)
library(mfstylr)
```


These notes are primarily from [PSU STAT 504](https://online.stat.psu.edu/stat504) which uses Alan Agresti's **Categorical Data Analysis** [@Agresti2013].  I also reviewed [PSU STAT 501](https://newonlinecourses.science.psu.edu/stat501/lesson/15), [DataCamp's Generalized Linar Models in R](https://www.datacamp.com/courses/generalized-linear-models-in-r), [DataCamp's Multiple and Logistic Regression](https://www.datacamp.com/courses/multiple-and-logistic-regression), and **Interpretable machine learning*"** [@Molner2020].

The linear regression model, $E(Y|X) = X \beta$, structured as $y_i = X_i \beta + \epsilon_i$ where $X_i \beta = \mu_i$, assumes the response is a linear function of the predictors and the residuals are independent random variables normally distributed with mean zero and constant variance, $\epsilon \sim N \left(0, \sigma^2 \right)$.  This implies that given some set of predictors, the response is normally distributed about its expected value, $y_i \sim N \left(\mu_i, \sigma^2 \right)$.  However, there are many situations where this assumption of normality fails.  Generalized linear models (GLMs) are a generalization of the linear regression model that addresses non-normal response distributions.

The response given a set of predictors will not have a normal distribution if its underlying data-generating process is binomial or multinomial (proportions), Poisson (counts), or exponential (time-to-event).  In these situations a regular linear regression can predict proportions outside [0, 100] or counts or times that are negative.  GLMs solve this problem by modeling *a function* of the expected value of $y$, $f(E(Y|X)) = X \beta$.  There are three components to a GLM: the *random component* is the probability distribution of the response variable (normal, binomial, Poisson, [etc](https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions).); the *systematic component* is the explanatory variables $X\beta$; and the *link function* $\eta$ specifies the link between random and systematic components, converting the response range to $[-\infty, +\infty]$.  

Linear regression is thus a special case of GLM where link function is the identity function, $f(E(Y|X)) = E(Y|X)$.  For a logistic regression, where the data generating process is binomial, the link function is 

$$f(E(Y|X)) = \ln \left( \frac{E(Y|X)}{1 - E(Y|X)} \right) = \ln \left( \frac{\pi}{1 - \pi} \right) = logit(\pi)$$

where $\pi$ is the event probability.  (As an aside, you have probably heard of the related "probit" regression. The probit regression link function is $f(E(Y|X)) = \Phi^{-1}(E(Y|X)) = \Phi^{-1}(\pi)$. The difference between the logistic and probit link function is theoretical, and [the practical significance is slight](https://www.theanalysisfactor.com/the-difference-between-logistic-and-probit-regression/).  You can probably safely ignore probit).  

For a Poisson regression, the link function is

$$f(E(Y|X)) = \ln (E(Y|X)) = \ln(\lambda)$$

where $\lambda$ is the expected event rate.  

For an exponential regression, the link function is 

$$f(E(Y|X) = -E(Y|X) = -\lambda$$

where $\lambda$ is the expected time to event.

GLM uses maximum likelihood estimation (MLE) rather than ordinary least squares (OLS) to estimate the parameters, and thus relies on large-sample approximations. 

In R, specify a GLM just like an linear model, but with the `glm()` function, specifying the distribution with the `family` parameter.

* `family = "gaussian"`: linear regression
* `family = "binomial"`: logistic regression
* `family = binomial(link = "probit")`: probit regression
* `family = "poisson"`: Poisson regression


## Logistic Regression

Logistic regression estimates the probability of a particular level of a categorical response variable given a set of predictors. The response levels can be binary, nominal (multiple categories), or ordinal (multiple levels).  

The **binary** logistic regression model is

$$y = logit(\pi) = \ln \left( \frac{\pi}{1 - \pi} \right) = X \beta$$

where $\pi$ is the event probability. The model predicts the *log odds* of the response variable.  The maximum likelihood estimator maximizes the likelihood function

$$L(\beta; y, X) = \prod_{i=1}^n \pi_i^{y_i}(1 - \pi_i)^{(1-y_i)} = \prod_{i=1}^n\frac{\exp(y_i X_i \beta)}{1 + \exp(X_i \beta)}.$$

There is no closed-form solution, so GLM estimates coefficients with interatively reweighted least squares.  

Here is a case study to illustrate the points.  Dataset `donner` contains observations of 45 members of the Donner party with response variable (`surv`) an explanatory variables `age` and `sex`.

```{r include=FALSE}
donner <- tribble(
  ~age, ~sex, ~surv,
  23, 1, 0,
  40, 0, 1,
  40, 1, 1,
  30, 1, 0,
  28, 1, 0,
  40, 1, 0,
  45, 0, 0,
  62, 1, 0,
  65, 1, 0,
  45, 0, 0,
  25, 0, 0,
  28, 1, 1,
  28, 1, 0,
  23, 1, 0,
  22, 0, 1,
  23, 0, 1,
  28, 1, 1,
  15, 0, 1,
  47, 0, 0,
  57, 1, 0,
  20, 0, 1,
  18, 1, 1,
  25, 1, 0,
  60, 1, 0,
  25, 1, 1,
  20, 1, 1,
  32, 1, 1,
  32, 0, 1,
  24, 0, 1,
  30, 1, 1,
  15, 1, 0,
  50, 0, 0,
  21, 0, 1,
  25, 1, 0,
  46, 1, 1,
  32, 0, 1,
  30, 1, 0,
  25, 1, 0,
  25, 1, 0,
  25, 1, 0,
  30, 1, 0,
  35, 1, 0,
  23, 1, 1,
  24, 1, 0,
  25, 0, 1,
) %>%
  mutate(
    surv = factor(surv, levels = c(0:1), labels = c("Died", "Lived")),
    sex = factor(sex, levels = c(0:1), labels = c("F", "M"))
  )
```

```{r}
glimpse(donner)
```

Fit a logistic regression $SURV = SEX + AGE + SEX : AGE$.

```{r}
m <- glm(surv ~ sex*age, data = donner, family = binomial(link = logit))
summary(m)
```

The "z value" in the Coefficients table is the Wald z statistic, $z = \hat{\beta} / SE(\hat{\beta})$, which if squared is the Wald chi-squared statistic, $z^2$.  The p.value is the area to the right of $z^2$ in the $\chi_1^2$ density curve:

```{r}
m %>% tidy() %>% 
  mutate(
    z = estimate / std.error, 
    p_z2 = pchisq(z^2, df = 1, lower.tail = FALSE)
  ) %>%
  select(term, estimate, z, p_z2) 
```

Below the Coefficients table, the "dispersion parameter" refers to overdispersion, a common issue with GLM.  For a logistic regression, the response variable should be distributed $y_i \sim Bin(n_i, \pi_i)$ with $\mu_i = n_i \pi_i$ and $\sigma^2 = \pi (1 - \pi)$.  Overdispersion means the data shows evidence of variance greater than $\sigma^2$.

"Fisher scoring" is a method for ML estimation. Logistic regression uses an iterative procedure to fit the model, so this section indicates whether the algorithm converged.

The null deviance is the likelihood ratio $G^2 = 61.827$ of the intercept-only model.  The residual deviance is the likelihood ratio $G^2 = 47.346$ after including all model covariates.  $G^2$ is large, so reject the null hypothesis of no age and sex effects.  The ANOVA table shows the change in deviance from adding each variable successively to the model.

```{r}
anova(m)
```

```{r}
glance(m)
```

Plug in values to interpret the model.  The log odds of a 24 year-old female surviving is $\hat{y} = 2.59$.  The log odds of a 24 year-old male surviving is $\hat{y} = -0.46$.

```{r results=FALSE}
coef(m)["(Intercept)"] + coef(m)["sexM"]*0 + coef(m)["age"]*24 +
  coef(m)["sexM:age"]*0*24
coef(m)["(Intercept)"] + coef(m)["sexM"]*1 + coef(m)["age"]*24 +
  coef(m)["sexM:age"]*1*24

# Or use predict()
(lo_f <- predict(m, newdata = data.frame(sex = "F", age = 24)))
(lo_m <- predict(m, newdata = data.frame(sex = "M", age = 24)))
```

Log odds are not easy to interpet.  Exponentiate the log odds to get the **odds**.

$$odds(\hat{y}) = \exp (\hat{y}) = \frac{\pi}{1 - \pi}.$$

The odds of a 24 year-old female surviving is $\exp(\hat{y}) = 13.31$.  The odds of a 24 year-old male surviving is $\exp(\hat{y}) = 0.63$.

```{r results=FALSE}
exp(lo_f)
exp(lo_m)
```

Solve for $\pi$ to get the **probability**.  

$$\pi = \frac{\exp (\hat{y})}{1 + \exp (\hat{y})}$$

The probability of a 24 year-old female surviving is $\pi = 0.93$.  The probability of a female of average age surviving is $\pi = 0.39$.  The `predict()` function for a logistic model returns log-odds, but can also return $\pi$ by specifying parameter `type = "response"`.

```{r results=FALSE}
exp(lo_f) / (1 + exp(lo_f))
exp(lo_m) / (1 + exp(lo_m))

# Or use predict(..., type = "response")
(p_f <- predict(m, newdata = data.frame(sex = "F", age =24), type = "response"))
(p_m <- predict(m, newdata = data.frame(sex = "M", age =24), type = "response"))
```

Interpret the *coefficient* estimates using the **odds ratio**, the ratio of the odds before and after an increment to the predictors.  The odds ratio is how much the odds would be multiplied after a $\delta = X_1 - X_0$ unit increase in $X$.

$$\theta = \frac{\pi / (1 - \pi) |_{X = X_1}}{\pi / (1 - \pi) |_{X = X_0}} = \frac{\exp (X_1 \hat{\beta})}{\exp (X_0 \hat{\beta})} = \exp ((X_1-X_0) \hat{\beta}) = \exp (\delta \hat{\beta})$$

The odds of a female surviving are multiplied by a factor of $\exp(1 \cdot (-0.19)) = 0.824$ per additional year of age. The odds of a male surviving are multiplied by a factor of $\exp(1 \cdot (-0.161-0.19)) = 0.968$ per additional year of age.   

```{r results=FALSE}
exp(1 * (coef(m)["age"] + 0*coef(m)["sexM:age"]))  # female
exp(1 * (coef(m)["age"] + 1*coef(m)["sexM:age"]))  # male
```

`oddsratio::or_glm()` calculates the odds ratio from an increment in the predictor values.

```{r}
oddsratio::or_glm(donner, m, incr = list(age = 1))
```

The predicted values can also be expressed as the probabilities $\pi$.  This produces the familiar signmoidal shape of the binary relationship.

```{r fig.height=3.5}
augment(m, type.predict = "response") %>%
  ggplot(aes(x = age)) +
  geom_point(aes(y = surv)) +
  geom_line(aes(y = .fitted+1)) +
  theme_mf() +
  labs(x = "AGE",
       y = "Probability of SURVIVE",
       title = "Binary Fitted Line Plot")
```

Evaluate a logistic regression using a [Gain curve or ROC curve](https://community.tibco.com/wiki/gains-vs-roc-curves-do-you-understand-difference). 

In the **gain curve**, the x-axis is the fraction of items seen when sorted by the predicted value, and the y-axis is the cumulative summed true outcome. The "wizard" curve is the gain curve when the data is sorted by the true outcome.  If the model's gain curve is close to the wizard curve, then the model predicted the response variable well. The grey area is the "gain" over a random prediction.

20 of the 45 members of the Donner party survived. 

* The gain curve encountered 10 survivors (50%) within the first 12 observations (27%).  It encountered all 20 survivors on the 37th observation.
* The bottom of the grey area is the outcome of a random model.  Only half the survivors would be observed within 50% of the observations.  
* The top of the grey area is the outcome of the perfect model, the "wizard curve".  Half the survivors would be observed in 10/45=22% of the observations.

```{r, fig.height=3.5}
options(yardstick.event_first = FALSE)  # set the second level as success
augment(m, type.predict = "response") %>%
yardstick::gain_curve(surv, .fitted) %>%
  autoplot() +
  labs(title = "Gain Curve")
```

The ROC (Receiver Operating Characteristics) curve plots sensitivity vs specificity at different cut-off values for the probability, ranging cut-off from 0 to 1.

```{r, fig.height=3.5}
options(yardstick.event_first = FALSE)  # set the second level as success
augment(m, type.predict = "response") %>%
yardstick::roc_curve(surv, .fitted) %>%
  autoplot() +
  labs(title = "ROC Curve")
```


## Multinomial Logistic Regression

The following notes rely on the [PSU STAT 504 course notes](https://online.stat.psu.edu/stat504/node/171/.

Multinomial logistic regression models the odds the multinomial response variable $Y \sim Mult(n, \pi)$ is in level $j$ relative to baseline category $j^*$ for all pairs of categories as a function of $k$ explanatory variables, $X = (X_1, X_2, ... X_k)$. 

$$\log \left( \frac{\pi_{ij}}{\pi_{ij^*}} \right) = x_i^T \beta_j, \hspace{5mm} j \ne j^2$$

Interpet the $k^{th}$ element of $\beta_j$ as the increase in log-odds of falling a response in category $j$ relative to category $j^*$ resulting from a one-unit increase in the $k^{th}$ predictor term, holding the other terms constant.

Multinomial model is a type of GLM.

Here is an example using multinomial logistic regression.  A researcher classified the stomach contents of $n = 219$ alligators according to $r = 5$ categories (*fish, Inv., Rept, Bird, Other*) as a function of covariates *Lake*, *Sex*, and *Size*..

```{r}
gator_dat <- tribble(
  ~profile, ~Gender, ~Size, ~Lake, ~Fish, ~Invertebrate, ~Reptile, ~Bird, ~Other,
  "1", "f", "<2.3", "george",  3, 9, 1, 0, 1,
  "2", "m", "<2.3", "george", 13, 10, 0, 2, 2,
  "3", "f", ">2.3", "george", 8, 1, 0, 0, 1,
  "4", "m", ">2.3", "george", 9, 0, 0, 1, 2,
  "5", "f", "<2.3", "hancock", 16, 3, 2, 2, 3,
  "6", "m", "<2.3", "hancock", 7, 1, 0, 0, 5,
  "7", "f", ">2.3", "hancock", 3, 0, 1, 2, 3,
  "8", "m", ">2.3", "hancock", 4, 0, 0, 1, 2,
  "9", "f", "<2.3", "oklawaha", 3, 9, 1, 0, 2,
  "10", "m", "<2.3", "oklawaha", 2, 2, 0, 0, 1,
  "11", "f", ">2.3", "oklawaha", 0, 1, 0, 1, 0,
  "12", "m", ">2.3", "oklawaha", 13, 7, 6, 0, 0,
  "13", "f", "<2.3", "trafford", 2, 4, 1, 1, 4,
  "14", "m", "<2.3", "trafford", 3, 7, 1, 0, 1,
  "15", "f", ">2.3", "trafford", 0, 1, 0, 0, 0,
  "16", "m", ">2.3", "trafford", 8, 6, 6, 3, 5
)
gator_dat <- gator_dat %>%
  mutate(
    Gender = as_factor(Gender),
    Lake = fct_relevel(Lake, "hancock"),
    Size = as_factor(Size)
  )
```

There are 4 equations to estimate:

$$\log \left( \frac{\pi_j} {\pi_{j^*}} \right) = \beta X$$

where $\pi_{j^*}$ is the probability of fish, the baseline category.  


Run a multivariate logistic regression model with `VGAM::vglm()`.

```{r message=FALSE}
library(VGAM)
```

`vglm()` fits 4 logit models.

```{r}
gator_vglm <- vglm(
  cbind(Bird,Invertebrate,Reptile,Other,Fish) ~ Lake + Size + Gender, 
  data = gator_dat, 
  family = multinomial
)
summary(gator_vglm)
```

The **residual deviance** is 50.2637 on 40 degrees of freedom.  Residual deviance tests the current model fit versus the saturated model. The saturated model, which fits a separate multinomial distribution to each of the 16 profiles (unique combinations of lake, sex and size), has 16 × 4 = 64 parameters. The current model has an intercept, three lake coefficients, one sex coefficient and one size coefficient for each of the four logit equations, for a total of 24 parameters. Therefore, the overall fit statistics have 64 − 24 = 40 degrees of freedom.

```{r}
E <- data.frame(fitted(gator_vglm) * rowSums(gator_dat[, 5:9]))
O <- gator_dat %>% select(Bird, Invertebrate, Reptile, Other, Fish) + .000001
(g2 <- 2 * sum(O * log(O / E)))
```


indicates the model fits okay, but not great. The Residual Deviance of 50.26 with 40 df from the table above output is reasonable, with p-value of 0.1282 and the statistics/df is close to 1 that is 1.256.



## Ordinal Logistic Regression

These notes rely on [UVA](https://data.library.virginia.edu/fitting-and-interpreting-a-proportional-odds-model/) and [PSU STAT 504 class notes](https://online.stat.psu.edu/stat504/node/171/).

The ordinal logistic regression model is

$$logit[P(Y \le j)] = \log \left[ \frac{P(Y \le j)}{P(Y \gt j)} \right] = \alpha_j - \beta X, \hspace{5mm} j \in [1, J-1]$$
where $j \in [1, J-1]$ are the levels of the ordinal outcome variable $Y$.  The proportional odds model assumes there is a common set of slope parameters $\beta$ for the predictors.  The ordinal outcomes are distinguished by the $J-1$ intercepts $\alpha_j$.  The benchmark level is $J$.

Technically, the model could be written $logit[P(Y \le j)] = \alpha_j + \zeta X$, replacing beta with zeta because the model fits $\alpha_j - \beta X$ instead of $\alpha_j + \beta X$.  
 
Suppose you want to model the probability a respondent holds a political ideology ["Socialist", "Liberal", "Moderate", "Conservative", "Libertarian"] given their party affiliation ["Republican", "Democrat"].

```{r include=FALSE}
ideo <- c("Socialist", "Liberal", "Moderate", "Conservative", "Libertarian")
ideo_cnt_rep <- c(30, 46, 148, 84, 99)
ideo_cnt_dem <- c(80, 81, 171, 41, 55)
ideology <- data.frame(
  party = factor(rep(c("Rep", "Dem"), c(407, 428)), levels = c("Rep", "Dem")),
  ideo = factor(
    c(rep(ideo, ideo_cnt_rep), rep(ideo, ideo_cnt_dem)), 
    levels = ideo
  )
)
```

```{r}
table(ideology)
```

Fit a proportional odds logistic regression.

```{r message=FALSE}
pom <- MASS::polr(ideo ~ party, data = ideology)
summary(pom)
```

The log-odds a Democrat identifies as *Socialist* vs *>Socialist*, or equivalently, the log-odds a Democrat identifies as *<=Socialist* vs *>=Liberal* is

$$logit[P(Y \le 1)] = -2.4690 - (-0.9745)(1) = -1.4945$$

which translates into an odds of 

$$odds(Y<=1) = exp(logit[P(Y \le 1)]) = \frac{exp(-2.469)}{exp(-0.9745)} = 0.2244$$

It is the same for Republicans, except multiply the slope coefficient by zero.

$$logit[P(Y \le 1)] = -2.4690 - (-0.9745)(0) = -2.4690$$

$$odds(Y<=1) = exp(logit[P(Y \le 1)]) = \frac{exp(-2.469)}{exp(0)} = -2.4690$$

The "proportional odds" part of the proportional odds model is that the ratios of the $J - 1$ odds are identical for each level of the predictors.  The numerators are always the same, and the denominators differ only by the exponent of the slope coefficient, $-0.9745$.  For all $j \in [1, J-1]$, the odds a Democrat's ideology is $\le j$ vs $>j$ is $exp(-0.9745) = 2.6498$ times that of a Republican's odds. 

You can translate the cumulative odds to cumulative probabilities by taking the ratio $\pi = exp(odds) / (1 + exp(odds))$.  The probability a Democrat identifies as *<=Socialist* vs *>Socialist* is 

$$P(Y \le 1) = \frac{exp(-1.4945)} {(1 + exp(-1.4945))} = 0.183.$$ 

The individual probabilities are just the successive differences in the cumulative probabilities. The log odds a Democrat identifies as *<=Liberal* vs *>Liberal* are $logit[P(Y \lt 2)] = -1.4745 - (-0.9745)(1) = -0.500$, which translates into a probability of $P(Y \le 2) = exp(-0.5) / (1 + exp(-0.5)) = 0.378$. The probability a Democrat identifies as *Liberal* is the difference in adjacent cumulative probabilities, $P(Y \le 2) - P(Y \le 1) = 0.378 = 0.183 = 0.194$.  This is how the model to predicts the level probabilities.

```{r}
x <- predict(pom, newdata = data.frame(party = c("Dem", "Rep")), type = "probs")
rownames(x) <- c("Dem", "Rep")
print(x)
```

Always check the assumption of proportional odds. One way to do this is by comparing the proportional odds model with a multinomial logit model, also called an unconstrained baseline logit model. The multinomial logit model models *unordered* responses and fits a slope to each level of the $J – 1$ responses. The proportional odds model is nested in the multinomial model, so you can use a likelihood ratio test to see if the models are statistically different.

```{r}
mlm <- nnet::multinom(ideo ~ party, data = ideology)
```

Calculate the difference in the deviance test statistics $D = -2 loglik(\beta)$.

```{r}
G <- -2 * (logLik(pom)[1] - logLik(mlm)[1])
pchisq(G, df = length(pom$zeta) - 1, lower.tail = FALSE)
```

The p-value is high, so do not reject the null hypothesis that the proportional odds model fits differently than the more complex multinomial logit model.

### Case Study

The General Social Survey for `year` 1972, 1973, and 1974 surveyed Caucasian Christians about their attitudes `att` toward abortion. Respondents were classified by years of education `edu` and religious group `att`. 

```{r include=FALSE}
abort <- tribble(
  ~year, ~rel, ~edu, ~att, ~cnt,
  1972, "Prot", "Low", "Neg", 9,
  1972, "Prot", "Low", "Mix", 12,
  1972, "Prot", "Low", "Pos", 48,
  1972, "Prot", "Med", "Neg", 13,
  1972, "Prot", "Med", "Mix", 43,
  1972, "Prot", "Med", "Pos", 197,
  1972, "Prot", "High", "Neg", 4,
  1972, "Prot", "High", "Mix", 9,
  1972, "Prot", "High", "Pos", 139,
  1972, "SProt", "Low", "Neg", 9,
  1972, "SProt", "Low", "Mix", 17,
  1972, "SProt", "Low", "Pos", 30,
  1972, "SProt", "Med", "Neg", 6,
  1972, "SProt", "Med", "Mix", 10,
  1972, "SProt", "Med", "Pos", 97,
  1972, "SProt", "High", "Neg", 1,
  1972, "SProt", "High", "Mix", 8,
  1972, "SProt", "High", "Pos", 68,
  1972, "Cath", "Low", "Neg", 14,
  1972, "Cath", "Low", "Mix", 12,
  1972, "Cath", "Low", "Pos", 32,
  1972, "Cath", "Med", "Neg", 18,
  1972, "Cath", "Med", "Mix", 50,
  1972, "Cath", "Med", "Pos", 131,
  1972, "Cath", "High", "Neg", 8,
  1972, "Cath", "High", "Mix", 13,
  1972, "Cath", "High", "Pos", 64,
  1973, "Prot", "Low", "Neg", 4,
  1973, "Prot", "Low", "Mix", 16,
  1973, "Prot", "Low", "Pos", 59,
  1973, "Prot", "Med", "Neg", 6,
  1973, "Prot", "Med", "Mix", 24,
  1973, "Prot", "Med", "Pos", 197,
  1973, "Prot", "High", "Neg", 4,
  1973, "Prot", "High", "Mix", 11,
  1973, "Prot", "High", "Pos", 124,
  1973, "SProt", "Low", "Neg", 4,
  1973, "SProt", "Low", "Mix", 16,
  1973, "SProt", "Low", "Pos", 34,
  1973, "SProt", "Med", "Neg", 6,
  1973, "SProt", "Med", "Mix", 29,
  1973, "SProt", "Med", "Pos", 118,
  1973, "SProt", "High", "Neg", 1,
  1973, "SProt", "High", "Mix", 4,
  1973, "SProt", "High", "Pos", 82,
  1973, "Cath", "Low", "Neg", 2,
  1973, "Cath", "Low", "Mix", 14,
  1973, "Cath", "Low", "Pos", 32,
  1973, "Cath", "Med", "Neg", 16,
  1973, "Cath", "Med", "Mix", 45,
  1973, "Cath", "Med", "Pos", 141,
  1973, "Cath", "High", "Neg", 7,
  1973, "Cath", "High", "Mix", 20,
  1973, "Cath", "High", "Pos", 72,
  1974, "Prot", "Low", "Neg", 7,
  1974, "Prot", "Low", "Mix", 16,
  1974, "Prot", "Low", "Pos", 49,
  1974, "Prot", "Med", "Neg", 10,
  1974, "Prot", "Med", "Mix", 26,
  1974, "Prot", "Med", "Pos", 219,
  1974, "Prot", "High", "Neg", 4,
  1974, "Prot", "High", "Mix", 10,
  1974, "Prot", "High", "Pos", 131,
  1974, "SProt", "Low", "Neg", 1,
  1974, "SProt", "Low", "Mix", 19,
  1974, "SProt", "Low", "Pos", 30,
  1974, "SProt", "Med", "Neg", 5,
  1974, "SProt", "Med", "Mix", 21,
  1974, "SProt", "Med", "Pos", 106,
  1974, "SProt", "High", "Neg", 2,
  1974, "SProt", "High", "Mix", 11,
  1974, "SProt", "High", "Pos", 87,
  1974, "Cath", "Low", "Neg", 3,
  1974, "Cath", "Low", "Mix", 9,
  1974, "Cath", "Low", "Pos", 29,
  1974, "Cath", "Med", "Neg", 15,
  1974, "Cath", "Med", "Mix", 30,
  1974, "Cath", "Med", "Pos", 149,
  1974, "Cath", "High", "Neg", 11,
  1974, "Cath", "High", "Mix", 18,
  1974, "Cath", "High", "Pos", 69
)

abort <- abort %>%
  mutate(
    edu = factor(edu, levels = c("Low", "Med", "High"), 
                 labels = c("0-8", "9-12", "12+")),
    rel = factor(rel, levels = c("Prot", "SProt", "Cath"),
                 labels = c("Prot.", "S. Prot.", "Cath.")),
    att = factor(att, levels = c("Neg", "Mix", "Pos"),
                 labels = c("Negative", "Mixed", "Positive"))
  )
```

```{r}
abort %>% pivot_wider(names_from = att, values_from = cnt) %>%
  flextable::flextable() %>%
  flextable::add_header_row(values = c("", "Attitude"), colwidths = c(3, 3))
```



## Poisson Regression

Poisson models count data, like "traffic tickets per day", or "website hits per day".  The response is an expected *rate* or intensity.  For count data, specify the generalized model, this time with `family = poisson` or `family = quasipoisson`. 

Recall that the probability of achieving a count $y$ when the expected rate is $\lambda$ is distributed 

$$P(Y = y|\lambda) = \frac{e^{-\lambda} \lambda^y}{y!}.$$


The poisson regression model is

$$\lambda = \exp(X \beta).$$ 
 
You can solve this for $y$ to get

$$y = X\beta = \ln(\lambda).$$

That is, the model predicts the log of the response rate.  For a sample of size *n*, the likelihood function is

$$L(\beta; y, X) = \prod_{i=1}^n \frac{e^{-\exp({X_i\beta})}\exp({X_i\beta})^{y_i}}{y_i!}.$$

The log-likelihood is

$$l(\beta) = \sum_{i=1}^n (y_i X_i \beta - \sum_{i=1}^n\exp(X_i\beta) - \sum_{i=1}^n\log(y_i!).$$

Maximizing the log-likelihood has no closed-form solution, so the coefficient estimates are found through interatively reweighted least squares.  

Poisson processes assume the variance of the response variable equals its mean.  "Equals" means the mean and variance are of a similar order of magnitude.  If that assumption does not hold, use the quasi-poisson.  Use Poisson regression for large datasets.  If the predicted counts are much greater than zero (>30), the linear regression will work fine.  Whereas RMSE is not useful for logistic models, it is a good metric in Poisson.


Dataset `fire` contains response variable `injuries` counting the number of injuries during the month and one explanatory variable, the month `mo`.

```{r}
fire <- read_csv(file = "C:/Users/mpfol/OneDrive/Documents/Data Science/Data/CivilInjury_0.csv")
fire <- fire %>% 
  mutate(mo = as.POSIXlt(`Injury Date`)$mon + 1) %>%
  rename(dt = `Injury Date`,
         injuries = `Total Injuries`)
str(fire)
```

In a situation like this where there the relationship is bivariate, start with a visualization.

```{r}
ggplot(fire, aes(x = mo, y = injuries)) +
  geom_jitter() +
  geom_smooth(method = "glm", method.args = list(family = "poisson")) +
  labs(title = "Injuries by Month")
```


Fit a poisson regression in R using `glm(formula, data, family = poisson)`.  But first, check whether the mean and variance of `injuries` are the same magnitude?  If not, then use `family = quasipoisson`.

```{r}
mean(fire$injuries)
var(fire$injuries)
```

They are of the same magnitude, so fit the regression with `family = poisson`.

```{r}
m2 <- glm(injuries ~ mo, family = poisson, data = fire)
summary(m2)
```

The *predicted* value $\hat{y}$ is the estimated **log** of the response variable, 

$$\hat{y} = X \hat{\beta} = \ln (\lambda).$$

Suppose `mo` is January (mo = `), then the log of `injuries` is $\hat{y} = 0.323787$. Or, more intuitively, the expected count of injuries is $\exp(0.323787) = 1.38$  

```{r}
predict(m2, newdata = data.frame(mo=1))
predict(m2, newdata = data.frame(mo=1), type = "response")
```

Here is a plot of the predicted counts in red.

```{r}
augment(m2, type.predict = "response") %>%
  ggplot(aes(x = mo, y = injuries)) +
  geom_point() +
  geom_point(aes(y = .fitted), color = "red") + 
  scale_y_continuous(limits = c(0, NA)) +
  labs(x = "Month",
       y = "Injuries",
       title = "Poisson Fitted Line Plot")
```

Evaluate a logistic model fit with an analysis of deviance.  

```{r}
(perf <- glance(m2))
(pseudoR2 <- 1 - perf$deviance / perf$null.deviance)
```

The deviance of the null model (no regressors) is 139.9.  The deviance of the full model is 132.2.  The psuedo-R2 is very low at .05.  How about the RMSE?

```{r}
RMSE(pred = predict(m2, type = "response"), obs = fire$injuries)
```

The average prediction error is about 0.99.  That's almost as much as the variance of `injuries` - i.e., just predicting the mean of `injuries` would be almost as good!  Use the `GainCurvePlot()` function to plot the gain curve.

```{r}
augment(m2, type.predict = "response") %>%
  ggplot(aes(x = injuries, y = .fitted)) +
  geom_point() +
  geom_smooth(method ="lm") +
  labs(x = "Actual",
       y = "Predicted",
       title = "Poisson Fitted vs Actual")
```


```{r}
augment(m2) %>% data.frame() %>% 
  GainCurvePlot(xvar = ".fitted", truthVar = "injuries", title = "Poisson Model")
```

It seems that `mo` was a poor predictor of `injuries`.  


<!--chapter:end:07-glm.Rmd-->

# Classification

<!--chapter:end:08-chisq.Rmd-->

# Classification

<!--chapter:end:09-class.Rmd-->


# Decision Trees

Placeholder


## Classification Tree
### Confusion Matrix
### ROC Curve
### Caret Approach
## Regression Trees
### Caret Approach
## Bagging
## Random Forests
#### Bagging Classification Example
#### Random Forest Classification Example
#### Bagging Regression Example
#### Random Forest Regression Example
## Gradient Boosting
#### Gradient Boosting Classification Example
#### Gradient Boosting Regression Example
## Summary
## Reference

<!--chapter:end:10-cart.Rmd-->

# Regularization

<!--chapter:end:11-regularization.Rmd-->


# Non-linear Models

Placeholder


## Splines
## MARS
## GAM

<!--chapter:end:12-nonlin.Rmd-->


# Support Vector Machines

Placeholder


## Maximal Margin Classifier
## Support Vector Classifier
## Support Vector Machines
## Example
## Using Caret

<!--chapter:end:13-svm.Rmd-->

# Principal Components Analysis

<!--chapter:end:14-pca.Rmd-->

# Clustering

<!--chapter:end:15-cluster.Rmd-->

# Text Mining

<!--chapter:end:16-text-mining.Rmd-->


# Appendix {-}

Placeholder


## Publishing to BookDown {-}
## Shiny Apps {-}
## Packages {-}
### Create a package {-}
### Document Functions with roxygen
### Create Data {-}
### Create Vignette {-}
#### Step 2: Create an R Markdown template {-}

<!--chapter:end:17-appendix.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:18-references.Rmd-->

