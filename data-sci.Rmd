--- 
title: "My Data Science Notes"
author: "Michael Foley"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output:
  bookdown::gitbook:
    lib_dir: assets
    split_by: section
    config:
      toolbar:
        position: static
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "This is a compendium of notes from classes, tutorials, etc. that I reference from time to time."
---

# Intro {-}

These notes are pulled from various classes, tutorials, books, etc. and are intended for my own consumption.  If you are finding this on the internet, I hope it is useful to you, but you should know that I am just a student and there's a good chance whatever you're reading here is mistaken.  In fact, that should probably be your null hypothesis... or your prior.  Whatever.

<!--chapter:end:index.Rmd-->

```{r include=FALSE}
library(tidyverse)
library(mfstylr)
my_colors <- list(blue = "#4a5F70",
                  beige = "#7f825f",
                  mauve = "#c2ae95",
                  red = "#824e4e",
                  grey = "#66777d")
```

# Probability {#probability}

## Principles

Here are three rules that come up all the time.

* $Pr(A \cup B) = Pr(A)+Pr(B) - Pr(AB)$.  This rule generalizes to 
$Pr(A \cup B \cup C)=Pr(A)+Pr(B)+Pr(C)-Pr(AB)-Pr(AC)-Pr(BC)+Pr(ABC)$.

* $Pr(A|B) = \frac{P(AB)}{P(B)}$

* If A and B are independent, $Pr(A \cap B) = Pr(A)Pr(B)$, and $Pr(A|B)=Pr(A)$. 

Uniform distributions on finite sample spaces often reduce to counting the elements of *A* and the sample space *S*, a process called combinatorics.  Here are three important combinatorial rules.

**Multiplication Rule**.  $|S|=|S_1 |⋯|S_k|$.

*How many outcomes are possible from a sequence of 4 coin flips and 2 rolls of a die?*
$|S|=|S_1| \cdot |S_2| \dots |S_6| = 2 \cdot 2 \cdot 2 \cdot 2 \cdot 6 \cdot 6 = 288$.

*How many subsets are possible from a set of n=10 elements?*
In each subset, each element is either included or not, so there are $2^n = 1024$ subsets.

*How many subsets are possible from a set of n=10 elements taken k at a time with replacement?*
Each experiment has $n$ possible outcomes and is repeated $k$ times, so there are $n^k$ subsets.

**Permutations**.  The number of *ordered* arrangements (permutations) of a set of $|S|=n$ items taken $k$ at a time *without* replacement has $n(n-1) \dots (n-k+1)$ subsets because each draw is one of k experiments with decreasing number of possible outcomes.  

$$_nP_k = \frac{n!}{(n-k)!}$$

Notice that if $k=0$ then there is 1 permutation; if $k=1$ then there are $n$ permutations; if $k=n$ then there are $n!$ permutations.

*How many ways can you distribute 4 jackets among 4 people?*
$_nP_k = \frac{4!}{(4-4)!} = 4! = 24$

*How many ways can you distribute 4 jackets among 2 people?*
$_nP_k = \frac{4!}{(4-2)!} = 12$

**Subsets**.  The number of *unordered* arrangements (combinations) of a set of $|S|=n$ items taken $k$ at a time *without* replacement has 

$$_nC_k = {n \choose k} = \frac{n!}{k!(n-k)!}$$

combinations and is called the binomial coefficient.  The binomial coefficient is the number of different subsets.  Notice that if k=0 then there is 1 subset; if k=1 then there are n subsets; if k=n then there is 1 subset.  The connection with the permutation rule is that there are $n!/(n-k)!$ permutations and each permutation has $k!$ permutations.  

*How many subsets of 7 people can be taken from a set of 12 persons?*
$_{12}C_7 = {12 \choose 7} = \frac{12!}{7!(12-7)!} = 792$

*If you are dealt five cards, what is the probability of getting a "full-house" hand containing three kings and two aces (KKKAA)?*
$$P(F) = \frac{{4 \choose 3} {4 \choose 2}}{{52 \choose 5}}$$

**Distinguishable permutations**.  The number of *unordered* arrangements (distinguishable permutations) of a set of $|S|=n$ items in which $n_1$ are of one type, $n_2$ are of another type, etc., is 

$${n \choose {n_1, n_2, \dots, n_k}} = \frac{n!}{n_{1}! n_{2}! \dots n_{k}!}$$

*How many ordered arrangements are there of the letters in the word PHILIPPINES?*  There are n=11 objects.  $|P|=n_1=3$;  $|H|=n_2=1$; $|I|=n_3=3$; $|L|=n_4=1$; $|N|=n_5=1$; $|E|=n_6=1$; $|S|=n_7=1$.

$${n \choose {n_1, n_2, \dots, n_k}} = \frac{11!}{3! 1! 3! 1! 1! 1! 1!} = 1,108,800$$

*How many ways can a research pool of 15 subjects be divided into three equally sized test groups?*

$${n \choose {n_1, n_2, \dots, n_k}} = \frac{15!}{5! 5! 5!} = 756,756$$


## Discrete Distributions {#disc_dist}

These notes rely heavily on PSU STATS 504 [course notes](https://online.stat.psu.edu/stat504/node/209/).

The most important discrete distributions are the Binomial, Poisson, and Multinomial.  Sometimes useful are the related Bernoulli, negative binomial, geometric, and hypergeometric distributions.

A discrete random variable $X$ is described by its probability mass function $f(x) = P(X = x)$.  The set of $x$ values for which $f(x) > 0$ is called the *support*. If the distribution depends on unknown parameter(s) $\theta$ we write it as $f(x; \theta)$ (frequentist) or $f(x | \theta)$ (Bayesian). 


### Bernoulli

If $X$ is the result of a trial with two outcomes of probability $P(X = 1) = \pi$ and $P(X = 0) = 1 - \pi$, then $X$ is a random variable with a Bernoulli distribution 

$$f(x) = \pi^x (1 - \pi)^{1 - x}, \hspace{1cm} x \in (0, 1)$$

with $E(X) = \pi$ and $Var(X) = \pi(1 - \pi)$.


### Binomial

If $X$ is the count of successful events in $n$ identical and independent Bernoulli trials of success probability $\pi$, then $X$ is a random variable with a binomial distribution $X \sim  Bin(n,\pi)$ 

$$f(x;n, \pi) = \frac{n!}{x!(n-x)!} \pi^x (1-\pi)^{n-x} \hspace{1cm} x \in (0, 1, ..., n), \hspace{2mm} \pi \in [0, 1]$$

with $E(X)=n\pi$ and $Var(X) = n\pi(1-\pi)$.  

Binomial sampling is used to model counts of one level of a categorical variable over a *fixed sample size*.  Here is a simple analysis of data from a Binomial process.  Data set `dat` contains frequencies of high-risk drinkers vs non-high-risk drinkers in a college survey.

```{r echo=FALSE}
dat <- data.frame(
  subject = 1:1315,
  high_risk = factor(c(rep("Yes", 630), rep("No", 685)))
)
table(dat$high_risk)
```

The MLE of $\pi$ from the Binomial distribution is the sample mean.

```{r}
x <- sum(dat$high_risk == "Yes")
n <- nrow(dat)
p <- x / n
print(p)
```

Here is the binomial distribution $f(x; \pi), \hspace{5mm} x \in [550, 700]$.

```{r fig.height=3.5}
events <- round(seq(from = 550, to = 700, length = 20), 0)
density <- dbinom(x = events, prob = p, size = n)
prob <- pbinom(q = events, prob = p, size = n, lower.tail = TRUE)
df <- data.frame(events, density, prob)
ggplot(df, aes(x = factor(events))) +
#  geom_col(aes(y = density)) +
  geom_col(aes(y = density), fill = mf_pal()(1), alpha = 0.8) +
  geom_text(
    aes(label = round(density, 3), y = density + 0.001),
    position = position_dodge(0.9),
    size = 3,
    vjust = 0
  ) +
  geom_line(
    data = df, 
    aes(x = as.numeric(factor(events)), y = prob/40), 
    color = mf_pal()(1), 
    size = 1) +
  scale_y_continuous(sec.axis = sec_axis(~.*40, name = "Cum Prob")) +
  theme_mf() +
  labs(title = "PMF and CDF of Binomial Distribution",
       subtitle = "Bin(1315, 0.479).",
       x = "Events (x)",
       y = "Density")
```

There are several ways to calculate a confidence interval for $\pi$.  One method is the **normal approximation** (Wald) interval.

$$\pi = p \pm z_{\alpha /2} \sqrt{\frac{p (1 - p)}{n}}$$

```{r}
alpha <- .05
z <- qnorm(1 - alpha / 2)
se <- sqrt(p * (1 - p) / n)
p + c(-z*se, z*se)
```

This method is easy to understand and calculate by hand, but its accuracy suffers when $np<5$ or $n(1-p)<5$ and it does not work at all when $p = 0$ or $p = 1$.  Option two is the **Wilson** method.

$$\frac{p + \frac{z^2}{2n}}{1 + \frac{z^2}{n}} \pm \frac{z}{1 + \frac{z^2}{n}} \sqrt{\frac{p(1 - p)}{n} + \frac{z^2}{4n^2}}$$

```{r}
est <- (p + (z^2)/(2*n)) / (1 + (z^2) / n)
pm <- z / (1 + (z^2)/n) * sqrt(p*(1-p)/n + (z^2) / (4*(n^2)))
est + c(-pm, pm)
```

This is what `prop.test()` does when you set `correct = FALSE`.

```{r}
prop.test(x = x, n = n, correct = FALSE)
```

There is a second version of the Wilson interval that applies a "continuity correction" that aligns the "minimum coverage probability", rather than the "average probability", with the nominal value.  *I'll need to learn what's inside those quotations at some point.*

```{r}
prop.test(x = x, n = n)
```

Finally, there is the Clopper-Pearson **exact confidence interval**.  Clopper-Pearson inverts two single-tailed binomial tests at the desired alpha.  This is a non-trivial calculation, so there is no easy formula to crank through.  Just use the `binom.test()` function and pray no one asks for an explanation.

```{r}
binom.test(x = x, n = n)
```

The expected probability of no one being a high-risk drinker is $f(0;0.479) = \frac{1315!}{0!(1315-0)!} 0.479^0 (1-0.479)^{1315-0} = 0$.

```{r}
dbinom(x = 0, size = n, p = p)
```

The expected probability of half the population being a high-risk drinker, $f(658, 0.479)$, is impossible to write out, and slow to calculate.

```{r}
pbinom(q = .5*n, size = n, prob = p, lower.tail = FALSE)
```

As n increases for fixed $\pi$, the binomial distribution approaches normal distribution $N(n\pi, n\pi(1−\pi))$.   The normal distribution is a good approximation when $n$ is large.

```{r}
pnorm(q = 0.5, mean = p, sd = se, lower.tail = FALSE)
```

Here are some more examples using smaller sample sizes.  The probability 2 out of 10 coin flips are heads if the probability of heads is 0.3: 

```{r}
dbinom(x = 2, size = 10, prob = 0.3)
```

Here is a simulation from n = 10,000 random samples of size 10.  `rbinom()` generates a random sample of numbers from the binomial distribution.

```{r message=FALSE, warning=FALSE, fig.height=3.5, fig.width=5}
data.frame(cnt = rbinom(n = 10000, size = 10, prob = 0.3)) %>%
  count(cnt) %>%
  ungroup() %>%
  mutate(pct = n / sum(n),
         X_eq_x = cnt == 2) %>%
  ggplot(aes(x = as.factor(cnt), y = n, fill = X_eq_x, label = pct)) +
  geom_col(alpha = 0.8) +
  scale_fill_mf() +
  geom_label(aes(label = round(pct, 2)), size = 3, alpha = .6) +
  theme_mf() +
  theme(legend.position = "none") +
  labs(title = "Binomial Distribution", 
       subtitle = paste0(
         "P(X=2) successes in 10 trials when p = 0.3 is ", 
         round(dbinom(2, 10, 0.3), 4), "."
       ),
       x = "Successes",
       y = "Count",
       caption = "Simulation from n = 10,000 binomial random samples.") 
```

What is the probability of <=2 heads in 10 coin flips where probability of heads is 0.3?  The cumulative probability is the sum of the first three bars in the simulation above.  Function `pbinom()` calculates the *cumulative* binomial probability.

```{r}
pbinom(q = 2, size = 10, prob = 0.3, lower.tail = TRUE)
```

What is the expected number of heads in 25 coin flips if the probability of heads is 0.3?

The expected value, $\mu = np$, is `r 25 * .3`.  Here's an empirical test from 10,000 samples.

```{r}
mean(rbinom(n = 10000, size = 25, prob = .3))
```

The variance, $\sigma^2 = np (1 - p)$, is `r 25 * .3 * (1 - .3)`.  Here's an empirical test.

```{r}
var(rbinom(n = 10000, size = 25, prob = .3))
```

Suppose X and Y are independent random variables distributed $X \sim Bin(10, .6)$ and $Y \sim Bin(10, .7)$.  What is the probability that either variable is <=4?

Let $P(A) = P(X<=4)$ and $P(B) = P(Y<=4)$.  Then $P(A|B) = P(A) + P(B) - P(AB)$, and because the events are independent, $P(AB) = P(A)P(B)$.

```{r}
p_a <- pbinom(q = 4, size = 10, prob = 0.6, lower.tail = TRUE)
p_b <- pbinom(q = 4, size = 10, prob = 0.7, lower.tail = TRUE)
p_a + p_b - (p_a * p_b)
```

Here's an empirical test.

```{r}
df <- data.frame(
  x = rbinom(10000, 10, 0.6),
  y = rbinom(10000, 10, 0.7)
  )
mean(if_else(df$x <= 4 | df$y <= 4, 1, 0))
```

A couple other points to remember:

* The Bernoulli distribution is a special case of the binomial with $n = 1$. 
* The binomial distribution assumes independent trials. If you sample *without replacement from a finite population*, use the hypergeometric distribution.


### Poission

If $X$ is the number of successes in $n$ (many) trials when the probability of success $\lambda / n$ is small, then $X$ is a random variable with a Poisson distribution $X \sim  Poisson(\lambda)$ 

$$f(x;\lambda) = \frac{e^{-\lambda} \lambda^x}{x!} \hspace{1cm} x \in (0, 1, ...), \hspace{2mm} \lambda > 0$$

with $E(X)=\lambda$ and $Var(X) = \lambda$.  

The Poisson likelihood function is

$$L(\lambda; x) = \prod_{i=1}^N f(x_i; \lambda) = \prod_{i=1}^N \frac{e^{-\lambda} \lambda^x_i}{x_i !} = \frac{e^{-n \lambda} \lambda^{\sum x_i}}{\prod x_i}.$$

The Poisson loglikelihood function is

$$l(\lambda; x) = \sum_{i=1}^N x_i \log \lambda - n \lambda.$$

One can show that the loglikelihood function is maximized at 

$$\hat{\lambda} = \sum_{i=1}^N x_i / n.$$

Thus, for a Poisson sample, the MLE for $\lambda$ is just the sample mean.

Poisson sampling is used to model counts of events that occur randomly over a *fixed period of time*. Here is a simple analysis of data from a Poisson process.  Data set `dat` contains frequencies of goal counts during the first round matches of the 2002 World Cup.

```{r echo=FALSE}
dat <- data.frame(
  goals = c(0, 1, 2, 3, 4, 5, 6, 7, 8),
  freq = c(23, 37, 20, 11, 2, 1, 0, 0, 1)
)
print(dat)
```

The MLE of $\lambda$ from the Poisson distribution is the sample mean.

```{r}
lambda <- weighted.mean(dat$goals, dat$freq)
print(lambda)
```

The 0.95 CI is $\lambda \pm z_{.05/2} \sqrt{\lambda / n}$

```{r}
n <- sum(dat$freq)
z <- qnorm(0.975)
se <- sqrt(lambda / n)
paste0("[", round(lambda - z*se, 2), ", ", round(lambda + z*se, 2),"]")
```

The expected probability of scoring 2 goals in a match is $\frac{e^{-1.38} 1.38^2}{2!} = 0.239$.

```{r}
dpois(x = 2, lambda = lambda)
```

```{r fig.height=3.5}
events <- 0:10
density <- dpois(x = events, lambda = 3)
prob <- ppois(q = events, lambda = 3, lower.tail = TRUE)
df <- data.frame(events, density, prob)
ggplot(df, aes(x = factor(events), y = density)) +
  geom_col() +
  geom_text(
    aes(label = round(density, 3), y = density + 0.01),
    position = position_dodge(0.9),
    size = 3,
    vjust = 0
  ) +
  geom_line(
    data = df, 
    aes(x = events, y = prob/4), 
    size = 1) +
  scale_y_continuous(sec.axis = sec_axis(~.*4, name = "Cum Prob")) +
  theme_mf() +
  scale_fill_mf() +
  labs(title = "PMF and CDF of Poisson Distribution",
       subtitle = "Poisson(3).",
       x = "Events (x)",
       y = "Density")
```

The expected probability of scoring 2 to 4 goals in a match is

```{r}
sum(dpois(x = c(2:4), lambda = lambda))
```

Or, using the cumulative probability distribution,

```{r}
ppois(q = 4, lambda = lambda) - ppois(q = 1, lambda = lambda)
```

How well does the Poisson distribution fit the 2002 World Cup data?

```{r fig.height=3, message=FALSE}
dat %>%
  mutate(pred = n * dpois(x = goals, lambda = lambda)) %>%
  rename(obs = freq) %>%
  pivot_longer(cols = -goals) %>%
  ggplot(aes(x = goals, y = value, color = name)) +
  geom_point() +
  theme_mf() +
  scale_color_mf() +
  geom_smooth(se = FALSE) +
  labs(
    title = "Poisson Dist: Observed vs Expected",
    color = "",
    y = "frequencey"
  )
```

It fits the data pretty good!

$Poison(\lambda) \rightarrow Bin(n, \pi)$ when $n\pi = \lambda$ and $n \rightarrow \infty$ and $\pi \rightarrow 0$. Because the Poisson is limit of the $Bin(n, \pi)$, it is useful as an approximation to the binomial when $n$ is large ($n>=20$) and $\pi$ small ($p<=0.05$).

For example, suppose a baseball player has a p=.03 chance of hitting a homerun.  What is the probability of X>=20 homeruns in 500 at-bats?  This is a binomial process because the sample size is fixed.

```{r}
pbinom(q = 20, size = 500, prob = 0.03, lower.tail = FALSE)
```

But $n$ is large and $\pi$ is small, so the Poission distribution will work well too.

```{r}
ppois(q = 20, lambda = 0.03 * 500, lower.tail = FALSE)
```

What is the distribution of successes from a sample of n = 50 when the probability of success is p = .03?
```{r fig.height=3, fig.width=5}
n = 500
p = 0.03
x = 0:30
data.frame(
  events = x, 
  Poisson = dpois(x = x, lambda = p * n),
  Binomial = dbinom(x = x, size = n, p = p)
) %>%
  pivot_longer(cols = -events) %>%
  ggplot(aes(x = events, y = value, color = name)) +
  geom_point() +
  theme_mf() +
  scale_color_mf() +
  labs(title = "Poisson(15) vs. Bin(500, .03)",
       subtitle = "Poisson approximation to binomial.",
       x = "Events",
       y = "Density",
       color = "")

```

When the observed variance is greater than $\lambda$ (overdispersion), the Negative Binomial distribution can be used instead of Poisson.


Suppose the probability that a drug produces a certain side effect is p =  = 0.1% and n = 1,000 patients in a clinical trial receive the drug. What is the probability 0 people experience the side effect?

The expected value is np, `r 1000 * .001`.  The probability of measuring 0 when the expected value is 1 is `dpois(x = 0, lambda = 1000 * .001) = ` `r dpois(x = 0, lambda = 1000 * .001)`.

```{r echo=FALSE, fig.height=3, fig.width=5}
options(scipen = 999, digits = 2) # sig digits

x <- 0:10
density <- dpois(x = x, lambda = 1000 * .001)
prob <- ppois(q = x, lambda = 1000 * .001, lower.tail = TRUE)
df <- data.frame(x, density, prob)
ggplot(df, aes(x = x, y = density)) +
  geom_col() +
  geom_text(
    aes(label = round(density,2), y = density + 0.01),
    position = position_dodge(0.9),
    size = 3,
    vjust = 0
  ) +
  theme_mf() +
  scale_fill_mf() +
  labs(title = "Poisson(1)",
       subtitle = "PMF and CDF of Poisson(1) distribution.",
       x = "Events (x)",
       y = "Density") +
  geom_line(data = df, aes(x = x, y = prob))
```


### Multinomial

If $X = (X_1, X_2, \cdots, X_k)$ are the counts of successful events in $n$ identical and independent trials of success probabilities $\pi = (\pi_1, \pi_2, \cdots, \pi_k)$, then $X$ is a random variable with a multinomial distribution $X \sim  Mult(n,\pi)$ 

$$f(x; n, \pi) = \frac{n!}{x_{1}! x_{2}! \cdots x_{k}!} \pi^{x_1} \pi^{x_2} \cdots \pi^{x_k} \hspace{1cm} x \in \{0, 1, ..., n \}, \hspace{2mm} \pi \in [0, 1]$$

with expected values vector $E(X_j) = n\pi_j$ and covariance matrix

$$Var(X) = \begin{bmatrix}n\pi_{1}(1-\pi_{1}) & -n\pi_{1}\pi_{2} & \cdots & -n\pi_{1}\pi_{k}\\
-n\pi_{1}\pi_{2} & n\pi_{2}(1-\pi_{2}) & \cdots & -n\pi_{2}\pi_{k}\\
\vdots & \vdots & \ddots & \vdots\\
-n\pi_{1}\pi_{k} & -n\pi_{2}\pi_{k} & \cdots & n\pi_{k}(1-\pi_{k})
\end{bmatrix}$$

so $Var(X_j) = n \pi_j (1 - \pi_j)$ and $cov(X_j, X_k) = -n \pi_j \pi_k$.

The individual components of a multinomial random vector are binomial and have a binomial distribution, $X_i = Bin(n, \pi_i)$. Binomial is a special case of multinomial for k = 2. 

Suppose a city population is 20% black, 15% Hispanic, and 65% other.  From a random sample of $n = 12$ persons, what is the probability of 4 black and 8 other?

$$f(x;\pi) = \frac{12!}{4! 0! 8!} (0.20)^4 (0.15)^0 (0.65)^8 = 0.0252$$

Function `dmultinom()` calculates the multinomial probability.

```{r}
dmultinom(x = c(4, 0, 8), prob = c(0.20, 0.15, 0.65))
```

To calculate the probability of *<= 1* black, combine Hispanic and other, then sum the probability of black = 1 and black = 2. 

$$f(x;\pi) = \frac{12!}{0! 12!} (0.20)^0 (0.80)^{12} + \frac{12!}{1! 11!} (0.20)^1 (0.80)^{11} = 0.2748$$

```{r}
dmultinom(x = c(0, 12), prob = c(0.20, 0.80)) + 
  dmultinom(x = c(1, 11), prob = c(0.20, 0.80))
```


### Negative-Binomial

If $X$ is the count of failure events ocurring prior to reaching $r$ successful events in a sequence of Bernouli trias of success probability $p$, then $X$ is a random variable with a negative-binomial distribution $X \sim NB(r, p)$. The probability of $X = x$ failures prior to $r$ successes is

$$f(x;r, p) = {{x + r - 1} \choose {r - 1}} p^r (1-p)^{x}.$$

with $E(X) = r (1 - p) / p$ and $Var(X) = r (1-p) / p^2$.

When the data has overdispersion, model the data with the negative-binomial distribution instead of Poission.

#### Examples {-}

An oil company has a $p = 0.20$ chance of striking oil when drilling a well.  What is the probability the company drills $x + r = 7$ wells to strike oil $r = 3$ times?  Note that the question is formulated as counting total events, $x + r = 7$, so translate it to total *failed* events, $x = 4$.

$$f(x;r, p) = {{4 + 3 - 1} \choose {3 - 1}} (0.20)^3 (1 - 0.20)^4 = 0.049.$$

Function `dnbinom()` calculates the negative-binomial probability.  Parameter `x` equals the number of failures, $x - r$.

```{r}
dnbinom(x = 4, size = 3, prob = 0.2)
```

The expected number of failures prior to 3 successes is $E(X) = 3 (1 - 0.20) / 0.20 = 12$ with variance $Var(X) = 3 (1 - 0.20) / 0.20^2 = 60$. Confirm this with a simulation from n = 10,000 random samples using `rnbinom()`.

```{r}
my_dat <- rnbinom(n = 10000, size = 3, prob = 0.20)
mean(my_dat)
var(my_dat)
```


```{r message=FALSE, warning=FALSE, fig.height=3, fig.width=6, echo=FALSE}
data.frame(x = 0:40, d = dnbinom(x = 0:40, size = 3, prob = 0.20)) %>%
  ggplot(aes(x = x, y = d, fill = x == 12)) +
  geom_col(alpha = 0.8) + 
  theme_mf() +
  scale_fill_mf() +
  theme(legend.position = "none") +
  labs(title = "NB(x; r = 3, p = 0.20)", 
       subtitle = "Expected number of failures is 12.",
       y = "dnbinom") 
```


### Geometric

If $X$ is the count of Bernoulli trials of success probability $p$ required to achieve the first successful event, then $X$ is a random variable with a geometric distribution $X \sim G(p)$. The probability of $X = x$ trials is

$$f(x; p) = p(1-p)^{x-1}.$$

with $E(X)=\frac{{n}}{{p}}$ and $Var(X) = \frac{(1-p)}{p^2}$.  The probability of $X<=n$ trials is 

$$F(X=n) = 1 - (1-p)^n.$$ 

#### Examples {-}

What is the probability a marketer encounters x = 3 people on the street who did not attend a sporting event before the first success if the population probability is p = 0.20?

$$f(4; 0.20) = 0.20(1-0.20)^{4-1} = 0.102.$$

Function `dgeom()` calculates the geometric distribution probability.  Parameter `x` is the number of *failures*, not the number of trials. 

```{r}
dgeom(x = 3, prob = 0.20)
```

```{r message=FALSE, warning=FALSE, fig.height=3, fig.width=5}
data.frame(cnt = rgeom(n = 10000, prob = 0.20)) %>%
  count(cnt) %>%
  top_n(n = 15, wt = n) %>%
  ungroup() %>%
  mutate(pct = round(n / sum(n), 3),
         X_eq_x = cnt == 3) %>%
  ggplot(aes(x = as.factor(cnt), y = n, fill = X_eq_x, label = pct)) +
  geom_col(alpha = 0.8) +
  scale_fill_mf() +
  geom_text(size = 3) +
  theme_mf() +
  theme(legend.position = "none") +
  labs(title = "Distribution of trials prior to first success",
       subtitle = paste("P(X = 3) | X ~ G(.2) = ", round(dgeom(2, .2), 3)),
       x = "Unsuccessful trials",
       y = "Count",
       caption = "simulation of n = 10,000 samples from geometric dist.") 
```

What is the probability the marketer fails to find someone who attended a game in x <= 5 trials before finding someone who attended a game on the sixth trial when the population probability is p = 0.20?

```{r}
p = 0.20
n = 5
# exact
pgeom(q = n, prob = p, lower.tail = TRUE)
# simulated
mean(rgeom(n = 10000, prob = p) <= n)
```


```{r fig.width=5, echo=FALSE}
data.frame(
  x = 0:10, 
  pmf = dgeom(x = 0:10, prob = p),
  cdf = pgeom(q = 0:10, prob = p, lower.tail = TRUE)
) %>%
  mutate(Failures = ifelse(x <= n, n, "other")) %>%
  ggplot(aes(x = factor(x), y = cdf, fill = Failures)) +
  geom_col(alpha = 0.8) +
  geom_text(
    aes(label = round(cdf,2), y = cdf + 0.01),
    position = position_dodge(0.9),
    size = 3,
    vjust = 0
  ) +
  theme_mf() +
  scale_fill_mf() +
  labs(
    title = "Cumulative Probability of X = 5 Failures.",
    subtitle = "G(.3)",
    x = "Failures prior to first success (x)",
    y = "probability"
  ) 
```

What is the probability the marketer fails to find someone who attended a game on x >= 5 trials before finding someone who attended a game on the next trial?

```{r}
p = 0.20
n = 5
# exact
pgeom(q = n, prob = p, lower.tail = FALSE)
# simulated
mean(rgeom(n = 10000, prob = p) > n)
```


```{r fig.width=5, echo=FALSE}
data.frame(x = 0:10, 
           pmf = dgeom(x = -1:9, prob = p),
           cdf = pgeom(q = -1:9, prob = p, lower.tail = FALSE)) %>%
  mutate(Failures = ifelse(x >= n + 1, n + 1, "other")) %>%
ggplot(aes(x = factor(x), y = cdf, fill = Failures)) +
  geom_col() +
  theme_mf() +
  scale_fill_mf() +
  geom_text(
    aes(label = round(cdf,2), y = cdf + 0.01),
    position = position_dodge(0.9),
    size = 3,
    vjust = 0
  ) +
  labs(title = "Cumulative Probability of X = 6 Failures (Right Tail).",
       subtitle = "G(.3)",
       x = "Failures prior to first success (x)",
       y = "probability") 
```

The expected number of trials to achieve the first success is `1 / 0.20 = ` `r 1 / 0.20`, `Var(X) = (1 - 0.20) / 0.20^2 = ` `r (1 - 0.20) / 0.20^2`? 

```{r}
p = 0.20
# mean
# exact
1 / p
# simulated
mean(rgeom(n = 10000, prob = p)) + 1

# Variance
# exact
(1 - p) / p^2
# simulated
var(rgeom(n = 100000, prob = p))
```


### Hypergeometric

If $X$ is the count of successful events in a sample of size $n$ *without replacement* from a population of size $N$ containing $K$ successes and $N-K$ non-successes, then $X$ is a random variable with a hypergeometric distribution

$$f(x|N,K,n) = \frac{{{K}\choose{k}}{{N-K}\choose{n-k}}}{{N}\choose{n}}.$$

with $E(X) = n\frac{K}{N}$ and $Var(X) = n \frac{K}{N} \cdot \frac{N-n}{N} \cdot \frac{N-K}{N-1}$.  

The formula follows from the frequency table of the possible outcomes.

| |Sampled |Not Sampled |Total |
|---|---|---|---|
|success|k|K-k|K|
|non-success|n-k|(N-K)-(n-k)|N-K|
|Total|n|N-n|N|


If $X$ is the count of successful events in a sample of size $k$ *without replacement* from a population containing $M$ successes and $N$ non-successes, then $X$ is a random variable with a hypergeometric distribution

$$f(x|m,n,k) = \frac{{{m}\choose{x}}{{n}\choose{k-x}}}{{m+n}\choose{k}}.$$

with $E(X)=k\frac{m}{m+n}$ and $Var(X) = k\frac{m}{m+n}\cdot\frac{m+n-k}{m+n}\cdot\frac{n}{m+n-1}$.  

`phyper` returns the cumulative probability (percentile) `p` at the specified value (quantile) `q`.  `qhyper` returns the value (quantile) `q` at the specified cumulative probability (percentile) `p`.


#### Example {-}

What is the probability of selecting $X = 14$ red marbles from a sample of $k = 20$ taken from an urn containing $m = 70$ red marbles and $n = 30$ green marbles?

Function `dhyper()` calculates the hypergeometric probability.

```{r}
x = 14
m = 70
n = 30
k = 20

dhyper(x = x, m = m, n = n, k = k)
```

The expected value is `r k * m / (m + n)` and variance is `r k * m / (m + n) * (m + n - k) / (m + n) * n / (m + n - 1)`. 

```{r message=FALSE, warning=FALSE, echo=FALSE, fig.height=3, fig.width=5}
options(scipen = 999, digits = 2) # sig digits

density = dhyper(x = 1:20, m = m, n = n, k = k)
data.frame(red = 1:20, density) %>%
  mutate(red14 = ifelse(red == 14, "x = 14", "other")) %>%
ggplot(aes(x = factor(red), y = density, fill = red14)) +
  geom_col() +
  geom_text(
    aes(label = round(density,2), y = density + 0.01),
    position = position_dodge(0.9),
    size = 3,
    vjust = 0
  ) +
  theme_mf() +
  scale_fill_mf() +
  labs(title = "PMF of X = x Red Balls",
       subtitle = "Hypergeometric(k = 20, M = 70, N = 30)",
       x = "Number of red balls (x)",
       y = "Density",
       fill = "")
```

The hypergeometric random variable is similar to the binomial random variable except that it applies to situations of sampling *without* replacement from a small population.  As the population size increases, sampling without replacement converges to sampling *with* replacement, and the hypergeometric distribution converges to the binomial. What if the total population size is 250? 500? 1000?

```{r message=FALSE, warning=FALSE, echo=FALSE, fig.height=3, fig.width=5}
library(tidyr)
library(ggplot2)
library(dplyr)
options(scipen = 999, digits = 2) # sig digits

x = 14
m = 7000
n = 3000
k = 20

d_binom <- dbinom(x = 1:20, size = k, prob = m / (m + n))
df_binom <- data.frame(x = 1:20, Binomial = d_binom)
p <- ggplot(df_binom, aes(x = x, y = Binomial)) +
  geom_col()

d_hyper_100 <- dhyper(x = 1:20, m = 70, n = 30, k = k)
d_hyper_250 <- dhyper(x = 1:20, m = 175, n = 75, k = k)
d_hyper_500 <- dhyper(x = 1:20, m = 350, n = 150, k = k)
d_hyper_1000 <- dhyper(x = 1:20, m = 700, n = 300, k = k)
df_hyper = data.frame(x = 1:20, 
                Hyper_0100 = d_hyper_100, 
                Hyper_0250 = d_hyper_250, 
                Hyper_0500 = d_hyper_500, 
                Hyper_1000 = d_hyper_1000)
df_hyper_tidy <- gather(df_hyper, key = "dist", value = "density", -c(x))
p + 
  geom_line(data = df_hyper_tidy, aes(x = x, y = density, color = dist)) +
  theme_mf() +
  scale_color_mf() +
  labs(title = "Hypergeometric Appox. to Binomial",
       subtitle = "Hypergeometric approaches Binomial as population size increases.",
       x = "Number of successful observations (x)",
       y = "Density",
       color = "")
```


### Gamma

If $X$ is the interval until the $\alpha^{th}$ successful event when the average interval is $\theta$, then $X$ is a random variable with a gamma distribution $X \sim \Gamma(\alpha, \theta)$. The probability of an interval of $X = x$ is

$$f(x; \alpha, \theta) = \frac{1}{\Gamma(\alpha)\theta^\alpha}x^{\alpha-1}e^{-x/\theta}.$$

where $\Gamma(\alpha) = (1 - \alpha)!$ with $E(X) = \alpha \theta$ and $Var(X) = \alpha \theta^2$.  

#### Examples {-}

On average, someone sends a money order once per 15 minutes ($\theta = .25$).  What is the probability someone sends $\alpha = 10$ money orders in less than $x = 3$ hours?*

```{r}
theta = 0.25
alpha = 10
pgamma(q = 3, shape = alpha, scale = 0.25)
```


```{r message=FALSE, warning=FALSE}
data.frame(x = 0:1000 / 100, prob = pgamma(q = 0:1000 / 100, shape = alpha, scale = theta, lower.tail = TRUE)) %>%
  mutate(Interval = ifelse(x >= 0 & x <= 3, "0 to 3", "other")) %>%
ggplot(aes(x = x, y = prob, fill = Interval)) +
  geom_area(alpha = 0.9) +
  theme_mf() +
  scale_fill_mf() +
  labs(title = "X ~ Gam(alpha = 10, theta = .25)",
       subtitle = "Probability of 10 events in X hours when the mean time to an event is .25 hours.",
       x = "Interval (x)",
       y = "pgamma") 

```


## Continuous Distributions {#cont_dist}

### Normal

Random variable $X$ is distributed $X \sim N(\mu, \sigma^2)$ if

$$f(X)=\frac{{1}}{{\sigma \sqrt{{2\pi}}}}e^{-.5(\frac{{x-\mu}}{{\sigma}})^2}$$.

#### Example {-}

*IQ scores are distributed $X \sim N(100, 16^2$. What is the probability a randomly selected person's IQ is <90?*

```{r message=FALSE, warning=FALSE}
my_mean = 100
my_sd = 16
my_x = 90
# exact
pnorm(q = my_x, mean = my_mean, sd = my_sd, lower.tail = TRUE)
# simulated
mean(rnorm(n = 10000, mean = my_mean, sd = my_sd) <= my_x)

library(dplyr)
library(ggplot2)

data.frame(x = 0:1500 / 10, 
           prob = pnorm(q = 0:1500 / 10, 
                        mean = my_mean, 
                        sd = my_sd, 
                        lower.tail = TRUE)) %>%
  mutate(cdf = ifelse(x > 0 & x <= my_x, prob, 0)) %>%
ggplot() +
  geom_line(aes(x = x, y = prob)) +
  geom_area(aes(x = x, y = cdf), alpha = 0.3) +
  labs(title = bquote('X~N('~mu==.(my_mean)~','~sigma^{2}==.(my_sd)^{2}~')'),
       subtitle = bquote('P(X<='~.(my_x)~') when mean is'~.(my_mean)~' and variance is'~.(my_sd)^{2}~'.'),
       x = "x",
       y = "Probability") 
```


#### Example
*IQ scores are distributed *$X \sim N(100, 16^2$*. What is the probability a randomly selected person's IQ is >140?*
```{r message=FALSE, warning=FALSE}
my_mean = 100
my_sd = 16
my_x = 140
# exact
pnorm(q = my_x, mean = my_mean, sd = my_sd, lower.tail = FALSE)
# simulated
mean(rnorm(n = 10000, mean = my_mean, sd = my_sd) > my_x)

library(dplyr)
library(ggplot2)

data.frame(x = 0:1500 / 10, 
           prob = pnorm(q = 0:1500 / 10, 
                        mean = my_mean, 
                        sd = my_sd, 
                        lower.tail = TRUE)) %>%
  mutate(cdf = ifelse(x > my_x & x < 1000, prob, 0)) %>%
ggplot() +
  geom_line(aes(x = x, y = prob)) +
  geom_area(aes(x = x, y = cdf), alpha = 0.3) +
  labs(title = bquote('X~N('~mu==.(my_mean)~','~sigma^{2}==.(my_sd)^{2}~')'),
       subtitle = bquote('P(X<='~.(my_x)~') when mean is'~.(my_mean)~' and variance is'~.(my_sd)^{2}~'.'),
       x = "x",
       y = "Probability") 
```

#### Example
*IQ scores are distributed *$X \sim N(100, 16^2$*. What is the probability a randomly selected person's IQ is between 92 and 114?*
```{r message=FALSE, warning=FALSE}
my_mean = 100
my_sd = 16
my_x_l = 92
my_x_h = 114
# exact
pnorm(q = my_x_h, mean = my_mean, sd = my_sd, lower.tail = TRUE) -
  pnorm(q = my_x_l, mean = my_mean, sd = my_sd, lower.tail = TRUE)

library(dplyr)
library(ggplot2)

data.frame(x = 0:1500 / 10, 
           prob = pnorm(q = 0:1500 / 10, 
                        mean = my_mean, 
                        sd = my_sd, 
                        lower.tail = TRUE)) %>%
  mutate(cdf = ifelse(x > my_x_l & x <= my_x_h, prob, 0)) %>%
ggplot() +
  geom_line(aes(x = x, y = prob)) +
  geom_area(aes(x = x, y = cdf), alpha = 0.3) +
  labs(title = bquote('X~N('~mu==.(my_mean)~','~sigma^{2}==.(my_sd)^{2}~')'),
       subtitle = bquote('P(X<='~.(my_x)~') when mean is'~.(my_mean)~' and variance is'~.(my_sd)^{2}~'.'),
       x = "x",
       y = "Probability") 
```


#### Example
*Class scores are distributed *$X \sim N(70, 10^2$*. If the instructor wants to give A's to >=85th percentile and B's to 75th-85th percentile, what are the cutoffs?*
```{r message=FALSE, warning=FALSE}
my_mean = 70
my_sd = 10
my_pct_l = .75
my_pct_h = .85

qnorm(p = my_pct_l, mean = my_mean, sd = my_sd, lower.tail = TRUE)
qnorm(p = my_pct_h, mean = my_mean, sd = my_sd, lower.tail = TRUE)

library(dplyr)
library(ggplot2)

data.frame(x = 0:1000 / 10, 
           prob = pnorm(q = 0:1000 / 10, 
                        mean = my_mean, 
                        sd = my_sd, 
                        lower.tail = TRUE)) %>%
  mutate(cdf = ifelse(prob > my_pct_l & prob <= my_pct_h, prob, 0)) %>%
ggplot() +
  geom_line(aes(x = x, y = prob)) +
  geom_area(aes(x = x, y = cdf), alpha = 0.3) +
  labs(title = bquote('X~N('~mu==.(my_mean)~','~sigma^{2}==.(my_sd)^{2}~')'),
       subtitle = bquote('P(X<=x) = ['~.(my_pct_l)~','~.(my_pct_h)~'] when mean is'~.(my_mean)~' and variance is'~.(my_sd)^{2}~'.'),
       x = "x",
       y = "Probability") 
```


#### Normal Approximation to Binomial

The CLT implies that certain distributions can be approximated by the normal distribution.  

The binomial distribution $X \sim B(n,p)$ is approximately normal with mean $\mu = n p$ and variance $\sigma^2=np(1-p)$.  The approximation is useful when the expected number of successes and failures is at least 5:  $np>=5$ and $n(1-p)>=5$.

#### Example
*A measure requires p>=50% popular to pass.  A sample of n=1,000 yields x=460 approvals. What is the probability that the overall population approves, P(X)>0.5?*
```{r message=FALSE, warning=FALSE}
my_x = 460
my_p = 0.50
my_n = 1000

my_mean = my_p * my_n
my_sd = round(sqrt(my_n * my_p * (1 - my_p)), 1)

# Exact binomial
pbinom(q = my_x, size = my_n, prob = my_p, lower.tail = TRUE)

# Normal approximation
pnorm(q = my_x, mean = my_p * my_n, sd = sqrt(my_n * my_p * (1 - my_p)), lower.tail = TRUE)

library(dplyr)
library(ggplot2)
library(tidyr)

data.frame(x = 400:600, 
           Normal = pnorm(q = 400:600, 
                        mean = my_p * my_n, 
                        sd = sqrt(my_n * my_p * (1 - my_p)), 
                        lower.tail = TRUE),
           Binomial = pbinom(q = 400:600, 
                        size = my_n, 
                        prob = my_p, 
                        lower.tail = TRUE)) %>%
  gather(key = "Distribution", value = "cdf", c(-x)) %>%
  ggplot(aes(x = x, y = cdf, color = Distribution)) +
  geom_line() +
  labs(title = bquote('X~B(n='~.(my_n)~', p='~.(my_p)~'),  '~'X~N('~mu==.(my_mean)~','~sigma^{2}==.(my_sd)^{2}~')'),
       subtitle = "Normal approximation to the binomial",
       x = "x",
       y = "Probability") 
```


The Poisson distribution $x~P(\lambda)$ is approximately normal with mean $\mu = \lambda$ and variance $\sigma^2 = \lambda$, for large values of $\lambda$.

#### Example
*The annual number of earthquakes registering at least 2.5 on the Richter Scale and having an epicenter within 40 miles of downtown Memphis follows a Poisson distribution with mean *$\lambda=6.5$*. What is the probability that at least *$x>=9$* such earthquakes will strike next year?*
```{r message=FALSE, warning=FALSE}
my_x = 9
my_lambda = 6.5
my_sd = round(sqrt(my_lambda), 2)

# Exact Poisson
ppois(q = my_x - 1, lambda = my_lambda, lower.tail = FALSE)

# Normal approximation
pnorm(q = my_x - 0.5, mean = my_lambda, sd = my_sd, lower.tail = FALSE)

library(dplyr)
library(ggplot2)
library(tidyr)

data.frame(x = 0:200 / 10, 
           Normal = pnorm(q = 0:200 / 10, 
                        mean = my_lambda, 
                        sd = my_sd, 
                        lower.tail = TRUE),
           Poisson = ppois(q = 0:200 / 10, 
                        lambda = my_lambda, 
                        lower.tail = TRUE)) %>%
  gather(key = "Distribution", value = "cdf", c(-x)) %>%
  ggplot(aes(x = x, y = cdf, color = Distribution)) +
  geom_line() +
  labs(title = bquote('X~P('~lambda~'='~.(my_lambda)~'),  '~'X~N('~mu==.(my_lambda)~','~sigma^{2}==.(my_lambda)~')'),
       subtitle = "Normal approximation to the Poisson",
       x = "x",
       y = "Probability") 
```

#### From Sample to Population

*Suppose a person's blood pressure typically measures 160?20 mm.  If one takes n=5 blood pressure readings, what is the probability the average will be <=150?*
```{r message=FALSE, warning=FALSE}
my_mu = 160
my_sigma = 20
my_n = 5
my_x = 150

my_se = round(my_sigma / sqrt(my_n), 1)

pnorm(q = my_x, mean = my_mu, sd = my_sigma / sqrt(my_n), lower.tail = TRUE)

library(dplyr)
library(ggplot2)

data.frame(x = 1000:2000 / 10, 
           prob = pnorm(q = 1000:2000 / 10, 
                        mean = my_mu, 
                        sd = my_sigma / sqrt(my_n), 
                        lower.tail = TRUE)) %>%
  mutate(cdf = ifelse(x > 0 & x <= my_x, prob, 0)) %>%
ggplot() +
  geom_line(aes(x = x, y = prob)) +
  geom_area(aes(x = x, y = cdf), alpha = 0.3) +
  labs(title = bquote('X~N('~mu==.(my_mu)~','~sigma^{2}==.(my_se)^{2}~')'),
       subtitle = bquote('P(X<='~.(my_x)~') when mean is'~.(my_mu)~' and variance is'~sigma~'/sqrt(n)'~.(my_se)^{2}~'.'),
       x = "x",
       y = "Probability") 
```

```{r}
knitr::include_app("https://mpfoley73.shinyapps.io/shiny_dist/", 
  height = "600px")
```


## Join Distributions

## Likelihood

The *likelihood function* is the likelihood of a parameter $\theta$ given an observed value of the random variable $X$.  The likelihood function is identical to the probability distribution function, except that it reverses which variable is considered fixed.  E.g., the binomial *probability* distribution expresses the probability that $X = x$ given the success probability $\theta = \pi$.

$$f(x|\pi) = \frac{n!}{x!(n-x)!} \pi^x (1-\pi)^{n-x}.$$

The corresponding *likelihood* function expresses the probability that $\pi = p$ given the observed value $x$.

$$L(p|x) = \frac{n!}{x!(n-x)!} p^x (1-p)^{n-x}.$$

You usually want to know the value of $\theta$ at the *maximum* of the likelihood function.  When taking derivatives, any multiplicative constant is irrevelant and can be discarded.  So for the binomial distribution, the likelihood function for $\pi$ may instead be expressed as

$$L(p|x) \propto p^x (1-p)^{n-x}$$

Calculating the maximum is usually simplified using the *log-likelihood*, $l(\theta|x) = \log L(\theta|x)$.  For the binomial distribution, $l(p|x) = x \log p + (n - x) \log (1 - p)$.  Frequently you derive loglikelihood from a sample.  The overall likelihood is the product of the individual likelihoods, and the overall loglikelihood is the log of the overall likelihood.

$$l(\theta|x) = \log \prod_{i=1}^n f(x_i|\theta)$$

Here are plots of the binomial log-likelihood of $pi$ for several values of $X$ from a sample of size $n = 5$.

```{r fig.height=3, fig.width=5, echo=FALSE}
data.frame(
  n = rep(5, 303),
  p = rep((0:100)/100, 3),
  x = c(rep(0, 101), rep(1, 101), rep(2, 101))
) %>%
  mutate(
    L = p^x * (1 - p)^(n - x),
    l = log(L),
    l2 = x * log(p) + (n - x) * log(1 - p)
  ) %>%
  ggplot(aes(x = p, y = l, color = as.factor(x))) +
    geom_point() +
  theme_mf() +
  scale_color_mf() +
  labs(
    x = "pi", 
    y = "log-likelihood", 
    title = "Binomial Log-Likelihood for pi",
    subtitle = "Selected values of X from sample size n = 5",
    caption = "",
    color = "X")
```

As the total sample size $n$ grows, the loglikelihood function becomes more sharply peaked around its maximum, and becomes nearly quadratic (i.e. a  parabola, if there is a single parameter).  Here is the same plot with $n = 500$.

```{r fig.height=3, fig.width=5, echo=FALSE}
data.frame(
  n = rep(500, 303),
  p = rep((0:100)/100, 3),
  x = c(rep(0, 101), rep(100, 101), rep(200, 101))
) %>%
  mutate(
    L = p^x * (1 - p)^(n - x),
    l = log(L),
    l2 = x * log(p) + (n - x) * log(1 - p)
  ) %>%
  ggplot(aes(x = p, y = l, color = as.factor(x))) +
    geom_point() +
  theme_mf() +
  scale_color_mf() +
  labs(
    x = "pi", 
    y = "log-likelihood", 
    title = "Binomial Log-Likelihood for pi",
    subtitle = "Selected values of X from sample size n = 5",
    caption = "",
    color = "X")
```

The value of $\theta$ that maximizes $l$ (and $L$) is the *maximum-likelihood estimator* (MLE) of $\theta$, $\hat{\theta}$. E.g., suppose you have an experiment of $n = 5$ Bernoulli trials  $\left(X \sim Bin(5, \pi) \right)$ with and $X = 3$ successful events. A plot of $L(p|x) = p^3(1 - p)^2$ shows the MLE is at $p = 0.6$.

```{r fig.height=3, fig.width=5, echo=FALSE}
data.frame(
  p = 0:100*.01
) %>%
  mutate(L = p^3 * (1 - p)^2) %>%
ggplot(aes(x = p, y = L)) +
  geom_line() +
  geom_vline(xintercept = 0.6) +
  theme_mf() +
  labs(
    x = "pi",
    y = "Likelihood",
    title = "Likelihood Function for Binomial Dist.",
    subtitle = "n = 5 trials with X = 3 successful events. Max is at 0.6."
  )
```

This approach is called *maximum-likelihood* estimation. MLE usually involves setting the derivatives to zero and solving for $theta$. 




<!--chapter:end:01-probability.Rmd-->

```{r include=FALSE}
library(tidyverse)
library(mfstylr)
```

# Categorical Analysis - Nonmodel {#discrete_analysis}

This section describes interactions or associations between two or three categorical variables mostly via single summary statistics and with significance testing. This non-model based analysis does not handle more complicated situations such as simultaneous effects of multiple variables, or mixtures of categorical and continuous variables. 

## Chi-Square Test

These notes rely on [PSU STAT 500](https://newonlinecourses.science.psu.edu/stat500/node/56/),  [Wikipedia](https://en.wikipedia.org/wiki/Chi-squared_test), and [Disha M](http://www.yourarticlelibrary.com/project-reports/chi-square-test/chi-square-test-meaning-applications-and-uses-statistics/92394).

The chi-square test compares observed categorical variable frequency counts $O$ with their expected values $E$.  The test statistic $X^2 = \sum (O - E)^2 / E$ is distributed $\chi^2$.  $H_0: O = E$ and $H_a$ is at least one pair of frequency counts differ. The chi-square test relies on the central limit theorem, so it is valid for independent, normally distributed samples, typically affirmed with at least 5 successes and failures in each cell.  There a small variations in the chi-square for its various applications.

* The **chi-square goodness-of-fit test** tests whether observed frequency counts $O_j$ of the $j \in (0, 1, \cdots k)$ levels of a single categorical variable differ from expected frequency counts $E_j$. $H_0$ is $O_j = E_j$.

* The **chi-square independence test** tests whether observed joint frequency counts $O_{ij}$ of the $i \in (0, 1, \cdots I)$ levels of categorical variable $Y$ and the $j \in (0, 1, \cdots J)$ levels of categorical variable $Z$ differ from expected frequency counts $E_{ij}$ under the *independence model* where $\pi_{ij} = \pi_{i+} \pi_{+j}$, the joint densities. $H_0$ is $O_{ij} = E_{ij}$.

* The **chi-square homogeneity test** tests whether frequency counts of the $R$ levels of a categorical variable are distributed identically across $C$ different populations.

## One-Way Tables

These notes rely on PSU STATS 504 [course notes](https://online.stat.psu.edu/stat504/node/60/).

A one-way table is a frequency table for a single categorical variable.  You usually construct a one-way table to test whether the frequency counts differ from a hypothesized distribution using the chi-square goodness-of-fit test.  You may also simply want to construct a confidence interval around a proportion.

Here is an example.  A researcher crosses tall cut-leaf tomatoes with dwarf potato-leaf tomatoes, then classifies the ($n = 1,611$) offspring phenotypes.

```{r}
o <- c(926, 288, 293, 104)
cell_names <- c("tall cut-leaf", "tall potato-leaf", "dwarf cut-leaf", "dwarf potato-leaf")
names(o) <- cell_names
print(o)
```

The four phenotypes are expected to occur with relative frequencies 9:3:3:1.

```{r}
pi <- c(9, 3, 3, 1) / (9 + 3 + 3 + 1)
print(pi)
```

```{r}
e <- sum(o) * pi
names(e) <- cell_names
print(e)
```


```{r fig.height=4}
data.frame(O = o, E = e) %>%
  rownames_to_column(var = "i") %>%
  pivot_longer(cols = -i, values_to = "freq") %>%
  group_by(name) %>%
  mutate(pct = freq / sum(freq)) %>%
  ungroup() %>%
  ggplot(aes(x = i, y = freq, fill = name, 
             label = paste0(round(freq, 0), "\n", 
                            scales::percent(pct, accuracy = 0.1)))
         ) +
  geom_col(position = position_dodge()) +
  geom_text(position = position_dodge(width = 0.9), size = 2.8) +
  theme_mf() +
  scale_fill_mf() +
  labs(title = "Observed vs Expected", fill = "")
```

Do the observed phenotype counts conform to the expected proportions?  This is a goodness-of-fit question because you are comparing frequencies from a single categorical variable to a set of hypothesized frequencies.


### Chi-Square Goodness-of-Fit Test

The **chi-square goodness-of-fit test** tests whether observed frequency counts $O_j$ of the $J$ levels of a categorical variable differ from expected frequency counts $E_j$ in a sample. $H_0$ is $O_j = E_j$.

There are two possible test statistics for this test, Pearson $X^2$ and deviance $G^2$.  The sampling distributions of $X^2$ and $G^2$ approach the $\chi_{J-1}^2$ as the sample size $n \rightarrow \infty$.  It's a good idea to calculate both test statistics.

The Pearson goodness-of-fit statistic is

$$X^2 = \sum \frac{(O_j - E_j)^2}{E_j}$$

where $O_j = p_j n$ and $E_j = \pi_j n$.  There is a variation of the $X^2$ statistic that corrects for small cell counts by subtracting 0.5 from each cell, the Yates Continuity Correction.  

$$X^2 = \sum \frac{(O_j - E_j - 0.5)^2}{E_j}$$

The deviance statistic, aka likelihood-ratio chi-square test statistic, is 

$$G^2 = 2 \sum O_j \log \left[ \frac{O_j}{E_j} \right]$$

If the saturated model (the observed data represent the fit of the saturated model, the most complex model possible with the data) proportions $p_j$ equal equal the expected proportions $\pi_j$, $X^2$ and $G^2$ will equal zero.  Large values indicate the data do not agree well with the proposed model.

You can perform a chi-square test of significance with the $G^2$ and $X^2$ test statistics with $dof$ degrees of freedom (d.f.).  The chi-square test is reliable when at least 80% of $E_j >= 5$.  

```{r echo=FALSE}
x2 <- sum((o - e)^2 / e)
g2 <- 2 * sum(o * log(o / e))
dof <- length(o) - 1
```

Calculate $X^2$ as `x2 <- sum((o - e)^2 / e) = ` `r  x2` and the $G^2$ as `g2 <- 2 * sum(o * log(o / e)) = ` `r g2`. The degrees of freedom are `length(o) - 1 = ` `r dof`.  The chi-sq test p-values are nearly identical.

```{r}
pchisq(q = x2, df = dof, lower.tail = FALSE)
pchisq(q = g2, df = dof, lower.tail = FALSE)
```

`chisq.test()` performs the chi-square test of the Pearson test statistic.

```{r}
chisq.test(o, p = pi)
```

The p-values based on the $\chi^2$ distribution with 3 d.f. are about 0.69, so the test fails to reject the null hypothesis that the observed frequencies are consistent with the theory. The plot of the chi-squared distribution shows $X^2$  well outside the $\alpha = 0.05$ range of rejection.

```{r warning=FALSE, message=FALSE, fig.height=4}
alpha <- 0.05
dof <- length(e) - 1
lrr = -Inf
p_val <- pchisq(q = x2, df = length(o) - 1, lower.tail = FALSE)
urr = qchisq(p = alpha, df = dof, lower.tail = FALSE)
 data.frame(chi2 = seq(from = 0, to = 20, by = .1)) %>%
   mutate(density = dchisq(x = chi2, df = dof)) %>%
   mutate(rr = ifelse(chi2 < lrr | chi2 > urr, density, 0)) %>%
 ggplot() +
   geom_line(aes(x = chi2, y = density), color = mf_pal(12)(12)[12], size = 0.8) +
   geom_area(aes(x = chi2, y = rr), fill = mf_pal(12)(12)[2], alpha = 0.8) +
   geom_vline(aes(xintercept = x2), color = mf_pal(12)(12)[11], size = 0.8) +
   labs(title = bquote("Chi-Square Goodness-of-Fit Test"),
        subtitle = paste0("X^2=", round(x2,2), ", ",
                          "Critical value=", round(urr,2), ", ",
                          "p-value=", round(p_val,3), "."
                          ),
        x = "chisq",
        y = "Density") +
   theme(legend.position="none") +
   theme_mf()
```

If you reject $H_0$, you can inspect the residuals to learn which differences may have lead to rejecting the rejection.  $X^2$ and $G^2$ are sums of squared cell comparisons, or "residuals".  The expected value of a $\chi^2$ random variable is its d.f., $k - 1$, so the average residual size is $(k - 1) / k$. The typical residual should be within 2 $\sqrt{(k - 1) / k}$.

```{r}
e2_res <- sqrt((o - e)^2 / e)
g2_res <- sign(o - e) * sqrt(abs(2 * o * log(o / e)))
```

```{r fig.height=4}
data.frame(e2_res) %>%
  rownames_to_column() %>%
  # pivot_longer(cols = e2_res:g2_res) %>%
  ggplot(aes(x = rowname, y = e2_res)) +
  geom_point(size = 3, color = mf_pal(12)(12)[2], alpha = 0.8) +
  theme_mf() +
  labs(title = "X^2 Residuals by Cell", color = "", x = "", y = "")
```

If you want to test whether the data conform to a particular distribution instead of some set of theoretical values, the test is nearly the same except for an adjustment to the d.f.  Your first step is the estimate the distribution's parameter(s).  Then you perform the goodness of fit test, but with degrees of freedom reduced for each estimated parameter.

For example, suppose you sample $n = 100$ families and count the number of children.  The count of children should be a Poisson random variable, $J \sim  Pois(\lambda)$.  

```{r}
dat <- data.frame(j = 0:5, o = c(19, 26, 29, 13, 10, 3))
```

The ML estimate for $\lambda$ is

$$\hat{\lambda} = \frac{j_0 O_0 + j_1 O_1, + \cdots j_k O_k}{O}$$

```{r}
lambda_hat <- sum(dat$j * dat$o) / sum(dat$o)
print(lambda_hat)
```

The probabilities for each possible count are 

$$f(j; \lambda) = \frac{e^{-\hat{\lambda}} \hat{\lambda}^j}{j!}.$$

```{r}
f <- exp(-lambda_hat) * lambda_hat^dat$j / factorial(dat$j)
E <- f * sum(dat$o)
dat <- cbind(dat, e = E)
```

```{r fig.height=4}
dat %>%
  rename(pois = e) %>%
  pivot_longer(cols = -j, values_to = "freq") %>%
  group_by(name) %>%
  mutate(pct = freq / sum(freq)) %>%
  ungroup() %>%
  ggplot(aes(x = fct_inseq(as.factor(j)), y = freq, fill = name, 
             label = paste0(round(freq, 0), "\n", 
                            scales::percent(pct, accuracy = 0.1)))
         ) +
  geom_col(position = position_dodge()) +
  geom_text(position = position_dodge(width = 0.9), size = 2.8) +
  theme_mf() +
  scale_fill_mf() +
  labs(title = "Observed vs Expected", fill = "", x = "children in family")
```

Compare the expected values to the observed values with the $\chi^2$ goodness of fit test.  In this case, $df = 6 - 1 - 1$ because the estimated paramater $\lambda$ reduces d.f. by 1.

```{r}
(X2 <- sum((dat$o - dat$e)^2 / dat$e))
(dof <- nrow(dat) - 1 - 1)
pchisq(q = X2, df = dof)
```

Be careful of this adjustment to the d.f. because `chisq.test()` does not take this into account, and you cannot override the d.f..

```{r}
chisq.test(dat$o, p = dat$e / sum(dat$e))
```


### Proportion Test

A special case of the one-way table is the $2 \times 1$ table for a binomial random variable.  When you calculate a single proportion $p$, you can compare it to a hypothesized $\pi_0$, or create a confidence interval around the estimate.

Suppose a company claims to resolve at least 70% of maintenance requests within 24 hours. In a random sample of $n = 50$ repair requests, the company resolves $O_1 = 33$ ($p_1 = 66\%)$ within 24 hours. At a 5% level of significance, is the maintenance company’s claim valid?

```{r}
o <- c(33, 17)
n <- sum(o)
cell_names <- c("resolved", "not resolved")
names(o) <- cell_names
print(o)
```

The null hypothesis is that the maintenance company resolves $\pi_0 = 0.70$ of requests within 24 hours, $H_0: \pi = \pi_0$ with alternative hypothesis $H_a: \pi < \pi_0$. This is a left-tailed test with an $\alpha = 0.05$ level of significance.

```{r}
pi_0 <- 0.70
alpha <- 0.05
```

The sample is independently drawn without replacement from <10% of the population (by assumption) and there were >=5 successes, so you can use the Clopper-Pearson exact binomial test. *Clopper-Pearson inverts two single-tailed binomial tests at the desired alpha*.

```{r}
binom.test(x = o, p = pi_0, alternative = "less", conf.level = 1 - alpha)
```

There is insufficient evidence (p = 0.3161) to reject $H_0$ that true probability of success is less than 0.7.

```{r fig.height=3.5}
x <- c(0:50)
p_x <- dbinom(x = x, size = n, prob = pi_0)
observed <- factor(if_else(x == o[1], 1, 0))
data.frame(x, p_x, observed) %>%
  ggplot(aes(x = x, y = p_x, fill = observed)) +
  geom_col() +
  theme_mf() +
  scale_fill_mf() +
  labs(title = "Exact Binomial")
```

There were >=5 failures, >=30 observations, and the measured probability of success was within (.2,.80), so you can also use the Wald normal approximation method where $\pi = p \pm z_{\alpha/2} SE$ and $Z = (p - \pi_0) / SE$ where $SE = \sqrt{\pi_0 (1 - \pi_0) / n}$.

```{r}
p <- o[1] / sum(o)
se <- sqrt(pi_0 * (1 - pi_0) / sum(o))
z <- (p - pi_0) / se
pnorm(q = p, mean = pi_0, sd = se, lower.tail = TRUE)
```

Again, there is insufficient evidence (p = 0.2685) to reject $H_0$ that true probability of success is less than 0.7.  The 95% CI around the measured p = 0.66 is

```{r}
z_alpha <- qnorm(0.95, mean = p, sd = se, lower.tail = FALSE)
c(0, p + z_alpha * se)
```


## Two-Way Tables

These notes rely on PSU STATS 504 [course notes](https://online.stat.psu.edu/stat504/node/69/).

A two-way frequency table is a frequency table for *two* categorical variables.  You usually construct a two-way table to test whether the frequency counts in one categorical variable differ from the other categorical variable using the chi-square independence test.  If there is a significant difference (i.e., the variables are related), then describe the relationship with an analysis of the residuals, calculations of measures of association (difference in proportions, relative risk, or odds ratio), and partition tests. 

Here are three case studies that illustrate the concepts.  The first is a simple 2x2 table.  The second is a 3x2 table that extends some of the concepts.  The third is a 2x4 table where one factor is *ordinal*.

**Study 1: "Vitamin C" 2x2 Table**.  A double blind study investigated whether vitamin C prevents common colds on a sample of *n* = 279 persons. This study has two categorical variables each with two levels, a 2x2 two way table.

```{r}
vitc_o <- matrix(
  c(31, 17, 109, 122), 
  ncol = 2, 
  dimnames = list(
    treat = c("Placebo", "VitaminC"), 
    resp = c("Cold", "NoCold")
  )
)
vitc_o %>% data.frame() %>% rownames_to_column(var = " ") %>%
janitor::adorn_totals(where = c("row", "col"))
```

**Study 2: "Smoking" 3x2 Table**.  An analysis classifies *n* = 5375 high school students by their smoking behavior and the smoking behavior of their parents.

```{r}
smoke_o <- matrix(
  c(400, 416, 188, 1380, 1823, 1168), 
  ncol = 2,
  dimnames = list(
    parents = c("Both", "One", "Neither"),
    student = c("Smoker", "Non-smoker"))
)
smoke_o %>% data.frame() %>% rownames_to_column(var = " ") %>%
janitor::adorn_totals(where = c("row", "col"))
```

**Study 3: "CHD" Ordinal Table**.  A study of classified *n* = 1329 patients by cholesterol level and whether they had been diagnosed with coronary heart disease (CHD).

```{r}
# tribble() is a little easier.
chd_o <- tribble(
  ~L_0_199, ~L_200_219, ~L_220_259, ~L_260p,
  12, 8, 31, 41,
  307, 246, 439, 245
) %>% as.matrix()
rownames(chd_o) <- c("CHD", "No CHD")
chd_o %>% data.frame() %>% rownames_to_column(var = " ") %>%
janitor::adorn_totals(where = c("row", "col"))
```


### Chi-Square Independence Test

The **chi-square independence test** tests whether observed joint frequency counts $O_{ij}$ differ from expected frequency counts $E_{ij}$ under the *independence model* (the model of independent explanatory variables, $\pi_{ij} = \pi_{i+} \pi_{+j}$. $H_0$ is $O_{ij} = E_{ij}$.

There are two possible test statistics for this test, Pearson $X^2$ (and the continuity adjusted $X^2$), and deviance $G^2$.  As $n \rightarrow \infty$ their sampling distributions approach $\chi_{df}^2$ with degrees of freedom (df) equal to the saturated model df $I \times J - 1$ minus the independence model df $(I - 1) + (J - 1)$, which you can algebraically solve for $df = (I - 1)(J - 1)$.

The Pearson goodness-of-fit statistic is

$$X^2 = \sum \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$$

where $O_{ij}$ is the observed count, and $E_{ij}$ is the product of the row and column marginal probabilities.  For the Vitamin C study, $X^2$ is 

```{r}
vitc_e <- sum(vitc_o) * prop.table(vitc_o, 1) * prop.table(vitc_o, 2)
X2 <- sum((vitc_o - vitc_e)^2 / vitc_e)
print(X2)
```

and the deviance statistic is 

$$G^2 = 2 \sum_{ij} O_{ij} \log \left( \frac{O_{ij}}{E_{ij}} \right)$$

```{r}
G2 <- - 2 * sum(vitc_o * log(vitc_o / vitc_e))
print(G2)
```

$X^2$ and $G^2$ increase with the disagreement between the saturated model proportions $p_{ij}$ and the independence model proportions $\pi_{ij}$.

The degrees of freedom is

```{r}
vitc_dof <- (nrow(vitc_o) - 1) * (ncol(vitc_o) - 1)
print(vitc_dof)
```

The associated p-values are
```{r}
pchisq(q = G2, df = vitc_dof, lower.tail = FALSE)
pchisq(q = X2, df = vitc_dof, lower.tail = FALSE)
```

The `chisq.test()` function applies the Yates continuity correcton by default to correct for situations with small cell counts.  The Yates continuity correction  subtracts 0.5 from the $O_{ij} - E_{ij}$ differences.  Set `correct = FALSE` to suppress Yates.

```{r}
vitc_chisq_test <- chisq.test(vitc_o, correct = FALSE)
print(vitc_chisq_test)
```

The Yates correction yields more conservative p-values.

```{r}
chisq.test(vitc_o)
```

These p-values are evidence for rejecting the independence model.

Here is the chi-square test applied to the CHD data.  Recall this data set is 4x2, so the degrees of freedom are $(4-1)(2-1) = 3$.  The Yates continuity correction does not apply to data other than 2x2, so the `correct = c(TRUE, FALSE)` has no effect in `chisq.test()`.

```{r}
(chd_chisq_test <- chisq.test(chd_o))
```

The p-value is very low, so reject the null hypothesis of independence. This demonstrates that a relationship exists between cholesterol and CHD.  Now you should describe that relationship by evaluating the (i) residuals, (ii) measures of association, and (iii) partitioning chi-square.


### Residuals Analysis

If the chi-squared independence test rejects $H_0$ of identical frequency distributions, the next step is to identify *which* cells may be driving the lack of fit.  The Pearson residuals in the two-way table are 

$$r_{ij} = \frac{O_{ij} - E_{ij}}{\sqrt{E_{ij}}}$$

where $X^2 = \sum{r_{ij}}$.  The $r_{ij}$ values have a normal distribution with mean 0, but with *unequal* variances.  The *standardized* Pearson residual for a two-way table is 

$$r_{ij} = \frac{O_{ij} - E_{ij}}{\sqrt{E_{ij}(1 - p_{i+})(1 - p_{+j})}}$$

and the $r_{ij}$ values *do* have a $\sim N(0, 1)$ distribution.  $r_{ij}^2 > 4$ is a sign of lack of fit. The `chissq.test()` object includes `residuals` that match the manual calculation.

```{r}
(vitc_o - vitc_e) / sqrt(vitc_e)
vitc_chisq_test$residuals
```

It also includes `stdres` that match the manual standardized calculation.  (*well, no it doesn't, but I don't know what my mistake is.*)

```{r}
(vitc_e - vitc_o) / 
  sqrt(vitc_e * (1 - prop.table(vitc_o, margin = 1)) 
              * (1 - prop.table(vitc_o, margin = 2))
  )
vitc_chisq_test$stdres
```

Here are the squared Pearson residuals for the CHD data.  The squared Pearson residuals for CHD 0-199, 200-219, and 260+ are greater than 4, and seem to be driving the lack of independence.

```{r}
chd_chisq_test$residuals^2
```


### Difference in Proportions

The difference in proportions measure is the difference in the probabilities of characteristic $Z$ conditioned on two groups $Y = 1$ and $Y = 2$: $\delta = \pi_{1|1} - \pi_{1|2}$.  In social sciences and epidemiology $\pi_{1|1}$ and $\pi_{1|2}$ are sometimes referred to as "risk" values. The point estimate for $\delta$ is $r = p_{1|1} - p_{1|2}$.  

Under the normal approximation method, the sampling distribution of the difference in population proportions has a normal distribution centered at $d$ with variance $Var(\delta)$. The point estimate for $Var(\delta)$ is $Var(d)$.

$$Var(d) = \frac{p_{1|1} (1 - p_{1|1})}{n_{1+}} + \frac{p_{1|2} (1 - p_{1|2})}{n_{2+}}$$

In the vitamin C acid example, $\delta$ is the difference in the row conditional frequencies.

```{r}
p <- prop.table(vitc_o, margin = 1)
d <- p[2, 1] - p[1, 1]
print(d)
```

The variance is

```{r}
var_d <- (p[2, 1])*(1 - p[2, 1]) / sum(vitc_o[2, ]) +
  (p[1, 1])*(1 - p[1, 1]) / sum(vitc_o[1, ])
print(var_d)
```

The 95% CI is

```{r}
d + c(-1, 1) * qnorm(.975) * sqrt(var_d)
```

This is how `prop.test()` without the continuity correction calculates the confidence interval.

```{r}
(prop.test.result <- prop.test(vitc_o, correct = FALSE))
```

```{r fig.height=3.5}
lcl <- -round(prop.test.result$conf.int[2], 3)
ucl <- -round(prop.test.result$conf.int[1], 3)
data.frame(d_i = -300:300 / 1000) %>%
  mutate(density = dnorm(x = d_i, mean = d, sd = sqrt(var_d))) %>%
  mutate(rr = ifelse(d_i < lcl | d_i > ucl, density, 0)) %>%
ggplot() +
  geom_line(aes(x = d_i, y = density)) +
  geom_area(aes(x = d_i, y = rr, fill = mf_pal(12)(12)[2]), alpha = 0.8) +
  geom_vline(aes(xintercept = d), color = "blue") +
  theme_mf() +
  labs(title = bquote("Difference in Proportions Confidence Interval"),
       subtitle = paste0(
         "d = ", round(d, 3) 
        ),
       x = "d",
       y = "Density") +
  theme(legend.position="none")

```


The normal approximation method applies when the central limit theorem conditions hold:

* the sample is independently drawn (random sampling without replacement from $n < 10\%$ of the population in observational studies, or random assignment in experiments), 
* there are at least $n_i p_i >= 5$ successes and $n_i (1 - p_i) >= 5$ failures for each group, 
* the sample sizes are both $>=30$, and 
* the probability of success for each group is not extreme, $(0.2, 0.8)$.   

Test $H_0: d = \delta_0$ for some hypothesized population $\delta$ (usually 0) with test statistic 

$$Z = \frac{d - \delta_0}{se_{d}}$$ 

where 

$$se_{d} = \sqrt{p (1 - p) \left( \frac{1}{n_{1+}} + \frac{1}{n_{2+}} \right)}$$ 

approximates $se_{\delta_0}$ where $p$ is the pooled proportion 

$$p = \frac{n_{11} + n_{21}}{n_{1+} + n_{2+}}.$$

```{r}
p_pool <- (vitc_o[1, 1] + vitc_o[2, 1]) / sum(vitc_o)
se_d <- sqrt(p_pool * (1 - p_pool) * (1 / sum(vitc_o[1, ]) + 1 / sum(vitc_o[2, ])))
z <- (d - 0) / se_d
pnorm(z) * 2
```

```{r message=FALSE, warning=FALSE, fig.height=3.5}
lrr = qnorm(p = .05/2, mean = 0, sd = se_d, lower.tail = TRUE)
urr = qnorm(p = .05/2, mean = 0, sd = se_d, lower.tail = FALSE)
data.frame(d_i = -300:300 / 1000) %>%
  mutate(density = dnorm(x = d_i, mean = 0, sd = se_d)) %>%
  mutate(rr = ifelse(d_i < lrr | d_i > urr, density, 0)) %>%
ggplot() +
  geom_line(aes(x = d_i, y = density)) +
  geom_area(aes(x = d_i, y = rr, fill = mf_pal(12)(12)[2]), alpha = 0.8) +
  geom_vline(aes(xintercept = d), color = "blue") +
  geom_vline(aes(xintercept = 0), color = "black") +
  theme_mf() +
  labs(title = "Hypothesis Test of Difference in Proportions",
       subtitle = paste0(
         "d = ", round(d, 3), 
         " (Z = ", round(z, 2), 
         ", p = ", round(pnorm(z) * 2, 4), ")."
        ),
       x = "d",
       y = "Density") +
  theme(legend.position="none") 
```

The null hypothesis $H_0: \delta_0 = 0$ is equivalent to saying that two variables are independent, $\pi_{1|1} = \pi_{1|2}$, so you can also use the $\chi^2$ or $G^2$ test for independence in a 2 × 2.  That's what `prop.test()` is doing.  The square of the z-statistic is algebraically equal to $\chi^2$.  The two-sided test comparing $Z$ to a $N(0, 1)$ is identical to comparing $\chi^2$ to a chi-square distribution with df = 1.  Compare the $Z^2$ to the output from `prop.test()`.

```{r}
z^2
prop.test.result$statistic
```

The difference in proportions is easy to interpret, but when $Z = 1$ is a rare event, the individual probabilities $\pi_{1|1}$ and $\pi_{1|2}$ are both small and $\delta$ is nearly zero even when the effect is strong.

In the CHD study, two of the conditional probabilities of CHD within the four cholesterol groups are similar, 0-199 (0.038) and 200-219 (.031).

```{r}
round(prop.table(chd_o, margin = 2), 3)
```

Is the difference in these proportions statistically signficant?  You can test this with the difference in proportions test or a chisq test.

```{r}
prop.test(t(chd_o[, c(1:2)]))
chisq.test(t(chd_o[, c(1:2)]))
```

You could go on to try other pairwise tests to establish which levels differ from the others.


### Relative Risk

The relative risk measure is the ratio of the probabilities of characteristic $Z$ conditioned on two groups $Y = 1$ and $Y = 2$: $\rho = \pi_{1|1} / \pi_{1|2}$.  In social sciences and epidemiology $\rho$ is sometimes referred to as the "relative risk". The point estimate for $\rho$ is $r = p_{1|1} / p_{1|2}$.  

Because $\rho$ is non-negative, a normal approximation for $\log \rho$ has a less skewed distribution than $\rho$. The approximate variance of $\log \rho$ is

$$Var(\log \rho) = \frac{1 - \pi_{11}/\pi_{1+}}{n_{1+}\pi_{11}/\pi_{1+}} + \frac{1 - \pi_{21}/\pi_{2+}}{n_{2+}\pi_{21}/\pi_{2+}}$$

and is estimated by 

$$Var(\log r) = \left( \frac{1}{n_{11}} - \frac{1}{n_{1+}} \right) + \left( \frac{1}{n_{21}} - \frac{1}{n_{2+}} \right)$$

In the vitamin C acid example, $r$ is the ratio of the row conditional frequencies.

```{r}
vitc_prop <- prop.table(vitc_o, margin = 1)
vitc_risk <- vitc_prop[2, 1] / vitc_prop[1, 1]
print(vitc_risk)
```

The variance is

```{r}
vitc_risk_var <- 1 / vitc_o[1, 1] - 1 / sum(vitc_o[1, ]) + 
  1 / vitc_o[2, 1] - 1 / sum(vitc_o[2, ])
print(vitc_risk_var)
```

The 95% CI is

```{r}
exp(log(vitc_risk) + c(-1, 1) * qnorm(.975) * sqrt(vitc_risk_var))
```

Thus, at 0.05 level, you can reject the independence model. People taking vitamin C are half as likely to catch a cold.

In the CHD study, you could summarize the relationship between CHD and cholesterol level by a set of three relative risks using 0-199 as the baseline:

* 200–219 versus 0–199, 
* 220–259 versus 0–199, and 
* 260+ versus 0–199. 

```{r}
(chd_prop <- prop.table(chd_o, margin = 2))
(chd_risk <- chd_prop[1, ] / chd_prop[1, 1])
```


### Odds Ratio

The odds ratio is the most commonly used measure of association.  It is also a natural parameter for many of the log-linear and logistic models. The odds is the ratio of probabilities of "success" and "failure".  When conditioned on a variable, the odds ratio is 

$$\theta = \frac{\pi_{1|1} / \pi_{2|1}} {\pi_{1|2} / \pi_{2|2}}$$

and is estimated by the sample frequencies

 $$\hat{\theta} = \frac{n_{11} n_{22}} {n_{12} n_{21}}$$
 
The log-odds ratio has a better normal approximation than the odds ratio, so define the confidence interval on the log scale.

$$Var(\log \hat{\theta}) = \frac{1}{n_{11}} + \frac{1}{n_{12}} + \frac{1}{n_{21}} + \frac{1}{n_{22}}$$

For the Vitamin C example, the odds of getting a cold after taking a placebo pill are $0.22 / 0.78 = 0.28$ and the odds of getting a cold after taking Vitamin C are $0.12 / 0.88 = 0.14$.

```{r}
vitc_odds <- vitc_prop[, 1] / vitc_prop[, 2]
print(vitc_odds)
```

The odds of getting a cold given vitamin C are $0.14 / 0.28 = 0.49$ times the odds of getting cold given a placebo.  

```{r}
vitc_theta_hat <- vitc_odds[2] / vitc_odds[1]
print(vitc_theta_hat)
```

with variance 

```{r}
(var_vitc_theta_hat <- sum(1 / vitc_o))
```

The 95% CI is

```{r}
z_alpha <- qnorm(p = 0.975)
exp(log(vitc_theta_hat) + c(-1, 1) * z_alpha * sqrt(var_vitc_theta_hat))
```

Keep in mind the following properties of odds ratios.

* You can convert an odds pack to probabilities by solving $\pi / (1 - \pi)$ for $\pi = odds / (1 + odds)$. 

* If two variables are independent, then the conditional probabilities $\pi_{1|1}$ and $\pi_{1|2}$ will be equal and therefore the odds ratio will equal 1.

* If $\pi_{1|1} > \pi_{1|2}$ then the odds ratio will be $1 < \theta < \infty$.

* If $\pi_{1|1} < \pi_{1|2}$ then the odds ratio will be $0 < \theta < 1$.

* the sample odds ratio will equal $0$ or $\infty$ if any $n_{ij} = 0$. If you have any empty cells add 1/2 to each cell count.


### Partitioning Chi-Square

Besides looking at the residuals or the measures of association, another way to describe the effects is to form a sequence of smaller tables by combining or collapsing rows and/or columns in a meaningful way. 

For the smoking study, you might ask whether a student is more likely to smoke if *either* parent smokes.  Collapse the first two rows (1 parent smokes, both parents smoke) and run the chi-squared test.

```{r}
smoke_clps_1 <- rbind(smoke_o[1, ] + smoke_o[2, ], smoke_o[3, ])
smoke_clps_1_theta <- (smoke_clps_1[1, 1] / smoke_clps_1[1, 2]) /
  (smoke_clps_1[2, 1] / smoke_clps_1[2, 2])
(smoke_clps_1_chisq <- chisq.test(smoke_clps_1))
```

The estimated odds a student smokes if at least one parent smokes is `r round(smoke_clps_1_theta, 2)` (X^2 = `r round(smoke_clps_1_chisq$statistic, 1)`, p = `r smoke_clps_1_chisq$p.value`.

Or you may ask, whether among students with at least one smoking parent, there is a difference between those with one smoking parent and those with two smoking parents. Answer this by running a chi-square test on the first two rows of the data table, discarding the row where neither parent smokes.

```{r}
smoke_clps_2_theta <- (smoke_o[1, 1] / smoke_o[1, 2]) /
  (smoke_o[2, 1] / smoke_o[2, 2])
(smoke_clps_2_chisq <- chisq.test(smoke_o[c(1:2), ]))
```

The estimated odds a student smokes if both parents smoke compared to one parent is `r round(smoke_clps_2_theta, 2)` (X^2 = `r round(smoke_clps_2_chisq$statistic, 1)`, p = `r smoke_clps_2_chisq$p.value`.

### Correlation

When classification is ordinal, there may exist a linear trend among the levels of the characteristics.  Measure the linear relationship with Pearson's correlation coefficient, or its nonparametric alternatives, Spearman's correlation coefficient and Kendall's tau.

In the CHD study, the four levels of cholesterol (0-199, 200-219, 220-259, and 260+) may be treated as ordinal data.  You can also treat the presence of heart disease as ordinal. The Pearson correlation, 

$$r = \frac{cov(X, Y)}{s_X s_Y}$$

is 

```{r}
#dim=dim(table) 
rbar <- sum(margin.table(chd_o, 1) * c(1, 2)) / sum(chd_o) 
rdif <- c(1, 2) - rbar 
cbar <- sum(margin.table(chd_o, 2) * c(1, 2, 3, 4)) / sum(chd_o) 
cdif <- c(1, 2, 3, 4) - cbar 
ssr <- sum(margin.table(chd_o, 1) * (rdif^2)) 
ssc <- sum(margin.table(chd_o, 2) * (cdif^2)) 
ssrc <- sum(t(chd_o * rdif) * cdif) 
pcor <- ssrc / (sqrt(ssr * ssc)) 
pcor 
M2 <- (sum(chd_o) - 1) * pcor^2
M2
```

## K-Way Tables

These notes rely on PSU STATS 504 [course notes](https://online.stat.psu.edu/stat504/node/102/).

A *k*-way table has *k* independent variables.  Each cell in the *k*-way table has joint probability $\pi_{ij..k}$.  You can collapse a *k*-way table along a dimension to create a *marginal* table and the resulting joint probabilities in the marginal table are referred to as *marginal distributions*.  Alternatively, you can consider a single level of a dimension, creating a conditional distribution.

Here are two case studies to illustrate the concepts.  

**Study 1: Death Penalty**.  Data is collected for *n = 326* murder cases along three dimensions: defendant's color, victim's color, and whether or not the defendant received the death penalty. 

```{r}
deathp <- array(c(19, 132, 11, 52, 0, 9, 6, 97), dim=c(2,2,2))
dimnames(deathp) <- list(
  DeathPen = c("yes","no"),
  Defendant=c("white","black"),
  Victim=c("white","black")
)
ftable(deathp, row.vars = c("Defendant", "Victim"), col.vars = "DeathPen")
```

**Study 2: Boy Scouts**.  Data is collected for *n = 800* Boy Scouts along three dimensions: socioeconomic status, scout status, and delinquency status.

```{r}
scout <- expand.grid(
  delinquent = c("no","yes"),
  scout = c("no", "yes"),
  SES = c("low", "med","high")
)
scout <- cbind(scout, count = c(169,42,43,11,132,20,104,14,59,2,196,8))

scout_ct <- xtabs(count ~ ., scout)

ftable(scout_ct, row.vars = c("SES", "scout"), col.vars = "delinquent")
```


### Odds Ratio

In the *DeathP* study, the *marginal odds ratio* is 1.18, meaning the odds of death penalty for a white defendant are 1.18 times as high as they are for a black defendant.

```{r}
deathp_m <- margin.table(deathp, margin = c(1, 2))
deathp_p <- prop.table(deathp_m, margin = 2)
deathp_odds <- deathp_p[1, ] / deathp_p[2, ]
deathp_or <- deathp_odds[1] / deathp_odds[2]
print(deathp_or)
```

The *conditional odds ratio* given the victim is white is 0.68, and 0.79 given the victim is black, meaning the odds of death penalty for a white defendant are 0.68 times as high as they are for a black defendent if the victim is white, and 0.79 times as high as they are for a black defendent if the victim is black.

```{r}
deathp_m <- margin.table(deathp[, , 1], margin = c(2, 1))
deathp_p <- prop.table(deathp_m, margin = 2)
deathp_odds <- deathp_p[1, ] / deathp_p[2, ]
deathp_or <- deathp_odds[1] / deathp_odds[2]
print(deathp_or)

deathp_m <- margin.table(deathp[, , 2], margin = c(2, 1)) + 0.5
deathp_p <- prop.table(deathp_m, margin = 2)
deathp_odds <- deathp_p[1, ] / deathp_p[2, ]
deathp_or <- deathp_odds[1] / deathp_odds[2]
print(deathp_or)
```

Notice above that the second margin table had a zero in one cell.  To get an odds ratio in this case, the convention is to add 0.5 to all cells.

Interesting that the marginal odds of a white defendent receiving the death penalty are >1, but the conditional odds of a white defendent receiving the death penalty given the race of the victim are <1.  *Simpson's paradox* is the phenomenon that a pair of variables can have marginal association  and partial (conditional) associations in opposite direction. Another way to think about this is that the nature and direction of association changes due to presence or absence of a third (possibly confounding) variable.


### Chi-Square Independence Test

There are several ways to think about "independence" when dealing with a *k*-way table.

* **Mutual independence**.  All variables are independent from each other, (A, B, C).
* **Joint independence**.  Two variables are jointly independent of the third, (AB, C).
* **Marginal independence**. Two variables are independent if you ignore the third, (A, B).
* **Conditional independence** Two variables are independent given the third, (AC, BC).
* **Homogeneous associations** Conditional (partial) odds-ratios are not related on the value of the third, (AB, AC, BC).


Under the assumption that the model of independence is true, once you know the marginal probability values, we have enough information to estimate all unknown cell probabilities. Because each of the marginal probability vectors must add up to one, the number of free parameters in the model is (I − 1) + (J − 1) + (K −I ). This is exactly like the two-way table


#### Mutual Independence

The simplest model is that ALL variables are independent of one another (A, B, C). The joint probabilities are equal to the product of the marginal probabilities, $\pi_{ijk} = \pi_i \pi_j \pi_k$. In terms of odds ratios, the model (A, B, C) implies that the odds ratios in the marginal tables A × B, B × C, and A × C are equal to 1.

The **chi-square independence test** tests whether observed joint frequency counts $O_{ijk}$ differ from expected frequency counts $E_{ijk}$ under the *independence model* $\pi_{ijk} = \pi_{i+} \pi_{+j} \pi_{k+}$. $H_0$ is $O_{ijk} = E_{ijk}$.  This is essentially a one-way table chi-squre goodness-of-fit test.

For the *Boy Scouts* study, $X^2$ and its associated p-value is 

```{r}
scout_e <- array(NA, dim(scout_ct))
for (i in 1:dim(scout_ct)[1]) {
  for (j in 1:dim(scout_ct)[2]) {
    for (k in 1:dim(scout_ct)[3]) {
      scout_e[i,j,k] <- (
        margin.table(scout_ct, 3)[k] *
        margin.table(scout_ct, 2)[j] * 
        margin.table(scout_ct, 1)[i]) /
        (sum(scout_ct))^2
    }
  }
}
scout_df <- (prod(dim(scout_ct)) - 1) - sum(dim(scout_ct) - 1)
scout_x2 <- sum((scout_ct - scout_e)^2 / scout_e)
print(scout_x2)
pchisq(scout_x2, df = scout_df, lower.tail = FALSE)
```

You can also get X^2 this way, although longer code.

```{r}
x <- scout %>% group_by(delinquent) %>% 
  summarize(n = sum(count)) %>% ungroup() %>% mutate(p = n / sum(n))
p_d <- x$p
names(p_d) <- x$delinquent

x <- scout %>% group_by(scout) %>% 
  summarize(n = sum(count)) %>% ungroup() %>% mutate(p = n / sum(n))
p_b <- x$p
names(p_b) <- x$scout

x <- scout %>% group_by(SES) %>% 
  summarize(n = sum(count)) %>% ungroup() %>% mutate(p = n / sum(n))
p_s <- x$p
names(p_s) <- x$SES

scout_e <- scout %>% 
  mutate(e = as.numeric(p_d[delinquent] * p_b[scout] * p_s[SES] * sum(count)))

x2 <- sum((scout_e$count - scout_e$e)^2 / scout_e$e)
print(x2)
```

The deviance statistic is 

```{r}
scout_g2 <- 2 * sum(scout_ct * log(scout_ct / scout_e$e))
print(scout_g2)
pchisq(scout_g2, df = scout_df, lower.tail = FALSE)
```

Safe to say the mutual independence model does not fit.

`chisq.test()` does not test the complete independence model for *k*-way tables.  Instead, conduct tests on all ${3 \choose 2} = 3$ embedded 2-way tables.  For the *Boy Scouts* study, you can conduct a series of 2-way chi-sq tests.

SES*scout:

```{r}
(scout_ses_scout <- margin.table(scout_ct, c(3, 2)))
chisq.test(scout_ses_scout)
```

SES*delinquent

```{r}
(scout_ses_delinquent <- margin.table(scout_ct, c(3, 1)))
chisq.test(scout_ses_delinquent)
```

delinquent*scout

```{r}
(scout_delinquent_scout <- margin.table(scout_ct, c(1, 2)))
chisq.test(scout_delinquent_scout)
```

The odds ratio for scout delinquency vs non-scout delinquency is 0.54 with 95% CI (0.347, 0.845), so reject the null hypothesis that boy scout status and delinquent status are independent of one another (OR = 1), and thus that boy scout status, delinquent status, and socioeconomic status are not mutually independent.

```{r}
(scout_or <- vcd::oddsratio(scout_delinquent_scout, log = FALSE))
confint(scout_or)
```

#### Joint Independence

The joint independence model is that two variables are jointly independent of a third (AB, C). The joint probabilities are equal to the product of the AB marginal probability and the C marginal probability, $\pi_{ijk} = \pi_{ij} \pi_k$. 

The degrees of freedom in the associated chi-sq test are the number of free parameters in the saturated model minus the number of free parameters in the joint independence model. $df = (IJK-1) - ((IJ-1) + (K-1)) = (IJ-1)(K-1)$.

From the *Boy Scouts* study, suppose juvenile delinquency is the response variable. Test the null hypothesis that juvenile delinquency is independent of boy scout status and socioeconomic status.

```{r}
scout_d_bs <- ftable(
  scout_ct, 
  row.vars = c("SES", "scout"), 
  col.vars = "delinquent"
)
print(scout_d_bs)
chisq.test(scout_d_bs)
```

Notice how the contingency table is constructed to not indicate whether *SES* and *scout* are related.

#### Marginal Independence

The marginal independence model is that two variables are independent while *ignoring* the third (A, B). The joint probabilities are equal to the product of the two marginal probabilities, $\pi_{ij} = \pi_{i} \pi_j$, just like a 2-way table. 

The degrees of freedom in the associated chi-sq test are the number of free parameters in the saturated model minus the number of free parameters in the marginal independence model. $df = (I - 1) + (J - 1) = (I-1)(J-1)$.

From the *Boy Scouts* study, suppose juvenile delinquency is the response variable. Test the null hypothesis that juvenile delinquency is independent of boy scout status and socioeconomic status.

```{r}
ct <- xtabs(count ~ scout + SES, scout)
print(ct)
chisq.test(ct)
```

#### Conditional Independence

The conditional independence model is that two variables are independent given a third (AB, AC). The joint probabilities are equal to the product of the conditional probabilities (B|A and C|A) and the marginal probability of A, $\pi_{ijk} = \pi_{j|i}\pi_{k|i}\pi_{i++}$. 

You can test for conditional independence (AB, AC) by individually testing the independence (B, C) for each level of A.  $X^2$ and $G^2$ are the sum of the individual values and the degrees of freedom are $I(J-1)(K-1)$.  A better way to do it is with the Cochran-Mantel-Haenszel Test.

The degrees of freedom in the associated chi-sq test are the number of free parameters in the saturated model minus the number of free parameters in the joint independence model. $df = (IJK-1) - ((IJ-1) + (K-1)) = (IJ-1)(K-1)$.

From the *Boy Scouts* study, the section on joint independence found that scouting and SES were jointly independent of delinquency, so it must be that either scouting is independent of delinquency, or SES is independent of delinquency, or *both* are independent of delinquency.  The marginal test below establishes the independance of (scout, delinquent).  

```{r}
ct <- xtabs(count ~ scout + delinquent, scout)
print(ct)
chisq.test(ct, correct = FALSE)
```

The odds ratio 0.54 is a strong negative relationship between boy scout status and delinquency - boy scouts are 46% less likely (on the odds scale) to be delinquent than non-boy scouts. 

```{r}
ct <- xtabs(count ~ scout + delinquent, scout)
p <- prop.table(ct, margin = 1)
(or <- (p[2, 2] / p[2, 1]) / (p[1, 2] / p[1, 1]))
```

However, one may ask *is scouting and delinquency conditionally independent given SES?*  You can answer this question with three chi-square tests, one for each level of SES.

```{r}
ct <- xtabs(count ~ scout + delinquent, scout[scout$SES == "low", ])
print(ct)
chisq.test(ct, correct = FALSE)
```

```{r}
ct <- xtabs(count ~ scout + delinquent, scout[scout$SES == "med", ])
print(ct)
chisq.test(ct, correct = FALSE)
```

```{r}
ct <- xtabs(count ~ scout + delinquent, scout[scout$SES == "high", ])
print(ct)
chisq.test(ct, correct = FALSE)
```

You can add up the three $X^2$ values and run a chi-sq test with $df = 3$.  p = 0.984 indicating that the conditional independence model fits extremely well.

```{r}
pchisq(0.0058145 + 0.10098 + 0.053447, df = 3, lower.tail = FALSE)
```

The other way of testing for independence is the Cochran-Mantel-Haenszel test. 

The Cochran-Mantel-Haenszel (CMH) test statistic 

$$M^2 = \frac{\left[ \sum_k{(n_{11k} - \mu_{11k})}\right]^2}{\sum_k{Var(n_{11k})}}$$

For the *Boy Scouts* study, the test is

```{r}
mantelhaen.test(scout_ct)
```

X2= 0.008 (df = 1, p-value = 0.9287) indicating the conditional independence model is a good fit for this data, so do not reject the null hypothesis.


#### Homogenous Associations

Whereas the conditional independence model, (AB, AC) requires the BC odds ratios at each level of A to equal 1, the homogeneous association model, (AB, AC, BC), requires the BC odds ratios at each level of A to be identical, but not necessarily equal to 1.

The Breslow-Day statistic is

$$X^2 = \sum_{ijk}{\frac{\left(O_{ijk} - E_{ijk}\right)^2}{E_{ijk}}}$$

```{r}
DescTools::BreslowDayTest(scout_ct)
```






<!--chapter:end:02-discrete_analysis.Rmd-->

```{r include=FALSE}
library(tidyverse)
library(mfstylr)
```

# Continuous Variable Analysis {#continuous_analysis}


### Correlation

Correlation measures the strength and direction of association between two variables.  There are three common correlation tests: the Pearson product moment (Pearson's r), Spearman's rank-order (Spearman's rho), and Kendall's tau (Kendall's tau). 

Use the **Pearson's r** if both variables are quantitative (interval or ratio), normally distributed, and the relationship is linear with homoscedastic residuals.

The **Spearman's rho** and **Kendal's tao** correlations are [non-parametric](https://www.statisticshowto.datasciencecentral.com/parametric-and-non-parametric-data/) measures, so they are valid for both quantitative and ordinal variables and do not carry the normality and homoscedasticity conditions.  However, non-parametric tests have less statistical power than parametric tests, so only use these correlations if Pearson does not apply.


#### Pearson's r

Pearson's $r$ 

$$r = \frac{\sum{(X_i - \bar{X})(Y_i - \bar{Y})}}{\sqrt{\sum{(X_i - \bar{X})^2 \sum{(Y_i - \bar{Y})^2}}}} = \frac{cov(X,Y)}{s_X s_Y}$$

estimates the population correlation $\rho$.  Pearson's $r$ ranges from $-1$ (perfect negative linear relationship) to $+1$ (perfect positive linear relationship, and $r = 0$ when there is no linear relationship.  A correlation in the range $(.1, .3)$ is condidered small, $(.3, .5)$ medium, and $(.5, 1.0)$ large.

Pearson's $r$ only applies if the variables are interval or ratio, normally distributed, linearly related, there are minimal outliers, and the residuals are homoscedastic.

Test $H_0: \rho = 0$ with test statistic 

$$T = r \sqrt{\frac{n-2}{1-r^2}}.$$


The `nascard` data set consists of $n = 898$ races from 1975 - 2003.  

```{r message=FALSE, warning=FALSE, cache=TRUE}
nascard <- read.fwf(
  file = url("http://jse.amstat.org/datasets/nascard.dat.txt"),
  widths = c(5, 6, 4, 4, 4, 5, 9, 4, 11, 30),
  col.names = c('series_race', 'year', 'race_year', 'finish_pos', 'start_pos',
                'laps_comp', 'winnings', 'num_cars','car_make', 'driver')
)
nascard_sr1 <- nascard[nascard$series_race == 1,]
glimpse(nascard_sr1)
```

In race 1 of 1975 $(n = 35)$, what was the correlation between the driver's finishing position and prize?

Explore the relationship with a scatterplot.  As expected, there is a negative relationship between finish position and winnings, but it is non-linear.  However, 1/winnings may be linearly related to finish position. 

```{r message=FALSE, warning=FALSE, fig.height=3.5, fig.width=6.5}
p1 <- ggplot(data = nascard_sr1, aes(x = finish_pos, y = winnings)) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  theme_mf() +
  labs(title = "Pearson's Rho", subtitle = "")
p2 <- ggplot(data = nascard_sr1, aes(x = finish_pos, y = 1/winnings)) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  theme_mf() +
  labs(title = "Pearson's Rho", subtitle = "y = 1 / winnings")
gridExtra::grid.arrange(p1, p2, nrow = 1)
```

(*I tried several [variable tranformations](https://stattrek.com/regression/linear-transformation.aspx) and chose the one producing the highest $R^2$ that also had a normal distribution in each variable.)

```{r}
#summary(lm(winnings ~ finish_pos, data = nascard_sr1)) # r2 = .5474
#summary(lm(log(winnings) ~ finish_pos, data = nascard_sr1)) # r2 = .9015
nascard_lm <- lm(1/winnings ~ finish_pos, data = nascard_sr1) # r2 = .9509
#summary(lm(winnings ~ log(finish_pos), data = nascard_sr1)) # r2 = .883
#summary(lm(log(winnings) ~ log(finish_pos), data = nascard_sr1)) # r2 = .9766
#summary(lm(winnings ~ log(finish_pos), data = nascard_sr1)) # r2 = .883
coef(nascard_lm)
```

Finish position and prize are ratio variables, so the Pearson's r applies.  Check whether each variable is normally distributed.  

```{r warning=FALSE, message=FALSE, fig.height=3.5, fig.width=6.5}
p1 <- ggplot(nascard_sr1, aes(sample = finish_pos)) +
  stat_qq() +
  stat_qq_line() +
  theme_mf() +
  labs(title = "Q-Q Plots", subtitle = "Finish Position")
p2 <- ggplot(nascard_sr1, aes(sample = winnings)) +
  stat_qq() +
  stat_qq_line() +
  theme_mf() +
  labs(title = "", subtitle = "Winnings")
gridExtra::grid.arrange(p1, p2, nrow = 1)
```

The normal distribution plots look good.  You can also use the Anderson-Darling statistical test.
```{r}
nortest::ad.test(nascard_sr1$finish_pos)
nortest::ad.test(1/nascard_sr1$winnings)
```

Both fail to reject the normality null hypothesis.

Pearson's $r$ is 

```{r}
x <- nascard_sr1$finish_pos
y <- 1/nascard_sr1$winnings
(r = sum((x - mean(x)) * (y - mean(y))) /
   sqrt(sum((x - mean(x))^2) * sum((y - mean(y))^2)))
```

Test $H_0: \rho = 0$ with test statistic $T$.

```{r}
n <- nrow(nascard_sr1)
(t = r * sqrt((n - 2) / (1 - r^2)))
```

```{r}
pt(q = t, df = n - 2, lower.tail = FALSE) * 2
```

$P(T>.9752) < .0001$, so reject $H_0$ that the correlation is zero.  `cor.test()` performs these calculations.  Specify `method = "pearson"`.

```{r}
cor.test(
  x = nascard_sr1$finish_pos,
  y = 1/nascard_sr1$winnings,
  alternative = "two.sided",
  method = "pearson"
)
```


#### Spearman's Rho

Spearman's $\rho$ is the Pearson's r applied to the sample variable *ranks*.  Let $(X_i, Y_i)$ be the ranks of the $n$ sample pairs with mean ranks $\bar{X} = \bar{Y} = (n+1)/2$.  Spearman's rho is   

$$\hat{\rho} = \frac{\sum{(X_i - \bar{X})(Y_i - \bar{Y})}}{\sqrt{\sum{(X_i - \bar{X})^2 \sum{(Y_i - \bar{Y})^2}}}}$$

Spearman's rho is a non-parametric test, so there is no associated confidence interval.

From the `nascard` study, what was the correlation between the driver's starting position `start_pos` and finishing position `finish_pos`?  From the scatterplot, there does not appear to be much of a relationship.

```{r fig.height=3.5, fig.width=3.75}
nascard_sr1 %>%
  ggplot(aes(x = start_pos, y = finish_pos)) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  theme_mf() +
  labs(title = "Spearman's Rho")
```

Both `star_pos` and `finish_pos` are ordinal variables, so use Spearman's rho instead of Pearson. Normally, you replace the variable values with their ranks, but in this case, the values *are* their ranks.  After that, Spearman's $\rho$ is the same as Pearson's r.

```{r}
x <- rank(nascard_sr1$start_pos)
y <- rank(nascard_sr1$finish_pos)

(rho = sum((x - mean(x)) * (y - mean(y))) /
   sqrt(sum((x - mean(x))^2) * sum((y - mean(y))^2)))
```

Test $H_0: \rho = 0$ with test statistic $T$.

```{r}
n <- nrow(nascard_sr1)
(t = rho * sqrt((n - 2) / (1 - rho^2)))
```

```{r}
pt(q = abs(t), df = n - 2, lower.tail = FALSE) * 2
```

$P(T>.2206) = .8268$, so *do not* reject $H_0$ that the correlation is zero.  `cor.test()` performs these calculations.  Specify `method = "spearman"`.

```{r}
cor.test(
  x = nascard_sr1$start_pos,
  y = nascard_sr1$finish_pos,
  alternative = "two.sided",
  method = "spearman"
)
```


#### Kendall's Tau

The Kendall Rank correlation coefficient (Kendall's tau) measures the relationship between ranked variables. (see also [Statistics How To](https://www.statisticshowto.datasciencecentral.com/kendalls-tau/)).  Let $(X_i, Y_i)$ be the ranks of the $n$ sample pairs.  For each $X_i$, count $Y > X_i$. The total count is $k$.   Kendall's tau is

$$\hat{\tau} = \frac{4k}{n(n-1)} -1$$

If $X$ and $Y$ have the same rank orders, $\hat{\tau} = 1$, and if they have opposite rank orders $\hat{\tau} = -1$.  Test $H_0: \tau = 0$ with test statistic 

$$Z = \hat{\tau} \sqrt{\frac{9n(n - 1)}{2(2n + 5)}}.$$

Kendall's tau is a non-parametric test, so there is no associated confidence interval.

From the `nascard` study, what was the correlation between the driver's starting position `start_pos` and finishing position `finish_pos`?  Spearman's rho was $\hat{\rho} = -0.038$.  For Kendall's tau, for each observation, count the number of other observations where both `start_pos` and  `finish_pos` is larger.  The sum is $k = 293$.  Then calculate Kendall's $\tau$ is $\hat{\tau} = \frac{4 * 293}{35(35-1)} -1 = -.0151$.

```{r}
n <- nrow(nascard_sr1)
k <- 0
for (i in 1:n) {
  for(j in i:n) {
    k = k + if_else(
      nascard_sr1[j, ]$start_pos > nascard_sr1[i, ]$start_pos
      & nascard_sr1[j, ]$finish_pos > nascard_sr1[i, ]$finish_pos, 1, 0)
  }
}
(k)
(tau = 4.0*k / (n * (n - 1)) - 1)
```

Test $H_0: \tau = 0$ with test statistic $T$.

```{r}
n <- nrow(nascard_sr1)
(t = tau * sqrt(9 * n * (n-1) / (2 * (2*n + 5))))
```

```{r}
pt(q = abs(t), df = n - 2, lower.tail = FALSE) * 2
```

$P(|T| > 0.128) = .8991$, so *do not* reject $H_0$ that the correlation is zero.  `cor.test()` performs these calculations.  Specify `method = "kendall"`.

```{r}
cor.test(
  x = nascard_sr1$start_pos,
  y = nascard_sr1$finish_pos,
  alternative = "two.sided",
  method = "kendall"
)
```

Here is a fun way to tie Pearson, Spearman, and Kendal together: Calculate Spearman's rho and Kendall's tau for all 898 races in the `nascard` data set and calculate the Pearson correlation of their values!

```{r message=FALSE, warning=FALSE}
nascard_smry <- nest(nascard, -c(series_race, year)) %>%
  mutate(
    spearman = map(data, ~ cor.test(.$start_pos, .$finish_pos, 
                                    alternative = "two.sided", 
                                    method = "spearman")),
    spearman_est = unlist(map(spearman, ~ .$estimate)),
    spearman_p = unlist(map(spearman, ~ .$p.value)),
    kendall = map(data, ~ cor.test(.$start_pos, .$finish_pos,
                                    alternative = "two.sided",
                                    method = "kendall")),
    kendall_est = unlist(map(kendall, ~ .$estimate)),
    kendall_p = unlist(map(kendall, ~ .$p.value))
  )
```

```{r message=FALSE, warning=FALSE, fig.height=3.5}
nascard_smry2 <- nascard_smry %>% 
  group_by(year) %>%
  summarize(
    spearman_r = mean(spearman_est),
    kendall_tau = mean(kendall_est)
  ) %>%
  pivot_longer(cols = c(spearman_r, kendall_tau), 
               names_to = "method", values_to = "estimate") 
ggplot(nascard_smry2, aes(x = year, color = method)) +
  geom_line(aes(y = estimate)) +
  theme_mf() +
  scale_color_mf() +
  labs(title = "Average Rank Correlations vs Year", color = "")
```

In terms of significance, the two tests usually return the same results.  

```{r}
table(
  ifelse(nascard_smry$spearman_p < .05, "sig", "insig"),
  ifelse(nascard_smry$kendall_p < .05, "sig", "insig")
)
```

Calculate the Pearson coefficient of the relationship between Spearman's rho and Kendall's tau.  
```{r fig.height=3.5}
ggplot(data = nascard_smry, aes(x = kendall_est, y = spearman_est)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  theme_mf() +
  labs(title = "Pearson's r of Spearman's rho vs Kendall's tau")
```

The scatterplot with fitted line has r-squared equal to the Pearson coefficient squared.

```{r}
cor.test(
  x = nascard_smry$spearman_est,
  y = nascard_smry$kendall_est,
  alternative = "two.sided",
  method = "pearson"
)$estimate^2

summary(lm(kendall_est ~ spearman_est, data = nascard_smry))$r.squared
```


<!--chapter:end:03-continuous_analysis.Rmd-->

```{r include=FALSE}
library(tidyverse)
library(mfstylr)
```

# Experiment Design

These notes are structured from the [PSU STAT 503](https://online.stat.psu.edu/stat503/) course.

## Single Factor

## Blocking

## Nested

## Split Plot

<!--chapter:end:04-experiment_design.Rmd-->

# PART 2: Supervised Machine Learning {-}

Machine learning (ML) develops algorithms to identify patterns in data (unsupervised ML) or make predictions and inferences (supervised ML). 

Supervised ML trains the machine to learn from prior examples to *predict* either a categorical outcome (classification) or a numeric outcome (regression), or to *infer* the relationships between the outcome and its explanatory variables.  

Two early forms of supervised ML are *[linear regression (OLS)](http://rpubs.com/mpfoley73/527767)* and *[generalized linear models (GLM)](http://rpubs.com/mpfoley73/527573)* (Poisson and logistic regression).  These methods have been improved with advanced linear methods, including *stepwise selection*, [*regularization*](http://rpubs.com/mpfoley73/521922) (ridge, lasso, elastic net), *principal components regression*, and *partial least squares*.  With greater computing capacity, non-linear models are now in use, including *polynomial regression*, *step functions*, *splines*, and generalized additive models (*GAM*).  [Decision trees](http://rpubs.com/mpfoley73/529130) (*bagging*, *random forests*, and, *boosting*) are additional options for regression and classification, and *support vector machines* is an additional option for classification.


# Ordinary Least Squares

```{r message=FALSE}
library(caret)
library(Metrics)
library(tidyverse)
library(corrplot)
library(gridExtra)
library(car)  # for avPlots
library(AppliedPredictiveModeling)
library(e1071)  # for skewness()
library(purrr)  # for map()
library(broom)  # for augment()
```




These notes cover *linear regression*.  


## Linear Regression Model

The population regression model $E(Y) = X \beta$ summarizes the trend between the predictors and the mean responses.  The individual responses vary about the population regression, $y_i = X_i \beta + \epsilon_i$ with assumed mean structure $y_i \sim N(\mu_i, \sigma^2)$ and assumed constant variance $\sigma^2$.  Equivalently, the model presumes a linear relationship between $y$ and $X$ with residuals $\epsilon$ that are independent normal random variables with mean zero and constant variance $\sigma^2$.  Estimate the population regression model coefficients as $\hat{y} = X \hat{\beta}$, and the population variance as $\hat{\sigma}^2$.  The most common method of estimating the $\beta$ coefficients and $\sigma$ is ordinary least squares (OLS).  OLS minimizes the sum of squared residuals from a random sample.  The individual predicted values vary about the actual value, $e_i = y_i - \hat{y}_i$, where $\hat{y}_i = X_i \hat{\beta}$.

The OLS model is the best linear unbiased estimator (BLUE) if the residuals are independent random variables normally distributed with mean zero and constant variance $\sigma^2$.  Recall these conditions with the LINE pneumonic: **L**inear, **I**ndependent, **N**ormal, and **E**qual.

**Linearity**.  The explanatory variables are each linearly related to the response variable: $E(\epsilon | X_j) = 0$.  

**Independence**.  The residuals are unrelated to each other.  Independence is violated when repeated measurements are taken, or when there is a temporal component in the model. 

**Normality**.  The residuals are normally distributed: $\epsilon|X \sim N(0, \sigma^2I)$.  

**Equal Variances**.  The variance of the residuals is constant (homoscedasticity): $E(\epsilon \epsilon' | X) = \sigma^2I$

Additionally, you should make sure you model has "little" or no **multicollinearity** among the variables.


## Parameter Estimation

There are two model parameters to estimate: $\hat{\beta}$ estimates the coefficient vector $\beta$, and $\hat{\sigma}$ estimates the variance of the residuals along the regression line.

Derive the coefficient estimators by minimizing the sum of squared residuals $SSE = (y - X \hat{\beta})' (y - X \hat{\beta})$.  The result is

$$\hat{\beta} = (X'X)^{-1}X'y.$$

The residual standard error (RSE) estimates the sample deviation around the population regression line. *(Think of each value of $X$ along the regression line as a subpopulation with mean $y_i$ and variance $\sigma^2$.  This variance is assumed to be the same for all $X$.)* 

$$\hat{\sigma} = \sqrt{(n-k-1)^{-1} e'e}.$$

The standard error for the coefficient estimators is the square root of the error variance divided by $(X'X)$.

$$SE(\hat{\beta}) = \sqrt{\hat{\sigma}^2 (X'X)^{-1}}.$$

#### Example

Dataset `mtcars` contains response variable fuel consumption `mpg` and 10 aspects of automobile design and performance for 32 automobiles.  What is the relationship between the response variable and its predictors?

```{r message=FALSE, warning=FALSE}
d <- mtcars %>%
  mutate(vs = factor(vs, labels = c("V", "S")),
         am = factor(am, labels = c("automatic", "manual")),
         cyl = ordered(cyl),
         gear = ordered(gear), 
         carb = ordered(carb))
glimpse(d)
```

The data consists of 32 observations.  A scatterplot matrix of the numeric variables shows the strongest individual association with `mpg` is from `wt` (corr = -0.87) followed by `disp` (corr = -0.85) and `hp` (corr = -0.78), `drat` is moderately correlated with `mpg` (corr = 0.68), and `qsec` is weakly correlated with `mpg` (corr = 0.42).  

```{r message=FALSE, warning=FALSE}
corrplot(cor(subset(d, select = c(mpg, disp, hp, drat, wt, qsec))), 
         type = "upper", 
         method = "number")
```

Many of the Predictor variables are strongly correlated with each other.  Boxplots of the categorical variables shows differences in levels, although ordinal variables `gear` and and `carb` do not have a monotonic relationshiop with `mpg`.

```{r}
p_list <- list()
for(i in c("cyl", "vs", "am", "gear", "carb")) {
  p <- ggplot(d, aes_string(x = i, y = "mpg")) + geom_boxplot()
  p_list <- c(p_list, list(p))
}
do.call("grid.arrange", c(p_list, ncol = 2))
```

```{r include=FALSE}
rm(p_list, i, p)
```

I'll drop the `gear` and `carb` predictors, and fit a population model to the remaining predictors.  

```{r}
m <- lm(mpg ~ ., data = d[,1:9])
summary(m)
```

The `summary()` function shows $\hat{\beta}$ as `Estimate`, $SE({\hat{\beta}})$ as `Std. Error`, and $\hat{\sigma}$ as `Residual standard error`.  You can verify this by manually peforming these calculations using matrix algebra (see matrix algebra in r notes at [R for Dummies](https://www.dummies.com/programming/r/how-to-do-matrix-arithmetic-in-r/)).  Here are the coefficient estimators, $\hat{\beta} = (X'X)^{-1}X'y$.

```{r}
X <- model.matrix(m)
y <- d$mpg

beta_hat <- solve(t(X) %*% X) %*% t(X) %*% y
round(beta_hat, 5)
```

Here is the residual standard error, $\hat{\sigma} = \sqrt{(n-k-1)^{-1} \hat{e}'\hat{e}}$.

```{r}
n <- nrow(X)
k <- ncol(X) - 1  # exclude the intercept term
y_hat <- X %*% beta_hat
sse <- sum((y - y_hat)^2)
rse <- sqrt(sse / (n - k - 1))
cat("Residual standard error: ", round(rse, 3), " on ", (n - k - 1), " degrees of freedom.")
```

Use the residual standard errors to derive the standard errors of the coefficients, $SE(\hat{\beta}) = \sqrt{\hat{\sigma}^2 (X'X)^{-1}}$.

```{r}
se_beta_hat <- sqrt(diag(rse^2 * solve(t(X) %*% X)))
matrix(round(se_beta_hat, 5), dimnames = list(names(se_beta_hat), "Std. Error"))
```


## Model Assumptions

The linear regression model assumes the relationship between the predictors and the response is linear with the residuals that are independent random variables normally distributed with mean zero and constant variance.

Additionally, you will want to check for multicollinearity in the predictors because it can produce unreliable coefficient estimates and predicted values.

Use a residuals vs fits plot $\left( e \sim \hat{Y} \right)$ to detect non-linearity and unequal error variances, including outliers.  The polynomial trend line should show that the residuals vary around $e = 0$ in a straight line (linearity).  The variance should be of constant width (especially no fan shape at the low or high ends).

Use a residuals normal probability plot to compares the theoretical percentiles of the normal distribution versus the observed sample percentiles.  It should be approximately linear.  

A scale-location plot $\sqrt{e / sd(e)} \sim \hat{y}$ checks the homogeneity of variance of the residuals (homoscedasticity). The square root of the absolute value of the residuals should be spread equally along a horizontal line.

A residuals vs leverage plot identifies influential observations.  A plot of the standardized residuals vs the leverage should fall within the 95% probability band.

```{r warning=FALSE, message=FALSE}
par(mfrow = c(2, 2))
plot(m, labels.id = NULL)
```


### Linearity

The explanatory variables should each be linearly related to the response variable: $E(\epsilon | X_j) = 0$.  A good way to test this condition is with a residuals vs fitted values plot.  A curved pattern in the residuals indicates a curvature in the relationship between the response and the predictor that is not explained by our model.  A linear model does not adequately describe the relationship between the predictor and the response.

Test for linearity four ways: 

* Residuals vs fits plot $(e \sim \hat{Y})$ should bounce randomly around 0. 
* Observed vs fits plot $(Y \sim \hat{Y})$ should be symmetric along the 45-degree line.  
* Each $(Y \sim X_j )$ plot should have correlation $\rho \sim 1$.  
* Each $(e \sim X_j)$ plot should exhibit no pattern.  

If the linearity condition fails, change the functional form of the model with non-linear transformations of the explanatory variables.  A common way to do this is with Box-Cox transformations. 

$$w_t = \begin{cases} \begin{array}{l} log(y_t) \quad \quad \lambda = 0 \\
(y_t^\lambda - 1) / \lambda \quad \text{otherwise} \end{array} \end{cases}$$

$\lambda$ can take any value, but values near the following yield familiar transformations.

* $\lambda = 1$ yields no substantive transformation.  
* $\lambda = 0.5$ is a square root plus linear transformation.
* $\lambda = 0.333$ is a cube root plus linear transformation.
* $\lambda = 0$ is a natural log transformation.
* $\lambda = -1$ is an inverse transformation.

A common source of non-linearity in a model is skewed response or independent variables (see discussion [here](https://blog.minitab.com/blog/applying-statistics-in-quality-projects/how-could-you-benefit-from-a-box-cox-transformation)).  `mtcars` has some skewed variables.

```{r}
tmp <- map(mtcars, skewness) %>% 
  unlist() %>% 
  as.data.frame() %>%
  rownames_to_column() 
colnames(tmp) <- c("IV", "skew")
ggplot(tmp, aes(x = order(IV, skew), y = skew)) +
  geom_col()

df <- mtcars
m <- lm(mpg ~ hp, data = df)
par(mfrow = c(2, 2))
plot(m)
plot(m$model$mpg, m$fitted.values)
abline(0, 1)
cor(df$mpg, df$hp)
postResample(pred = m$fitted.values, obs = m$model$mpg)


bc <- BoxCoxTrans(mtcars$hp)
df$hp_bc <- predict(bc, mtcars$hp)
m_bc <- lm(mpg ~ hp_bc, data = df)
plot(m_bc)
plot(m_bc$model$mpg, m_bc$fitted.values)
abline(0, 1)
cor(df$mpg, df$hp_bc)
postResample(pred = m_bc$fitted.values, obs = m_bc$model$mpg)

# Which vars are skewed?
map(mtcars, skewness)

# Benchmark model: mpg ~ hp
d0 <- mtcars
m0 <- lm(mpg ~ hp, data = d0)
d0 <- augment(m0, d0)
d0.cor <- round(cor(d0$mpg, d0$hp), 2)

# Benchmark diagnostics
p0a <- ggplot(d0, aes(x = hp, y = mpg)) + geom_point() + geom_smooth(method = "lm", se = FALSE) + 
  labs(title = "Response vs IV",
       subtitle = paste0("Correlation ~ 1?  (rho = ", d0.cor, ")"))
p0b <- ggplot(d0, aes(x = .fitted, y = .resid)) + geom_point() + geom_abline(intercept = 0, slope = 0) + 
  labs(title = "Residuals vs Fits",
       subtitle = "Random scatter?")
p0c <- ggplot(d0, aes(x = .fitted, y = mpg)) + geom_point() + geom_abline(intercept = 0, slope = 1) +
  expand_limits(x = c(0, 35), y = c(0, 35)) +
  labs(title = "Observed vs Fits",
       subtitle = "Symmetric along 45 degree line?")
p0d <- ggplot(d0, aes(x = hp, y = .resid)) + geom_point() + geom_abline(intercept = 0, slope = 0) + 
  labs(title = "Residuals vs IV",
       subtitle = "Random scatter?")
grid.arrange(p0a, p0b, p0c, p0d, nrow = 2)

# Benchmark performance
postResample(pred = m0$fitted.values, obs = m0$model$mpg)

# Box-Cox transform hp
d1 <- mtcars
bc <- BoxCoxTrans(d1$hp)
d1$hp_bc <- predict(bc, d1$hp)
m1 <- lm(mpg ~ hp_bc, data = d1)
d1 <- augment(m1, d1)
d1.cor <- round(cor(d1$mpg, d1$hp_bc), 2)

p1a <- ggplot(d1, aes(x = hp_bc, y = mpg)) + geom_point() + geom_smooth(method = "lm", se = FALSE) + 
  labs(title = "Response vs IV",
       subtitle = paste0("Correlation ~ 1?  (rho = ", d1.cor, ")"))
p1b <- ggplot(d1, aes(x = .fitted, y = .resid)) + geom_point() + geom_abline(intercept = 0, slope = 0) + 
  labs(title = "Residuals vs Fits",
       subtitle = "Random scatter?")
p1c <- ggplot(d1, aes(x = .fitted, y = mpg)) + geom_point() + geom_abline(intercept = 0, slope = 1) +
  expand_limits(x = c(0, 35), y = c(0, 35)) +
  labs(title = "Observed vs Fits",
       subtitle = "Symmetric along 45 degree line?")
p1d <- ggplot(d1, aes(x = hp, y = .resid)) + geom_point() + geom_abline(intercept = 0, slope = 0) + 
  labs(title = "Residuals vs IV",
       subtitle = "Random scatter?")
grid.arrange(p1a, p1b, p1c, p1d, nrow = 2)

postResample(pred = m1$fitted.values, obs = m1$model$mpg)

```


### Multicollinearity

The multicollinearity condition is violated when two or more of the predictors in a regression model are correlated.  Muticollinearity can occur for *structural* reasons, as when one variable is a transformation of another variable, or for *data* reasons, as occurs in observational studies.  Multicollinearity is a problem because it inflates the variances of the estimated coefficients, resulting in larger confidence intervals.

When predictor variables are correlated, the precision of the estimated regression coefficients decreases with each added correlated predictor variable.  The usual interpretation of a slope coefficient as the change in the mean response for each additional unit increase in the predictor when all the other predictors are held constant breaks down because changing one predictor necessarily changes the others.

A residuals vs fits plot $(\epsilon \sim \hat{Y})$ should have correlation $\rho \sim 0$.   A correlation matrix is helpful for picking out the correlation strengths.  A good rule of thumb is correlation coefficients should be less than 0.80.  However, this test may not work when a variable is correlated with a function of other variables.  A model with multicollinearity may have a significant F-test with insignificant individual slope estimator t-tests.  Another way to detect multicollinearity is by calculating variance inflation factors.  The predictor variance $Var(\hat{\beta_k})$ increases by a factor 

$$VIF_k = \frac{1}{1 - R_k^2}$$

where $R_k^2$ is the $R^2$ of a regression of the $k^{th}$ predictor on the remaining predictors.  A $VIF_k$ of $1$ indicates no inflation (no corellation).  A $VIF_k >= 4$ warrants investigation.  A $VIF_k >= 10$ requires correction.

#### Example

Does the model `mpg ~ .` exhibit multicollinearity?

The correlation matrix above (and presented again below) has several correlated covariates.  `disp` is strongly correlated with `wt` (r = 0.89) and `hp` (r = 0.79).

```{r}
m <- lm(mpg ~ ., data = mtcars)
corrplot(cor(subset(d, select = c(mpg, disp, hp, drat, wt, qsec))), type = "upper", method = "number")
```

Calculate the VIFs.

```{r}
round(vif(m), 2)
```

There are two predictors with VIFs greater than 10, cyl (GVIF = 21.36) and disp (GVIF = 13.76).  One way to address multicollinearity is removing one or more of the violating predictors from the regression model.  Try removing `cyl`.

```{r}
vif(m <-lm(mpg ~ . - cyl, data = d[,1:9]))
summary(m)
```

Removing `cyl` reduced the VIFs of the other variables below 10.  `disp` is still right up there (VIF = 9.87), so it may be worth dropping it from the model too.  The model summary still shows that there is only one significant (at .05 level a significance) variable (`wt`, p = .00457).  What if I drop `disp` too?

```{r}
vif(m <- lm(mpg ~ . - cyl - disp, data = d[,1:9]))
summary(m)
```

The model is not improved, so keep `disp`.

```{r}
m <-lm(mpg ~ . - cyl, data = d[,1:9])
```


If the multicollinearity occurs because you are using a polynomial regression model, *center* the predictor variables (subtract their means).  


#### Example

Data set `exerimmun` ([exerimun.txt](https://newonlinecourses.science.psu.edu/stat501/sites/onlinecourses.science.psu.edu.stat501/files/data/exerimmun/index.txt)) contains observations of immunoglobin in blood (a measure of immunity) and maximal oxygen uptake (a measure of exercise level) for $n = 30$ individuals.  

* `igg` = amount of immunoglobin in blood (mg) 
* `oxygent` = maximal oxygen uptake (ml/kg)

How does exercise affect the immune system?

```{r}
#exerimmun <- read_tsv(file = "./Data/exerimmun.txt")
exerimmun <- tribble(
  ~igg, ~oxygen,
  881,	34.6,
  1290,	45,
  2147,	62.3,
  1909,	58.9,
  1282,	42.5,
  1530,	44.3,
  2067,	67.9,
  1982,	58.5,
  1019,	35.6,
  1651,	49.6,
  752,	33,
  1687,	52,
  1782,	61.4,
  1529,	50.2,
  969,	34.1,
  1660,	52.5,
  2121,	69.9,
  1382,	38.8,
  1714,	50.6,
  1959,	69.4,
  1158,	37.4,
  965,	35.1,
  1456,	43,
  1273,	44.1,
  1418,	49.8,
  1743,	54.4,
  1997,	68.5,
  2177,	69.5,
  1965,	63,
  1264,	43.2
)
head(exerimmun)
```

The scatterplot `oxygen ~ igg` shows some curvature.  Formulate a quadratic polynomial regression function, $igg_i = \beta_0 + \beta_1 oxygen_i + \beta_2 oxygen_i^2 + \epsilon_i$ where the error terms are assumed to be independent, and normally distributed with equal variance.

```{r}
ggplot(exerimmun, aes(y = igg, x = oxygen)) +
  geom_point() +
  geom_smooth(method = lm, formula = y ~ poly(x, 2), se = FALSE) +
  labs(title = "Immunoglobin in Blood")
```

The formulated regression fits the data well ($adj R^2 = .933$), but the terms `oxygen` and `oxygen^2` are strongly correlated.

```{r}
m_blood <- lm(igg ~ poly(oxygen, 2), data = exerimmun)
summary(m_blood)
cor(exerimmun$oxygen, exerimmun$oxygen^2)
```

Remove the structural multicollinearity by centering the predictors.  You can scale the predictors with `scale()`, but be careful to scale new data when predicting new observations with `predict(newdata=)`!  Whenever possible, perform the transformation right in the model.

```{r}
m_blood <- lm(igg ~ I(oxygen - mean(exerimmun$oxygen)) +
                I((oxygen - mean(exerimmun$oxygen))^2), 
              data = exerimmun)
summary(m_blood)
```

The estimated intercept coefficient $\hat{\beta}_0 = 1632$ means a person whose maximal oxygen uptake is $50.64$ ml/kg (the mean value) is predicted to have $1632$ mg of immunoglobin in his blood. The estimated coefficient $\hat{\beta}_1 = 34.0$ means a person whose maximal oxygen uptake is near $50.64$ ml/kg is predicted to increase by 34.0 mg for every 1 ml/kg increase in maximal oxygen uptake.

By performing all transformations in the model, it is straightforward to perform predictions.  Here is the predicted value of immunoglobin when maximal oxygen uptake = 90.00 ml/kg.

```{r}
predict(m_blood, newdata = data.frame(oxygen = 90), interval = "prediction")
```

### Normality

A normal probability plot or a normal quantile plot should have values near the line with no bow-shaped deviations.  A histogram should be normally distributed.  A residuals vs fits plot $(\epsilon \sim \hat{Y})$ should be randomly scattered around 0.  Sometimes the normality check fails when linearity assumption does not hold, so check for linearity first.  Parameter estimation is not sensitive to this condition, but prediction intervals are.

### Equal Variances

The residuals should be the same size at both low and high values of the response variable.  A residuals vs fits plot $(\epsilon \sim \hat{Y})$ should have random scatter in a band of constant width around 0, and no fan shape at the low and high ends.   All tests and intervals are sensitive to this condition.

## Prediction

The standard error in the **expected value** of $\hat{y}$ at some new set of predictors $X_n$ is 

$$SE(\mu_\hat{y}) = \sqrt{\hat{\sigma}^2 (X_n (X'X)^{-1} X_n')}.$$  

The standard error increases the further $X_n$ is from $\bar{X}$.  If $X_n = \bar{X}$, the equation reduces to $SE(\mu_\hat{y}) = \sigma / \sqrt{n}$.  If $n$ is large, or the predictor values are spread out, $SE(\mu_\hat{y})$ will be relatively small.  The $(1 - \alpha)\%$ **confidence interval** is $\hat{y} \pm t_{\alpha / 2} SE(\mu_\hat{y})$.

The standard error in the **predicted value** of $\hat{y}$ at some $X_{new}$ is 

$$SE(\hat{y}) = SE(\mu_\hat{y})^2 + \sqrt{\hat{\sigma}^2}.$$

Notice the standard error for a predicted value is always greater than the standard error of the expected value.  The $(1 - \alpha)\%$ **prediction interval** is $\hat{y} \pm t_{\alpha / 2} SE(\hat{y})$.


#### Example

What is the expected value of `mpg` if the predictor values equal their mean values?  

R performs this calucation with the `predict()` function with parameter `interval = confidence`.

```{r message=FALSE}
m <-lm(mpg ~ ., data = d[,1:9])
X_new <- data.frame(Const = 1,
                    cyl = factor(round(mean(as.numeric(as.character(d$cyl))),0), levels = levels(d$cyl)), 
                    disp = mean(d$disp),
                    hp = mean(d$hp),
                    drat = mean(d$drat),
                    wt = mean(d$wt),
                    qsec = mean(d$qsec),
                    vs = factor("S", levels = levels(d$vs)), 
                    am = factor("manual", levels = levels(d$am)))
predict.lm(object = m, 
           newdata = X_new, 
           interval = "confidence")
```

You can verify this by manually calculating $SE(\mu_\hat{y}) = \sqrt{\hat{\sigma}^2 (X_{new} (X'X)^{-1} X_{new}')}$ using matrix algebra.

```{r message=FALSE, warning=FALSE}
X2 <- lapply(data.frame(model.matrix(m)), mean) %>% unlist() %>% t()
X2[2] <- contr.poly(3)[2,1]  # cyl linear
X2[3] <- contr.poly(3)[2,2]  # cyl quadratic
X2[9] <- 1
X2[10] <- 1

y_exp <- sum(m$coefficients * as.numeric(X2))
se_y_exp <- as.numeric(sqrt(rse^2 * 
                              X2 %*% 
                              solve(t(X) %*% X) %*% 
                              t(X2)))

t_crit <- qt(p = .05 / 2, df = n - k - 1, lower.tail = FALSE)
me <- t_crit * se_y_exp
cat("fit: ", round(y_exp, 6),
    ", 95% CI: (", round(y_exp - me, 6), ", ", round(y_exp + me, 6), ")")
```


#### Example 

What is the predicted value of mpg if the predictor values equal their mean values?

R performs this calucation with the `predict()` with parameter `interval = prediction`.

```{r message=FALSE}
predict.lm(object = m, 
           newdata = X_new, 
           interval = "prediction")
```

```{r message=FALSE, warning=FALSE}
se_y_hat <- sqrt(rse^2 + se_y_exp^2)
me <- t_crit * se_y_hat
cat("fit: ", round(y_exp, 6),
    ", 95% CI: (", round(y_exp - me, 6), ", ", round(y_exp + me, 6), ")")
```


## Inference

Draw conclusions about the significance of the coefficient estimates with the *t*-test and/or *F*-test.

### *t*-Test

By assumption, the residuals are normally distributed, so the *Z*-test statistic could evaluate the parameter estimators, 

$$Z = \frac{\hat{\beta} - \beta_0}{\sqrt{\sigma^2 (X'X)^{-1}}}$$ 

where $\beta_0$ is the null-hypothesized value, usually 0.  $\sigma$ is unknown, but $\frac{\hat{\sigma}^2 (n - k)}{\sigma^2} \sim \chi^2$.  The ratio of the normal distribution divided by the adjusted chi-square $\sqrt{\chi^2 / (n - k)}$ is t-distributed, 

$$t = \frac{\hat{\beta} - \beta_0}{\sqrt{\hat{\sigma}^2 (X'X)^{-1}}} = \frac{\hat{\beta} - \beta_0}{SE(\hat{\beta})}$$

The $(1 - \alpha)$ confidence intervals are $CI = \hat{\beta} \pm t_{\alpha / 2, df} SE(\hat{\beta})$ with *p*-value equaling the probability of measuring a $t$ of that extreme, $p = P(t > |t|)$.  For a one-tail test, divide the reported p-value by two.  The $SE(\hat{\beta})$ decreases with 1) a better fitting regression line (smaller $\hat{\sigma}^2$), 2) greater variation in the predictor (larger $X'X$), and 3) larger sample size (larger n).


#### Example

Define a 95% confidence interval around the slope parameters.

The `summary()` output shows the t values and probabilities in the `t value` and `Pr(>|t|)` columns.  You can verify this manually using matrix algebra for $t = \frac{(\hat{\beta} - \beta_1)}{SE(\hat{\beta})}$ with $\beta_1 = 0$.  The $(1 - \alpha)$ confidence interval is $CI = \hat{\beta} \pm t_{\alpha / 2, df} SE(\hat{\beta})$.  The table below gathers the parameter estimators and t-test results.

```{r}
t <- beta_hat / se_beta_hat
p_value <- pt(q = abs(t), 
              df = n - k - 1, 
              lower.tail = FALSE) * 2
t_crit <- qt(p = .05 / 2, df = n - k - 1, lower.tail = FALSE)
lcl = beta_hat - t_crit * se_beta_hat
ucl = beta_hat + t_crit * se_beta_hat
data.frame(beta = round(beta_hat, 4), 
           se = round(se_beta_hat, 4), 
           t = round(t, 4), 
           p = round(p_value, 4),
           lcl = round(lcl,4), 
           ucl = round(ucl, 4))
```


### *F*-Test

The *F*-test for the model is a test of the null hypothesis that none of the independent variables linearly predict the dependent variable, that is, the model parameters are jointly zero: $H_0 : \beta_1 = \ldots = \beta_k = 0$.  The regression mean sum of squares $MSR = \frac{(\hat{y} - \bar{y})'(\hat{y} - \bar{y})}{k-1}$ and the error mean sum of squares $MSE = \frac{\hat{\epsilon}'\hat{\epsilon}}{n-k}$ are each chi-square variables.  Their ratio has an *F* distribution with $k - 1$ numerator degrees of freedom and $n - k$ denominator degrees of freedom.  The F statistic can also be expressed in terms of the coefficient of correlation $R^2 = \frac{MSR}{MST}$.  

$$F(k - 1, n - k) = \frac{MSR}{MSE} = \frac{R^2}{1 - R^2} \frac{n-k}{k-1}$$

*MSE* is $\sigma^2$.  If $H_0$ is true, that is, there is no relationship between the predictors and the response, then $MSR$ is also equal to $\sigma^2$, so $F = 1$.  As $R^2 \rightarrow 1$, $F \rightarrow \infty$, and as $R^2 \rightarrow 0$, $F \rightarrow 0$.  F increases with $n$ and decreases with $k$.

#### Example

What is the probability that all parameters are jointly equal to zero?

The F-statistic is presented at the bottom of the `summary()` function.  You can verify this manually.

```{r}
ssr <- sum((m$fitted.values - mean(d$mpg))^2)
sse <- sum(m$residuals^2)
sst <- sum((m$mpg - mean(d$mpg))^2)
msr <- ssr / k
mse <- sse / (n - k - 1)
f = msr / mse
p_value <- pf(q = f, df1 = k, df2 = n - k - 1, lower.tail = FALSE)
cat("F-statistic: ", round(f, 4), " on 3 and 65 DF,  p-value: ", p_value)
```

There is sufficient evidence $(F = 17.35, P < .0001)$ to reject $H_0$ that the parameter estimators are jointly equal to zero.

The `aov` function calculates the sequential sum of squares.  The regression sum of squares SSR for `mpg ~ cyl` is 824.8.  Adding `disp` to the model increases SSR by 57.6.  Adding `hp` to the model increases SSR by 18.5.  It would seem that `hp` does not improve the model.

```{r}
summary(aov(m))
```

Order matters.  Had we started with `disp`, then added `hp` we would find both estimators were significant.
```{r}
summary(aov(lm(mpg ~ disp + hp + drat + wt + qsec + vs + am + cyl, data = d)))
```


## Interpretation

A plot of the standardized coefficients shows the relative importance of each variable.  The distance the coefficients are from zero shows how much a change in a standard deviation of the regressor changes the mean of the predicted value.  The CI shows the precision. The plot shows not only which variables are significant, but also which are important.

```{r warning=FALSE}
d_sc <- d %>% mutate_at(c("mpg", "disp", "hp", "drat", "wt", "qsec"), scale)
m_sc <- lm(mpg ~ ., d_sc[,1:9])
lm_summary <- summary(m_sc)$coefficients
df <- data.frame(Features = rownames(lm_summary),
                 Estimate = lm_summary[,'Estimate'],
                 std_error = lm_summary[,'Std. Error'])
df$lower = df$Estimate - qt(.05/2, m_sc$df.residual) * df$std_error
df$upper = df$Estimate + qt(.05/2, m_sc$df.residual) * df$std_error
df <- df[df$Features != "(Intercept)",]

ggplot(df) +
  geom_vline(xintercept = 0, linetype = 4) +
  geom_point(aes(x = Estimate, y = Features)) +
  geom_segment(aes(y = Features, yend = Features, x=lower, xend=upper), 
               arrow = arrow(angle=90, ends='both', length = unit(0.1, 'cm'))) +
  scale_x_continuous("Standardized Weight") +
  labs(title = "Model Feature Importance")
```

The added variable plot shows the bivariate relationship between $Y$ and $X_i$ after accounting for the other variables.  For example, the partial regression plots of `y ~ x1 + x2 + x3` would plot the residuals of `y ~ x2 + x3` vs `x1`, and so on.

```{r}
library(car)
avPlots(m)
```


## Model Validation

Evaluate predictive accuracy by training the model on a training data set and testing on a test data set.

### Accuracy Metrics

The most common measures of model fit are R-squared, RMSE, RSE, MAE, Adjusted R-squared, AIC, AICc, BIC, and Mallow's Cp.

#### R-Squared

The coefficient of determination (**R-squared**) is the percent of total variation in the response variable that is explained by the regression line.  

$$R^2 = 1 - \frac{SSE}{SST}$$

where $SSE = \sum_{i=1}^n{(y_i - \hat{y}_i)^2}$ is the sum squared differences between the predicted and observed value, $SST = \sum_{i = 1}^n{(y_i - \bar{y})^2}$ is the sum of squared differences between the observed and overall mean value, and $RSS = \sum_{i=1}^n{(\hat{y}_i - \bar{y})^2}$ is the sum of squared differences between the predicted and overall mean "no-relationship line" value.  At the extremes, $R^2 = 1$ means all data points fall perfectly on the regression line - the predictors account for *all* variation in the response; $R^2 = 0$ means the regression line is horizontal at $\bar{y}$ - the predictors account for *none* of the variation in the response.  In the simple case of a single predictor variable, $R^2$ equals the squared correlation between $x$ and $y$, $r = Cor(x,y)$.

```{r}
ssr <- sum((m$fitted.values - mean(d$mpg))^2)
sse <- sum(m$residuals^2)
sst <- sum((d$mpg - mean(d$mpg))^2)
(r2 <- ssr / sst)
(r2 <- 1 - sse / sst)
summary(m)$r.squared
```

$R^2$ is also equal to the correlation between the fitted value and observed values, $R^2 = Cor(Y, \hat{Y})^2$.

```{r}
cor(m$fitted.values, d$mpg)^2
```

R-squared is proportional to the the variance in the response, *SST*.  Given a constant percentage error in predictions, a test set with relatively low variation in the reponse will have a lower R-squared.  Conversely, test sets with large variation, e.g., housing data with home sale ranging from $60K to $2M may have a large R-squared despite average prediction errors of >$10K.

A close variant of R-squared is the non-parametric Spearman's rank correlation.  This statistic is the correlation of the *ranks* of the response and the predicted values.  It is used when the model goal is ranking.  

#### RMSE

The root mean squared error (**RMSE**) is the average prediction error (square root of mean squared error).

$$RMSE = \sqrt{\frac{\sum_{i=1}^n{(y_i - \hat{y}_i)^2}}{n}}$$

```{r}
sqrt(mean((d$mpg - m$fitted.values)^2))
```

The `rmse()` function from the `Metrics` package, and the `postResample()` function in `caret` calculate RMSE.

```{r warning=FALSE, message=FALSE}
rmse(actual = d$mpg, predicted = m$fitted.values)
postResample(pred = m$fitted.values, obs = d$mpg)[1]
```

The mean squared error of a model with theoretical residual of mean zero and constant variance $\sigma^2$ can be decomposed into the model's bias and the model's variance:

$$E[MSE] = \sigma^2 + Bias^2 + Var.$$

A model that predicts the response closely will have low bias, but be relatively sensitive to the training data and thus have high variance.  A model that predicts the response conservatively (e.g., a simple mean) will have large bias, but be relatively insensitive to nuances in the training data.  Here is an example of a simulated sine wave.  A model predicting the mean value at the upper and lower levels has low variance, but high bias, and a model of an actual sine wave has low bias and high variance.  This is referred to as the variance-bias trade-off. 

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.height=3}
library(xts)
library(forecast)
temp_x <- runif(50, 2, 10)
temp_ts <- ts(sin(temp_x[order(temp_x)]), start = 2, frequency = 6)
ggplot(temp_ts, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = c(as.numeric(NA), rollmean(temp_ts, k = 3, align = "center"), as.numeric(NA)), color = "Roll Mean (k = 3)")) +
  geom_line(aes(y = ifelse(x <= 6, mean(sin(temp_ts[x <= 6])), mean(sin(temp_ts[x > 6]))), color = "Roll Mean (k = 50)")) +
  labs(color = "Fit")
rm(temp_x, temp_ts)
```


#### RSE

The residual standard error (**RSE**, or model sigma $\hat{\sigma}$) is an estimate of the standard deviation of $\epsilon$.  It is roughly the average amount the response deviates from the true regression line.  

$$\sigma = \sqrt{\frac{\sum_{i=1}^n{(y_i - \hat{y}_i)^2}}{n-k-1}}$$

```{r}
sqrt(sum((d$mpg - m$fitted.values)^2) / (n - k - 1))

# sd is sqrt(sse / (n-1)), sigma = sqrt(sse / (n - k - 1))
sd(m$residuals) * sqrt((n - 1) / (n - k - 1))  

summary(m)$sigma 

sigma(m)
```


#### MAE

The mean absolute error (**MAE**) is the average absolute prediction arror.  It is less sensitive to outliers.

$$MAE = \frac{\sum_{i=1}^n{|y_i - \hat{y}_i|}}{n}$$

```{r}
sum(abs(d$mpg - m$fitted.values)) / n
```


The `postResample()` function in `caret` conveniently calculates all three.

```{r}
postResample(pred = m$fitted.values, obs = d$mpg)

defaultSummary(data = data.frame(obs = d$mpg, pred = m$fitted.values), model = m)

apply(as.matrix(m$fitted.values), 2, postResample, obs = d$mpg)
```

These metrics are good for evaluating a model, but less useful for comparing models.  The problem is that they tend to improve with additional variables added to the model, even if the improvement is not significant.  The following metrics aid model comparison by penalizing added variables. 

#### Adjusted R-squared

The adjusted R-squared ($\bar{R}^2$) penalizes the R-squared metric for increasing number of predictors. 

$$\bar{R}^2 = 1 - \frac{SSE}{SST} \cdot \frac{n-1}{n-k-1}$$

```{r}
(adj_r2 <- 1 - sse/sst * (n - 1) / (n - k - 1))

summary(m)$adj.r.squared
```


#### AIC

Akaike's Information Criteria (**AIC**) is a penalization metric.  The lower the AIC, the better the model.

```{r}
AIC(m)
```


#### AICc

**AICc** corrects AIC for small sample sizes.

```{r}
AIC(m) + (2 * k * (k + 1)) / (n - k - 1)
```


#### BIC
The Basiean information criteria (**BIC**) is like AIC, but with a stronger penalty for additional variables.

```{r}
BIC(m)
```


#### Mallows Cp

Mallows Cp is a variant of AIC.


##### Example

Compare the full model to a model without `cyl`.

The `glance()` function from the `broom` package calculates many validation metrics.  Here are the validation stats for the full model and then the reduced model.

```{r}
library(broom)

glance(m) %>% select(adj.r.squared, sigma, AIC, BIC, p.value)
glance(lm(mpg ~ . - cyl, d[, 1:9])) %>% select(adj.r.squared, sigma, AIC, BIC, p.value)
```

The ajusted R2 increased and AIC and BIC decreased, meaning the full model is less efficient at explaining the variability in the response value.  The residual standard error `sigma` is smaller for the reduced model.  Finally, the *F* statistic p-value is smaller for the reduced model, meaning the reduced model is statistically more significant.

Note that these regression metrics are all internal measures, that is they have been computed on the training dataset, not the test dataset.


### Cross-Validation

Cross-validation is a set of methods for measuring the performance of a predictive model on a test dataset.  The main measures of prediction performance are R2, RMSE and MAE. 

#### Validation Set

To perform validation set cross validation, randomly split the data into a training data set and a test data set.  Fit models to the training data set, then predict values with the validation set.  The model that produces the best prediction performance is the preferred model.

The `caret` package provides useful methods for cross-validation.

##### Example

```{r warning=FALSE, message=FALSE}
library(caret)

set.seed(123)
train_idx <- createDataPartition(y = d$mpg, p = 0.80, list = FALSE)
d.train <- d[train_idx, ]
d.test <- d[-train_idx, ]
```

Build the model using `d.train`, make predictions, then calculate the R2, RMSE, and MAE.  Use the `train()` function from the `caret` package.  Use `method = "none"` to simply fit the model to the entire data set.

```{r}
set.seed(123)
m1 <- train(mpg ~ ., 
            data = d.train[, 1:9],
            method = "lm",
            trControl = trainControl(method = "none"))
print(m1)
postResample(pred = predict(m1, newdata = d.test), 
             obs = d.test$mpg)
```

The validation set method is only useful when you have a large data set to partition.  A second disadvantage is that building a model on a fraction of the data leaves out information. The test error will vary with which observations are included in the training set.

#### LOOCV

Leave one out cross validation (LOOCV) works by successively modeling with training sets leaving out one data point, then averaging the prediction errors.

```{r}
set.seed(123)
m2 <- train(mpg ~ ., 
            data = d.train[, 1:9],
            method = "lm",
            trControl = trainControl(method = "LOOCV"))
print(m2)
postResample(pred = predict(m2, newdata = d.test), 
             obs = d.test$mpg)
```

This method isn't perfect either.  It repeats as many times as there are data points, so the execution time may be long.  LOOCV is also sensitive to outliers.

#### K-fold Cross-Validation

K-fold cross-validation splits the dataset into *k* folds (subsets), then uses *k-1* of the folds for a training set and the remaining fold for a test set, then repeats for all permutations of k taken k-1 at a time.  E.g., 3-fold cross-validation will partition the data into sets A, B, and C, then create train/test splits of [AB, C], [AC, B], and [BC, A].  

K-fold cross-validation is less computationally expensive than LOOCV, and often yields more accurate test error rate estimates.  What is the right value of k?  The lower is *k* the more biased the estimates; the higher is *k* the larger the estimate variability. At the extremes *k* = 2 is the validation set method, and *k = n* is the LOOCV method.  In practice, one typically performs k-fold cross-validation using k = 5 or k = 10 because these values have been empirically shown to balence bias and variance. 

```{r}
set.seed(123)
m3 <- train(mpg ~ ., 
            data = d.train[, 1:9],
            method = "lm",
            trControl = trainControl(method = "cv",
                                     number = 5))
print(m3)
postResample(pred = predict(m3, newdata = d.test), 
             obs = d.test$mpg)
```

#### Repeated K-fold CV

You can also perform k-fold cross-validation multiple times and average the results.  Specify `method = "repeatedcv"` and `repeats = 3` in the `trainControl` object for three repeats.

```{r}
set.seed(123)
m4 <- train(mpg ~ ., 
            data = d.train[, 1:9],
            method = "lm",
            trControl = trainControl(method = "repeatedcv",
                                     number = 5,
                                     repeats = 3))
print(m4)
postResample(pred = predict(m4, newdata = d.test), 
             obs = d.test$mpg)
```


#### Bootstrapping

Bootstrapping randomly selects a sample of n observations with replacement from the original dataset to evaluate the model.  The procedure is repeated many times.

Specify `method = "boot"` and `number = 100` to perform 100 bootstrap samples.

```{r}
set.seed(123)
m5 <- train(mpg ~ ., 
            data = d.train[, 1:9],
            method = "lm",
            trControl = trainControl(method = "boot",
                                     number = 100))
print(m5)
postResample(pred = predict(m5, newdata = d.test), 
             obs = d.test$mpg)
```


### Gain Curve

For supervised learning purposes, a visual way to evaluate a regression model is with the gain curve.  This visualization compares a predictive model score to an actual outcome (either binary (0/1) or continuous). The gain curve plot measures how well the model score sorts the data compared to the true outcome value.  The x-axis is the fraction of items seen when sorted by score, and the y-axis is the cumulative summed true outcome when sorted by score.  For comparison, GainCurvePlot also plots the "wizard curve": the gain curve when the data is sorted according to its true outcome.  A relative Gini score close to 1 means the model sorts responses well.

```{r message=FALSE, warning=FALSE}
library(WVPlots)
d$fitted <- m$fitted.values
GainCurvePlot(d, xvar = "fitted", truthVar = "mpg", title = "Model Gain Curve")
```


## Reference

Penn State University, STAT 501, Lesson 12: Multicollinearity & Other Regression Pitfalls. [https://newonlinecourses.science.psu.edu/stat501/lesson/12](https://newonlinecourses.science.psu.edu/stat501/lesson/12).

STHDA.  Bootstrap Resampling Essentials in R.  [http://www.sthda.com/english/articles/38-regression-model-validation/156-bootstrap-resampling-essentials-in-r/](http://www.sthda.com/english/articles/38-regression-model-validation/156-bootstrap-resampling-essentials-in-r/)

Molnar, Christoph. "Interpretable machine learning. A Guide for Making Black Box Models Explainable", 2019. [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/).

<!--chapter:end:06-ols.Rmd-->

# Generalized Linear Models

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(broom)  # for augment()
library(WVPlots)  # for GainCurvePlot()
library(caret)  # for RMSE()
library(readr)
library(mfstylr)
```


These notes are primarily from [PSU STAT 504](https://online.stat.psu.edu/stat504) which uses Alan Agresti's **Categorical Data Analysis** [@Agresti2013].  I also reviewed [PSU STAT 501](https://newonlinecourses.science.psu.edu/stat501/lesson/15), [DataCamp's Generalized Linar Models in R](https://www.datacamp.com/courses/generalized-linear-models-in-r), [DataCamp's Multiple and Logistic Regression](https://www.datacamp.com/courses/multiple-and-logistic-regression), and **Interpretable machine learning*"** [@Molner2020].

The linear regression model, $E(Y|X) = X \beta$, structured as $y_i = X_i \beta + \epsilon_i$ where $X_i \beta = \mu_i$, assumes the response is a linear function of the predictors and the residuals are independent random variables normally distributed with mean zero and constant variance, $\epsilon \sim N \left(0, \sigma^2 \right)$.  This implies that given some set of predictors, the response is normally distributed about its expected value, $y_i \sim N \left(\mu_i, \sigma^2 \right)$.  However, there are many situations where this assumption of normality fails.  Generalized linear models (GLMs) are a generalization of the linear regression model that addresses non-normal response distributions.

The response given a set of predictors will not have a normal distribution if its underlying data-generating process is binomial or multinomial (proportions), Poisson (counts), or exponential (time-to-event).  In these situations a regular linear regression can predict proportions outside [0, 100] or counts or times that are negative.  GLMs solve this problem by modeling *a function* of the expected value of $y$, $f(E(Y|X)) = X \beta$.  There are three components to a GLM: the *random component* is the probability distribution of the response variable (normal, binomial, Poisson, [etc](https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions).); the *systematic component* is the explanatory variables $X\beta$; and the *link function* $\eta$ specifies the link between random and systematic components, converting the response range to $[-\infty, +\infty]$.  

Linear regression is thus a special case of GLM where link function is the identity function, $f(E(Y|X)) = E(Y|X)$.  For a logistic regression, where the data generating process is binomial, the link function is 

$$f(E(Y|X)) = \ln \left( \frac{E(Y|X)}{1 - E(Y|X)} \right) = \ln \left( \frac{\pi}{1 - \pi} \right) = logit(\pi)$$

where $\pi$ is the event probability.  (As an aside, you have probably heard of the related "probit" regression. The probit regression link function is $f(E(Y|X)) = \Phi^{-1}(E(Y|X)) = \Phi^{-1}(\pi)$. The difference between the logistic and probit link function is theoretical, and [the practical significance is slight](https://www.theanalysisfactor.com/the-difference-between-logistic-and-probit-regression/).  You can probably safely ignore probit).  

For a Poisson regression, the link function is

$$f(E(Y|X)) = \ln (E(Y|X)) = \ln(\lambda)$$

where $\lambda$ is the expected event rate.  

For an exponential regression, the link function is 

$$f(E(Y|X) = -E(Y|X) = -\lambda$$

where $\lambda$ is the expected time to event.

GLM uses maximum likelihood estimation (MLE) rather than ordinary least squares (OLS) to estimate the parameters, and thus relies on large-sample approximations. 

In R, specify a GLM just like an linear model, but with the `glm()` function, specifying the distribution with the `family` parameter.

* `family = "gaussian"`: linear regression
* `family = "binomial"`: logistic regression
* `family = binomial(link = "probit")`: probit regression
* `family = "poisson"`: Poisson regression


## Logistic Regression

Logistic regression estimates the probability of a particular level of a categorical response variable given a set of predictors. The response levels can be binary, nominal (multiple categories), or ordinal (multiple levels).  

The **binary** logistic regression model is

$$y = logit(\pi) = \ln \left( \frac{\pi}{1 - \pi} \right) = X \beta$$

where $\pi$ is the event probability. The model predicts the *log odds* of the response variable.  The maximum likelihood estimator maximizes the likelihood function

$$L(\beta; y, X) = \prod_{i=1}^n \pi_i^{y_i}(1 - \pi_i)^{(1-y_i)} = \prod_{i=1}^n\frac{\exp(y_i X_i \beta)}{1 + \exp(X_i \beta)}.$$

There is no closed-form solution, so GLM estimates coefficients with interatively reweighted least squares.  

Here is a case study to illustrate the points.  Dataset `donner` contains observations of 45 members of the Donner party with response variable (`surv`) an explanatory variables `age` and `sex`.

```{r include=FALSE}
donner <- tribble(
  ~age, ~sex, ~surv,
  23, 1, 0,
  40, 0, 1,
  40, 1, 1,
  30, 1, 0,
  28, 1, 0,
  40, 1, 0,
  45, 0, 0,
  62, 1, 0,
  65, 1, 0,
  45, 0, 0,
  25, 0, 0,
  28, 1, 1,
  28, 1, 0,
  23, 1, 0,
  22, 0, 1,
  23, 0, 1,
  28, 1, 1,
  15, 0, 1,
  47, 0, 0,
  57, 1, 0,
  20, 0, 1,
  18, 1, 1,
  25, 1, 0,
  60, 1, 0,
  25, 1, 1,
  20, 1, 1,
  32, 1, 1,
  32, 0, 1,
  24, 0, 1,
  30, 1, 1,
  15, 1, 0,
  50, 0, 0,
  21, 0, 1,
  25, 1, 0,
  46, 1, 1,
  32, 0, 1,
  30, 1, 0,
  25, 1, 0,
  25, 1, 0,
  25, 1, 0,
  30, 1, 0,
  35, 1, 0,
  23, 1, 1,
  24, 1, 0,
  25, 0, 1,
) %>%
  mutate(
    surv = factor(surv, levels = c(0:1), labels = c("Died", "Lived")),
    sex = factor(sex, levels = c(0:1), labels = c("F", "M"))
  )
```

```{r}
glimpse(donner)
```

```{r, fig.height=3.5}
donner %>%  mutate(surv = as.numeric(surv)-1) %>%
  ggplot(aes(x = age, y = surv, color = sex)) +
  geom_jitter() +
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE) +
  theme_mf() +
  labs(title = "Donner Party Survivorship", color = "")
```

Fit a logistic regression $SURV = SEX + AGE + SEX : AGE$.

```{r}
m <- glm(surv ~ sex*age, data = donner, family = binomial(link = logit))
summary(m)
```

The "z value" in the Coefficients table is the Wald z statistic, $z = \hat{\beta} / SE(\hat{\beta})$, which if squared is the Wald chi-squared statistic, $z^2$.  The p.value is the area to the right of $z^2$ in the $\chi_1^2$ density curve:

```{r}
m %>% tidy() %>% 
  mutate(
    z = estimate / std.error, 
    p_z2 = pchisq(z^2, df = 1, lower.tail = FALSE)
  ) %>%
  select(term, estimate, z, p_z2) 
```

Below the Coefficients table, the "dispersion parameter" refers to overdispersion, a common issue with GLM.  For a logistic regression, the response variable should be distributed $y_i \sim Bin(n_i, \pi_i)$ with $\mu_i = n_i \pi_i$ and $\sigma^2 = \pi (1 - \pi)$.  Overdispersion means the data shows evidence of variance greater than $\sigma^2$.

"Fisher scoring" is a method for ML estimation. Logistic regression uses an iterative procedure to fit the model, so this section indicates whether the algorithm converged.

The null deviance is the likelihood ratio $G^2 = 61.827$ of the intercept-only model.  The residual deviance is the likelihood ratio $G^2 = 47.346$ after including all model covariates.  $G^2$ is large, so reject the null hypothesis of no age and sex effects.  The ANOVA table shows the change in deviance from adding each variable successively to the model.

```{r}
anova(m)
```

```{r}
glance(m)
```

Plug in values to interpret the model.  The log odds of a 24 year-old female surviving is $\hat{y} = 2.59$.  The log odds of a 24 year-old male surviving is $\hat{y} = -0.46$.

```{r results=FALSE}
coef(m)["(Intercept)"] + coef(m)["sexM"]*0 + coef(m)["age"]*24 +
  coef(m)["sexM:age"]*0*24
coef(m)["(Intercept)"] + coef(m)["sexM"]*1 + coef(m)["age"]*24 +
  coef(m)["sexM:age"]*1*24

# Or use predict()
(lo_f <- predict(m, newdata = data.frame(sex = "F", age = 24)))
(lo_m <- predict(m, newdata = data.frame(sex = "M", age = 24)))
```

Log odds are not easy to interpet.  Exponentiate the log odds to get the **odds**.

$$odds(\hat{y}) = \exp (\hat{y}) = \frac{\pi}{1 - \pi}.$$

The odds of a 24 year-old female surviving is $\exp(\hat{y}) = 13.31$.  The odds of a 24 year-old male surviving is $\exp(\hat{y}) = 0.63$.

```{r results=FALSE}
exp(lo_f)
exp(lo_m)
```

Solve for $\pi$ to get the **probability**.  

$$\pi = \frac{\exp (\hat{y})}{1 + \exp (\hat{y})}$$

The probability of a 24 year-old female surviving is $\pi = 0.93$.  The probability of a female of average age surviving is $\pi = 0.39$.  The `predict()` function for a logistic model returns log-odds, but can also return $\pi$ by specifying parameter `type = "response"`.

```{r results=FALSE}
exp(lo_f) / (1 + exp(lo_f))
exp(lo_m) / (1 + exp(lo_m))

# Or use predict(..., type = "response")
(p_f <- predict(m, newdata = data.frame(sex = "F", age =24), type = "response"))
(p_m <- predict(m, newdata = data.frame(sex = "M", age =24), type = "response"))
```

Interpret the *coefficient* estimates using the **odds ratio**, the ratio of the odds before and after an increment to the predictors.  The odds ratio is how much the odds would be multiplied after a $\delta = X_1 - X_0$ unit increase in $X$.

$$\theta = \frac{\pi / (1 - \pi) |_{X = X_1}}{\pi / (1 - \pi) |_{X = X_0}} = \frac{\exp (X_1 \hat{\beta})}{\exp (X_0 \hat{\beta})} = \exp ((X_1-X_0) \hat{\beta}) = \exp (\delta \hat{\beta})$$

The odds of a female surviving are multiplied by a factor of $\exp(1 \cdot (-0.19)) = 0.824$ per additional year of age (or the odds fall by $1 - 0.824 = 17.6\%$). The odds of a male surviving are multiplied by a factor of $\exp(1 \cdot (-0.161-0.19)) = 0.968$ per additional year of age.   

```{r results=FALSE}
exp(1 * (coef(m)["age"] + 0*coef(m)["sexM:age"]))  # female
exp(1 * (coef(m)["age"] + 1*coef(m)["sexM:age"]))  # male
```

`oddsratio::or_glm()` calculates the odds ratio from an increment in the predictor values.

```{r}
oddsratio::or_glm(donner, m, incr = list(age = 1))
```

The predicted values can also be expressed as the probabilities $\pi$.  This produces the familiar signmoidal shape of the binary relationship.

```{r fig.height=3.5}
augment(m, type.predict = "response") %>%
  ggplot(aes(x = age)) +
  geom_point(aes(y = surv)) +
  geom_line(aes(y = .fitted+1)) +
  theme_mf() +
  labs(x = "AGE",
       y = "Probability of SURVIVE",
       title = "Binary Fitted Line Plot")
```

Evaluate a logistic regression using a [Gain curve or ROC curve](https://community.tibco.com/wiki/gains-vs-roc-curves-do-you-understand-difference). 

In the **gain curve**, the x-axis is the fraction of items seen when sorted by the predicted value, and the y-axis is the cumulative summed true outcome. The "wizard" curve is the gain curve when the data is sorted by the true outcome.  If the model's gain curve is close to the wizard curve, then the model predicted the response variable well. The grey area is the "gain" over a random prediction.

20 of the 45 members of the Donner party survived. 

* The gain curve encountered 10 survivors (50%) within the first 12 observations (27%).  It encountered all 20 survivors on the 37th observation.
* The bottom of the grey area is the outcome of a random model.  Only half the survivors would be observed within 50% of the observations.  
* The top of the grey area is the outcome of the perfect model, the "wizard curve".  Half the survivors would be observed in 10/45=22% of the observations.

```{r, fig.height=3.5}
options(yardstick.event_first = FALSE)  # set the second level as success
augment(m, type.predict = "response") %>%
yardstick::gain_curve(surv, .fitted) %>%
  autoplot() +
  labs(title = "Gain Curve")
```

The ROC (Receiver Operating Characteristics) curve plots sensitivity vs specificity at different cut-off values for the probability, ranging cut-off from 0 to 1.

```{r, fig.height=3.5}
options(yardstick.event_first = FALSE)  # set the second level as success
augment(m, type.predict = "response") %>%
yardstick::roc_curve(surv, .fitted) %>%
  autoplot() +
  labs(title = "ROC Curve")
```


## Multinomial Logistic Regression

The following notes rely on the [PSU STAT 504 course notes](https://online.stat.psu.edu/stat504/node/171/.

Multinomial logistic regression models the odds the multinomial response variable $Y \sim Mult(n, \pi)$ is in level $j$ relative to baseline category $j^*$ for all pairs of categories as a function of $k$ explanatory variables, $X = (X_1, X_2, ... X_k)$. 

$$\log \left( \frac{\pi_{ij}}{\pi_{ij^*}} \right) = x_i^T \beta_j, \hspace{5mm} j \ne j^2$$

Interpet the $k^{th}$ element of $\beta_j$ as the increase in log-odds of falling a response in category $j$ relative to category $j^*$ resulting from a one-unit increase in the $k^{th}$ predictor term, holding the other terms constant.

Multinomial model is a type of GLM.

Here is an example using multinomial logistic regression.  A researcher classified the stomach contents of $n = 219$ alligators according to $r = 5$ categories (*fish, Inv., Rept, Bird, Other*) as a function of covariates *Lake*, *Sex*, and *Size*..

```{r}
gator_dat <- tribble(
  ~profile, ~Gender, ~Size, ~Lake, ~Fish, ~Invertebrate, ~Reptile, ~Bird, ~Other,
  "1", "f", "<2.3", "george",  3, 9, 1, 0, 1,
  "2", "m", "<2.3", "george", 13, 10, 0, 2, 2,
  "3", "f", ">2.3", "george", 8, 1, 0, 0, 1,
  "4", "m", ">2.3", "george", 9, 0, 0, 1, 2,
  "5", "f", "<2.3", "hancock", 16, 3, 2, 2, 3,
  "6", "m", "<2.3", "hancock", 7, 1, 0, 0, 5,
  "7", "f", ">2.3", "hancock", 3, 0, 1, 2, 3,
  "8", "m", ">2.3", "hancock", 4, 0, 0, 1, 2,
  "9", "f", "<2.3", "oklawaha", 3, 9, 1, 0, 2,
  "10", "m", "<2.3", "oklawaha", 2, 2, 0, 0, 1,
  "11", "f", ">2.3", "oklawaha", 0, 1, 0, 1, 0,
  "12", "m", ">2.3", "oklawaha", 13, 7, 6, 0, 0,
  "13", "f", "<2.3", "trafford", 2, 4, 1, 1, 4,
  "14", "m", "<2.3", "trafford", 3, 7, 1, 0, 1,
  "15", "f", ">2.3", "trafford", 0, 1, 0, 0, 0,
  "16", "m", ">2.3", "trafford", 8, 6, 6, 3, 5
)
gator_dat <- gator_dat %>%
  mutate(
    Gender = as_factor(Gender),
    Lake = fct_relevel(Lake, "hancock"),
    Size = as_factor(Size)
  )
```

There are 4 equations to estimate:

$$\log \left( \frac{\pi_j} {\pi_{j^*}} \right) = \beta X$$

where $\pi_{j^*}$ is the probability of fish, the baseline category.  


Run a multivariate logistic regression model with `VGAM::vglm()`.

```{r message=FALSE}
library(VGAM)
```

`vglm()` fits 4 logit models.

```{r}
gator_vglm <- vglm(
  cbind(Bird,Invertebrate,Reptile,Other,Fish) ~ Lake + Size + Gender, 
  data = gator_dat, 
  family = multinomial
)
summary(gator_vglm)
```

The **residual deviance** is 50.2637 on 40 degrees of freedom.  Residual deviance tests the current model fit versus the saturated model. The saturated model, which fits a separate multinomial distribution to each of the 16 profiles (unique combinations of lake, sex and size), has 16 × 4 = 64 parameters. The current model has an intercept, three lake coefficients, one sex coefficient and one size coefficient for each of the four logit equations, for a total of 24 parameters. Therefore, the overall fit statistics have 64 − 24 = 40 degrees of freedom.

```{r}
E <- data.frame(fitted(gator_vglm) * rowSums(gator_dat[, 5:9]))
O <- gator_dat %>% select(Bird, Invertebrate, Reptile, Other, Fish) + .000001
(g2 <- 2 * sum(O * log(O / E)))
```


indicates the model fits okay, but not great. The Residual Deviance of 50.26 with 40 df from the table above output is reasonable, with p-value of 0.1282 and the statistics/df is close to 1 that is 1.256.



## Ordinal Logistic Regression

These notes rely on [UVA](https://data.library.virginia.edu/fitting-and-interpreting-a-proportional-odds-model/), [PSU STAT 504 class notes](https://online.stat.psu.edu/stat504/node/171/), and [Laerd Statistics](https://statistics.laerd.com/spss-tutorials/ordinal-regression-using-spss-statistics.php).

The ordinal logistic regression model is

$$logit[P(Y \le j)] = \log \left[ \frac{P(Y \le j)}{P(Y \gt j)} \right] = \alpha_j - \beta X, \hspace{5mm} j \in [1, J-1]$$
where $j \in [1, J-1]$ are the levels of the ordinal outcome variable $Y$.  The proportional odds model assumes there is a common set of slope parameters $\beta$ for the predictors.  The ordinal outcomes are distinguished by the $J-1$ intercepts $\alpha_j$.  The benchmark level is $J$.

Technically, the model could be written $logit[P(Y \le j)] = \alpha_j + \zeta X$, replacing beta with zeta because the model fits $\alpha_j - \beta X$ instead of $\alpha_j + \beta X$.  
 
Suppose you want to model the probability a respondent holds a political ideology ["Socialist", "Liberal", "Moderate", "Conservative", "Libertarian"] given their party affiliation ["Republican", "Democrat"].

```{r include=FALSE}
ideo <- c("Socialist", "Liberal", "Moderate", "Conservative", "Libertarian")
ideo_cnt_rep <- c(30, 46, 148, 84, 99)
ideo_cnt_dem <- c(80, 81, 171, 41, 55)
ideology <- data.frame(
  party = factor(rep(c("Rep", "Dem"), c(407, 428)), levels = c("Rep", "Dem")),
  ideo = factor(
    c(rep(ideo, ideo_cnt_rep), rep(ideo, ideo_cnt_dem)), 
    levels = ideo
  )
)
```

```{r}
table(ideology)
```

### Assumptions

Ordinal regression makes four assumptions about the underlying data.  One is that the response variable is ordinal (duh).  The second is that the explanatory variables are continuous or categorical.  You can include ordinal variables, but you need to treat them either as continous or categorical.  Third, there is no multicollinearity.  Fourth, the odds are proportional, meaning each independent variable has an identical effect at each cumulative split of the ordinal dependent variable. Test for proportionality with a full likelihood ratio test comparing the fitted location model to a model with varying location parameters. This test can sometimes flag violations that do not exist, so can also run separate binomial logistic regressions on cumulative dichotomous dependent variables to further determine if this assumption is met. 

### Modeling 
Fit a proportional odds logistic regression.

```{r message=FALSE}
pom <- MASS::polr(ideo ~ party, data = ideology)
summary(pom)
```

The log-odds a Democrat identifies as *Socialist* vs *>Socialist*, or equivalently, the log-odds a Democrat identifies as *<=Socialist* vs *>=Liberal* is

$$logit[P(Y \le 1)] = -2.4690 - (-0.9745)(1) = -1.4945$$

which translates into an odds of 

$$odds(Y<=1) = exp(logit[P(Y \le 1)]) = \frac{exp(-2.469)}{exp(-0.9745)} = 0.2244$$

It is the same for Republicans, except multiply the slope coefficient by zero.

$$logit[P(Y \le 1)] = -2.4690 - (-0.9745)(0) = -2.4690$$

$$odds(Y<=1) = exp(logit[P(Y \le 1)]) = \frac{exp(-2.469)}{exp(0)} = -2.4690$$

The "proportional odds" part of the proportional odds model is that the ratios of the $J - 1$ odds are identical for each level of the predictors.  The numerators are always the same, and the denominators differ only by the exponent of the slope coefficient, $-0.9745$.  For all $j \in [1, J-1]$, the odds a Democrat's ideology is $\le j$ vs $>j$ is $exp(-0.9745) = 2.6498$ times that of a Republican's odds. 

You can translate the cumulative odds to cumulative probabilities by taking the ratio $\pi = exp(odds) / (1 + exp(odds))$.  The probability a Democrat identifies as *<=Socialist* vs *>Socialist* is 

$$P(Y \le 1) = \frac{exp(-1.4945)} {(1 + exp(-1.4945))} = 0.183.$$ 

The individual probabilities are just the successive differences in the cumulative probabilities. The log odds a Democrat identifies as *<=Liberal* vs *>Liberal* are $logit[P(Y \lt 2)] = -1.4745 - (-0.9745)(1) = -0.500$, which translates into a probability of $P(Y \le 2) = exp(-0.5) / (1 + exp(-0.5)) = 0.378$. The probability a Democrat identifies as *Liberal* is the difference in adjacent cumulative probabilities, $P(Y \le 2) - P(Y \le 1) = 0.378 = 0.183 = 0.194$.  This is how the model to predicts the level probabilities.

```{r}
x <- predict(pom, newdata = data.frame(party = c("Dem", "Rep")), type = "probs")
rownames(x) <- c("Dem", "Rep")
print(x)
```

Always check the assumption of proportional odds. One way to do this is by comparing the proportional odds model with a multinomial logit model, also called an unconstrained baseline logit model. The multinomial logit model models *unordered* responses and fits a slope to each level of the $J – 1$ responses. The proportional odds model is nested in the multinomial model, so you can use a likelihood ratio test to see if the models are statistically different.

```{r}
mlm <- nnet::multinom(ideo ~ party, data = ideology)
```

Calculate the difference in the deviance test statistics $D = -2 loglik(\beta)$.

```{r}
G <- -2 * (logLik(pom)[1] - logLik(mlm)[1])
pchisq(G, df = length(pom$zeta) - 1, lower.tail = FALSE)
```

The p-value is high, so do not reject the null hypothesis that the proportional odds model fits differently than the more complex multinomial logit model.

### Case Study

The General Social Survey for `year` 1972, 1973, and 1974 surveyed caucasian Christians about their attitudes `att` toward abortion. Respondents were classified by years of education `edu` and religious group `att`. 

```{r include=FALSE}
abort <- tribble(
  ~year, ~rel, ~edu, ~att, ~cnt,
  1972, "Prot", "Low", "Neg", 9,
  1972, "Prot", "Low", "Mix", 12,
  1972, "Prot", "Low", "Pos", 48,
  1972, "Prot", "Med", "Neg", 13,
  1972, "Prot", "Med", "Mix", 43,
  1972, "Prot", "Med", "Pos", 197,
  1972, "Prot", "High", "Neg", 4,
  1972, "Prot", "High", "Mix", 9,
  1972, "Prot", "High", "Pos", 139,
  1972, "SProt", "Low", "Neg", 9,
  1972, "SProt", "Low", "Mix", 17,
  1972, "SProt", "Low", "Pos", 30,
  1972, "SProt", "Med", "Neg", 6,
  1972, "SProt", "Med", "Mix", 10,
  1972, "SProt", "Med", "Pos", 97,
  1972, "SProt", "High", "Neg", 1,
  1972, "SProt", "High", "Mix", 8,
  1972, "SProt", "High", "Pos", 68,
  1972, "Cath", "Low", "Neg", 14,
  1972, "Cath", "Low", "Mix", 12,
  1972, "Cath", "Low", "Pos", 32,
  1972, "Cath", "Med", "Neg", 18,
  1972, "Cath", "Med", "Mix", 50,
  1972, "Cath", "Med", "Pos", 131,
  1972, "Cath", "High", "Neg", 8,
  1972, "Cath", "High", "Mix", 13,
  1972, "Cath", "High", "Pos", 64,
  1973, "Prot", "Low", "Neg", 4,
  1973, "Prot", "Low", "Mix", 16,
  1973, "Prot", "Low", "Pos", 59,
  1973, "Prot", "Med", "Neg", 6,
  1973, "Prot", "Med", "Mix", 24,
  1973, "Prot", "Med", "Pos", 197,
  1973, "Prot", "High", "Neg", 4,
  1973, "Prot", "High", "Mix", 11,
  1973, "Prot", "High", "Pos", 124,
  1973, "SProt", "Low", "Neg", 4,
  1973, "SProt", "Low", "Mix", 16,
  1973, "SProt", "Low", "Pos", 34,
  1973, "SProt", "Med", "Neg", 6,
  1973, "SProt", "Med", "Mix", 29,
  1973, "SProt", "Med", "Pos", 118,
  1973, "SProt", "High", "Neg", 1,
  1973, "SProt", "High", "Mix", 4,
  1973, "SProt", "High", "Pos", 82,
  1973, "Cath", "Low", "Neg", 2,
  1973, "Cath", "Low", "Mix", 14,
  1973, "Cath", "Low", "Pos", 32,
  1973, "Cath", "Med", "Neg", 16,
  1973, "Cath", "Med", "Mix", 45,
  1973, "Cath", "Med", "Pos", 141,
  1973, "Cath", "High", "Neg", 7,
  1973, "Cath", "High", "Mix", 20,
  1973, "Cath", "High", "Pos", 72,
  1974, "Prot", "Low", "Neg", 7,
  1974, "Prot", "Low", "Mix", 16,
  1974, "Prot", "Low", "Pos", 49,
  1974, "Prot", "Med", "Neg", 10,
  1974, "Prot", "Med", "Mix", 26,
  1974, "Prot", "Med", "Pos", 219,
  1974, "Prot", "High", "Neg", 4,
  1974, "Prot", "High", "Mix", 10,
  1974, "Prot", "High", "Pos", 131,
  1974, "SProt", "Low", "Neg", 1,
  1974, "SProt", "Low", "Mix", 19,
  1974, "SProt", "Low", "Pos", 30,
  1974, "SProt", "Med", "Neg", 5,
  1974, "SProt", "Med", "Mix", 21,
  1974, "SProt", "Med", "Pos", 106,
  1974, "SProt", "High", "Neg", 2,
  1974, "SProt", "High", "Mix", 11,
  1974, "SProt", "High", "Pos", 87,
  1974, "Cath", "Low", "Neg", 3,
  1974, "Cath", "Low", "Mix", 9,
  1974, "Cath", "Low", "Pos", 29,
  1974, "Cath", "Med", "Neg", 15,
  1974, "Cath", "Med", "Mix", 30,
  1974, "Cath", "Med", "Pos", 149,
  1974, "Cath", "High", "Neg", 11,
  1974, "Cath", "High", "Mix", 18,
  1974, "Cath", "High", "Pos", 69
)

abort <- abort %>%
  mutate(
    year = factor(year),
    rel = factor(rel, levels = c("Prot", "SProt", "Cath")),
    edu = factor(edu, levels = c("Low", "Med", "High")),
    att = factor(att, levels = c("Neg", "Mix", "Pos"))
  )
```

```{r}
abort %>% pivot_wider(names_from = att, values_from = cnt) 
```

Fit a proportional-odds cumulative logit model with just main effects. There are two main effects for `year`, two for `rel`, and two for `edu`, plus two logit equations for the response for a total of eight parameters. 

```{r message=FALSE}
abort_mdl <- MASS::polr(att ~ year + rel + edu, data = abort, weights = cnt)
summary(abort_mdl)
```

All predictors are significant.  Now fit the saturated model. 

```{r message=FALSE}
abort_mdl_sat <- MASS::polr(att ~ year*rel*edu, weights = cnt, data = abort)
summary(abort_mdl_sat)
```

Compare the two models.  

```{r}
anova(abort_mdl, abort_mdl_sat)
```

The likelihood ratio test indicates the main-effects model fits poorly in comparison to the saturated model (*LR* = 41.0, *df* = 20, *p* < 0.01).  From the table of coefficients,the effects of religion and education appear to be much more powerful than the year, so consider modeling an interaction between `rel` and `edu`.  This is also what the stepwise AIC algorithm recommends.

```{r include=FALSE}
abort_mdl_step <- MASS::stepAIC(abort_mdl_sat)
```

```{r warning=FALSE, message=FALSE}
summary(abort_mdl_step)
```

Compare the model with the `rel:edu` interaction to the saturated model.  

```{r}
anova(abort_mdl_step, abort_mdl_sat)
```

Great, this time they are not significantly different (*LR* = 22.2, *df* = 16, *p* = 0.138).  Interpret the results.

Positive coefficients mean attitudes toward legalizing abortion are more positive relative to the reference year, 1972. The odds of supporting legalization in 1973 compared to 1972 were $exp(0.2281) = 1.26$. The odds for 1974 vs 1972 were $exp(0.2410) = 1.27$, so attitudes toward abortion became more positive from 1972 to 1973, but remained nearly unchanged from 1973 to 1974.

Among Protestants (reference religious group), increasing education is associated with more positive attitudes toward abortion legalization. The odds of a Protestant with medium education vs low education supporting legalization are $exp(0.7504) = 2.12$.  Among Southern Protestants, odds are $exp(0.7504 + 0.2526) = 2.73$.  Therefore, the estimated effects of education for Southern Protestants are in the same direction as for Protestants but are somewhat larger. Note, however, that the interaction effect coefficient is not not significantly different from zero, so the effect of education among Protestants and Southern Protestants is not significantly different.

Among Catholics, the medium vs low education odds are $exp(0.7504- 0.3892) = 1.44$.  And the high vs low education odds are $exp(1.3689 - 0.9442) = 1.53$.  Increasing education is still associated with more positive attitudes, but the effects are smaller than they are among Protestants and Southern Protestants.

```{r fig.height=3.5, echo=FALSE}
dat <- bind_cols(
  abort_mdl_step$model, 
  data.frame(abort_mdl_step$fitted.values)
)
dat %>%
  filter(year == 1974) %>%
  mutate(prob = case_when(att == "Neg" ~ Neg, att == "Mix" ~ Mix, TRUE ~ Pos)) %>%
  select(-Neg, -Mix, -Pos) %>%
  filter(att == "Pos") %>%
  ggplot(aes(x = edu, y = prob, color = rel)) +
  geom_point() +
  geom_line(aes(x = as.numeric(factor(edu)), y = prob, col = rel)) +
  theme_mf() +
  labs(
    title = "Interaction of Education and Religion",
    subtitle = "on Positive Attitudes about Abortion Legalization",
    color = "",
    x = "Level of Education",
    y = "Prob of Positive Att."
  )
```


**Example Summarization**

We used logistic regression to investigate whether groups with the Christian religion might moderate the effects of education on attitudes toward abortion legalization. For Protestants, higher education education was associated with higher, significant, increase of odds of a more positive attitude toward abortion legalization, *b* = 0.7504, *SE* = 0.1774, *OR* = 2.12, *p* < .01.  There was a significant interaction for Catholics at high levels of education, *b* = -0.9442, *SE* = 0.3066, *p* < .01, relative to the Protestant reference group, but no significant interaction at medium education, and no interaction at all for Southern Protestants relative to the reference group.  The figure above graphs the interaction, showing the change in the expected probability of positive attitude by education level for Protestant, Southern Protestant, and Catholic religious groups. Overall, the significant interaction for Catholics at high levels of education suggests that education has a different relationship to attitudes toward abortion depending on the individual's religious group, but the difference between Protestant and Southern Protestant is minimal.


## Poisson Regression

Poisson models count data, like "traffic tickets per day", or "website hits per day".  The response is an expected *rate* or intensity.  For count data, specify the generalized model, this time with `family = poisson` or `family = quasipoisson`. 

Recall that the probability of achieving a count $y$ when the expected rate is $\lambda$ is distributed 

$$P(Y = y|\lambda) = \frac{e^{-\lambda} \lambda^y}{y!}.$$


The poisson regression model is

$$\lambda = \exp(X \beta).$$ 
 
You can solve this for $y$ to get

$$y = X\beta = \ln(\lambda).$$

That is, the model predicts the log of the response rate.  For a sample of size *n*, the likelihood function is

$$L(\beta; y, X) = \prod_{i=1}^n \frac{e^{-\exp({X_i\beta})}\exp({X_i\beta})^{y_i}}{y_i!}.$$

The log-likelihood is

$$l(\beta) = \sum_{i=1}^n (y_i X_i \beta - \sum_{i=1}^n\exp(X_i\beta) - \sum_{i=1}^n\log(y_i!).$$

Maximizing the log-likelihood has no closed-form solution, so the coefficient estimates are found through interatively reweighted least squares.  

Poisson processes assume the variance of the response variable equals its mean.  "Equals" means the mean and variance are of a similar order of magnitude.  If that assumption does not hold, use the quasi-poisson.  Use Poisson regression for large datasets.  If the predicted counts are much greater than zero (>30), the linear regression will work fine.  Whereas RMSE is not useful for logistic models, it is a good metric in Poisson.


Dataset `fire` contains response variable `injuries` counting the number of injuries during the month and one explanatory variable, the month `mo`.

```{r}
fire <- read_csv(file = "C:/Users/mpfol/OneDrive/Documents/Data Science/Data/CivilInjury_0.csv")
fire <- fire %>% 
  mutate(mo = as.POSIXlt(`Injury Date`)$mon + 1) %>%
  rename(dt = `Injury Date`,
         injuries = `Total Injuries`)
str(fire)
```

In a situation like this where there the relationship is bivariate, start with a visualization.

```{r}
ggplot(fire, aes(x = mo, y = injuries)) +
  geom_jitter() +
  geom_smooth(method = "glm", method.args = list(family = "poisson")) +
  labs(title = "Injuries by Month")
```


Fit a poisson regression in R using `glm(formula, data, family = poisson)`.  But first, check whether the mean and variance of `injuries` are the same magnitude?  If not, then use `family = quasipoisson`.

```{r}
mean(fire$injuries)
var(fire$injuries)
```

They are of the same magnitude, so fit the regression with `family = poisson`.

```{r}
m2 <- glm(injuries ~ mo, family = poisson, data = fire)
summary(m2)
```

The *predicted* value $\hat{y}$ is the estimated **log** of the response variable, 

$$\hat{y} = X \hat{\beta} = \ln (\lambda).$$

Suppose `mo` is January (mo = `), then the log of `injuries` is $\hat{y} = 0.323787$. Or, more intuitively, the expected count of injuries is $\exp(0.323787) = 1.38$  

```{r}
predict(m2, newdata = data.frame(mo=1))
predict(m2, newdata = data.frame(mo=1), type = "response")
```

Here is a plot of the predicted counts in red.

```{r}
augment(m2, type.predict = "response") %>%
  ggplot(aes(x = mo, y = injuries)) +
  geom_point() +
  geom_point(aes(y = .fitted), color = "red") + 
  scale_y_continuous(limits = c(0, NA)) +
  labs(x = "Month",
       y = "Injuries",
       title = "Poisson Fitted Line Plot")
```

Evaluate a logistic model fit with an analysis of deviance.  

```{r}
(perf <- glance(m2))
(pseudoR2 <- 1 - perf$deviance / perf$null.deviance)
```

The deviance of the null model (no regressors) is 139.9.  The deviance of the full model is 132.2.  The psuedo-R2 is very low at .05.  How about the RMSE?

```{r}
RMSE(pred = predict(m2, type = "response"), obs = fire$injuries)
```

The average prediction error is about 0.99.  That's almost as much as the variance of `injuries` - i.e., just predicting the mean of `injuries` would be almost as good!  Use the `GainCurvePlot()` function to plot the gain curve.

```{r}
augment(m2, type.predict = "response") %>%
  ggplot(aes(x = injuries, y = .fitted)) +
  geom_point() +
  geom_smooth(method ="lm") +
  labs(x = "Actual",
       y = "Predicted",
       title = "Poisson Fitted vs Actual")
```


```{r}
augment(m2) %>% data.frame() %>% 
  GainCurvePlot(xvar = ".fitted", truthVar = "injuries", title = "Poisson Model")
```

It seems that `mo` was a poor predictor of `injuries`.  


<!--chapter:end:07-glm.Rmd-->

# Multivariate Statistical Analysis

These notes are structured from the [PSU STAT 504](https://online.stat.psu.edu/stat505/) course.


## Background

## MANOVA

## Repeated Measures

## LDA

Linear Discriminant Analysis (LDA) is a supervised machine learning classification (binary or multimonial) and dimension reduction method.  LDA finds linear combinations of variables that best "discriminate" the response classes.

One approach (Welch) to LDA assumes the predictor variables are continuous random variables normally distributed and with equal variance.  You will typically scale the data to meet these conditions.

For a response variable of $k$ levels, LDA produces $k-1$ discriminants using Bayes Theorem.

$$Pr[Y = C_l | X] = \frac{P[Y = C_l] P[X | Y = C_l]}{\sum_{l=1}^C Pr[Y = C_l] Pr[X | Y = C_l]}$$

The probability that $Y$ equals class level $C_l$ given the predictors $X$ equals the *prior probability* of $Y$ multiplied by the probability of observing $X$ if $Y = C_l$ divided by the sum of all priors and probabilities of $X$ given the priors.  The predicted value for any $X$ is just the $C_l$ with the maximimum probability.

One way to calculate the probabilities is by assuming $X$ has a multivariate normal distribution with means $\mu_l$ and common variance $\Sigma$. Then the linear discriminant function group $l$ is

$$X'\Sigma^{-1}\mu_l - 0.5 \mu_l^{'}\Sigma^{-1}\mu_l + \log(Pr[Y = C_l])$$
The theoretical means and covariance matrix is estimated by the sample mean $\mu = \bar{x}_l$ and covariance $\Sigma = S$, and the population predictors $X$ are replaced with the sample predictors $u$.

Another approach (Fisher) to LDA is to find a linear combination of predictors that maximizes the between-group covariance matrix $B$ relative to the within-group covariance matrix $W$.

$$\frac{b'Bb}{b'Wb}$$

The solution to this optimization problem is the eigenvector corresponding to the largest eigenvalue of $W^{-1}B$.  This vector is a linear discrminant.  Solving for two-group setting gives the discriminant function $S^{-1}(\bar{x}_1 - \bar{x}_2)$ where $S^{-1}$ is the inverse of the covariance matrix of the data and $\bar{x}_1$ and $\bar{x}_2$ are the means of each predictor in response groups 1 and 2.  In practice, a new sample, $u$, is projected onto the discriminant function as $uS^{-1}(\bar{x}_1 - \bar{x}_2)$, which returns a discriminant score. A new sample is then classified into group 1 if the sample is closer to the group 1 mean than the group 2 mean in the projection:

$$\left| b'(u - \bar{x}_1) \right| - \left| b'(u - \bar{x}_2) \right| < 0$$

In general, the model requires $CP +  P(P + 1)/2$ parameters with $P$ predictors and $C$ classes. The value of the extra parameters in LDA models is that the between-predictor correlations are explicitly handled by the model. This should provide some advantage to LDA over logistic regression when there are substantial correlations, although both models will break down  when the multicollinearity becomes extreme. 

Fisher’s formulation is intuitive, easy to solve mathematically, and, unlike Welch’s approach, involves no assumptions about  the underlying distributions of the data.

In practice, it is best to center and scale predictors and remove near-zero-variance predictors.  If the matrix is still not invertible, use PLS or regularization.


## PCA

## Factor Analysis

## Canonical Correlation

## Cluster Analysis

This section is based on [PSU STAT 504](https://online.stat.psu.edu/stat505/) and [PSU STAT 508](https://online.stat.psu.edu/stat508/lesson/12).

<!--chapter:end:08-multivariate.Rmd-->

# Classification

<!--chapter:end:09-pls.Rmd-->

# Regularization

These notes are from this [tutorial](https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net) on DataCamp, the [Machine Learning Toolbox](https://campus.datacamp.com/courses/machine-learning-toolbox) DataCamp class, and Interpretable Machine Learning [@Molner2020].

Regularization is a set of methods that manage the bias-variance trade-off problem in linear regression. 

The linear regression model is $Y = X \beta + \epsilon$, where $\epsilon \sim N(0, \sigma^2)$. OLS estimates the coefficients by minimizing the loss function

$$L = \sum_{i = 1}^n \left(y_i - x_i^{'} \hat\beta \right)^2.$$

The resulting estimate for the coefficients is 

$$\hat{\beta} = \left(X'X\right)^{-1}\left(X'Y\right).$$

There are two important characteristics of any estimator: its *bias* and its *variance*.  For OLS, these are

$$Bias(\hat{\beta}) = E(\hat{\beta}) - \beta = 0$$
and

$$Var(\hat{\beta}) = \sigma^2(X'X)^{-1}$$

where the unknown population variance $\sigma^2$ is estimated from the residuals

$$\hat\sigma^2 = \frac{\epsilon' \epsilon}{n - k}.$$

The OLS estimator is unbiased, but can have a large variance when the predictor variables are highly correlated with each other, or when there are many predictors (notice how $\hat{\sigma}^2$ increases as $k \rightarrow n$).  Stepwise selection balances the trade-off by eliminating variables, but this throws away information.  *Regularization* keeps all the predictors, but reduces coefficient magnitudes to reduce variance at the expense of some bias.

In the sections below, I'll use the `mtcars` data set to predict `mpg` from the other variables using the `caret::glmnet()` function.  `glmnet()` uses penalized maximum likelihood to fit generalized linear models such as ridge, lasso, and elastic net.  I'll compare the model performances by creating a training and validation set, and a common `trainControl` object to make sure the models use the same observations in the cross-validation folds.

```{r warning=FALSE, message=FALSE}
library(tidyverse)
library(caret)

data("mtcars")

set.seed(123)
partition <- createDataPartition(mtcars$mpg, p = 0.8, list = FALSE)
training <- mtcars[partition, ]
testing <- mtcars[-partition, ]

train_control <- trainControl(
  method = "repeatedcv",
  number = 5,  
  repeats = 5,
  savePredictions = "final"  # saves predictions from optimal tuning parameters
)
```


## Ridge

Ridge regression estimates the linear model coefficients by minimizing an augmented loss function which includes a term, $\lambda$, that penalizes the magnitude of the coefficient estimates,

$$L = \sum_{i = 1}^n \left(y_i - x_i^{'} \hat\beta \right)^2 + \lambda \sum_{j=1}^k \hat{\beta}_j^2.$$

The resulting estimate for the coefficients is 

$$\hat{\beta} = \left(X'X + \lambda I\right)^{-1}\left(X'Y \right).$$

As $\lambda \rightarrow 0$, ridge regression approaches OLS.  The bias and variance for the ridge estimator are

$$Bias(\hat{\beta}) = -\lambda \left(X'X + \lambda I \right)^{-1} \beta$$
$$Var(\hat{\beta}) = \sigma^2 \left(X'X + \lambda I \right)^{-1}X'X \left(X'X + \lambda I \right)^{-1}$$

The estimator bias increases with $\lambda$ and the estimator variance decreases with $\lambda$.  The optimal level for $\lambda$ is the one that minimizes the root mean squared error (RMSE) or the Akaike or Bayesian Information Criterion (AIC or BIC), or R-squared.


#### Example {-}

Specify `alpha = 0` in a tuning grid for ridge regression (the following sections reveal how alpha distinguishes ridge, lasso, and elastic net). Note that I standardize the predictors in the `preProcess` step - ridge regression requires standardization.

```{r}
set.seed(1234)
mdl_ridge <- train(
  mpg ~ .,
  data = training,
  method = "glmnet",
  metric = "RMSE",  # Choose from RMSE, RSquared, AIC, BIC, ...others?
  preProcess = c("center", "scale"),
  tuneGrid = expand.grid(
    .alpha = 0,  # optimize a ridge regression
    .lambda = seq(0, 5, length.out = 101)
  ),
  trControl = train_control
  )
mdl_ridge
```

The model printout shows the RMSE, R-Squared, and mean absolute error (MAE) values at each lambda specified in the tuning grid.  The final three lines summarize what happened. It did not tune alpha because I held it at 0 for ridge regression; it optimized using RMSE; and the optimal tuning values (at the minimum RMSE) were alpha = 0 and lambda = 2.75.  You plot the model to see the tuning results.

```{r}
ggplot(mdl_ridge) +
  labs(title = "Ridge Regression Parameter Tuning", x = "lambda")
```

`varImp()` ranks the predictors by the absolute value of the coefficients in the tuned model. The most important variables here were `wt`, `disp`, and `am`.

```{r}
plot(varImp(mdl_ridge))
```

## Lasso

Lasso stands for “least absolute shrinkage and selection operator”.  Like ridge, lasso adds a penalty for coefficients, but instead of penalizing the sum of squared coefficients (L2 penalty), lasso penalizes the sum of absolute values (L1 penalty). As a result, for high values of $\lambda$, coefficients can be zeroed under lasso.

The loss function for lasso is

$$L = \sum_{i = 1}^n \left(y_i - x_i^{'} \hat\beta \right)^2 + \lambda \sum_{j=1}^k \left| \hat{\beta}_j \right|.$$

#### Example {-}

Continuing with prediction of `mpg` from the other variables in the `mtcars` data set, follow the same steps as before, but with ridge regression.  This time specify parameter `alpha = 1` for ridge regression (it was 0 for ridge, and for elastic net it will be something in between and require optimization).

```{r}
set.seed(1234)
mdl_lasso <- train(
  mpg ~ .,
  data = training,
  method = "glmnet",
  metric = "RMSE",
  preProcess = c("center", "scale"),
  tuneGrid = expand.grid(
    .alpha = 1,  # optimize a lasso regression
    .lambda = seq(0, 5, length.out = 101)
  ),
  trControl = train_control
  )
mdl_lasso$bestTune
```

The summary output shows the model did not tune alpha because I held it at 1 for lasso regression. The optimal tuning values (at the minimum RMSE) were alpha = 1 and lambda = 0.65.  You can see the RMSE minimum on the the plot.

```{r}
ggplot(mdl_ridge) +
  labs(title = "Lasso Regression Parameter Tuning", x = "lambda")
```

## Elastic Net

Elastic Net combines the penalties of ridge and lasso to get the best of both worlds. The loss function for elastic net is

$$L = \frac{\sum_{i = 1}^n \left(y_i - x_i^{'} \hat\beta \right)^2}{2n} + \lambda \frac{1 - \alpha}{2}\sum_{j=1}^k \hat{\beta}_j^2 + \lambda \alpha\left| \hat{\beta}_j \right|.$$

In this loss function, new parameter $\alpha$ is a "mixing" parameter that balances the two approaches.  If $\alpha$ is zero, you are back to ridge regression, and if $\alpha$ is one, you are back to lasso.


#### Example {-}

Continuing with prediction of `mpg` from the other variables in the `mtcars` data set, follow the same steps as before, but with elastic net regression there are two parameters to optimize: $\lambda$ and $\alpha$.

```{r}
set.seed(1234)
mdl_elnet <- train(
  mpg ~ .,
  data = training,
  method = "glmnet",
  metric = "RMSE",
  preProcess = c("center", "scale"),
  tuneGrid = expand.grid(
    .alpha = seq(0, 1, length.out = 10),  # optimize an elnet regression
    .lambda = seq(0, 5, length.out = 101)
  ),
  trControl = train_control
  )
mdl_elnet$bestTune
```

The optimal tuning values (at the mininum RMSE) were alpha = 0.0 and lambda = 2.75, so the mix is 100% ridge, 0% lasso.  You can see the RMSE minimum on the the plot.  Alpha is on the horizontal axis and the different lambdas are shown as separate series.  

```{r}
ggplot(mdl_elnet) +
  labs(title = "Elastic Net Regression Parameter Tuning", x = "lambda")
```


## Model Summary {-}

Make predictions on the validation data set for each of the three models.

```{r}
pr_ridge <- postResample(pred = predict(mdl_ridge, newdata = testing), obs = testing$mpg)
pr_lasso <- postResample(pred = predict(mdl_lasso, newdata = testing), obs = testing$mpg)
pr_elnet <- postResample(pred = predict(mdl_elnet, newdata = testing), obs = testing$mpg)
```

```{r}
rbind(pr_ridge, pr_lasso, pr_elnet)
```

It looks like ridge/elnet was the big winner today based on RMSE and MAE.  Lasso had the best Rsquared though.  On average, ridge/elnet will miss the true value of `mpg` by 3.75 mpg (RMSE) or 2.76 mpg (MAE).  The model explains about 90% of the variation in `mpg`. 

You can also compare the models by resampling.  

```{r}
model.resamples <- resamples(list(Ridge = mdl_ridge,
                                  Lasso = mdl_lasso,
                                  ELNet = mdl_elnet))
summary(model.resamples)
```

You want the smallest mean RMSE, and a small range of RMSEs.  Ridge/elnet had the smallest mean, and a relatively small range.  Boxplots are a common way to visualize this information.  

```{r}
bwplot(model.resamples, metric = "RMSE", main = "Model Comparison on Resamples")
```

Now that you have identified the optimal model, capture its tuning parameters and refit the model to the entire data set.

```{r}
set.seed(123)
mdl_final <- train(
  mpg ~ .,
  data = training,
  method = "glmnet",
  metric = "RMSE",
  preProcess = c("center", "scale"),
  tuneGrid = data.frame(
    .alpha = mdl_ridge$bestTune$alpha,  # optimized hyperparameters
    .lambda = mdl_ridge$bestTune$lambda),  # optimized hyperparameters
  trControl = train_control
  )
mdl_final
```

The model is ready to predict on new data! Here are some final conclusions on the models.

* Lasso can set some coefficients to zero, thus performing variable selection.
* Lasso and Ridge address multicollinearity differently: in ridge regression, the coefficients of correlated predictors are similar; In lasso, one of the correlated predictors has a larger coefficient, while the rest are (nearly) zeroed.
* Lasso tends to do well if there are a small number of significant parameters and the others are close to zero. Ridge tends to work well if there are many large parameters of about the same value.
* In practice, you don't know which will be best, so run cross-validation pick the best.



<!--chapter:end:10-glmnet.Rmd-->

# Decision Trees

Decision trees, also known as classification and regression tree (CART) models, are tree-based methods for supervised machine learning.  Simple *classification trees* and *regression trees* are easy to use and interpret, but are not competitive with the best machine learning methods. However, they form the foundation for **bagged trees**, **random forests**, and **boosted trees** ensemble models, which although less interpretable, are very accurate.

CART models segment the predictor space into $K$ non-overlapping terminal nodes (leaves), $A_1, A_2, \dots, A_K$.  Each node is described by a set of rules which can be used to predict new responses. The predicted value $\hat{y}$ for each node is the mode (classification) or mean (regression).

CART models define the nodes through a *top-down greedy* process called *recursive binary splitting*. The process is *top-down* because it begins at the top of the tree with all observations in a single region and successively splits the predictor space. It is *greedy* because at each splitting step, the best split is made at that particular step without consideration to subsequent splits.

The best split is the predictor variable and cutpoint that minimizes a cost function. For a regression tree, the most common cost function is the sum of squared residuals, 

$$RSS = \sum_{k=1}^K\sum_{i \in A_k}{\left(y_i - \hat{y}_{A_k} \right)^2}.$$

For a classification tree, the most common cost functions are the Gini index, 

$$G = \sum_{c=1}^C{\hat{p}_{kc}(1 - \hat{p}_{kc})},$$

and the entropy (aka information statistic)

$$D = - \sum_{c=1}^C{\hat{p}_{kc} \log \hat{p}_{kc}}$$

where $\hat{p}_{kc}$ is the proportion of training observations in node $k$ that are class $c$.  For a completely *pure* node in a binary tree, $\hat{p} \in \{ 0, 1 \}$ and $G = D = 0$. For a completely *impure* node in a binary tree, $\hat{p} = 0.5$ and $G = 0.5^2 \cdot 2 = 0.25$ and $D = -(0.5 \log(0.5)) \cdot 2 = 0.69$.

CART repeats the splitting process for each of the child nodes until a *stopping criterion* is satisfied, usually when no node size surpasses a predefined maximum, or continued splitting does not improve the model significantly.  CART may also impose a minimum number of observations in each node.

The resulting tree likely over-fits the training data and therefore does not generalize well to test data, so CART *prunes* the tree, minimizing the cross-validated prediction error. Rather than cross-validating every possible subtree to find the one with minimum error, CART uses *cost-complexity pruning*. Cost-complexity is the tradeoff between error (cost) and tree size (complexity) where the tradeoff is quantified with cost-complexity parameter $c_p$.  In the equation below, the cost complexity of the tree $R_{c_p}(T)$ is the sum of its risk (error) plus a "cost complexity" factor $c_p$ multiple of the tree size $|T|$.  

$$R_{c_p}(T) = R(T) + c_p|T|$$

$c_p$ can take on any value from $[0..\infty]$, but it turns out there is an optimal tree for *ranges* of $c_p$ values, so there are only a finite set of *interesting* values for $c_p$ [@James2013] [@Therneau2019] [@Kuhn2016].  A parametric algorithm identifies the interesting $c_p$ values and their associated pruned trees, $T_{c_p}$. CART uses cross-validation to determine which $c_p$ is optimal.


## Classification Tree

A simple classification tree is rarely performed on its own; the bagged, random forest, and gradient boosting methods build on this logic. However, it is good to start here to build understanding. I'll learn by example. Using the `ISLR::OJ` data set, I will use 17 feature variables to predict which brand of orange juice, Citrus Hill (CH) or Minute Maid = (MM), customers `Purchase`.

```{r warning=FALSE, message=FALSE}
library(tidyverse)
library(caret)
library(rpart)  # classification and regression trees 
library(rpart.plot)  # better formatted plots than the ones in rpart
library(plotROC)  # ROC curves
library(ROCR)

oj_dat <- ISLR::OJ
```

I'll split `oj_dat` (n = 1,070) into `oj_train` (80%, n = 857) to fit various models, and `oj_test` (20%, n = 213) to compare their performance on new data.

```{r}
set.seed(12345)
partition <- createDataPartition(y = oj_dat$Purchase, p = 0.8, list = FALSE)
oj_train <- oj_dat[partition, ]
oj_test <- oj_dat[-partition, ]
```

Function `rpart::rpart()` builds a full tree, minimizing the Gini index $G$ by default (`parms = list(split = "gini")`), until the stopping criterion is satisfied.  The default stopping criterion is 

* only attempt a split if the current node has at least `minsplit = 20` observations,
* only accept a split if 
   * the resulting nodes have at least `minbucket = round(minsplit/3)` observations, and 
   * the resulting overall fit improves by `cp = 0.01` (i.e., $\Delta G <= 0.01$).

```{r}
# Use method = "class" for classification, method = "anova" for regression
set.seed(123)
oj_mdl_cart_full <- rpart(formula = Purchase ~ ., data = oj_train, method = "class")
print(oj_mdl_cart_full)
```

The output starts with the root node.  The predicted class at the root is `CH` and this prediction produces 334 errors on the 857 observations for a success rate of 0.61026838 and an error rate of 0.38973162.  The child nodes of node "x" are labeled 2x) and 2x+1), so the child nodes of 1) are 2) and 3), and the child nodes of 2) are 4) and 5).  Terminal nodes are labeled with an asterisk (*).  

Surprisingly, only 3 of the 17 features were used the in full tree: `LoyalCH` (Customer brand loyalty for CH), `PriceDiff` (relative price of MM over CH), and `SalePriceMM` (absolute price of MM).  The first split is at `LoyalCH` = 0.48285.  Here is a diagram of the full (unpruned) tree.

```{r}
rpart.plot(oj_mdl_cart_full, yesno = TRUE)
```

The boxes show the node classification (based on mode), the proportion of observations that are *not* `CH`, and the proportion of observations included in the node. 

`rpart()` not only grew the full tree, it identified the set of cost complexity parameters, and measured the model performance of each corresponding tree using cross-validation. `printcp()` displays the candidate $c_p$ values. You can use this table to decide how to prune the tree.

```{r}
printcp(oj_mdl_cart_full)
```

There are 4 $c_p$ values in this model.  The model with the smallest complexity parameter allows the most splits (`nsplit`).  The highest complexity parameter corresponds to a tree with just a root node.  `rel error` is the error rate relative to the root node.  The root node absolute error is 0.38973162 (the proportion of MM), so its `rel error` is 0.38973162/0.38973162 = 1.0.  That means the absolute error of the full tree (at CP = 0.01) is 0.42814 * 0.38973162 = 0.1669.  You can verify that by calculating the error rate of the predicted values:

```{r}
data.frame(pred = predict(oj_mdl_cart_full, newdata = oj_train, type = "class")) %>%
   mutate(obs = oj_train$Purchase,
          err = if_else(pred != obs, 1, 0)) %>%
   summarize(mean_err = mean(err))
```

Finishing the CP table tour, `xerror` is the relative cross-validated error rate and `xstd` is its standard error. If you want the lowest possible error, then prune to the tree with the smallest relative CV error, $c_p$ = 0.01.  If you want to balance predictive power with simplicity, prune to the smallest tree within 1 SE of the one with the smallest relative error.  The CP table is not super-helpful for finding that tree, so add a column to find it.

```{r}
oj_mdl_cart_full$cptable %>%
   data.frame() %>%
   mutate(
      min_idx = which.min(oj_mdl_cart_full$cptable[, "xerror"]),
      rownum = row_number(),
      xerror_cap = oj_mdl_cart_full$cptable[min_idx, "xerror"] + 
                   oj_mdl_cart_full$cptable[min_idx, "xstd"],
      eval = case_when(rownum == min_idx ~ "min xerror",
                       xerror < xerror_cap ~ "under cap",
                       TRUE ~ "")
   ) %>%
   select(-rownum, -min_idx) 
```

The simplest tree using the 1-SE rule is $c_p = 0.01347305, CV error = `r round(0.4700599*0.38973162, 4)`).  Fortunately, `plotcp()` presents a nice graphical representation of the relationship between `xerror` and `cp`.

```{r}
plotcp(oj_mdl_cart_full, upper = "splits")
```

The dashed line is set at the minimum `xerror` + `xstd`.  The top axis shows the number of splits in the tree.  I'm not sure why the CP values are not the same as in the table (they are close, but not the same).  The figure suggests I should prune to 5 or 3 splits.  I see this curve never really hits a minimum - it is still decreasing at 5 splits.  The default tuning parameter value `cp = 0.01` may be too large, so I'll set it to `cp = 0.001` and start over.

```{r}
set.seed(123)
oj_mdl_cart_full <- rpart(
   formula = Purchase ~ .,
   data = oj_train,
   method = "class",
   cp = 0.001
   )
print(oj_mdl_cart_full)
```

This is a much larger tree.  Did I find a `cp` value that produces a local min?

```{r}
plotcp(oj_mdl_cart_full, upper = "splits")
```

Yes, the min is at CP = 0.011 with 5 splits.  The min + 1 SE is at CP = 0.021 with 3 splits.  I'll prune the tree to 3 splits.

```{r}
oj_mdl_cart <- prune(
   oj_mdl_cart_full,
   cp = oj_mdl_cart_full$cptable[oj_mdl_cart_full$cptable[, 2] == 3, "CP"]
)
rpart.plot(oj_mdl_cart, yesno = TRUE)
```

The most "important" indicator of `Purchase` appears to be `LoyalCH`.  From the **rpart** [vignette](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf) (page 12), 

> "An overall measure of variable importance is the sum of the goodness of split measures for each split for which it was the primary variable, plus goodness (adjusted agreement) for all splits in which it was a surrogate."

Surrogates refer to alternative features for a node to handle missing data. For each split, CART evaluates a variety of alternative "surrogate" splits to use when the feature value for the primary split is NA. Surrogate splits are splits that produce results similar to the original split. 

A variable's importance is the sum of the improvement in the overall Gini (or RMSE) measure produced by the nodes in which it appears. Here is the variable importance for this model.

```{r}
oj_mdl_cart$variable.importance
```

```{r}
oj_mdl_cart$variable.importance %>% 
   data.frame() %>%
   rownames_to_column(var = "Feature") %>%
   rename(Overall = '.') %>%
   ggplot(aes(x = fct_reorder(Feature, Overall), y = Overall)) +
   geom_pointrange(aes(ymin = 0, ymax = Overall), color = "cadetblue", size = .3) +
   theme_minimal() +
   coord_flip() +
   labs(x = "", y = "", title = "Variable Importance with Simple Classication")
```

`LoyalCH` is by far the most important variable, as expected from its position at the top of the tree, and one level down. 

You can see how the surrogates appear in the model with the `summary()` function.

```{r}
summary(oj_mdl_cart)
```

I'll evaluate the predictions and record the accuracy (correct classification percentage) for comparison to other models. Two ways to evaluate the model are the confusion matrix, and the ROC curve. 


### Confusion Matrix

Print the confusion matrix with `caret::confusionMatrix()` to see how well does this model performs against the test data set.

```{r}
oj_preds_cart <- predict(oj_mdl_cart, oj_test, type = "class")
oj_cm_cart <- confusionMatrix(oj_preds_cart, reference = oj_test$Purchase)
oj_cm_cart
```

The confusion matrix is at the top.  It also includes a lot of statistics.  It's worth getting familiar with the stats.  The model accuracy and 95% CI are calculated from the binomial test.

```{r}
binom.test(x = 113 + 70, n = 213)
```

The "No Information Rate" (NIR) statistic is the class rate for the largest class.  In this case CH is the largest class, so NIR = 130/213 = 0.6103.  "P-Value [Acc > NIR]" is the binomial test that the model accuracy is significantly better than the NIR (i.e., significantly better than just always guessing CH).

```{r}
binom.test(x = 113 + 70, n = 213, p = 130/213, alternative = "greater")
```

The "Accuracy" statistic indicates the model predicts 0.8590 of the observations correctly.  That's good, but less impressive when you consider the prevalence of CH is 0.6103 - you could achieve 61% accuracy just by predicting CH every time. A measure that controls for the prevalence is Cohen's kappa statistic. The kappa statistic is explained [here](https://standardwisdom.com/softwarejournal/2011/12/confusion-matrix-another-single-value-metric-kappa-statistic/). It compares the accuracy to the accuracy of a "random system".  It is defined as

$$\kappa = \frac{Acc - RA}{1-RA}$$

where 

$$RA = \frac{ActFalse \times PredFalse + ActTrue \times PredTrue}{Total \times Total}$$

is the hypotheical probability of a chance agreement.  ActFalse will be the number of "MM" (13 + 70 = 83) and actual true will be the number of "CH" (113 + 17 = 130).  The predicted counts are

```{r}
table(oj_preds_cart)
```

So, $RA = (83*87 + 130*126) / 213^2 = 0.5202$ and $\kappa = (0.8592 - 0.5202)/(1 - 0.5202) = 0.7064$.  The kappa statistic varies from 0 to 1 where 0 means accurate predictions occur merely by chance, and 1 means the predictions are in perfect agreement with the observations.  In this case, a kappa statistic of 0.7064 is "substantial".  See chart [here](https://www.statisticshowto.datasciencecentral.com/cohens-kappa-statistic/).

The other measures from the `confusionMatrix()` output are various proportions and you can remind yourself of their definitions in the documentation with `?confusionMatrix`.

Visuals are almost always helpful.  Here is a plot of the confusion matrix.

```{r}
plot(oj_test$Purchase, oj_preds_cart, 
     main = "Simple Classification: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")
```


### ROC Curve

The ROC (receiver operating characteristics) curve [@Fawcett2005] is another measure of accuracy.  The ROC curve is a plot of the true positive rate (TPR, sensitivity) versus the false positive rate (FPR, 1 - specificity) for a set of thresholds. By default, the threshold for predicting the default classification is 0.50, but it could be any threshold. `precrec::evalmod()` calculates the confusion matrix values from the model using the holdout data set.  The AUC on the holdout set is 0.8848.  `pRoc::plot.roc()`, `plotROC::geom_roc()`, and `yardstick::roc_curve()` are all options for plotting a ROC curve.

```{r}
oj_probs_cart <- predict(oj_mdl_cart, oj_test, "prob")

precrec::evalmod(scores = oj_probs_cart[, "CH"], labels = oj_test$Purchase, posclass = "CH")

data.frame(M = oj_probs_cart[, "CH"],
           D = if_else(oj_test$Purchase == "CH", 1, 0)) %>%
   ggplot() + 
   geom_roc(aes(m = M, d = D), 
            hjust = -0.4, vjust = 1.5, linealpha = 0.6, labelsize = 3, 
            n.cuts = 10) + 
   geom_abline(intercept = 0, slope = 1, linetype = 2) +
   coord_equal() +
   theme_minimal() +
   labs(x = "FPR (1 - specificity)", y = "TPR (sensitivity)", title = "OJ CART ROC Curve")
```

A few points on the ROC space are helpful for understanding how to use it.  

* The lower left point (0, 0) is the result of *always* predicting "negative" or in this case "MM" if "CH" is taken as the default class. No false positives, but no true positives either.
* The upper right point (1, 1) is the result of *always* predicting "positive" ("CH" here).  You catch all true positives, but miss all the true negatives.
* The upper left point (0, 1) is the result of perfect accuracy.
* The lower right point (1, 0) is the result of perfect imbecility.  You made the exact wrong prediction every time. 
* The 45 degree diagonal is the result of randomly guessing positive (CH) X percent of the time.  If you guess positive 90% of the time and the prevalence is 50%, your TPR will be 90% and your FPR will also be 90%, etc.

The goal is for all nodes to bunch up in the upper left.

Points to the left of the diagonal with a low TPR can be thought of as "conservative" predictors - they only make positive (CH) predictions with strong evidence.  Points to the left of the diagonal with a high TPR can be thought of as "liberal" predictors - they make positive (CH) predictions with weak evidence.  

### Caret Approach

I can also fit the model with `caret::train()`.  There are two ways to tune hyperparameters in `train()`: 

* set the number of tuning parameter values to consider by setting `tuneLength`, or
* set particular values to consider for each parameter by defining a `tuneGrid`.

I'll build the model using 10-fold cross-validation to optimize the hyperparameter CP. If you don't have any idea what the tuning parameter ought to look like, use `tuneLength` to get close, then fine-tune with `tuneGrid`.  That's what I'll do.  I'll create a training control object that I can re-use in other model builds.  

```{r}
oj_trControl = trainControl(
   method = "cv",  # k-fold cross validation
   number = 10,  # 10 folds
   savePredictions = "final",       # save predictions for the optimal tuning parameter
   classProbs = TRUE  # return class probabilities in addition to predicted values
#   summaryFunction = twoClassSummary  # computes sensitivity, specificity and the area under the ROC curve.
   )
```

Now fit the model.

```{r}
set.seed(1234)
oj_model_2 = train(
   Purchase ~ ., 
   data = oj_train, 
   method = "rpart",
   tuneLength = 5,
   metric = "Accuracy",
   trControl = oj_trControl
   )
```

`caret` built a full tree using `rpart`'s default parameters: gini splitting index, at least 20 observations in a node in order to consider splitting it, and at least 6 observations in each node.  Caret then calculated the accuracy for each candidate value of $\alpha$.  Here is the results.

```{r}
print(oj_model_2)
```

The second `cp` (0.008982036) produced the highest accuracy.  I can drill into the best value of `cp` using a tuning grid.  I'll try that now.

```{r}
set.seed(1234)
oj_model_3 = train(
   Purchase ~ ., 
   data = oj_train, 
   method = "rpart",
   tuneGrid = expand.grid(cp = seq(from = 0.001, to = 0.010, length = 11)),  
   metric='Accuracy',
   trControl = oj_trControl
   )
print(oj_model_3)
```

The beset model is at cp = 0.009.  Here are the rules in the final model.  

```{r}
oj_model_3$finalModel
```

Here is the tree.

```{r}
rpart.plot(oj_model_3$finalModel)
```

Here is the ROC curve.

```{r}
library(plotROC)
ggplot(oj_model_3$pred) + 
    geom_roc(
       aes(
          m = MM, 
          d = factor(obs, levels = c("CH", "MM"))
       ),
       hjust = -0.4, vjust = 1.5
    ) +
   coord_equal()
```

Here are the cross-validated Accuracy for each candidate cp value.

```{r}
plot(oj_model_3)
```
 
Evaluate the model by making predictions with the test data set.  

```{r}
oj_model_3_preds <- predict(oj_model_3, oj_test, type = "raw")
```

The confusion matrix shows the true positives and true negatives.

```{r}
oj_model_3_cm <- confusionMatrix(
   data = oj_model_3_preds, 
   reference = oj_test$Purchase
)
oj_model_3_cm
```

The accuracy metric is the slightly worse than in my previous model.  Here is a graphical representation of the confusion matrix.

```{r}
plot(oj_test$Purchase, oj_model_3_preds, 
     main = "Simple Classification: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")
```

Finally, here is the variable importance plot.

```{r}
plot(varImp(oj_model_3), main="Variable Importance with Simple Classication")
```

Looks like the manual effort faired best.  Here is a summary the accuracy rates of the three models.

```{r}
rbind(data.frame(model = "Manual Class", Acc = round(oj_cm_cart$overall["Accuracy"], 5)), 
      data.frame(model = "Caret w/tuneGrid", Acc = round(oj_model_3_cm$overall["Accuracy"], 5))
)
```


## Regression Trees

A simple regression tree is built in a manner similar to a simple classificatioon tree, and like the simple classification tree, it is rarely invoked on its own; the bagged, random forest, and gradient boosting methods build on this logic.  I'll learn by example again. Using the `ISLR::Carseats` data set, I will predict `Sales` using from the 10 feature variables.  Load the data.

```{r}
carseats_dat <- ISLR::Carseats
#skim_with(numeric = list(p0 = NULL, p25 = NULL, p50 = NULL, p75 = NULL, 
#                                p100 = NULL, hist = NULL))
#skim(carseats_dat)
```

I'll split `careseats_dat` (n = 400) into `carseats_train` (80%, n = 321) and `carseats_test` (20%, n = 79).  I'll fit a simple decision tree with `carseats_train`, then later a bagged tree, a random forest, and a gradient boosting tree.  I'll compare their predictive performance with `carseats_test`.

```{r}
set.seed(12345)
partition <- createDataPartition(y = carseats_dat$Sales, p = 0.8, list = FALSE)
carseats_train <- carseats_dat[partition, ]
carseats_test <- carseats_dat[-partition, ]
```

The first step is to build a full tree, then perform k-fold cross-validation to help select the optimal cost complexity (cp).  The only difference here is the `rpart()` parameter `method = "anova"` to produce a regression tree.

```{r}
set.seed(1234)
carseats_model_1 <- rpart(
   formula = Sales ~ .,
   data = carseats_train,
   method = "anova", 
   xval = 10,
   model = TRUE  # to plot splits with factor variables.
)
print(carseats_model_1)
```

The output starts with the root node.  The predicted `Sales` at the root is the mean `Sales` for the training data set, 7.535950 (values are $000s).  The deviance at the root is the SSE, 2567.768.  The child nodes of node "x" are labeled 2x) and 2x+1), so the child nodes of 1) are 2) and 3), and the child nodes of 2) are 4) and 5).  Terminal nodes are labeled with an asterisk (*).

The first split is at `ShelveLoc` = [Bad, Medium] vs Good.  Here is what the full (unpruned) tree looks like.

```{r}
rpart.plot(carseats_model_1, yesno = TRUE)
```

The boxes show the node predicted value (mean) and the proportion of observations that are in the node (or child nodes). 

`rpart()` not only grew the full tree, it also used cross-validation to test the performance of the possible complexity hyperparameters. `printcp()` displays the candidate cp values. You can use this table to decide how to prune the tree.

```{r}
printcp(carseats_model_1)
```

There are 16 possible cp values in this model.  The model with the smallest complexity parameter allows the most splits (`nsplit`).  The highest complexity parameter corresponds to a tree with just a root node.  `rel error` is the SSE relative to the root node.  The root node SSE is 2567.76800, so its `rel error` is 2567.76800/2567.76800 = 1.0.  That means the absolute error of the full tree (at CP = 0.01) is 0.30963 * 2567.76800 = 795.058. You can verify that by calculating the SSE of the model predicted values:

```{r}
data.frame(pred = predict(carseats_model_1, newdata = carseats_train)) %>%
   mutate(obs = carseats_train$Sales,
          sq_err = (obs - pred)^2) %>%
   summarize(sse = sum(sq_err))
```

Finishing the CP table tour, `xerror` is the cross-validated SSE and `xstd` is its standard error. If you want the lowest possible error, then prune to the tree with the smallest relative SSE (`xerror`).  If you want to balance predictive power with simplicity, prune to the smallest tree within 1 SE of the one with the smallest relative SSE.  The CP table is not super-helpful for finding that tree. I'll add a column to find it.

```{r}
carseats_model_1$cptable %>%
   data.frame() %>%
   mutate(min_xerror_idx = which.min(carseats_model_1$cptable[, "xerror"]),
          rownum = row_number(),
          xerror_cap = carseats_model_1$cptable[min_xerror_idx, "xerror"] + 
             carseats_model_1$cptable[min_xerror_idx, "xstd"],
          eval = case_when(rownum == min_xerror_idx ~ "min xerror",
                           xerror < xerror_cap ~ "under cap",
                           TRUE ~ "")) %>%
   select(-rownum, -min_xerror_idx) 
```

Okay, so the simplest tree is the one with CP = 0.01544139 (8 splits).  Fortunately, `plotcp()` presents a nice graphical representation of the relationship between `xerror` and `cp`.

```{r}
plotcp(carseats_model_1, upper = "splits")
```

The dashed line is set at the minimum `xerror` + `xstd`.  The top axis shows the number of splits in the tree.  I'm not sure why the CP values are not the same as in the table (they are close, but not the same).  The smallest relative error is at `r carseats_model_1$cptable[which.min(carseats_model_1$cptable[, "xerror"]), "CP"]`, but the maximum CP below the dashed line (one standard deviation above the mimimum error) is at CP = .019 (8 splits). Use the `prune()` function to prune the tree by specifying the associated cost-complexity `cp`.  

```{r}
carseats_model_1_pruned <- prune(
   carseats_model_1,
   cp = carseats_model_1$cptable[carseats_model_1$cptable[, 2] == 8, "CP"]
)
rpart.plot(carseats_model_1_pruned, yesno = TRUE)
```

The most "important" indicator of `Sales` is `ShelveLoc`.  Here are the importance values from the model. 

```{r}
carseats_model_1_pruned$variable.importance %>% 
   data.frame() %>%
   rownames_to_column(var = "Feature") %>%
   rename(Overall = '.') %>%
   ggplot(aes(x = fct_reorder(Feature, Overall), y = Overall)) +
   geom_pointrange(aes(ymin = 0, ymax = Overall), color = "cadetblue", size = .3) +
   theme_minimal() +
   coord_flip() +
   labs(x = "", y = "", title = "Variable Importance with Simple Regression")
```

The most important indicator of `Sales` is `ShelveLoc`, then `Price`, then `Age`, all of which appear in the final model.  `CompPrice` was also important.

The last step is to make predictions on the validation data set. The root mean squared error ($RMSE = \sqrt{(1/2) \sum{(actual - pred)^2}})$ and mean absolute error ($MAE = (1/n) \sum{|actual - pred|}$) are the two most common measures of predictive accuracy. The key difference is that RMSE punishes large errors more harshly. For a regression tree, set argument `type = "vector"` (or do not specify at all). 

```{r message=FALSE, warning=FALSE}
carseats_model_1_preds <- predict(
   carseats_model_1_pruned, 
   carseats_test, 
   type = "vector"
)

carseats_model_1_pruned_rmse <- RMSE(
   pred = carseats_model_1_preds,
   obs = carseats_test$Sales
)
carseats_model_1_pruned_rmse
```

The pruning process leads to an average prediction error of `r round(carseats_model_1_pruned_rmse, 3)` in the test data set.  Not too bad considering the standard deviation of `Sales` is `r round(sd(carseats_test$Sales), 3)`. Here is a predicted vs actual plot. 

```{r message=FALSE, warning=FALSE}
plot(carseats_test$Sales, carseats_model_1_preds, 
     main = "Simple Regression: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")
abline(0, 1)
```

The 6 possible predicted values do a decent job of binning the observations.

### Caret Approach

I can also fit the model with `caret::train()`, specifying `method = "rpart"`.

I'll build the model using 10-fold cross-validation to optimize the hyperparameter CP. 

```{r}
carseats_trControl = trainControl(
   method = "cv",  # k-fold cross validation
   number = 10,  # 10 folds
   savePredictions = "final"       # save predictions for the optimal tuning parameter
)
```


I'll let the model look for the best CP tuning parameter with `tuneLength` to get close, then fine-tune with `tuneGrid`.  

```{r}
set.seed(1234)
carseats_model_2 = train(
   Sales ~ ., 
   data = carseats_train, 
   method = "rpart",  # for classification tree
   tuneLength = 5,  # choose up to 5 combinations of tuning parameters (cp)
   metric = "RMSE",  # evaluate hyperparamter combinations with RMSE
   trControl = carseats_trControl
)
print(carseats_model_2)
```

The first `cp` (0.04167149) produced the smallest RMSE.  I can drill into the best value of `cp` using a tuning grid.  I'll try that now.

```{r}
myGrid <-  expand.grid(cp = seq(from = 0, to = 0.1, by = 0.01))
carseats_model_3 = train(
   Sales ~ ., 
   data = carseats_train, 
   method = "rpart",  # for classification tree
   tuneGrid = myGrid,  # choose up to 5 combinations of tuning parameters (cp)
   metric = "RMSE",  # evaluate hyperparamter combinations with RMSE
   trControl = carseats_trControl
)
print(carseats_model_3)
```

It looks like the best performing tree is the unpruned one.

```{r}
plot(carseats_model_3)
```

Lets's see the final model.

```{r}
rpart.plot(carseats_model_3$finalModel)
```

What were the most important variables?
```{r}
plot(varImp(carseats_model_3), main="Variable Importance with Simple Regression")
```

Evaluate the model by making predictions with the test data set.  

```{r}
carseats_model_3_preds <- predict(carseats_model_3, carseats_test, type = "raw")
data.frame(Actual = carseats_test$Sales, Predicted = carseats_model_3_preds) %>%
ggplot(aes(x = Actual, y = Predicted)) +
   geom_point() +
   geom_smooth() +
   geom_abline(slope = 1, intercept = 0) + 
   scale_y_continuous(limits = c(0, 15)) +
   labs(title = "Simple Regression: Predicted vs. Actual")
```

Looks like the model over-estimates at the low end and undestimates at the high end.  Calculate the test data set RMSE.

```{r}
carseats_model_3_pruned_rmse <- RMSE(
   pred = carseats_model_3_preds,
   obs = carseats_test$Sales
)
carseats_model_3_pruned_rmse
```


Caret faired better in this model.  Here is a summary the RMSE values of the two models.

```{r}
rbind(data.frame(model = "Manual ANOVA", 
                 RMSE = round(carseats_model_1_pruned_rmse, 5)), 
      data.frame(model = "Caret", 
                 RMSE = round(carseats_model_3_pruned_rmse, 5))
)
```


## Bagging

Bootstrap aggregation, or *bagging*, is a general-purpose procedure for reducing the variance of a statistical learning method.  The algorithm constructs *B* regression trees using *B* bootstrapped training sets, and averages the resulting predictions. These trees are grown deep, and are not pruned. Hence each individual tree has high variance, but low bias. Averaging these *B* trees reduces the variance.  For classification trees, bagging takes the "majority vote" for the prediction.  Use a value of *B* sufficiently large that the error has settled down.

To test the model accuracy, the out-of-bag observations are predicted from the models that do not use them.  If B/3 of observations are in-bag, there are *B/3* predictions per observation.  These predictions are averaged for the test prediction.  Again, for classification trees, a majority vote is taken.

The downside to bagging is that it improves accuracy at the expense of interpretability.  There is no longer a single tree to interpret, so it is no longer clear which variables are more important than others.  

Bagged trees are a special case of random forests, so see the next section for an example.


## Random Forests

Random forests improve bagged trees by way of a small tweak that de-correlates the trees.  As in bagging, the algorithm builds a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of *mtry* predictors is chosen as split candidates from the full set of *p* predictors.  A fresh sample of *mtry* predictors is taken at each split.  Typically $mtry \sim \sqrt{b}$.  Bagged trees are thus a special case of random forests where *mtry = p*.


#### Bagging Classification Example

Again using the `OJ` data set to predict `Purchase`, this time I'll use the bagging method by specifying `method = "treebag"`.  I'll use `tuneLength = 5` and not worry about `tuneGrid` anymore.  Caret has no hyperparameters to tune with this model. 

```{r}
oj.bag = train(Purchase ~ ., 
               data = oj_train, 
               method = "treebag",  # for bagging
               tuneLength = 5,  # choose up to 5 combinations of tuning parameters
               metric = "ROC",  # evaluate hyperparamter combinations with ROC
               trControl = trainControl(
                 method = "cv",  # k-fold cross validation
                 number = 10,  # k=10 folds
                 savePredictions = "final",       # save predictions for the optimal tuning parameters
                      classProbs = TRUE,  # return class probabilities in addition to predicted values
                      summaryFunction = twoClassSummary  # for binary response variable
                      )
                    )
oj.bag
#plot(oj.bag$)
oj.pred <- predict(oj.bag, oj_test, type = "raw")
plot(oj_test$Purchase, oj.pred, 
     main = "Bagging Classification: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")

(oj.conf <- confusionMatrix(data = oj.pred, 
                            reference = oj_test$Purchase))
oj.bag.acc <- as.numeric(oj.conf$overall[1])
rm(oj.pred)
rm(oj.conf)
#plot(oj.bag$, oj.bag$finalModel$y)
plot(varImp(oj.bag), main="Variable Importance with Simple Classication")
```

#### Random Forest Classification Example

Now I'll try it with the random forest method by specifying `method = "ranger"`.  I'll stick with `tuneLength = 5`.  Caret tunes three hyperparameters: 

* `mtry`: number of randomly selected predictors.  Default is sqrt(p).
* `splitrule`: splitting rule.  For classification, options are "gini" (default) and "extratrees".
* `min.node.size`: minimal node size.  Default is 1 for classification.

```{r}
oj.frst = train(Purchase ~ ., 
               data = oj_train, 
               method = "ranger",  # for random forest
               tuneLength = 5,  # choose up to 5 combinations of tuning parameters
               metric = "ROC",  # evaluate hyperparamter combinations with ROC
               trControl = trainControl(
                 method = "cv",  # k-fold cross validation
                 number = 10,  # 10 folds
                 savePredictions = "final",       # save predictions for the optimal tuning parameter1
                 classProbs = TRUE,  # return class probabilities in addition to predicted values
                 summaryFunction = twoClassSummary  # for binary response variable
                 )
               )
oj.frst
plot(oj.frst)
oj.pred <- predict(oj.frst, oj_test, type = "raw")
plot(oj_test$Purchase, oj.pred, 
     main = "Random Forest Classification: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")

(oj.conf <- confusionMatrix(data = oj.pred, 
                            reference = oj_test$Purchase))
oj.frst.acc <- as.numeric(oj.conf$overall[1])
rm(oj.pred)
rm(oj.conf)
#plot(oj.bag$, oj.bag$finalModel$y)
#plot(varImp(oj.frst), main="Variable Importance with Simple Classication")
```

The model algorithm explains *"ROC was used to select the optimal model using the largest value. The final values used for the model were mtry = 9, splitrule = extratrees and min.node.size = 1."*  You can see the results of tuning grid combinations in the associated plot of ROC AUC vs mtry grouped by splitting rule.

The bagging (accuracy = 0.80751) and random forest (accuracy = 0.81690) models faired pretty well, but the manual classification tree is still in first place.  There's still gradient boosting to investigate!

```{r}
rbind(data.frame(model = "Manual Class", Accuracy = round(oj_cm_cart$overall["Accuracy"], 5)), 
      data.frame(model = "Caret w.tuneGrid", Accuracy = round(oj_model_3_cm$overall["Accuracy"], 5)),
      data.frame(model = "Bagging", Accuracy = round(oj.bag.acc, 5)),
      data.frame(model = "Random Forest", Accuracy = round(oj.frst.acc, 5))
) %>% arrange(desc(Accuracy))
```

#### Bagging Regression Example

Again using the `Carseats` data set to predict `Sales`, this time I'll use the bagging method by specifying `method = "treebag"`.  I'll use `tuneLength = 5` and not worry about `tuneGrid` anymore.  Caret has no hyperparameters to tune with this model. 

```{r}
carseats.bag = train(Sales ~ ., 
               data = carseats_train, 
               method = "treebag",  # for bagging
               tuneLength = 5,  # choose up to 5 combinations of tuning parameters
               metric = "RMSE",  # evaluate hyperparamter combinations with RMSE
               trControl = trainControl(
                 method = "cv",  # k-fold cross validation
                 number = 10,  # 10 folds
                 savePredictions = "final"       # save predictions for the optimal tuning parameter1
                 )
               )
carseats.bag
#plot(carseats.bag$finalModel)
carseats.pred <- predict(carseats.bag, carseats_test, type = "raw")
plot(carseats_test$Sales, carseats.pred, 
     main = "Bagging Regression: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")
abline(0, 1)

(carseats.bag.rmse <- RMSE(pred = carseats.pred,
                           obs = carseats_test$Sales))
rm(carseats.pred)
plot(varImp(carseats.bag), main="Variable Importance with Regression Bagging")
```


#### Random Forest Regression Example

Now I'll try it with the random forest method by specifying `method = "ranger"`.  I'll stick with `tuneLength = 5`.  Caret tunes three hyperparameters: 

* `mtry`: number of randomly selected predictors
* `splitrule`: splitting rule.  For regression, options are "variance" (default), "extratrees", and "maxstat". 
* `min.node.size`: minimal node size

```{r}
carseats.frst = train(Sales ~ ., 
               data = carseats_train, 
               method = "ranger",  # for random forest
               tuneLength = 5,  # choose up to 5 combinations of tuning parameters
               metric = "RMSE",  # evaluate hyperparamter combinations with RMSE
               trControl = trainControl(
                 method = "cv",  # k-fold cross validation
                 number = 10,  # 10 folds
                 savePredictions = "final"       # save predictions for the optimal tuning parameter1
                 )
               )
carseats.frst
plot(carseats.frst)
carseats.pred <- predict(carseats.frst, carseats_test, type = "raw")
plot(carseats_test$Sales, carseats.pred, 
     main = "Random Forest Regression: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")
abline(0, 1)
(carseats.frst.rmse <- RMSE(pred = carseats.pred,
                           obs = carseats_test$Sales))
rm(carseats.pred)
#plot(varImp(carseats.frst), main="Variable Importance with Regression Random Forest")
```

The model algorithm explains *"RMSE was used to select the optimal model using the smallest value. The final values used for the model were mtry = 11, splitrule = variance and min.node.size = 5."*  You can see the results of tuning grid combinations in the associated plot of ROC AUC vs mtry grouped by splitting rule.

The bagging and random forest models faired very well - they took over the first and second place!

```{r}
rbind(data.frame(model = "Manual ANOVA", RMSE = round(carseats_model_1_pruned_rmse, 5)), 
      data.frame(model = "ANOVA w.tuneGrid", RMSE = round(carseats_model_3_pruned_rmse, 5)),
      data.frame(model = "Bagging", RMSE = round(carseats.bag.rmse, 5)),
      data.frame(model = "Random Forest", RMSE = round(carseats.frst.rmse, 5))
) %>% arrange(RMSE)
```


## Gradient Boosting

Boosting is a method to improve (boost) the week learners sequentially and increase the model accuracy with a combined model. There are several boosting algorithms.  One of the earliest was AdaBoost (adaptive boost).  A more recent innovation is gradient boosting.

Adaboost creates a single split tree (decision stump) then weights the observations by how well the initial tree performed, putting more weight on the difficult observations.  It then creates a second tree using the weights so that it focuses on the difficult observations.  Observations that are difficult to classify receive increasing larger weights until the algorithm identifies a model that correctly classifies them.  The final model returns predictions that are a majority vode. (*I think Adaboost applies only to classification problems, not regressions*). 
 
Gradient boosting generalizes the AdaBoost method, so that the object is to minimize a loss function.  In the case of classification problems, the loss function is the log-loss; for regression problems, the loss function is mean squared error.  The regression trees are addative, so that the successive models can be added together to correct the residuals in the earlier models.  Gradient boosting constructs its trees in a "greedy" manner, meaning it chooses the best splits based on purity scores like Gini or minimizing the loss.  It is common to constrain the weak learners by setting maximum tree size parameters.  Gradient boosting continues until it reaches maximum number of trees or an acceptible error level.  This can result in overfitting, so it is common to employ regularization methods that penalize aspects of the model.

**Tree Constraints**.  In general the more constrained the tree, the more trees need to be grown.  Parameters to optimize include number of trees, tree depth, number of nodes, minimmum observations per split, and minimum improvement to loss.

**Learning Rate**.  Each successive tree can be weighted to slow down the learning rate.  Decreasing the learning rate increases the number of required trees.  Common growth rates are 0.1 to 0.3.

The gradient boosting algorithm fits a shallow tree $T_1$ to the data, $M_1 = T_1$.  Then it fits a tree $T_2$ to the residuals and adds a weighted sum of the tree to the original tree as $M_2 = M_1 + \gamma T_2$.  For regularized boosting, include a learning rate factor $\eta \in (0..1)$, $M_2 = M_1 + \eta \gamma T_2$.  A larger $\eta$ produces faster learning, but risks overfitting.  The process repeats until the residuals are small enough, or until it reaches the maximum iterations.  Because overfitting is a risk, use cross-validation to select the appropriate number of trees (the number of trees producing the lowest RMSE).


#### Gradient Boosting Classification Example

Again using the `OJ` data set to predict `Purchase`, this time I'll use the gradient boosting method by specifying `method = "gbm"`.  I'll use `tuneLength = 5` and not worry about `tuneGrid` anymore.  Caret tunes the following  hyperparameters (see `modelLookup("gbm")`). 

* `n.trees`: number of boosting iterations
* `interaction.depth`: maximum tree depth
* `shrinkage`: shrinkage
* `n.minobsinnode`: mimimum terminal node size

```{r}
oj.gbm <- train(Purchase ~ ., 
               data = oj_train, 
               method = "gbm",  # for bagged tree
               tuneLength = 5,  # choose up to 5 combinations of tuning parameters
               metric = "ROC",  # evaluate hyperparamter combinations with ROC
               trControl = trainControl(
                 method = "cv",  # k-fold cross validation
                 number = 10,  # 10 folds
                 savePredictions = "final",       # save predictions for the optimal tuning parameter1
                      classProbs = TRUE,  # return class probabilities in addition to predicted values
                      summaryFunction = twoClassSummary  # for binary response variable
                      )
                    )
oj.gbm
plot(oj.gbm)
oj.pred <- predict(oj.gbm, oj_test, type = "raw")
plot(oj_test$Purchase, oj.pred, 
     main = "Gradient Boosing Classification: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")

(oj.conf <- confusionMatrix(data = oj.pred, 
                            reference = oj_test$Purchase))
oj.gbm.acc <- as.numeric(oj.conf$overall[1])
rm(oj.pred)
rm(oj.conf)
#plot(oj.bag$, oj.bag$finalModel$y)
#plot(varImp(oj.gbm), main="Variable Importance with Gradient Boosting")
```


#### Gradient Boosting Regression Example

Again using the `Carseats` data set to predict `Sales`, this time I'll use the gradient boosting method by specifying `method = "gbm"`.  I'll use `tuneLength = 5` and not worry about `tuneGrid` anymore.  Caret tunes the following  hyperparameters. 

* `n.trees`: number of boosting iterations (increasing `n.trees` reduces the error on training set, but may lead to over-fitting)
* `interaction.depth`: maximum tree depth (the default six - node tree appears to do an excellent job)
* `shrinkage`: learning rate (reduces the impact of each additional fitted base-learner (tree) by reducing the size of incremental steps and thus penalizes the importance of each consecutive iteration.  The intuition is that it is better to improve a model by taking many small steps than by taking fewer large steps. If one of the boosting iterations turns out to be erroneous, its negative impact can be easily corrected in subsequent steps.)
* `n.minobsinnode`: mimimum terminal node size

```{r}
carseats.gbm <- train(Sales ~ ., 
                      data = carseats_train, 
                      method = "gbm",  # for bagged tree
                      tuneLength = 5,  # choose up to 5 combinations of tuning parameters
                      metric = "RMSE",  # evaluate hyperparamter combinations with ROC
                      trControl = trainControl(
                        method = "cv",  # k-fold cross validation
                        number = 10,  # 10 folds
                        savePredictions = "final",       # save predictions for the optimal tuning parameter1
                        verboseIter = FALSE,
                        returnData = FALSE
                        )
                      )
carseats.gbm
plot(carseats.gbm)
carseats.pred <- predict(carseats.gbm, carseats_test, type = "raw")
plot(carseats_test$Sales, carseats.pred, 
     main = "Gradient Boosing Regression: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")
abline(0,1)

(carseats.gbm.rmse <- RMSE(pred = carseats.pred,
                           obs = carseats_test$Sales))
rm(carseats.pred)

#plot(varImp(carseats.gbm), main="Variable Importance with Gradient Boosting")
```

## Summary

Okay, I'm going to tally up the results!  For the classification division, the winner is the manual classification tree!  Gradient boosting made a valiant run at it, but came up just a little short.

```{r}
rbind(data.frame(model = "Manual Class", Acc = round(oj_cm_cart$overall["Accuracy"], 5)), 
      data.frame(model = "Class w.tuneGrid", Acc = round(oj_model_3_cm$overall["Accuracy"], 5)),
      data.frame(model = "Bagging", Acc = round(oj.bag.acc, 5)),
      data.frame(model = "Random Forest", Acc = round(oj.frst.acc, 5)),
      data.frame(model = "Gradient Boosting", Acc = round(oj.gbm.acc, 5))
) %>% arrange(desc(Acc))
```

And now for the regression division, the winnner is... gradient boosting!

```{r}
rbind(data.frame(model = "Manual ANOVA", RMSE = round(carseats_model_1_pruned_rmse, 5)), 
      data.frame(model = "ANOVA w.tuneGrid", RMSE = round(carseats_model_3_pruned_rmse, 5)),
      data.frame(model = "Bagging", RMSE = round(carseats.bag.rmse, 5)),
      data.frame(model = "Random Forest", RMSE = round(carseats.frst.rmse, 5)),
      data.frame(model = "Gradient Boosting", RMSE = round(carseats.gbm.rmse, 5))
) %>% arrange(RMSE)
```


Here are plots of the ROC curves for all the models (one from each chapter) on the same graph. The ROCR package provides the prediction() and performance() functions which generate the data required for plotting the ROC curve, given a set of predictions and actual (true) values. 
The more "up and to the left" the ROC curve of a model is, the better the model. The AUC performance metric is literally the "Area Under the ROC Curve", so the greater the area under this curve, the higher the AUC, and the better-performing the model is.

```{r}
library(ROCR)
# List of predictions
oj.class.pred <- predict(oj_model_3, oj_test, type = "prob")[,2]
oj.bag.pred <- predict(oj.bag, oj_test, type = "prob")[,2]
oj.frst.pred <- predict(oj.frst, oj_test, type = "prob")[,2]
oj.gbm.pred <- predict(oj.gbm, oj_test, type = "prob")[,2]

preds_list <- list(oj.class.pred, oj.bag.pred, oj.frst.pred, oj.gbm.pred)
#preds_list <- list(oj.class.pred)

# List of actual values (same for all)
m <- length(preds_list)
actuals_list <- rep(list(oj_test$Purchase), m)

# Plot the ROC curves
pred <- prediction(preds_list, actuals_list)
#pred <- prediction(oj.class.pred[,2], oj_test$Purchase)
rocs <- performance(pred, "tpr", "fpr")
plot(rocs, col = as.list(1:m), main = "Test Set ROC Curves")
legend(x = "bottomright", 
       legend = c("Decision Tree", "Bagged Trees", "Random Forest", "GBM"),
       fill = 1:m)
```


## Reference

Penn State University, STAT 508: Applied Data Mining and Statistical Learning, "Lesson 11: Tree-based Methods". [https://newonlinecourses.science.psu.edu/stat508/lesson/11](https://newonlinecourses.science.psu.edu/stat508/lesson/11).

Brownlee, Jason. "Classification And Regression Trees for Machine Learning", Machine Learning Mastery.  [https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/](https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/). 

Brownlee, Jason. "A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning", Machine Learning Mastery. [https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/).

[DataCamp: Machine Learning with Tree-Based Models in R](https://campus.datacamp.com/courses/machine-learning-with-tree-based-models-in-r)

[An Introduction to Statistical Learning](http://faculty.marshall.usc.edu/gareth-james/ISL/) by Gareth James, et al.

[SAS Documentation](http://support.sas.com/documentation/cdl/en/stathpug/68163/HTML/default/viewer.htm#stathpug_hpsplit_details01.htm)

[StatMethods: Tree-Based Models](https://www.statmethods.net/advstats/cart.html)

[Machine Learning Plus](https://www.machinelearningplus.com/machine-learning/caret-package/)

[GBM (Boosted Models) Tuning Parameters](https://www.listendata.com/2015/07/gbm-boosted-models-tuning-parameters.html)  from Listen Data

[Harry Southworth](https://github.com/harrysouthworth/gbm/blob/master/demo/bernoulli.R) on GitHub

[Gradient Boosting Classification with GBM in R](https://www.datatechnotes.com/2018/03/classification-with-gradient-boosting.html) in DataTechNotes

Molnar, Christoph. "Interpretable machine learning. A Guide for Making Black Box Models Explainable", 2019. [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/).


<!--chapter:end:11-cart.Rmd-->

# Non-linear Models

Linear methods can model nonlinear relationships by including polynomial terms, interaction effects, and variable transformations.  However, it is often difficult to identify how to formulate the model. Nonlinear models may be preferable because you do not need to know the the exact form of the nonlinearity prior to model training.

## Splines

A regression spline fits a piecewise polynomial to the range of *X* partitioned by *knots* (*K* knots produce *K + 1* piecewise polynomials) **James et al** [@James2013].  The polynomials can be of any degree *d*, but are usually in the range [0, 3], most commonly 3 (a cubic spline).  To avoid discontinuities in the fit, a degree-*d* spline is constrained to have continuity in derivatives up to degree *d*−1 at each knot.

A cubic spline fit to a data set with *K* knots, performs least squares regression with an intercept and 3 + *K* predictors, of the form 

$$y_i = \beta_0 + \beta_1X + \beta_2X^2 + \beta_3X^3 + \beta_4h(X, \xi_1) + \beta_5h(X, \xi_2) + \dots + \beta_{K+3}h(X, \xi_K)$$

where $\xi_1, \dots, \xi_K$ are the knots are truncated power basis functions $h(X, \xi) = (X - \xi)^3$ if $X > \xi$, else 0.

Splines can have high variance at the outer range of the predictors.  A **natural spline** is a regression spline additionally constrained to be linear at the boundaries.

How many knots should there be, and Where should the knots be placed?  It is common to place knots in a uniform fashion, with equal numbers of points between each knot.  The number of knots is typically chosen by trial and error using cross-validation to minimize the RSS.  The number of knots is usually expressed in terms of degrees of freedom.  A cubic spline will have *K* + 3 + 1 degrees of freedom.  A natural spline has *K* + 3 + 1 - 5 degrees of freedom due to the constraints at the endpoints.

A further constraint can be added to reduce overfitting by enforcing smoothness in the spline.  Instead of minimizing the loss function $\sum{(y - g(x))^2}$ where $g(x)$ is a natural spline, minimize a loss function with an additional penalty for variability:

$$L = \sum{(y_i - g(x_i))^2 + \lambda \int g''(t)^2dt}.$$

The function $g(x)$ that minimizes the loss function is a *natural cubic spline* with knots at each $x_1, \dots, x_n$. This is called a **smoothing spline**.  The larger g is, the greater the penalty on variation in the spline.  In a smoothing spline, you do not optimize the number or location of the knots -- there is a knot at each training observation. Instead, you optimize $\lambda$. One way to optimze $\lambda$ is cross-validation to minimize RSS. Leave-one-out cross-validation (LOOCV) can be computed efficiently for smoothing splines.


## MARS

Multivariate adaptive regression splines (MARS) is a non-parametric algorithm that creates a piecewise linear model to capture nonlinearities and interactions effects. The resulting model is a weighted sum of *basis* functions $B_i(X)$:

$$\hat{y} = \sum_{i=1}^{k}{w_iB_i(x)}$$

The basis functions are either a constant (for the intercept), a *hinge* function of the form $\max(0, x - x_0)$ or $\max(0, x_0 - x)$ (a more concise representation is $[\pm(x - x_0)]_+$), or products of two or more hinge functions (for interactions).  MARS automatically selects which predictors to use and what predictor values to serve as the *knots* of the hinge functions.

MARS builds a model in two phases: the forward pass and the backward pass, similar to growing and pruning of tree models. MARS starts with a model consisting of just the intercept term equaling the mean of the response values.  It then asseses every predictor to find a basis function pair consisting of opposing sides of a mirrored hinge function which produces the maximum improvement in the model error.  MARS repeats the process until either it reaches a predefined limit of terms or the error improvement reaches a predefined limit.  MARS generalizes the model by removing terms according to the generalized cross validation (GCV) criterion.  GCV is a form of regularization: it trades off goodness-of-fit against model complexity. 

The `earth::earth()` function ([documentation](https://www.rdocumentation.org/packages/earth/versions/5.1.2/topics/earth)) performs the MARS algorithm (*the term "MARS" is trademarked, so open-source implementations use "Earth" instead*).  The caret implementation tunes two parameters: `nprune` and `degree`.  `nprune` is the maximum number of terms in the pruned model.  `degree` is the maximum degree of interaction (default is 1 (no interactions)).  However, there are other hyperparameters in the model that may improve performance, including `minspan` which regulates the number of knots in the predictors.

Here is an example using the Ames housing data set (following [this](http://uc-r.github.io/mars) tutorial. 

```{r model_mars, cache=TRUE, message=FALSE}
library(tidyverse)
library(earth)
library(caret)

# set up
ames <- AmesHousing::make_ames()
set.seed(12345)
idx <- createDataPartition(ames$Sale_Price, p = 0.80, list = FALSE)
ames_train <- ames[idx, ] %>% as.data.frame()
ames_test  <- ames[-idx, ]

m <- train(
  x = subset(ames_train, select = -Sale_Price),
  y = ames_train$Sale_Price,
  method = "earth",
  metric = "RMSE",
  minspan = -15,
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = expand.grid(
    degree = 1:3, 
    nprune = seq(2, 100, length.out = 10) %>% floor()
  )
)
```

The model plot shows the best tuning parameter combination.

```{r}
plot(m, main = "MARS Parameter Tuning")
m$bestTune
```

How does this model perform against the holdout data?

```{r}
caret::postResample(
  pred = log(predict(m, newdata = ames_test)),
  obs = log(ames_test$Sale_Price)
)
```


## GAM

Generalized additive models (GAM) allow for non-linear relationships between each feature and the response by replacing each linear component $\beta_j x_{ij}$ with a nonlinear function $f_j(x_{ij})$. The GAM model is of the form

$$y_i = \beta_0 + \sum{f_j(x_{ij})} + \epsilon_i.$$

It is called an additive model because we calculate a separate $f_j$ for each $X_j$, and then add together all of their contributions. 

The advantage of GAMs is that they automatically model non-linear relationships so you do not need to manually try out many diﬀerent transformations on each variable individually.  And because the model is additive, you can still examine the eﬀect of each $X_j$ on $Y$ individually while holding all of the other variables ﬁxed.  The main limitation of GAMs is that the model is restricted to be additive, so important interactions can be missed unless you explicitly add them. 

<!--chapter:end:12-nonlin.Rmd-->

# Support Vector Machines

These notes rely on [@James2013], [@Hastie2017], and [@Kuhn2016]. I also reviewed the material in PSU's Applied Data Mining and Statistical Learning ([STAT 508](https://online.stat.psu.edu/stat508/)), and the *e1071* [Support Vector Machines](https://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf) vignette.

The Support Vector Machines (SVM) algorithm finds the optimal separating hyperplane between members of two classes using an appropriate nonlinear mapping to a sufficiently high dimension. The hyperplane is defined by the observations that lie within a margin optimized by a cost hyperparameter. These observations are called the *support vectors*.

SVM is an extension of the *support vector classifier* which in turn is a generalization of the simple and intuitive *maximal margin classifier*.  


## Maximal Margin Classifier

The maximal margin classifier is the optimal hyperplane defined in the (rare) case where two classes are *linearly separable*.  Given an $n \times p$ data matrix $X$ with binary response variable defined as $y \in [-1, 1]$ it *may* be possible to define a *p*-dimensional hyperplane $h(X) = \beta_0 + \beta_1X_1 + \beta_2X_2 \dots + \beta_pX_p = x_i^T \beta + \beta_0 = 0$ such that all observations of each class fall on opposite sides of the hyperplane. This "separating hyperplane" has the property that if $\beta$ is constrained to be a unit vector, $||\beta|| = \sum\beta^2 = 1$, then the product of the hyperplane and response variables are positive perpendicular distances from the hyperplane, the smallest of which may be termed the hyperplane *margin*, $M$,  

$$y_i (x_i^{'} \beta + \beta_0) \ge M.$$

The maximal margin classifier is the hyperplane with the maximum margin.  That is, $\max \{M\}$ subject to $||\beta|| = 1$.  A separating hyperplane rarely exists.  In fact, even if a separating hyperplane does exist, its resulting margin is probably undesirably narrow. 


## Support Vector Classifier

The maximal margin classifier can be generalized to non-separable cases using a so-called "soft margin".  The generalization is called the *support vector classifier*.  The soft margin allows some misclassification in the interest of greater robustness to individual observations. The support vector classifier optimizes 

$$y_i (x_i^{'} \beta + \beta_0) \ge M(1 - \xi_i)$$

where the $\xi_i$ are positive *slack variables* whose sum is bounded by some constant tuning parameter $\sum{\xi_i} \le constant$.  The slack variable values indicate where the observation lies:  $\xi_i = 0$ observations lie on the correct side of the margin;  $\xi_i > 0$ observation lie on the wrong side of the margin;  $\xi_i > 1$ observations lie on the wrong side of the hyperplane.  The constant sets the tolerance for margin violation.  If $constant = 0$, then all observations must reside on the correct side of the margin, as in the maximal margin classifier.  The $constant$ controls the bias-variance trade-off.  As the $constant$ increases, the margin widens and allows more violations.  The classifier bias increases but its variance decreases.

The support vector classifier is usually defined by dropping the $||\beta|| = 1$ constraint, and defining $M = 1 / ||\beta||$.  The optimization problem then becomes

$$
 \min ||\beta|| \hspace{2mm} s.t. \hspace{2mm}  
  \begin{cases} 
   y_i(x_i^T\beta + \beta_0) \ge 1 - \xi_i, \hspace{2mm} \forall i &  \\
   \xi_i \ge 0, \hspace{2mm} \sum \xi_i \le constant.      
  \end{cases}
$$

This is a quadratic equation with linear inequality constraints, so it is a convex optimization problem which can be solved using Lagrange multipliers. Re-express the optimization problem as

$$
\min_{\beta_0, \beta} \frac{1}{2}||\beta||^2 = C\sum_{i = 1}^N \xi_i \\
s.t. \xi_i \ge 0, \hspace{2mm} y_i(x_i^T\beta + \beta_0) \ge 1 - \xi_i, \hspace{2mm} \forall i
$$

where the "cost" parameter $C$ replaces the constant and penalizes large residuals.  This optimization problem is equivalent to *another* optimization problem, the familiar *loss + penalty* formulation:

$$\min_{\beta_0, \beta} \sum_{i=1}^N{[1 - y_if(x_i)]_+} + \frac{\lambda}{2} ||\beta||^2 $$

where $\lambda = 1 / C$ and $[1 - y_if(x_i)]_+$ is a "hinge" loss function with $f(x_i) = sign[Pr(Y = +1|x) - 1 / 2]$.  
The parameter estimates can be written as functions of a set of unknown parameters $(\alpha_i)$ and data points. The solution to the optimization problem requires only the inner products of the observations, represented as $\langle x_i, x_j \rangle$,

$$f(x) = \beta_0 + \sum_{i = 1}^n {\alpha_i \langle x, x_i \rangle}$$
The solution has the interesting property that only observations on or within the margin affect the hyperplane.  These observations are known as support vectors.  As the constant increases, the number of violating observations increase, and thus the number of support vectors increases.  This property makes the algorithm robust to the extreme observations far away from the hyperplane.

The parameter estimators for $\alpha_i$ are nonzero only for the support vectors in the solution—that is, if a training observation is not a support vector, then its $\alpha_i$ equals zero.
 
The only shortcoming with the algorithm is that it presumes a linear decision boundary.  


## Support Vector Machines

Enlarging the feature space of the support vector classifier accommodates nonlinar relationships.  Support vector machines do this in a specific way, using *kernals*.  The kernal is a generalization of the inner product with form $K(x_i, x_i^{'})$.  So the linear kernal is simply 

$$K(x_i, x_i^{'}) = \langle x, x_i \rangle$$

and the solution is 

$$f(x) = \beta_0 + \sum_{i = 1}^n {\alpha_i K(x_i, x_i^{'})}$$

$K$ can take onother form instead, such as polynomial 

$$K(x, x') = (\gamma \langle x, x' \rangle + c_0)^d$$ 

or radial 

$$K(x, x') = \exp\{-\gamma ||x - x'||^2\}.$$


## Example

Here is a data set of two classes $y \in [-1, 1]$ described by two features $X1$ and $X2$.

```{r message=FALSE, warning=FALSE}
library(tidyverse)
set.seed(1)
x <- matrix(rnorm (20*2), ncol=2)
y <- c(rep(-1, 10), rep(1, 10))
x[y==1, ] <- x[y==1, ] + 1
train_data <- data.frame(x, y)
train_data$y <- as.factor(y)
```

A scatter plot reveals whether the classes are linearly separable.

```{r}
ggplot(train_data, aes(x = X1, y = X2, color = y)) +
  geom_point(size = 2) +
  labs(title = "Binary response with two features") +
  theme(legend.position = "top")
```

No, they are not linearly separable.  Now fit a support vector machine. The **e1071** library implements the SVM algorithm.   `svm(..., kernel="linear")` fits a support vector classifier. Change the kernal to `c("polynomial", "radial")` for SVM.  Try a cost of 10.

```{r}
library(e1071)
m <- svm(
  y ~ ., 
  data = train_data,
  kernel = "linear",
  type = "C-classification",  # (default) for classification
  cost = 10,  # default is 1
  scale = FALSE  # do not standardize features
)
plot(m, train_data)
```

The support vectors are plotted as "x's".  There are seven of them.

```{r}
m$index
```

The summary shows adds additional information, including the distribution of the support vector classes.

```{r}
summary(m)
```

The seven support vectors are comprised of four in one class, three in the other. What if we lower the cost of margin violations?  This will increase bias and lower variance.

```{r}
m <- svm(
  y ~ ., 
  data = train_data,
  kernel = "linear",
  type = "C-classification",  
  cost = 0.1,
  scale = FALSE
)
plot(m, train_data)
```

There are many more support vectors now.  *(In case you hoped to see the linear decision boundary formulation, or at least a graphical representation of the margins, keep hoping. The model is generalized beyond two features, so it evidently does not worry too much about supporting sanitized two-feature demos.)*

Which cost level yields the *best* predictive performance on holdout data?  Use cross validation to find out. SVM defaults to 10-fold CV.  I'll try seven candidate values for `cost`.

```{r}
set.seed(1)
m_tune <- tune(
  svm,
  y ~ .,
  data = train_data,
  kernel ="linear",
  ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100))
)
summary(m_tune)
```

The lowest cross-validation error rate is 0.10 with cost = 0.1.  `tune()` saves the best tuning parameter value.
```{r}
m_best <- m_tune$best.model
summary(m_best)
```

There are 16 support vectors, 8 in each class.  This is a pretty wide margin.

```{r}
plot(m_best, train_data)
```

What if the classes had been linearly separable?  Then we could create a maximal margin classifier.  

```{r}
train_data_2 <- train_data %>% 
  mutate(
    X1 = X1 + ifelse(y==1, 1.0, 0),
    X2 = X2 + ifelse(y==1, 1.0, 0)
  )
ggplot(train_data_2, aes(x = X1, y = X2, color = y)) +
  geom_point(size = 2) +
  labs(title = "Binary response with two features, linearly separable")
```

Specify a huge cost = 1e5 so that no support vectors violate the margin.  

```{r}
m2 <- svm(
  y ~ ., 
  data = train_data_2,
  kernel = "linear",
  cost = 1e5,
  scale = FALSE  # do not standardize features
)
plot(m2, train_data_2)
summary(m2)
```

This model will have very low bias, but very high variance.  To fit an SVM, use a different kernel.  You can use `kernal = c("polynomial", "radial", "sigmoid")`.  For a polynomial model, also specify the polynomial degree. For a radial model, include the gamma value.

```{r}
set.seed(1)
m3_tune <- tune(
  svm,
  y ~ .,
  data = train_data,
  kernel ="polynomial",
  ranges = list(
    cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100),
    degree = c(1, 2, 3)
  )
)
summary(m3_tune)
```


The lowest cross-validation error rate is 0.10 with cost = 1, polynomial degree 1.

```{r}
m3_best <- m3_tune$best.model
summary(m3_best)
```

There are 12 support vectors, 6 in each class.  This is a pretty wide margin.

```{r}
plot(m3_best, train_data)
```

## Using Caret

The model can also be fit using **caret**.  I'll used LOOCV since the data set is so small.  Normalize the variables to make their scale comparable.

```{r message=FALSE, warning=FALSE}
library(caret)
library(kernlab)

train_data_3 <- train_data %>%
  mutate(y = factor(y, labels = c("A", "B")))

m4 <- train(
  y ~ .,
  data = train_data_3,
  method = "svmPoly",
  preProcess = c("center", "scale"),
  trControl = trainControl(
    method = "cv",
    number = 5,
    summaryFunction = twoClassSummary,	# Use AUC to pick the best model
    classProbs=TRUE
  )
)

m4$bestTune
```

```{r svm_plot_m4}
#plot(m4)
```


<!--chapter:end:13-svm.Rmd-->

# Principal Components Analysis

<!--chapter:end:14-pca.Rmd-->

# Text Mining

```{r message=FALSE, warning=FALSE, echo=FALSE}
library(tidyverse)
library(tidytext)
library(janeaustenr)
library(gutenbergr)
library(scales)
library(wordcloud)
library(igraph)
library(ggraph)
library(widyr)
library(tm)
library(gridExtra)
```

Text mining may be thought of as the process of distilling actionable insights from text, typically by identifying patterns with statistical pattern learning. Typical text mining tasks include text categorization, sentiment analysis, and topic modeling.  

There are two approaches to text mining.  *Semantic parsing* identifies words by type and order (sentences, phrases, nouns/verbs, proper nouns, etc.).  *Bag of Words* treats words as simply attributes of the document.  Bag of words is obviously the simpler way to go.

The **qdap** package provides parsing tools for preparing transcript data. 
```{r warning=FALSE, message=FALSE}
library(qdap)
```

For example, `freq_terms()` parses text and counts the terms.

```{r}
freq_terms(DATA$state, top = 5)
```

You can also plot the terms.

```{r}
plot(freq_terms(DATA$state, top = 5))
```

There are two kinds of the corpus data types, the permanent corpus, **PCorpus**, and the volatile corpus, **VCorpus**. The volatile corpus is held in RAM rather than saved to disk.  Create a volatile corpus with `tm::vCorpous()`. vCorpous() takes either a text source created with `tm::VectorSource()` or a dataframe source created with `Dataframe Source()` where the input dataframe has cols `doc_id`, `text_id` and zero or more metadata columns.

```{r}
tweets <- read_csv(file = "https://assets.datacamp.com/production/repositories/19/datasets/27a2a8587eff17add54f4ba288e770e235ea3325/coffee.csv")
coffee_tweets <- tweets$text

coffee_source <- VectorSource(coffee_tweets)
coffee_corpus <- VCorpus(coffee_source)

# Text of first tweet
coffee_corpus[[1]][1]
```

In bag of words text mining, cleaning helps aggregate terms, especially words with common stems like "miner" and "mining".  There are several functions useful for preprocessing: `tolower()`, `tm::removePunctuation()`, `tm::removeNumbers()`, `tm::stripWhiteSpace()`, and `removeWords()`.  Apply these functions to the documents in a `VCorpus` object with `tm_map()`.  If the function is not one of the pre-defined functions, wrap it in `content_transformer()`.  Another preprocessing function is `stemDocument()`.

```{r}
# Create the object: text
text <- "<b>She</b> woke up at       6 A.M. It\'s so early!  She was only 10% awake and began drinking coffee in front of her computer."
tolower(text)
removePunctuation(text)
removeNumbers(text)
stripWhitespace(text)
```

The **qdap** package offers other preprocessing functions.

```{r}
text <- "<b>She</b> woke up at       6 A.M. It\'s so early!  She was only 10% awake and began drinking coffee in front of her computer."
bracketX(text)
replace_number(text)
replace_abbreviation(text)
replace_contraction(text)
replace_symbol(text)
```

`tm::stopwords("en")` returns a vector of stop words.  You can add to the list with concatenation.

```{r}
new_stops <- c("coffee", "bean", stopwords("en"))
removeWords(text, new_stops)
```

`tm::stemDocument()` and `tm::stemCompletion()` reduce the variation in terms.

```{r}
complicate <- c("complicated", "complication", "complicatedly")

stem_doc <- stemDocument(complicate)

comp_dict <- c("complicate")

complete_text <- stemCompletion(stem_doc, comp_dict)
complete_text
```


```{r}
clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, words = c(stopwords("en"), "coffee", "mug"))
  corpus <- tm_map(corpus, stripWhitespace)
  return(corpus)
}
clean_corp <- clean_corpus(coffee_corpus)
content(clean_corp[[1]])

# compare to original
content(coffee_corpus[[1]])
```

To perform the analysis of the tweets, convert the corpus into either a document term matrix (DTM, documents as rows, terms as cols), or a term document matrix (TDM, terms as rows, documents as cols). 

```{r}
coffee_tdm <- TermDocumentMatrix(clean_corp)

# Print coffee_tdm data
coffee_tdm

# Convert coffee_tdm to a matrix
coffee_m <- as.matrix(coffee_tdm)

# Print the dimensions of the matrix
dim(coffee_m)

# Review a portion of the matrix
coffee_m[c("star", "starbucks"), 25:35]
```

# Tidy Text

The tidy text format is a table with one token (meaningful unit of text, such as a word) per row.  The `tidytext` package assists with the major tasks in text analysis.  A typical text analysis using tidy data principles unnests tokens with `unnest_tokens()`, removes unimportant "stop words" tokens `anti_join(stop_words)`, and summarizes token counts `count()`.

Here are four Jane Austin books from the `janeaustenr`. "Sense & Sensibility" acts as the baseline count, and the other books are faceted for comparison.  Note the use of the "tribble" of custom stop words.

```{r}
custom_stop_words <- tribble(
  ~word, ~lexicon,
  "edward", "CUSTOM",
  "frank", "CUSTOM",
  "thomas", "CUSTOM",
  "fanny", "CUSTOM",
  "anne", "CUSTOM"
)

austin_tidy <- austen_books() %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))
    ) %>%
  ungroup() %>%
  unnest_tokens(output = word, input = text) %>%
  anti_join(stop_words) %>%
  anti_join(custom_stop_words)

austin_tidy %>%
  count(book, word) %>%
  group_by(book) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  pivot_wider(names_from = book, values_from = proportion) %>%
  pivot_longer(cols = `Pride & Prejudice`:`Persuasion`, names_to = "book", values_to = "proportion") %>%
  ggplot(aes(x = proportion, 
             y = `Sense & Sensibility`,
             color = abs(`Sense & Sensibility` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~book, ncol = 2) +
  theme(legend.position = "none") +
  labs(y = "Sense & Sensibility", x = NULL)
```

Words close to the 45-degree lines have similar frequencies in both books.  Words far from the line are found more in one book more than the other.  If there are few points near the low frequencies, then few infrequent words are shared.  Emma is similar to Sense & Sensibility because the points are fairly narrow on the 45-degree line, and they extend all the way to the origin.

A common way to visualize words is with a word cloud.  The `wordcloud` library is helpful.  NOte that word clouds do not contain any information not already in a bar plot.

```{r}
austin_word_cnt <- austin_tidy %>%
  filter(book == "Sense & Sensibility") %>%
  count(word)
pal <- brewer.pal(9,"BuGn")
pal <- pal[-(1:4)]
wordcloud(
  words = austin_word_cnt$word,
  freq = austin_word_cnt$n,
  max.words = 30,
  colors = pal
)
```

```{r}
pa_file <- readxl::read_excel("./../../PeoplaAnalyticsCloud.xlsx")
pa_tidy <- pa_file %>%
  mutate(
    linenumber = row_number(),
    ) %>%
  ungroup() %>%
  unnest_tokens(output = word, input = comment) %>%
#  unnest_tokens(output = bigram, input = comment, token = "ngrams", n = 2) 
  anti_join(stop_words) %>%
  anti_join(custom_stop_words)
pa_tidy_n <- pa_tidy %>%
  count(word)
pal <- brewer.pal(9,"BuGn")
pal <- pal[-(1:4)]
wordcloud(
  words = pa_tidy_n$word,
  freq = pa_tidy_n$n,
  max.words = 30,
  colors = pal
)

pa_2gram <- pa_tidy %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word &
           !word2 %in% stop_words$word &
           !is.na(word1) & !is.na(word2)) %>%
  unite(bigram, word1, word2, sep = " ")


pa_2gram %>%
  count(book, bigram, sort = TRUE)

pa_2gram2 <- pa_2gram %>%
  count(bigram)
pal <- brewer.pal(9,"BuGn")
pal <- pal[-(1:4)]
wordcloud(
  words = pa_2gram2$bigram,
  freq = pa_2gram2$n,
  max.words = 30,
  colors = pal
)
```


# Bag of Words
# Sentiment Analysis

A typical sentiment analysis involves unnesting tokens with `unnest_tokens()`, assigning sentiments with `inner_join(sentiments)`, counting tokens with `count()`, and summarizing and visualizing.

The tidytext package contains four sentiment lexicons, all based on unigrams. 

* **nrc**. binary “yes”/“no” for categories positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. 
* **bing**. “positive”/“negative” classification. 
* **AFINN**. score between -5 (most negative) and 5 (most positive).
* **loughran**. "positive"/"negative"/"litigious"/"uncertainty"/"constraining"/"superflous" classification.

You can view the sentiment assignments with `get_sentiments(lexicon = c("afinn", "bing", nrc", "laughlin"))`

```{r}
x1 <- get_sentiments(lexicon = "nrc") %>%
  count(sentiment) %>%
  mutate(lexicon = "nrc")
x2 <- get_sentiments(lexicon = "bing") %>%
  count(sentiment) %>%
  mutate(lexicon = "bing")
x3 <- get_sentiments(lexicon = "afinn") %>%
  count(value) %>%
  mutate(lexicon = "afinn") %>%
  mutate(sentiment = as.character(value)) %>%
  select(-value)
x4 <- get_sentiments(lexicon = "loughran") %>%
  count(sentiment) %>%
  mutate(lexicon = "loughran")
x <- bind_rows(x1, x2, x3, x4)

ggplot(x, aes(x = fct_reorder(sentiment, n), y = n, fill = lexicon)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(title = "Sentiment Counts", x = "", y = "") +
  facet_wrap(~ lexicon, scales = "free")
```

Here is a sentiment analysis of sections of 80 lines of Jane Austin's books.  *(Small sections may not have enough words to get a good estimate of sentiment, and large sections can wash out the narrative structure.  80 lines seems about right.)*

```{r}
austin_tidy %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = list(n = 0)) %>%
  mutate(sentiment = positive - negative) %>%
  ggplot(aes(x = index, y = sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```

Fair to say Jane Austin novels tend to have a happy ending?  The three sentiment lexicons provide different views of THE data.  Here is a comparison of the lexicons using one of Jane Austin's novels, "Pride and Prejudice".

```{r}
# AFINN lexicon measures sentiment with a numeric score between -5 and 5.
afinn <- austin_tidy %>% 
  filter(book == "Pride & Prejudice") %>%
  inner_join(get_sentiments("afinn"), by = "word") %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

# Bing and nrc categorize words in a binary fashion, either positive or negative.
bing <- austin_tidy %>%
  filter(book == "Pride & Prejudice") %>%
  inner_join(get_sentiments("bing"), by = "word") %>% 
  count(index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from =  n, values_fill = list(n = 0)) %>%
  mutate(sentiment = positive - negative) %>% 
  mutate(method = "Bing") %>%
  select(index, sentiment, method)

nrc <- austin_tidy %>%
  filter(book == "Pride & Prejudice") %>%
  inner_join(get_sentiments("nrc") %>% filter(sentiment %in% c("positive", "negative")), by = "word") %>% 
  count(index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = list(n = 0)) %>%
  mutate(sentiment = positive - negative) %>% 
  mutate(method = "NRC") %>%
  select(index, sentiment, method)

bind_rows(afinn, bing, nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```

In this example, and in general, **NRC** sentiment tends to be high, **AFINN** sentiment has more variance, and **Bing** sentiment finds longer stretches of similar text.  However, all three agree roughly on the overall trends in the sentiment through a narrative arc.

What are the top-10 positive and negative words?  Using the Bing lexicon, get the counts, then `group_by(sentiment)` and `top_n()` to the top 10 in each category.  

```{r}
austin_tidy %>%
  filter(book == "Pride & Prejudice") %>%
  inner_join(get_sentiments("bing"), by = "word") %>% 
  count(word, sentiment, sort = TRUE) %>%
  group_by(sentiment) %>%
  top_n(n = 10, wt = n) %>%
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  coord_flip() +
  labs(y = "Contribution to Sentiment",
       x = "")
```

Uh oh, "miss" is a red-herring - in Jane Austin novels it often refers to an unmarried woman.  Drop it from the analysis by appending it to the stop-words list.

```{r}
austin_tidy %>%
  anti_join(bind_rows(stop_words,
                      tibble(word = c("miss"), lexicon = c("custom")))) %>%
  filter(book == "Pride & Prejudice") %>%
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>%
  group_by(sentiment) %>%
  top_n(n = 10, wt = n) %>%
  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  coord_flip() +
  labs(y = "Contribution to Sentiment",
       x = "")
```

Better!

A common way to visualize sentiments is with a word cloud.  

```{r}
austin_tidy %>%
  anti_join(bind_rows(stop_words,
                      tibble(word = c("miss"), lexicon = c("custom")))) %>%
  filter(book == "Pride & Prejudice") %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

`comparison.cloud` is another implementation of a word cloud.  It takes a matrix input.

```{r}
x <- austin_tidy %>%
  anti_join(bind_rows(stop_words,
                      tibble(word = c("miss"), lexicon = c("custom")))) %>%
  inner_join(get_sentiments("bing")) %>% 
  filter(book == "Pride & Prejudice") %>%
  count(word, sentiment, sort = TRUE) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = list(n = 0)) %>%
  as.data.frame()
rownames(x) <- x[,1]
comparison.cloud(x[, 2:3])
rm(x)
```

Sometimes it makes more sense to analyze entire sentences.  Specify `unnest_tokens(..., token = "sentences")` to override the default `token = "word"`.

```{r}
austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(output = word, input = text, token = "sentences")

```

## N-Grams

Create n-grams by specifying `unnest_tokens(..., token = "ngrams", n)` where `n = 2` is a bigram, etc.  To remove the stop words, `separate` the n-grams, then filter on the `stop_words` data set.

```{r}
austin.2gram <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(output = bigram, input = text, token = "ngrams", n = 2)

austin.2gram <- austin.2gram %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word &
           !word2 %in% stop_words$word &
           !is.na(word1) & !is.na(word2)) %>%
  unite(bigram, word1, word2, sep = " ")

austin.2gram %>%
  count(book, bigram, sort = TRUE)
```

Here are the most commonly mentioned streets in Austin's novels.

```{r}
austin.2gram %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(word2 == "street") %>%
  count(book, word1, sort = TRUE)
```


Bind the tf-idf statistics.  Tf-idf is short for term frequency–inverse document frequency.  It is a statistic that indicates how important a word is to a document in a collection or corpus. Tf–idf increases with the number of times a word appears in the document and decreases with the number of documents in the corpus that contain the word.  The idf adjusts for the fact that some words appear more frequently in general.

```{r}
austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(output = bigram, input = text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word &
           !word2 %in% stop_words$word &
           !is.na(word1) & !is.na(word2)) %>%
  unite(bigram, word1, word2, sep = " ")
austin.2gram %>%
  count(book, bigram) %>%
  bind_tf_idf(bigram, book, n) %>%
  group_by(book) %>%
  top_n(n = 10, wt = tf_idf) %>%
  ggplot(aes(x = fct_reorder(bigram, n), y = tf_idf, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, scales = "free_y", ncol = 2) +
  labs(y = "tf-idf of bigram to novel") +
  coord_flip() 
```


A good way to visualize bigrams is with a network graph.  Packages `igraph` and `ggraph` provides tools for this purpose.

```{r}
set.seed(2016)

austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(output = bigram, input = text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word &
           !word2 %in% stop_words$word &
           !is.na(word1) & !is.na(word2)) %>%
  count(word1, word2) %>%
  filter(n > 20) %>%
  graph_from_data_frame() %>%  # creates unformatted "graph"
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), 
                 show.legend = FALSE,
                 arrow = grid::arrow(type = "closed", 
                                     length = unit(.15, "inches")), 
                 end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", 
                  size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```


If you want to count the number of times that two words appear within the same document, or to see how correlated they are, widen the data with the `widyr` package.

```{r}
austen_books() %>%
  filter(book == "Pride & Prejudice") %>%
  mutate(section = row_number() %/% 10) %>%
  filter(section > 0) %>%
  unnest_tokens(word, text) %>%
  filter(!word %in% stop_words$word) %>%
  pairwise_count(word, section, sort = TRUE)
```

The correlation among words is how often they appear together relative to how often they appear separately.  The phi coefficient is defined

$$\phi = \frac{n_{11}n_{00} - n_{10}n_{01}}{\sqrt{n_{1.}n_{0.}n_{.1}n_{.0}}}$$

where $n_{10}$ means number of times section has word x, but not word y, and $n_{1.}$ means total times section has word x.  This lets us pick particular interesting words and find the other words most associated with them.

```{r}
austen_books() %>%
  filter(book == "Pride & Prejudice") %>%
  mutate(section = row_number() %/% 10) %>%
  filter(section > 0) %>%
  unnest_tokens(word, text) %>%
  filter(!word %in% stop_words$word) %>%
  pairwise_cor(word, section, sort = TRUE) %>%
  filter(item1 %in% c("elizabeth", "pounds", "married", "pride")) %>%
  group_by(item1) %>%
  top_n(n = 4) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(x = item2, y = correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  coord_flip()
```

You can use the correlation to set a threshold for a graph.

```{r}
set.seed(2016)

austen_books() %>%
  filter(book == "Pride & Prejudice") %>%
  mutate(section = row_number() %/% 10) %>%
  filter(section > 0) %>%
  unnest_tokens(word, text) %>%
  filter(!word %in% stop_words$word) %>%
  # filter to relatively common words
  group_by(word) %>%
  filter(n() > 20) %>%
  pairwise_cor(word, section, sort = TRUE) %>%
  filter(correlation > .15) %>%  # relatively correlated words
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```


## Converting to and from non-tidy formats

One of the most common objects in text mining packages is the document term matrix (DTM) where each row is a document, each column a term, and each value an appearance count.  The `broom` package contains functions to convert between DTM and tidy formats.

Convert a DTM object into a tidy data frame with `tidy()`.  Convert a tidy object into a sparse matrix with `cast_sparse()`, into a DTM with `cast_dtm()`, and into a "dfm" for quanteda with `cast_dfm()`.  

```{r}
data("AssociatedPress", package = "topicmodels")
AssociatedPress
Terms(AssociatedPress) %>% head()
```

Create a tidy version of `AssociatedPress` with `tidy()`.

```{r}
ap_td <- tidy(AssociatedPress)
ap_td %>% 
  inner_join(get_sentiments("bing"), by = c(term = "word")) %>%
  count(sentiment, term, wt = count) %>%
  ungroup() %>%
  filter(n >= 200) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  arrange(n) %>%
  ggplot(aes(x = fct_inorder(term), y = n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  labs(title = "Most Common AP Sentiment Words",
       x = "",
       y = "Contribution to Sentiment") +
  theme(legend.position = "top", 
        legend.title = element_blank()) +
  coord_flip()
```

The document-feature matrix `dfm` class from the `quanteda` text-mining package is another implementation of a document-term matrix.  Here are the terms most specific (highest tf-idf) from each of four selected inaugural addresses.  

```{r}
data("data_corpus_inaugural", package = "quanteda")
inaug_dfm <- quanteda::dfm(data_corpus_inaugural, verbose = FALSE)
head(inaug_dfm)

inaug_td <- tidy(inaug_dfm)
head(inaug_td)

inaug_td %>% 
  bind_tf_idf(term = term, document = document, n = count) %>%
  group_by(document) %>%
  top_n(n = 10, wt = tf_idf) %>%
  ungroup() %>%
  filter(document %in% c("1861-Lincoln", "1933-Roosevelt", "1961-Kennedy", "2009-Obama")) %>%
  arrange(document, desc(tf_idf)) %>%
  ggplot(aes(x = fct_rev(fct_inorder(term)), y = tf_idf, fill = document)) +
  geom_col() +
  labs(x = "") +
  theme(legend.position = "none") +
  coord_flip() +
  facet_wrap(~document, ncol = 2, scales = "free")
```

And here is word frequency trend ocer time for six selected terms.

```{r}
inaug_td %>%
  extract(document, "year", "(\\d+)", convert = TRUE) %>%
  complete(year, term, fill = list(count = 0)) %>%
  group_by(year) %>%
  mutate(year_total = sum(count)) %>%
  filter(term %in% c("god", "america", "foreign", "union", "constitution", "freedom")) %>%
  ggplot(aes(x = year, y = count / year_total)) + 
  geom_point() +
  geom_smooth() +
  facet_wrap(~ term, scales = "free_y") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(y = "",
       title = "% frequency of word in inaugural address")
```

Cast tidy data into document-term matrix with `cast_dtm()`, quanteda's dfm with `cast_dfm()`, and sparese matrix with `cast_sparse()`.

```{r}
inaug_dtm <- cast_dtm(data = inaug_td, document = document, term = term, value = count)
inaug_dfm <- cast_dfm(data = inaug_td, document = document, term = term, value = count)
inaug_sparse <- cast_sparse(data = inaug_td, row = document, column = term, value = count)
```

An untokenized document collection is called a *corpus*. The corpuse may include metadata, such as ID, date/time, title, language, etc.  Corpus metadata is usually stored as lists.  Use `tidy()` to construct a table, one row per document.

```{r}
data("acq")
print(acq)
acq[[1]]  # first document

acq_td <- tidy(acq)
acq_td
```

## Example

Library `tm.plugin.webmining` connects to online feeds to retrieve news articles based on a keyword.

```{r}
library(tm.plugin.webmining)
library(purrr)

company <- c("Progressive", "Microsoft", "Apple")
symbol <- c("PRG", "MSFT", "AAPL")
download_articles <- function(symbol) {
  WebCorpus(GoogleFinanceSource(paste0("NASDAQ:", symbol)))
}
#stock_articles <- tibble(company = company,
#                         symbol = symbol) %>%
#  mutate(corpus = map(symbol, download_articles))
#download_articles("MSFT")
#?GoogleFinanceSource()
#corpus <- Corpus(GoogleFinanceSource("NASDAQ:MSFT"))
```

# Topic Modeling

Topic modeling is a method for unsupervised classification of documents, similar to clustering on numeric data, which finds natural groups of items.  Latent Dirichlet allocation (LDA) is a popular topic modeling algorithm. LDA treats each document as a mixture of topics (X% topic A, Y% topic B, etc.), and each topic as a mixture of words.  Each topic is a collection of word probabilities for all of the unique words used in the corpus. LDA is implemented in the `topicmodels` package.

```{r message=FALSE, warning=FALSE}
library(topicmodels)
```

Create a topic model with the `LDA` function.  Parameter `k` specifieds the number of topics.  Here is an example using the `AssociatedPress` data set.

```{r cache=TRUE}
data("AssociatedPress")

ap_lda <- LDA(AssociatedPress, k = 2, control = list(seed = 1234))
```

The tidytext package provides a tidy method for extracting the per-topic/word probabilities, called $\beta$ from the model.

```{r}
library(tidytext)

ap_topics <- tidy(ap_lda, matrix = "beta")

ap_topics %>% arrange(-beta)
```

The tidied format lends itself to plotting.

```{r}
ap_topics %>%
  group_by(topic) %>%
  top_n(n = 10, wt = beta) %>%
  ungroup() %>%
  arrange(topic, -beta) %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(x = term, y = beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()
```

Topic 1 appears to be related to the economy; topic 2 to politics.  What is the right number of topics?  That's a matter of subjectivity, but when the topics appear to be duplicative, then you've modeled too many topics.  

Another way to look at the data is to identify terms that had the greatest difference in beta between topic 1 and topic 2. A good way to do this is with the log ratio of the two, $log_2(\beta_2 / \beta_1)$.  Log ratios are useful because the differences are symmetrical ($log_2(2) = 1$, and $log_2(.5) = -1$).  To constrain the analysis to a set of especially relevant words, filter for relatively common words having a beta greater than 1/1000 in at least one topic.
  
```{r}
ap_topics %>% 
  mutate(topic = paste0("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = beta) %>%
  filter(topic1 > 0.001 | topic2 > 0.001) %>%
  mutate(log_ratio = log2(topic2 / topic1)) %>%
  top_n(n = 20, w = abs(log_ratio)) %>%
  arrange(-log_ratio) %>%
  ggplot(aes(x = fct_rev(fct_inorder(term)), y = log_ratio)) +
  geom_col() +
  coord_flip()
```
  

Examine the per-document-per-topic probabilities, called gamma with the `matrix = "gamma"` argument to `tidy()`.

```{r}
(ap_documents <- tidy(ap_lda, matrix = "gamma"))
```

As an example, use topic modeling to see whether the chapters for four books cluster into the right books.

```{r}
library(gutenbergr)

books <- gutenberg_works(title %in% c("Twenty Thousand Leagues under the Sea", 
                                      "The War of the Worlds",
                                      "Pride and Prejudice", 
                                      "Great Expectations")) %>%
  gutenberg_download(meta_fields = "title")

by_chapter <- books %>%
  group_by(title) %>%
  mutate(chapter = cumsum(str_detect(text, regex("^chapter ", ignore_case = TRUE)))) %>%
  ungroup() %>%
  filter(chapter > 0) %>%
  unite(document, title, chapter)

by_chapter_word <- by_chapter %>%
  unnest_tokens(output = word, input = text, token = "words")

word_counts <- by_chapter_word %>%
  anti_join(stop_words) %>%
  count(document, word, sort = TRUE) %>%
  ungroup()

word_counts
```

The `topmodels` library requires `DocumentTermMatrix` objects, so cast `word_counts`.

```{r}
chapters_dtm <- word_counts %>%
  cast_dtm(document = document, term = word, value = n)

chapters_dtm
```

Create a four-topic model.

```{r}
chapters_lda <- LDA(chapters_dtm, k = 4, control = list(seed = 1234))
chapters_lda
```

What are the per-topic/word probabilities?

```{r}
chapter_topics <- tidy(chapters_lda, matrix = "beta")
chapter_topics
```

For each combination, the model computes the probability of that term being generated from that topic. The top 5 terms per topic are:

```{r}
top_terms <- chapter_topics %>%
  group_by(topic) %>%
  top_n(n = 5, wt = beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  ggplot(aes(x = reorder_within(term, beta, topic), y = beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()
```

These topics are pretty clearly associated with the four books!  Each "document" in this analysis was a single chapter. Which topics are associated with each document - can we put the chapters back together into the correct books? Examining the per-document-per-topic probabilities, (gamma).

Separate the document name into title and chapter, then visualize the per-document-per-topic probability for each. 

```{r}
chapters_gamma <- tidy(chapters_lda, matrix = "gamma") %>%
  separate(document, c("title", "chapter"), sep = "_", convert = TRUE)

chapters_gamma %>%
  mutate(title = reorder(title, gamma * topic)) %>%
  ggplot(aes(factor(topic), y = gamma)) + 
  geom_boxplot() +
  facet_wrap(~ title)

```

Almost all chapters from Pride and Prejudice, War of the Worlds, and Twenty Thousand Leagues Under the Sea were uniquely identified as a single topic each.  Some chapters from Great Expectations (topic 4) were somewhat associated with other topics. 

Are there any cases where the topic most associated with a chapter belonged to another book? First we’d find the topic that was most associated with each chapter using top_n(), which is effectively the “classification” of that chapter.  We can then compare each to the “consensus” topic for each book (the most common topic among its chapters), and see which were most often misidentified.

```{r}
chapter_classifications <- chapters_gamma %>%
  group_by(title, chapter) %>%
  top_n(1, gamma) %>%
  ungroup()

chapter_classifications

book_topics <- chapter_classifications %>%
  count(title, topic) %>%
  group_by(title) %>%
  top_n(1, n) %>%
  ungroup() %>%
  transmute(consensus = title, topic)

chapter_classifications %>%
  inner_join(book_topics, by = "topic") %>%
  filter(title != consensus)
```

Only two chapters from Great Expectations were misclassified.

The `augment()` function adds model output (token count and topic classification) to the original observations. 

```{r}
assignments <- augment(chapters_lda, data = chapters_dtm)
assignments
```

Combine with the book_topics summarization to assess the misclassifications.

```{r}
assignments <- assignments %>%
  separate(document, c("title", "chapter"), sep = "_", convert = TRUE) %>%
  inner_join(book_topics, by = c(".topic" = "topic"))
```

A good way to visualize the misclassifications is with a confusion matrix.

```{r}
library(scales)

assignments %>%
  count(title, consensus, wt = count) %>%
  group_by(title) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(consensus, title, fill = percent)) +
  geom_tile() +
  scale_fill_gradient2(high = "red", label = percent_format()) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        panel.grid = element_blank()) +
  labs(x = "Book words were assigned to",
       y = "Book words came from",
       fill = "% of assignments")
```

We notice that almost all the words for Pride and Prejudice, Twenty Thousand Leagues Under the Sea, and War of the Worlds were correctly assigned, while Great Expectations had a fair number of misassigned words (which, as we saw above, led to two chapters getting misclassified).  What were the most commmonly mistaken words?

```{r}
wrong_words <- assignments %>%
  filter(title != consensus)

wrong_words %>%
  count(title, consensus, term, wt = count) %>%
  ungroup() %>%
  arrange(desc(n))
```


# Appendix: String Manipulation

```{r}
library(stringr)
```


`print()` outputs strings as you might enter them, so embedded quotes are escaped.  Use `writeLines()` to see text as you might prefer to read them. `writeLines()` is similar to `cat()`, but does not attempt to convert non-character objects to strings.

Unicode is a standard for representing characters that might not be on your keyboard. Each available character has a Unicode code point: a number that uniquely identifies it. These code points are generally written in hex notation, that is, using base 16 and the digits 0-9 and A-F. You can find the code point for a particular character by looking up a [code chart](http://www.unicode.org/charts/).

Format numbers with `format`.  Here is how to format with comma-separations.  `digits` is *not* the number of decimal places; it's the number of significant digits

```{r}
x <- c(72.19, 1030.18, 102091.93, 1189192.18)
format(x, digits = 2, big.mark = ",", trim = TRUE, scientific = FALSE)
```

## stringr package

The **stringr** package is a simple wrapper around the more complete **stringi** package.  There are a ton of functions (see `help(package = "stringr")`), but here are some particularly useful ones.

* `str_c()` concatenates strings, similar to with `paste()` and `paste0()`.  
```{r}
str_c("hello", "world", sep = " ")
```

* `str_replace(string, pattern, replacment)` replaces `pattern` with `replacement`.  
```{r}
str_replace("If the future's looking bleek", pattern = fixed("bleek"), replacement = "dark")
```

* `str_replace_na(string, replacement)` replaces NAs.
```{r}
str_replace_na(c("We're the ones ", NA, "have to shine"), replacement = "who ")
```

* `str_split(string, pattern, simplify = FALSE)` splits `string` by `pattern` into a list of vectors, or matrix if `simplify = TRUE`. 
```{r}
str_split("If there's no one in control", pattern = " ", simplify = TRUE)
```

* `str_c(..., sep)` concatenates a vector of strings, separated by `sep`.
```{r}
str_c("we're", "the", "ones", "who", "draw", "the", "line", sep = " ")
```

`str_sub(string, start, end)` returns substring of `string` from `start` to `end`.  Use negatives to start from the end of the string.
```{r}
my_str <- "Although we live in trying times"
str_sub(my_str, start = 1, end = 5)
str_sub(my_str, start = -4, end = -1)
```

* `str_length(string)` returns the number of characters in a string.
```{r}
str_length("We're the ones who have to try")
```

* `str_detect(string, pattern)` returns booleans where `string` matches `pattern`.
```{r}
str_detect(c("Although we know", "that time",  "has wings"), pattern = fixed("wings"))
```

* `str_match(string, pattern)` returns matching strings where `string` matches `pattern`.
```{r}
str_match(c("Although we know", "that time",  "has wings"), pattern = "wings")
```

* `str_subset(string, pattern)` returns string matches where `string` matches `pattern`.
```{r}
str_subset(c("Although we know", "that time",  "has wings"), pattern = fixed("wings"))
```

* `str_count(string, pattern)` returns a count of matches where `string` matches `pattern`.
```{r}
str_count(c("Although we know", "that time",  "has wings"), pattern = fixed("wings"))
```

* `str_extract(string, pattern)` returns the part of the `string` matching `pattern`.
```{r}
str_extract(c("We're the ones", "who have to fly"), pattern = " t..")
```

## Regular Expressions
The **rebus** package is a good resource for building regular expressions.

```{r}
library(rebus)
```

The `%R%` operator concatenates the regular expression.  `START` represents regex "^" meaning "starting with".  `END` represents regex "$" meaning "ending with".
```{r}
x <- austen_books() %>% 
  filter(book == "Sense & Sensibility") %>%
  select(text) %>%
  head(100) %>%
  pull()
```

Here are lines from Sense & Sensibility that start with "Mr".
```{r}
str_subset(x, pattern = START %R% "Mr")
```

`ANY_CHAR` represents regex "."  Here are lines from Sense & Sensibility with pattern "*handsome*"
```{r}
str_subset(x, pattern = ANY_CHAR %R% "handsome" %R% ANY_CHAR)
```

`char_class()` is similar to `ANY_CHAR` except that it matches any character from the string parameter.  It is the same as regex "[]".  The opposite is `negated_char_class()`, which is the same as "[^]".  `char_class()` can accept ranges, such as "0-9", and "a-z".  `DGT` is the same thing as "0-9".  
```{r}
str_subset(c("apple", "Aardvark", "Orukidn"), char_class("Aa"))
```


`DOT`, `CARAT`, and `DOLLAR` represent special characters, "\.", "\%", and "\$".  Function `or()` provides alteration.

```{r}
str_match(c("kittycat", "doggone"), pattern = or("dog", "cat"))
```

Look for repeating patterns with `optional()`, `zero_or_more()`, `one_or_more()`, and `repeated()`.

Wrap a rebus expression in `capture()` to create a column in the output for the match to each captured part of the regex.
```{r}
phone_string <- c("555-123-4567", 
                  "(555)123-4567", 
                  "555.123.4567", 
                  "555-123-4567 (M), 555-123-7654 (H)")
phone_pattern <- 
  capture(DGT %R% DGT %R% DGT) %R% zero_or_more(char_class("()-.")) %R% 
  capture(DGT %R% DGT %R% DGT) %R% zero_or_more(char_class("()-.")) %R%
  capture(DGT %R% DGT %R% DGT %R% DGT)

# first match with str_match()
phone_match <- str_match(phone_string, pattern = phone_pattern)
str_c("(", phone_match[, 2], ")", phone_match[, 3], "-", phone_match[, 4])

# all matches with str_match_all() and lapply()
phone_match_all <- str_match_all(phone_string, pattern = phone_pattern)
lapply(phone_match_all, function(x){str_c("(", x[, 2], ")", x[, 3], "-", x[, 4])}) %>% unlist()
```

You can refer to captured patterns with `REF[0-9]`.
```{r}
str_match(c("hello", "sweet", "kitten"), 
  pattern = capture(LOWER) %R% REF1)
```

Here is an exercise working with the Oscal Wilde play "The Importance of Being Earnest".

```{r}
earnest <- read_lines("http://s3.amazonaws.com/assets.datacamp.com/production/course_2922/datasets/importance-of-being-earnest.txt")
```

The text is between the lines with "START OF THE PROJECT" and "END OF THE PROJECT".  `str_which()` returns the indices where the string contains the pattern. The text consists of an introduction and the play itself.  The play starts at "FIRST ACT".
```{r}
start <- str_which(earnest, fixed("START OF THE PROJECT"))
end <- str_which(earnest, fixed("END OF THE PROJECT"))
earnest_sub <- earnest[(start+1):(end-1)]

play_start <- str_which(earnest_sub, "FIRST ACT")
intro_line_index <- 1:(play_start - 1)
intro_text <- earnest_sub[intro_line_index]
play_text <- earnest_sub[-intro_line_index]

# remove the emptly lines
play_lines <- play_text[str_length(play_text) > 0] %>% as.character()

# print first 20 lines
writeLines(play_lines[1:20])
```

How would you identify lines where the character is starting to speak?  You might look for a capitalized word followed by a ".".
```{r}
pattern <- START %R% ascii_upper() %R% one_or_more(WRD) %R% DOT

lines <- str_subset(play_lines, pattern)

# Extract the matching string (the character speaking)
who <- str_extract(lines, pattern)

# Let's see what we have
unique(who)
```

Close, but not perfect.  If you know the characters, just search for them directly.  `or1()` is like `or()` but lets you supply a vector of strings.
```{r}
characters <- c("Algernon", "Jack", "Lane", "Cecily", "Gwendolen", "Chasuble", 
  "Merriman", "Lady Bracknell", "Miss Prism")

pattern <- START %R% or1(characters) %R% DOT

lines <- str_subset(play_lines, pattern)

# Extract the matching string (the character speaking)
who <- str_extract(lines, pattern)

# Let's see what we have
unique(who)

# Lines per character 
table(who)
```


# Reference

Silge, J., & Robinson, D. (2019). Text Mining with R. O'Reilly Media. https://www.tidytextmining.com.
https://juliasilge.com/blog/evaluating-stm/

Dotson, Marc.  "Introduction to Text Analysis in R". DataCamp. https://www.datacamp.com/courses/introduction-to-text-analysis-in-r., 

String Manipulation in R with stringr,
Text Mining: Bag of Words, 
Sentiment Analysis in R.

Other DataCamp Courses
Sentiment Analysis in R: The Tidy Way, 
Topic Modeling in R,
Introduction to Natural Language Processing in R.  

There are also some projects in DataCamp, and tutorial.



<!--chapter:end:16-text-mining.Rmd-->

# Survival Analysis

These notes rely on the [Survival Analysis in R](https://campus.datacamp.com/courses/survival-analysis-in-r) DataCamp course, [STHDA](http://www.sthda.com/english/wiki/survival-analysis), and Applied Survival Analysis Using R [@Moore2016].  

Survival analysis models time to event.  Whereas linear regression outcomes are assumed to have a normal distribution, time-to-event outcomes have a Weibull or unknown distribution.  Survival analysis models also deal with censoring (unknown starting event and/or ending event).  These factors make survival analysis more complicated than linear regression.

Most survival analyses use the `survival` package for modeling and the `survminer` package for visualization.

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(survival)
library(survminer)
```


A typical survival analysis uses Kaplan-Meier plots to visualize survival curves, log-rank tests to compare survival curves among groups, and Cox proportional hazards regression to describe the effect of variables on survival. 


## Basic Concepts

This section reviews the fundamentals of survival analysis, including the hazard probability density, and survival functions.

You can specify the survival distribution function either as a *survival function* or as a *hazard function*.  Define $F(t) = Pr(T \le t), \hspace{3mm} 0 < t < \infty$ as the cumulative risk function, the probability of dying on or before time $t$.  Then the survival function is the probability of *surviving* up to time $t$, 

$$S(t) = 1 - F(t) = pr(T > t), \hspace{3mm} 0 < t < \infty.$$

The hazard function is the instantaneous death rate given survival up to time $t$,

$$h(t) = \lim_{\delta \rightarrow 0}{\frac{pr(t < T < t + \delta|T > t)}{\delta}}.$$

The survival function and the hazard function are related.  The probability of dying during the interval $(t, t + \delta)$, $f(t) = F'(t)$, is the probability of dying during the interval given survival up to point $t$ times the probability of surviving up to point $t$, $f(t) = h(t) S(t)$.

$S(t)$ is also the exponent of the negative cumulative hazard function,

$$S(t) = e^{-H(t)}.$$

You can use the survival function to estimate the mean and median survival times.  The mean survival time is $E(T) = \int S(t)dt$.  The median survival time is $S(t) = 0.5$.


## Survival Curve Estimation

There are parametric and non-parametric methods to estimate a survivor curve.  The usual non-parametric method is the *Kaplan-Meier* (KM) estimator.  The usual parametric method is the *Weibull* distribution, of which the exponential distribution is a special case.  In between the two is the *Cox proportional hazards model*, the most common way to estimate a survivor curve.

### Kaplan-Meier

The Kaplan-Meier estimator for the survival function is

$$\hat{S} = \prod_{i: t_i < t}{\frac{n_i - d_i}{n_i}}$$

where $n_i$ is the number of persons under observation at time $i$ and $d_i$ is the number of individuals dying at time $i$.  The Kaplan-Meier curve falls only when a subject dies, not when a subject is censored.  Confidence limits can be calculated using the "delta" method to obtain the variance of $\log \left(\hat{S}(t) \right)$ (see p27 of Moore).

Calculate the Kaplan-Meier survival function estimate with the `survfit()` function. Here is an example using the `lung` data from the `survival` package to death of 228 patients witha advanced lung cancer.  Column `status` indicates whether or not a person in the study has died (1 = censored, 2 = dead).

```{r}
data(lung, package = "survival")
head(lung)
```

`survfit()` creates survival curves from a formula or from a previously fitted Cox model.  The formula below is an intercept-only model.  Structure the response variable as a `Surv` object.  

```{r}
km_fit <- survfit(Surv(time, status) ~ sex, data = lung)
km_fit
```

The printed `survfit` object shows there were 138 records (patients) for males (`sex=1`) and 90 records for females (`sex=2`), and 112 events (deaths) for males and 53 events for females.  It also shows the 95% CI in days for the median time to event.

The `survfit` object contains more variables, including detailed `time` points with the number at risk `n.risk`, events `n.event`, and censors `n.censor` at each time point and strata `strata`.

```{r warning=FALSE}
data.frame(
  strata = km_fit$strata,
  time = km_fit$time,
  n.risk = km_fit$n.risk,
  n.event = km_fit$n.event,
  n.censor = km_fit$n.censor,
  surv = km_fit$surv,
  upper = km_fit$upper,
  lower = km_fit$lower
) %>% head()
```

Plot the fitted model with `ggsurvplot()`.  A vertical drop in the curves indicates an event. The vertical tick mark on the curves means that a patient was censored.

```{r warning=FALSE, fig.height=7, fig.width=6}
ggsurvplot(
  km_fit,
  linetype = "strata", # Change line type by groups
  pval = TRUE,
  conf.int = TRUE,
  risk.table = TRUE,
  surv.median.line = "hv", # median horizontal and vertical ref lines
  ggtheme = theme_bw(),
  palette = c("#E7B800", "#2E9FDF"),
  title = "Kaplan-Meier Survival Function Estimate"
)
```

Reading the figure, you can see the median survival is 270 days for `sex=1` and 426 days for `sex=2`.  These key values are available from the `summary()` object.

```{r}
summary(km_fit)$table
```

`ggsurvplot()` can plot the cumulative risk function (aka "cumulative incidence" or "cumulative events"), $F(t) = 1 - S(t)$, with argument `fun = "event"`, and the cumulative hazard function, $H(t) = -\log(S(t)).$, with argument `fun = "cumhaz"`.

```{r}
ggsurvplot(
  km_fit,
  fun = "event",
  linetype = "strata", # Change line type by groups
  pval = TRUE,
  conf.int = TRUE,
  ggtheme = theme_bw(),
  palette = c("#E7B800", "#2E9FDF"),
  title = "Kaplan-Meier Cumulative Risk Function Estimate"
)
```

```{r}
ggsurvplot(
  km_fit,
  fun = "cumhaz",
  linetype = "strata", # Change line type by groups
  pval = TRUE,
  conf.int = TRUE,
  ggtheme = theme_bw(),
  palette = c("#E7B800", "#2E9FDF"),
  title = "Kaplan-Meier Cumulative Hazard Function Estimate"
)
```

`ggsurvplot()` can produce faceted plots for more complicated analyses.  For example, fit a survival curve to the `survival::colon` data set with predictors `sex`, `rx`, and `adhere`.

```{r}
km_fit_colon <- survfit(Surv(time, status) ~ sex + rx + adhere, data = colon)

p <- km_fit_colon %>%
  ggsurvplot(fun = "event", 
             conf.int = TRUE,
             ggtheme = theme_bw()) 
p$plot + theme(legend.position = "right") + facet_grid(rx ~ adhere)
```

#### Log-Rank Test

Compare survival curves with the log-rank test ($H_0$: no difference). The log rank test is a non-parametric test, so it makes no assumptions about the survival distributions. The log rank test compares the observed and expected ($H_0$) number of events in each group. The log rank test statistic is approximately chi-square distributed.

Function `survdiff()` performs the log-rank test.

```{r}
km_diff <- survdiff(Surv(time, status) ~ sex, data = lung)
km_diff
```

The chi-sq test statistic is 10.3 on one d.f., for a p-value of 0.001, so yes, males and females had different survival patterns.


### Weibull

Kaplan-Meier curves and logrank tests are examples of univariate analysis. They describe the survival as a function of a single categorical factor variable. Weibull and other parametric models describe the function of *multiple* covariates.

Several parametric distributions are available for modeling survival data.  The exponential distribution is the easiest to use because it has a time-independent function,

$$\log h_i(t) = \alpha + \beta X_i$$
or

$$h_i(t) = e^{\left(\alpha + \beta X_i \right)} = \lambda$$

where $i$ is the observation number.   The constant $\alpha$ is a kind of baseline log-hazard, because $\log h_i(t) = \alpha$, or $h_i(t) = e^\alpha$, when $X$ is zero.

The cumulative hazard is $H(t) = \int_0^t \lambda dt = \lambda t$ and the corresponding survival function is 

$$S(t) = e^{-H(t)} = e^{-\lambda t}.$$

The expected survival time is $E(T) = \int_0^\infty S(t)dt = \int_0^\infty d^{-\lambda t} dt = 1 / \lambda.$.  The median survival time is $S(t) = e^{-\lambda t} = 0.5$, or $t_{med} = \log(2) / \lambda$.  

The Weibull distribution is more appropriate for modeling lifetimes, however.  The Weibull hazard function is $h(t) = \alpha \lambda (\lambda t)^{\alpha - 1} = \alpha \lambda^\alpha t^{\alpha-1}$.  

```{r}
data.frame(t = rep(1:80, 3),
           alpha = c(rep(1.5, 80), rep(1, 80), rep(0.75, 80)),
           lambda = rep(0.03, 240)) %>%
  mutate(
    f = dweibull(x = t, shape = alpha, scale = 1 / 0.03),
    S = pweibull(q = t, shape = alpha, scale = 1 / 0.03, lower.tail = FALSE),
    h = f / S  # same as alpha * lambda^alpha * t^(alpha-1)
  ) %>%
  ggplot(aes(x = t, y = h, color = as.factor(alpha))) +
  geom_line() +
  theme(legend.position = "top") +
  labs(y = "hazard", x = "time", color = "alpha",
       title = "Weibul hazard function at varying levels of alpha",
       subtitle = "Lambda = 0.03",
       caption = "alpha = 1 is special case of exponential function.")
```

The cumulative hazard function is $H(t) = (\lambda t)^\alpha$ and the corresponding survival function is

$$S(t) = e^{-(\lambda t)^\alpha}.$$

The exponential distribution is a special case of the Weibull where $\alpha = 1$.  The expected survival time is $E(t) = \frac{\Gamma (1 + 1 / \alpha)}{\lambda}$.  The median survival time is $t_{med} = \frac{[\log(2)]^{1 / \alpha}}{\lambda}$

The Kaplan-Meier estimate is used mainly as a descriptive tool.  The Weibull model produces a smooth survival curve instead of a step function.  The Weibull model assumes a Weibull distribution.  

Fit a Weibull model with the `survreg()` function.

```{r}
data(GBSG2, package = "TH.data")
wb <- survreg(Surv(time, cens) ~ 1, data = GBSG2)

# 90% of patients survive beyond time point 385
# Alternatively, 10% of patients die at time 385
predict(wb, type = "quantile", p = 1 - 0.9, newdata = data.frame(1))
# The median survival time is 1694
predict(wb, type = "quantile", p = 1 - 0.5, newdata = data.frame(1))

surv <- seq(.99, .01, by = -.01)
t <- predict(wb, type = "quantile", p = 1 - surv, newdata = data.frame(1))
head(data.frame(time = t, surv = surv))

surv_wb <- data.frame(time = t, surv = surv, 
                      upper = NA, lower = NA, std.err = NA)
ggsurvplot_df(fit = surv_wb, surv.geom = geom_line)
```

Fit a Weibull model controlling for hormonal therapy `horTh` and tumor size `tsize`.

```{r}
wbmod <- survreg(Surv(time, cens) ~ horTh + tsize, data = GBSG2)

coef(wbmod)
summary(wbmod)

surv <- seq(.99, .01, by = -.01)
newdata <- expand.grid(
    horTh = levels(GBSG2$horTh),
    tsize = quantile(GBSG2$tsize, probs = c(0.25, 0.50, 0.75))
)
t <- predict(wbmod, type = "quantile", p = 1 - surv, newdata = newdata)

surv_wbmod <- surv_wbmod_wide <- cbind(newdata, t) %>%
    pivot_longer(names_to = "surv_id", values_to = "time", cols = -c(1:2)) %>%
    mutate(tsize = as.numeric(tsize),
           surv_id = as.factor(as.numeric(surv_id))) %>%
    data.frame()
surv_wbmod$surv = surv[as.numeric(surv_wbmod$surv_id)]
surv_wbmod$upper = NA
surv_wbmod$lower = NA
surv_wbmod$std.err = NA
surv_wbmod$strata = NA
surv_wbmod[, c("upper", "lower", "std.err", "strata")] <- NA
ggsurvplot_df(surv_wbmod, surv.geom = geom_line,
  linetype = "horTh", color = "tsize", legend.title = NULL)
```

Interpret the coefficient as the probability of surviving falls by 0.012 per unit increase in the tumor size and increases by 0.312 if taking hormonal therapy.

You can fit other models with the `dist = c("lognormal", "exponential")` parameter.


### Cox

Whereas parametric models specify the baseline hazard function, $\alpha(t) = \log h_0(t)$, the Cox proportional hazards model is semi-parametric in that it leaves it unspecified.  

$$\log h_i(t) = \alpha(t) + \beta X_i$$

or

$$h_i(t) = h_0(t) \cdot e^{X_i \beta} = h_0(t) \psi_i$$

The $e^{\beta_j}$ values are called the hazard ratios.  A positive $e^{\beta_j}$ means the hazard increases with the covariate. The proportionality of the model comes from the lack of time dependence in the $X$ variables.  Two ratio of the hazard functions of two individuals is 

$$\frac{h_i(t)}{h_{i'}(t)} = \frac{h_0(t) \cdot e^{X_i \beta}}{h_0(t) \cdot e^{X_{i'} \beta}}$$

Remarkably, even though the baseline hazard is unspecified, the Cox model can still be estimated by the method of partial likelihood. Consider the first failure time, $t_1$.  The probability that patient $i$ is the one to fail is the proportion of patient $i$'s hazard divided by the sum of the hazards of all $R_1$ patients at risk,

$$p_1 = \frac{h_i(t_1)}{\sum_{k \in R_1} h_k(t_1)} = \frac{h_0(t_1) \psi_i}{\sum_{k \in R_1} h_0(t_1) \psi_k} = \frac{\psi_i}{\sum_{k \in R_1} \psi_k}$$

The next failure event has a reduced $R_2$ patients at risk.  The partial likelihood for $D$ failure times is the product $L_i = p_1 p_2 \cdots p_D$. The Cox model uses maximum partial likelihood estimation to find the value of $\phi$ that maximizes the likelihood function.

Fit a Cox proportional hazards model with `coxph()`.  Here is a simple Cox proportional hazards model for a single covariate, `sex`.

```{r}
lung_cox <- coxph(Surv(time, status) ~ sex, data = lung)
summary(lung_cox)
```

The Wald statistic (z) is the ratio of each regression coefficient to its standard error (`z = coef/se(coef)`), just as with linear regression.

The negative coefficient estimator sign means that the hazard (risk of death) decreases with increasing values of the variable. `sex` is encoded as a 1 = male, 2 = female, so sex = -0.5310 means females have a lower risk of death than males.

The exponentiated coefficients (`exp(coef)`) are the hazard ratios, the effect-size of the covariates. Being female (sex=2) reduces the hazard by a factor of 0.5880 (41%).

Below the coefficients estimators table is a table of hazard ratio Confidence intervals. 

The last section of the summary object is three tests for the overall significance of the model: the likelihood-ratio test, Wald test, and score logrank statistics. These three methods are asymptotically equivalent. The Likelihood ratio test has better behavior for small sample sizes, so it is generally preferred.

A multivariate analysis works the same way.  Here is the Cox model with two additional covariates: `age` and `ph.ecog`.

```{r}
lung_cox_2 <- coxph(Surv(time, status) ~ age + sex + ph.ecog, data = lung)
summary(lung_cox_2)
```

The p-values for all three overall tests (likelihood, Wald, and score) are significant, indicating that the model is significant (not all $\beta$ values are 0). 

Having fit a Cox model to the data, it’s possible to visualize the predicted survival proportion at any given time for a particular risk group. Function `survfit()` estimates the survival proportion, by default at the mean values of covariates.

```{r}
#ggsurvplot(survfit(lung_cox_2), data = lung, palette = "#2E9FDF", ggtheme = theme_minimal())
```

To display the effects of one or more particular covariates, construct a data frame with test cases and pass to `survfit()` with the `newdata` argument.

```{r}
newdata <- expand.grid(
    sex = unique(lung$sex),
#    age = quantile(lung$age, probs = c(0.25, 0.50, 0.75)),
    age = median(lung$age),
    ph.ecog = 1
)
rownames(newdata) <- c("male", "female")

# Create survival curves.  The rownames show up in the model
lung_pred <- survfit(lung_cox_2, newdata = newdata, data = lung)

# surv_summary() creates the data.frame with a summary of the survfit() results, including columns like time (survival time) and surv (survival probability).
lung_pred0 <- surv_summary(lung_pred)
# get the corresponding new_data cols
lung_pred1 <- cbind(lung_pred0, newdata[as.character(lung_pred0$strata), ])
ggsurvplot_df(
  lung_pred1, 
  color = "sex", 
  legend.labs = c("M", "F"),
  legend.title = "Sex", 
  conf.int = TRUE, 
  ggtheme = theme_minimal())
```


<!--chapter:end:17-survival_analysis.Rmd-->

# Appendix {-}

Here are miscellaneous skills, knowledge, and technologies I should know.

## Publishing to BookDown {-}

The **bookdown** package, written by Yihui Xie, is built on top of R Markdown and the **knitr** package.  Use it to publish a book or long manuscript where each chapter is a separate file.  There are instructions for how to author a book in his [bookdown book](https://bookdown.org/yihui/bookdown/) [@xie2019].  The main advantage of **bookdown** over R Markdown is that you can produce multi-page HTML output with numbered headers, equations, figures, etc., just like in a book.  I'm using **bookdown** to create a compendium of all my data science notes.  

The first step to using **bookdown** is installing the **bookdown* package with `install.packages("bookdown")`.

Next, create an account at [bookdown.org](http://bookdown.org), and connect the account to RStudio.  Follow the instructions at [https://bookdown.org/home/about/](https://bookdown.org/home/about/).

Finally, create a project in R Studio by creating a new project of type *Book Project using Bookdown*.

After creating all of your Markdown pages, knit the book or click the **Build Book** button in the Build panel.


## Shiny Apps {-}

## Packages {-}

**R Packages** [@Wickham2015] by Hadley Wickham is a good manual on packages, but it does not include a full tutorial.  The [Developing R Packages](https://campus.datacamp.com/courses/developing-r-packages/) Data Camp course is also helpful.  I will set up my own exercise and present it here.  I will create a package for my pretend organization, "MF".  The package will include the following:

* R Markdown template.  My template will integrate code, output, and commentary in a single R Markdown. The template will produce a familiar work product containing standard content (summary, data management, exploratory analysis, methods, results, conclusions), and a standard style (colors, typeface, size, logo).

* Functions.  Common I/O functions for database retrieval, writing to Excel.  Common graphing functions for ggplot styling.

I am mostly copying the logic and code from the ggthemes [economist.R](https://rdrr.io/cran/ggthemes/src/R/economist.R) script.


### Create a package {-}

1. In the RStudio IDE, click File > New Project.  Select "New Directory".  Select "R Package".  You can also use `devtools::create("mfstylr")`.  This will create the minimum items for an R package.

![](./images/create_pkg.png)

    + R directory: R scripts with function definitions.
    + man directory: documentation
    + NAMESPACE file: information about imported functions and functions made available (managed by **roxygen2**)
    + DESCRIPTION file: metadata about the package
  
2. Write functions in R scripts in R directory.  Document with tags readable by *roxygen2* package.

3. Select XYZ > Install and Restart.

### Document Functions with roxygen

Add roxygen documentation with `#'` characters.  The first three lines are always the title, Description, and Details.  They don't need any tags, but you need to separate them with blank lines.

```{r eval=FALSE}

```



### Create Data {-}

Add an RData file to your package with `use_data()`

### Create Vignette {-}

Add a directory and template vignette with `use_vignette(name, title)`.

```{r eval=FALSE}
use_vignette("Creating-Plots-with-mfstylr", "Creating Plots with mfstylr")
```


#### Step 2: Create an R Markdown template {-}

I relied on [this blog](http://freerangestats.info/blog/2017/09/09/rmarkdown) at *free range statistics* for a lot what follows.  There is also good information about R Markdown and templates in Yihui Xie's **R Markdown: The Definitive Guide** [@Xie2019b].  

Use `usethis::use_rmarkdown_template()` to create an Rmd template.  I will create a "Kaggle Report" template.  In the Console (or a script), enter

```{r eval=FALSE}
usethis::use_rmarkdown_template(
  template_name = "Kaggle Report",
  template_dir = "kaggle_report",
  template_description = "Template for creating Kaggle reports in RMarkdown.",
  template_create_dir = FALSE
)
```

Since my project directory is `C:\Users\mpfol\OneDrive\Documents\GitHub\mfstylr`, `use_rmarkdown_template()` creates subdirectories `.\inst\rmarkdown\templates\kaggle_report\skeleton` with three files

* `.\inst\rmarkdown\templates\kaggle_report\template.yaml`
* `.\inst\rmarkdown\templates\kaggle_report\skeleton\skeleton.Rmd`

My kaggle report template will include a logo.  Looks like there are two ways to embed an image in your document.  One is a direct image loading reference `!()`, but I don't think you can control the attributes this way.  A second way is adding html tags.

```{r eval=FALSE}
![](logo.png)

# or for more control
<img src="logo.png" style="position:absolute;top:0px;right:0px;" />
```

 

<!--chapter:end:18-appendix.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:19-references.Rmd-->

