--- 
title: "My Data Science Notes"
author: "Michael Foley"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output:
  bookdown::gitbook:
    lib_dir: assets
    split_by: section
    config:
      toolbar:
        position: static
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "This is a compendium of notes from classes, tutorials, etc. that I reference from time to time."
---

# Intro {-}

These notes are pulled from various classes, tutorials, books, etc. and are intended for my own consumption.  If you are finding this on the internet, I hope it is useful to you, but you should know that I am just a student and there's a good chance whatever you're reading here is mistaken.  In fact, that should probably be your null hypothesis... or your prior.  Whatever.

<!--chapter:end:index.Rmd-->

```{r include=FALSE}
library(tidyverse)
library(mfstylr)
my_colors <- list(blue = "#4a5F70",
                  beige = "#7f825f",
                  mauve = "#c2ae95",
                  red = "#824e4e",
                  grey = "#66777d")
```

# Probability {#probability}

## Principles

Here are three rules that come up all the time.

* $Pr(A \cup B) = Pr(A)+Pr(B) - Pr(AB)$.  This rule generalizes to 
$Pr(A \cup B \cup C)=Pr(A)+Pr(B)+Pr(C)-Pr(AB)-Pr(AC)-Pr(BC)+Pr(ABC)$.

* $Pr(A|B) = \frac{P(AB)}{P(B)}$

* If A and B are independent, $Pr(A \cap B) = Pr(A)Pr(B)$, and $Pr(A|B)=Pr(A)$. 

Uniform distributions on finite sample spaces often reduce to counting the elements of *A* and the sample space *S*, a process called combinatorics.  Here are three important combinatorial rules.

**Multiplication Rule**.  $|S|=|S_1 |⋯|S_k|$.

*How many outcomes are possible from a sequence of 4 coin flips and 2 rolls of a die?*
$|S|=|S_1| \cdot |S_2| \dots |S_6| = 2 \cdot 2 \cdot 2 \cdot 2 \cdot 6 \cdot 6 = 288$.

*How many subsets are possible from a set of n=10 elements?*
In each subset, each element is either included or not, so there are $2^n = 1024$ subsets.

*How many subsets are possible from a set of n=10 elements taken k at a time with replacement?*
Each experiment has $n$ possible outcomes and is repeated $k$ times, so there are $n^k$ subsets.

**Permutations**.  The number of *ordered* arrangements (permutations) of a set of $|S|=n$ items taken $k$ at a time *without* replacement has $n(n-1) \dots (n-k+1)$ subsets because each draw is one of k experiments with decreasing number of possible outcomes.  

$$_nP_k = \frac{n!}{(n-k)!}$$

Notice that if $k=0$ then there is 1 permutation; if $k=1$ then there are $n$ permutations; if $k=n$ then there are $n!$ permutations.

*How many ways can you distribute 4 jackets among 4 people?*
$_nP_k = \frac{4!}{(4-4)!} = 4! = 24$

*How many ways can you distribute 4 jackets among 2 people?*
$_nP_k = \frac{4!}{(4-2)!} = 12$

**Subsets**.  The number of *unordered* arrangements (combinations) of a set of $|S|=n$ items taken $k$ at a time *without* replacement has 

$$_nC_k = {n \choose k} = \frac{n!}{k!(n-k)!}$$

combinations and is called the binomial coefficient.  The binomial coefficient is the number of different subsets.  Notice that if k=0 then there is 1 subset; if k=1 then there are n subsets; if k=n then there is 1 subset.  The connection with the permutation rule is that there are $n!/(n-k)!$ permutations and each permutation has $k!$ permutations.  

*How many subsets of 7 people can be taken from a set of 12 persons?*
$_{12}C_7 = {12 \choose 7} = \frac{12!}{7!(12-7)!} = 792$

*If you are dealt five cards, what is the probability of getting a "full-house" hand containing three kings and two aces (KKKAA)?*
$$P(F) = \frac{{4 \choose 3} {4 \choose 2}}{{52 \choose 5}}$$

**Distinguishable permutations**.  The number of *unordered* arrangements (distinguishable permutations) of a set of $|S|=n$ items in which $n_1$ are of one type, $n_2$ are of another type, etc., is 

$${n \choose {n_1, n_2, \dots, n_k}} = \frac{n!}{n_1! n_2! \dots n_k!}$$

*How many ordered arrangements are there of the letters in the word PHILIPPINES?*  There are n=11 objects.  $|P|=n_1=3$;  $|H|=n_2=1$; $|I|=n_3=3$; $|L|=n_4=1$; $|N|=n_5=1$; $|E|=n_6=1$; $|S|=n_7=1$.

$${n \choose {n_1, n_2, \dots, n_k}} = \frac{11!}{3! 1! 3! 1! 1! 1! 1!} = 1,108,800$$

*How many ways can a research pool of 15 subjects be divided into three equally sized test groups?*

$${n \choose {n_1, n_2, \dots, n_k}} = \frac{15!}{5! 5! 5!} = 756,756$$


## Discrete Distributions

These notes rely heavily on PSU STATS 504 [course notes](https://online.stat.psu.edu/stat504/node/209/).

The most important discrete distributions are Bernoulli, Binomial, Poisson, and Multinomial.  Less important, but sometimes useful, are the negative binomial, geometric, and hypergeometric distributions.

A discrete random variable $X$ is described by its probability mass function $f(x) = P(X = x)$.  The set of $x$ values for which $f(x) > 0$ is called the *support*. If the distribution depends on unknown parameter(s) $\theta$ we write it as $f(x; \theta)$ (frequentists) or $f(x | \theta)$ (Bayesian). 


### Bernoulli

If $X$ is the result of a trial with two outcomes of probability $P(X = 1) = \pi$ and $P(X = 0) = 1 - \pi$, then $X$ is a random variable with a Bernoulli distribution 

$$f(x) = \pi^x (1 - \pi)^{1 - x}, \hspace{1cm} x \in (0, 1)$$

with $E(X) = \pi$ and $V(X) = \pi(1 - \pi)$.


### Binomial

If $X$ is the count of successful events in $n$ identical and independent Bernoulli trials of success probability $\pi$, then $X$ is a random variable with a binomial distribution $X \sim  Bin(n,\pi)$ 

$$f(x;\pi) = \frac{{n!}}{{x!(n-x)!}} \pi^x (1-\pi)^{n-x} \hspace{1cm} x \in (0, 1, ..., n), \hspace{2mm} \pi \in [0, 1]$$

with $E(X)=n\pi$ and $V(X) = n\pi(1-\pi)$.  

The Bernoulli distribution is a special case of the binomial with $n = 1$. As n increases for fixed $\pi$, the binomial distribution approaches normal distribution $N(n\pi, n\pi(1−\pi))$.  The binomial distribution assumes independent trials - if sampling *without replacement from a finite population*, then the hypergeometric distribution is appropriate.

#### Example {-}

What is the probability 2 out of 10 coin flips are heads if the probability of heads is 0.3?

Function `dbinom()` calculates the binomial probability.

```{r}
dbinom(x = 2, size = 10, prob = 0.3)
```

A simulation of n = 10,000 random samples of size 10 gives a similar result.  `rbinom()` generates a random sample of numbers from the binomial distribution.

```{r message=FALSE, warning=FALSE, fig.height=3, fig.width=5}
data.frame(cnt = rbinom(n = 10000, size = 10, prob = 0.3)) %>%
  count(cnt) %>%
  ungroup() %>%
  mutate(pct = n / sum(n),
         X_eq_x = cnt == 2) %>%
  ggplot(aes(x = as.factor(cnt), y = n, fill = X_eq_x, label = pct)) +
  geom_col(alpha = 0.8) +
  scale_fill_mf() +
  geom_label(aes(label = round(pct, 2)), size = 3, alpha = .6) +
  theme_mf() +
  theme(legend.position = "none") +
  labs(title = "Binomial Distribution", 
       subtitle = paste0(
         "P(X=2) successes in 10 trials when p = 0.3 is ", 
         round(dbinom(2, 10, 0.3), 4), "."
       ),
       x = "Successes",
       y = "Count",
       caption = "Simulation from n = 10,000 binomial random samples.") 
```

#### Example {-}

What is the probability of <=2 heads in 10 coin flips where probability of heads is 0.3?

The cumulative probability is the sum of the first three bars in the simulation above.  Function `pbinom()` calculates the *cumulative* binomial probability.

```{r}
pbinom(q = 2, size = 10, prob = 0.3, lower.tail = TRUE)
```

#### Example {-}

What is the expected number of heads in 25 coin flips if the probability of heads is 0.3?

The expected value, $\mu = np$, is `r 25 * .3`.  Here's an empirical test from 10,000 samples.

```{r}
mean(rbinom(n = 10000, size = 25, prob = .3))
```

The variance, $\sigma^2 = np (1 - p)$, is `r 25 * .3 * (1 - .3)`.  Here's an empirical test.

```{r}
var(rbinom(n = 10000, size = 25, prob = .3))
```


#### Example {-}

Suppose X and Y are independent random variables distributed $X \sim Bin(10, .6)$ and $Y \sim Bin(10, .7)$.  What is the probability that either variable is <=4?

Let $P(A) = P(X<=4)$ and $P(B) = P(Y<=4)$.  Then $P(A|B) = P(A) + P(B) - P(AB)$, and because the events are independent, $P(AB) = P(A)P(B)$.

```{r}
p_a <- pbinom(q = 4, size = 10, prob = 0.6, lower.tail = TRUE)
p_b <- pbinom(q = 4, size = 10, prob = 0.7, lower.tail = TRUE)
p_a + p_b - (p_a * p_b)
```

Here's an empirical test.

```{r}
df <- data.frame(
  x = rbinom(10000, 10, 0.6),
  y = rbinom(10000, 10, 0.7)
  )
mean(if_else(df$x <= 4 | df$y <= 4, 1, 0))
```


### Poission

If $X$ is the number of successes in $n$ (many) trials when the probability of success $\lambda / n$ is small, then $X$ is a random variable with a Poisson distribution $X \sim  Poisson(\lambda)$ 

$$f(x;\lambda) = \frac{e^{-\lambda} \lambda^x}{x!} \hspace{1cm} x \in (0, 1, ...), \hspace{2mm} \lambda > 0$$

with $E(X)=\lambda$ and $V(X) = \lambda$.  

$Poison(\lambda) \rightarrow Bin(n, \pi)$ when $n\pi = \lambda$ and $n \rightarrow \infty$ and $\pi \rightarrow 0$. Because the Poisson is limit of the $Bin(n, \pi)$, it is useful as an approximation to the binomial when $n$ is large ($n>=20$) and $\pi$ small ($p<=0.05$).

When the observed variance is greater than $\lambda$ (overdispersion), the Negative Binomial distribution can be used instead of Poisson.

#### Example {-}

What is the probability of making 2 to 4 sales in a week if the average sales rate is 3 per week?

Function `dpois()` calculates the binomial probability.

```{r warning=FALSE, message = FALSE}
# Using cumulative probability
ppois(q = 4, lambda = 3, lower.tail = TRUE) - 
  ppois(q = 1, lambda = 3, lower.tail = TRUE)

# Using exact probability
dpois(x = 2, lambda = 3) +
  dpois(x = 3, lambda = 3) +
  dpois(x = 4, lambda = 4)
```

```{r echo=FALSE, fig.height=3, fig.width=5}
options(scipen = 999, digits = 2) # sig digits

events <- 0:10
density <- dpois(x = events, lambda = 3)
prob <- ppois(q = events, lambda = 3, lower.tail = TRUE)
df <- data.frame(events, density, prob)
ggplot(df, aes(x = factor(events), y = density)) +
  geom_col() +
  geom_text(
    aes(label = round(density,2), y = density + 0.01),
    position = position_dodge(0.9),
    size = 3,
    vjust = 0
  ) +
  geom_line(data = df, aes(x = events, y = prob), color = "#814E4A", size = 1) +
  theme_mf() +
  scale_fill_mf() +
  labs(title = "PMF and CDF of Poisson Distribution",
       subtitle = "Poisson(3).",
       x = "Events (x)",
       y = "Density")
```

#### Example {-}

Suppose a baseball player has a p=.300 batting average.  What is the probability of X<=150 hits in n=500 at bats? X=150? X>150?

```{r}
ppois(q = 150, lambda = .300 * 500, lower.tail = TRUE)

dpois(x = 150, lambda = .300 * 500)

ppois(q = 150, lambda = .300 * 500, lower.tail = FALSE) 
```

The Poisson distribution approximates the binomial distribution with $\lambda=np$ if $n>=20$ and $p<=0.05$.

#### Example {-}

What is the distribution of successes from a sample of n = 50 when the probability of success is p = .03?
```{r fig.height=3, fig.width=5}
options(scipen = 999, digits = 2) # sig digits

n = 0:10
df <- data.frame(events = 0:10, 
                      Poisson = dpois(x = n, lambda = .03 * 50),
                      Binomial = dbinom(x = n, size = 50, p = .03))
df_tidy <- gather(df, key = "Distribution", value = "density", -c(events))
ggplot(df_tidy, aes(x = factor(events), y = density, fill = Distribution)) +
  geom_col(position = "dodge") +
  theme_mf() +
  scale_fill_mf() +
  labs(title = "Poisson(15) and Binomial(50, .03)",
       subtitle = "Poisson approximates binomial when n >= 20 and p <= .05.",
       x = "Events (x)",
       y = "Density",
       fill = "")

```


#### Example {-}

Suppose the probability that a drug produces a certain side effect is p =  = 0.1% and n = 1,000 patients in a clinical trial receive the drug. What is the probability 0 people experience the side effect?

The expected value is np, `r 1000 * .001`.  The probability of measuring 0 when the expected value is 1 is `dpois(x = 0, lambda = 1000 * .001) = ` `r dpois(x = 0, lambda = 1000 * .001)`.

```{r echo=FALSE, fig.height=3, fig.width=5}
options(scipen = 999, digits = 2) # sig digits

x <- 0:10
density <- dpois(x = x, lambda = 1000 * .001)
prob <- ppois(q = x, lambda = 1000 * .001, lower.tail = TRUE)
df <- data.frame(x, density, prob)
ggplot(df, aes(x = x, y = density)) +
  geom_col() +
  geom_text(
    aes(label = round(density,2), y = density + 0.01),
    position = position_dodge(0.9),
    size = 3,
    vjust = 0
  ) +
  theme_mf() +
  scale_fill_mf() +
  labs(title = "Poisson(1)",
       subtitle = "PMF and CDF of Poisson(1) distribution.",
       x = "Events (x)",
       y = "Density") +
  geom_line(data = df, aes(x = x, y = prob))
```


### Multinomial
### Negative-Binomial

If $X$ is the count of trials required to reach a target number $r$ of successful events in identical and independent Bernoulli trials of success probability $p$, then $X$ is a random variable with a negative-binomial distribution $X \sim  nb(r,p)$ with mean $\mu=r/p$ and variance $\sigma^2 = r(1-p)/p^2$. The probability of $X=x$ trials prior to $r$ successes is 

$$P(X=x) = {{x - 1} \choose {r - 1}} p^r (1-p)^{x-r}.$$

*An oil company has a p = 0.20 chance of striking oil when drilling a well.  What is the probability the company drills x = 7 wells to strike oil r = 3 times?*

$$P(X=7) = {{7 - 1} \choose {3 - 1}} (0.2)^3 (1-0.2)^{(7-3)} = 0.049.$$

Function `dnbinom()` calculates the negative-binomial probability.  Parameter `x` equals the number of failures, $x - r$.

```{r}
dnbinom(x = 4, size = 3, prob = 0.2)
```

Here is a simulation of n = 10,000 random samples.  `rnbinom()` generates a random sample of numbers from the negative-binomial distribution.

```{r message=FALSE, warning=FALSE, fig.height=3, fig.width=5}
data.frame(cnt = rnbinom(n = 10000, size = 3, prob = 0.2)) %>%
  count(cnt) %>%
  ungroup() %>%
  mutate(pct = n / sum(n),
         X_eq_x = cnt == 7-3,
         cnt = cnt + 3) %>%
  filter(cnt < 15) %>%
  ggplot(aes(x = as.factor(cnt), y = n, fill = X_eq_x, label = pct)) +
  geom_col(alpha = 0.8) +
  scale_fill_manual(values = c(my_colors$grey, my_colors$red)) +
  geom_label(aes(label = round(pct, 2)), size = 3, alpha = .6, check_overlap = TRUE) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Negative-Binomial Distribution", 
       subtitle = paste0("P(X=7) trials to reach 3 successes when p = 0.2 is ", round(dnbinom(4, 3, 0.2), 4), "."),
       x = "Trials",
       y = "Count",
       caption = "Simulation from n = 10,000 negative-binomial random samples.") 
```

### Geometric

If $X$ is the count of independent Bernoulli trials of success probability $p$ required to achieve the first successful trial, then $X$ is a random variable with a geometric distribution $X \sim  G(p)$ with mean $\mu=\frac{{n}}{{p}}$ and variance $\sigma^2 = \frac{{(1-p)}}{{p^2}}$ .  The probability of $X=n$ trials is 

$$f(X=n) = p(1-p)^{n-1}.$$

The probability of $X<=n$ trials is 

$$F(X=n) = 1 - (1-p)^n.$$ 

*Example.  A sports marketer randomly selects persons on the street until he encounters someone who attended a game last season. What is the probability the marketer encounters x = 3 people who did not attend a game before the first success if p = 0.20 of the population attended a game?*

Function `pgeom()` calculates the geometric distribution probability.

```{r}
dgeom(x = 3, prob = 0.20)
```

```{r message=FALSE, warning=FALSE, fig.height=3, fig.width=5}
data.frame(cnt = rgeom(n = 10000, prob = 0.20)) %>%
  count(cnt) %>%
  top_n(n = 15, wt = n) %>%
  ungroup() %>%
  mutate(pct = round(n / sum(n), 2),
         X_eq_x = cnt == 3) %>%
  ggplot(aes(x = as.factor(cnt), y = n, fill = X_eq_x, label = pct)) +
  geom_col(alpha = 0.8) +
  scale_fill_manual(values = c(my_colors$grey, my_colors$red)) +
  geom_text(size = 3) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Distribution of trials prior to first success",
       subtitle = paste("P(X = 3) | X ~ G(.2) = ", round(dgeom(3, .2), 4)),
       x = "Unsuccessful trials",
       y = "Count",
       caption = "simulation of n = 10,000 samples from geometric dist.") 
```

### Hypergeometric

If $X$ is the count of successful events in a sample sof size $k$ *without replacement* from a population containing $M$ successes and $N$ non-successes, then $X$ is a random variable with a hypergeometric distribution

$$f(x|m,n,k) = \frac{{{m}\choose{x}}{{n}\choose{k-x}}}{{m+n}\choose{k}}.$$

with $E(X)=k\frac{m}{m+n}$ and $V(X) = k\frac{m}{m+n}\cdot\frac{m+n-k}{m+n}\cdot\frac{n}{m+n-1}$.  

`phyper` returns the cumulative probability (percentile) `p` at the specified value (quantile) `q`.  `qhyper` returns the value (quantile) `q` at the specified cumulative probability (percentile) `p`.


#### Example {-}

What is the probability of selecting $X = 14$ red marbles from a sample of $k = 20$ taken from an urn containing $m = 70$ red marbles and $n = 30$ green marbles?

Function `dhyper()` calculates the hypergeometric probability.

```{r}
x = 14
m = 70
n = 30
k = 20

dhyper(x = x, m = m, n = n, k = k)
```

The expected value is `r k * m / (m + n)` and variance is `r k * m / (m + n) * (m + n - k) / (m + n) * n / (m + n - 1)`. 

```{r message=FALSE, warning=FALSE, echo=FALSE, fig.height=3, fig.width=5}
options(scipen = 999, digits = 2) # sig digits

density = dhyper(x = 1:20, m = m, n = n, k = k)
data.frame(red = 1:20, density) %>%
  mutate(red14 = ifelse(red == 14, "x = 14", "other")) %>%
ggplot(aes(x = factor(red), y = density, fill = red14)) +
  geom_col() +
  geom_text(
    aes(label = round(density,2), y = density + 0.01),
    position = position_dodge(0.9),
    size = 3,
    vjust = 0
  ) +
  theme_mf() +
  scale_fill_mf() +
  labs(title = "PMF of X = x Red Balls",
       subtitle = "Hypergeometric(k = 20, M = 70, N = 30)",
       x = "Number of red balls (x)",
       y = "Density",
       fill = "")
```

The hypergeometric random variable is similar to the binomial random variable except that it applies to situations of sampling *without* replacement from a small population.  As the population size increases, sampling without replacement converges to sampling *with* replacement, and the hypergeometric distribution converges to the binomial. What if the total population size is 250? 500? 1000?

```{r message=FALSE, warning=FALSE, echo=FALSE, fig.height=3, fig.width=5}
library(tidyr)
library(ggplot2)
library(dplyr)
options(scipen = 999, digits = 2) # sig digits

x = 14
m = 7000
n = 3000
k = 20

d_binom <- dbinom(x = 1:20, size = k, prob = m / (m + n))
df_binom <- data.frame(x = 1:20, Binomial = d_binom)
p <- ggplot(df_binom, aes(x = x, y = Binomial)) +
  geom_col()

d_hyper_100 <- dhyper(x = 1:20, m = 70, n = 30, k = k)
d_hyper_250 <- dhyper(x = 1:20, m = 175, n = 75, k = k)
d_hyper_500 <- dhyper(x = 1:20, m = 350, n = 150, k = k)
d_hyper_1000 <- dhyper(x = 1:20, m = 700, n = 300, k = k)
df_hyper = data.frame(x = 1:20, 
                Hyper_0100 = d_hyper_100, 
                Hyper_0250 = d_hyper_250, 
                Hyper_0500 = d_hyper_500, 
                Hyper_1000 = d_hyper_1000)
df_hyper_tidy <- gather(df_hyper, key = "dist", value = "density", -c(x))
p + 
  geom_line(data = df_hyper_tidy, aes(x = x, y = density, color = dist)) +
  theme_mf() +
  scale_color_mf() +
  labs(title = "Hypergeometric Appox. to Binomial",
       subtitle = "Hypergeometric approaches Binomial as population size increases.",
       x = "Number of successful observations (x)",
       y = "Density",
       color = "")
```


## Continuous Distributions

### Normal

Random variable $X$ is distributed $X \sim N(\mu, \sigma^2)$ if

$$f(X)=\frac{{1}}{{\sigma \sqrt{{2\pi}}}}e^{-.5(\frac{{x-\mu}}{{\sigma}})^2}$$.

#### Example {-}

*IQ scores are distributed $X \sim N(100, 16^2$. What is the probability a randomly selected person's IQ is <90?*

```{r message=FALSE, warning=FALSE}
my_mean = 100
my_sd = 16
my_x = 90
# exact
pnorm(q = my_x, mean = my_mean, sd = my_sd, lower.tail = TRUE)
# simulated
mean(rnorm(n = 10000, mean = my_mean, sd = my_sd) <= my_x)

library(dplyr)
library(ggplot2)

data.frame(x = 0:1500 / 10, 
           prob = pnorm(q = 0:1500 / 10, 
                        mean = my_mean, 
                        sd = my_sd, 
                        lower.tail = TRUE)) %>%
  mutate(cdf = ifelse(x > 0 & x <= my_x, prob, 0)) %>%
ggplot() +
  geom_line(aes(x = x, y = prob)) +
  geom_area(aes(x = x, y = cdf), alpha = 0.3) +
  labs(title = bquote('X~N('~mu==.(my_mean)~','~sigma^{2}==.(my_sd)^{2}~')'),
       subtitle = bquote('P(X<='~.(my_x)~') when mean is'~.(my_mean)~' and variance is'~.(my_sd)^{2}~'.'),
       x = "x",
       y = "Probability") 
```


### Example
*IQ scores are distributed *$X \sim N(100, 16^2$*. What is the probability a randomly selected person's IQ is >140?*
```{r message=FALSE, warning=FALSE}
my_mean = 100
my_sd = 16
my_x = 140
# exact
pnorm(q = my_x, mean = my_mean, sd = my_sd, lower.tail = FALSE)
# simulated
mean(rnorm(n = 10000, mean = my_mean, sd = my_sd) > my_x)

library(dplyr)
library(ggplot2)

data.frame(x = 0:1500 / 10, 
           prob = pnorm(q = 0:1500 / 10, 
                        mean = my_mean, 
                        sd = my_sd, 
                        lower.tail = TRUE)) %>%
  mutate(cdf = ifelse(x > my_x & x < 1000, prob, 0)) %>%
ggplot() +
  geom_line(aes(x = x, y = prob)) +
  geom_area(aes(x = x, y = cdf), alpha = 0.3) +
  labs(title = bquote('X~N('~mu==.(my_mean)~','~sigma^{2}==.(my_sd)^{2}~')'),
       subtitle = bquote('P(X<='~.(my_x)~') when mean is'~.(my_mean)~' and variance is'~.(my_sd)^{2}~'.'),
       x = "x",
       y = "Probability") 
```

### Example
*IQ scores are distributed *$X \sim N(100, 16^2$*. What is the probability a randomly selected person's IQ is between 92 and 114?*
```{r message=FALSE, warning=FALSE}
my_mean = 100
my_sd = 16
my_x_l = 92
my_x_h = 114
# exact
pnorm(q = my_x_h, mean = my_mean, sd = my_sd, lower.tail = TRUE) -
  pnorm(q = my_x_l, mean = my_mean, sd = my_sd, lower.tail = TRUE)

library(dplyr)
library(ggplot2)

data.frame(x = 0:1500 / 10, 
           prob = pnorm(q = 0:1500 / 10, 
                        mean = my_mean, 
                        sd = my_sd, 
                        lower.tail = TRUE)) %>%
  mutate(cdf = ifelse(x > my_x_l & x <= my_x_h, prob, 0)) %>%
ggplot() +
  geom_line(aes(x = x, y = prob)) +
  geom_area(aes(x = x, y = cdf), alpha = 0.3) +
  labs(title = bquote('X~N('~mu==.(my_mean)~','~sigma^{2}==.(my_sd)^{2}~')'),
       subtitle = bquote('P(X<='~.(my_x)~') when mean is'~.(my_mean)~' and variance is'~.(my_sd)^{2}~'.'),
       x = "x",
       y = "Probability") 
```


### Example
*Class scores are distributed *$X \sim N(70, 10^2$*. If the instructor wants to give A's to >=85th percentile and B's to 75th-85th percentile, what are the cutoffs?*
```{r message=FALSE, warning=FALSE}
my_mean = 70
my_sd = 10
my_pct_l = .75
my_pct_h = .85

qnorm(p = my_pct_l, mean = my_mean, sd = my_sd, lower.tail = TRUE)
qnorm(p = my_pct_h, mean = my_mean, sd = my_sd, lower.tail = TRUE)

library(dplyr)
library(ggplot2)

data.frame(x = 0:1000 / 10, 
           prob = pnorm(q = 0:1000 / 10, 
                        mean = my_mean, 
                        sd = my_sd, 
                        lower.tail = TRUE)) %>%
  mutate(cdf = ifelse(prob > my_pct_l & prob <= my_pct_h, prob, 0)) %>%
ggplot() +
  geom_line(aes(x = x, y = prob)) +
  geom_area(aes(x = x, y = cdf), alpha = 0.3) +
  labs(title = bquote('X~N('~mu==.(my_mean)~','~sigma^{2}==.(my_sd)^{2}~')'),
       subtitle = bquote('P(X<=x) = ['~.(my_pct_l)~','~.(my_pct_h)~'] when mean is'~.(my_mean)~' and variance is'~.(my_sd)^{2}~'.'),
       x = "x",
       y = "Probability") 
```


### Normal Approximation to Binomial

The CLT implies that certain distributions can be approximated by the normal distribution.  

The binomial distribution $X \sim B(n,p)$ is approximately normal with mean $\mu = n p$ and variance $\sigma^2=np(1-p)$.  The approximation is useful when the expected number of successes and failures is at least 5:  $np>=5$ and $n(1-p)>=5$.

### Example
*A measure requires p>=50% popular to pass.  A sample of n=1,000 yields x=460 approvals. What is the probability that the overall population approves, P(X)>0.5?*
```{r message=FALSE, warning=FALSE}
my_x = 460
my_p = 0.50
my_n = 1000

my_mean = my_p * my_n
my_sd = round(sqrt(my_n * my_p * (1 - my_p)), 1)

# Exact binomial
pbinom(q = my_x, size = my_n, prob = my_p, lower.tail = TRUE)

# Normal approximation
pnorm(q = my_x, mean = my_p * my_n, sd = sqrt(my_n * my_p * (1 - my_p)), lower.tail = TRUE)

library(dplyr)
library(ggplot2)
library(tidyr)

data.frame(x = 400:600, 
           Normal = pnorm(q = 400:600, 
                        mean = my_p * my_n, 
                        sd = sqrt(my_n * my_p * (1 - my_p)), 
                        lower.tail = TRUE),
           Binomial = pbinom(q = 400:600, 
                        size = my_n, 
                        prob = my_p, 
                        lower.tail = TRUE)) %>%
  gather(key = "Distribution", value = "cdf", c(-x)) %>%
  ggplot(aes(x = x, y = cdf, color = Distribution)) +
  geom_line() +
  labs(title = bquote('X~B(n='~.(my_n)~', p='~.(my_p)~'),  '~'X~N('~mu==.(my_mean)~','~sigma^{2}==.(my_sd)^{2}~')'),
       subtitle = "Normal approximation to the binomial",
       x = "x",
       y = "Probability") 
```


The Poisson distribution $x~P(\lambda)$ is approximately normal with mean $\mu = \lambda$ and variance $\sigma^2 = \lambda$, for large values of $\lambda$.

### Example
*The annual number of earthquakes registering at least 2.5 on the Richter Scale and having an epicenter within 40 miles of downtown Memphis follows a Poisson distribution with mean *$\lambda=6.5$*. What is the probability that at least *$x>=9$* such earthquakes will strike next year?*
```{r message=FALSE, warning=FALSE}
my_x = 9
my_lambda = 6.5
my_sd = round(sqrt(my_lambda), 2)

# Exact Poisson
ppois(q = my_x - 1, lambda = my_lambda, lower.tail = FALSE)

# Normal approximation
pnorm(q = my_x - 0.5, mean = my_lambda, sd = my_sd, lower.tail = FALSE)

library(dplyr)
library(ggplot2)
library(tidyr)

data.frame(x = 0:200 / 10, 
           Normal = pnorm(q = 0:200 / 10, 
                        mean = my_lambda, 
                        sd = my_sd, 
                        lower.tail = TRUE),
           Poisson = ppois(q = 0:200 / 10, 
                        lambda = my_lambda, 
                        lower.tail = TRUE)) %>%
  gather(key = "Distribution", value = "cdf", c(-x)) %>%
  ggplot(aes(x = x, y = cdf, color = Distribution)) +
  geom_line() +
  labs(title = bquote('X~P('~lambda~'='~.(my_lambda)~'),  '~'X~N('~mu==.(my_lambda)~','~sigma^{2}==.(my_lambda)~')'),
       subtitle = "Normal approximation to the Poisson",
       x = "x",
       y = "Probability") 
```

### From Sample to Population

*Suppose a person's blood pressure typically measures 160?20 mm.  If one takes n=5 blood pressure readings, what is the probability the average will be <=150?*
```{r message=FALSE, warning=FALSE}
my_mu = 160
my_sigma = 20
my_n = 5
my_x = 150

my_se = round(my_sigma / sqrt(my_n), 1)

pnorm(q = my_x, mean = my_mu, sd = my_sigma / sqrt(my_n), lower.tail = TRUE)

library(dplyr)
library(ggplot2)

data.frame(x = 1000:2000 / 10, 
           prob = pnorm(q = 1000:2000 / 10, 
                        mean = my_mu, 
                        sd = my_sigma / sqrt(my_n), 
                        lower.tail = TRUE)) %>%
  mutate(cdf = ifelse(x > 0 & x <= my_x, prob, 0)) %>%
ggplot() +
  geom_line(aes(x = x, y = prob)) +
  geom_area(aes(x = x, y = cdf), alpha = 0.3) +
  labs(title = bquote('X~N('~mu==.(my_mu)~','~sigma^{2}==.(my_se)^{2}~')'),
       subtitle = bquote('P(X<='~.(my_x)~') when mean is'~.(my_mu)~' and variance is'~sigma~'/sqrt(n)'~.(my_se)^{2}~'.'),
       x = "x",
       y = "Probability") 
```

```{r}
knitr::include_app("https://mpfoley73.shinyapps.io/shiny_dist/", 
  height = "600px")
```


## Likelihood

The *likelihood function* is the likelihood of a parameter $\theta$ given an observed value.  The likelihood function is identical to the probability distribution function, except that it reverses which variable is considered fixed.  E.g., the binomial distribution $Bin(n, \pi)$ expresses the probability that $X = x$ given a sample of size $n$ from a binomial distribution with success probability $\pi$.

$$f(x|\theta) = P(X=x|\pi) = \frac{{n!}}{{x!(n-x)!}} p^x (1-p)^{n-x}.$$

The corresponding *likelihood* function expresses the probability that $\pi = p$ given the observed value $x$.

$$L(\theta|x) = L(\pi|x) = \frac{{n!}}{{x!(n-x)!}} \pi^x (1-\pi)^{n-x} \propto \pi^x (1-\pi)^{n-x}.$$

Any multiplicative constant which does not depend on $\theta$ is irrevelant and can be discarded, so

$$L(\pi|x) =  \pi^x (1-\pi)^{n-x}$$

In most cases, you work with the *loglikelihood*, $l(\theta|x) = \log L(\theta|x)$.  So for the binomial distribution, $l(\pi|x) = x \log \pi + (n - x) \log (1 - \pi)$.  Frequently you derive loglikelihood from a sample.  The overall likelihood is the product of the individual likelihoods, and the overall loglikelihood is the log of the overall likelihood.

$$l(\theta|x) = \log \prod_{i=1}^n f(x_i|\theta)$$

You usually want to know the *maximum* of the likelihood function.  The value of $\theta$ that maximizes $L$ is the maximum-likelihood estimator of $\theta$.  Suppose an experiment had $n=5$ Bernoulli trials, and $X = 3$ where successes.  $X \sim Bin(5, p)$.  What is the MLE?  A plot of $L(p|x) = p^3(1 - p)^2$ is maximized at $p = 0.6$.

```{r}
library(tidyverse)
p <- 0:100*.01
L <- p^3 * (1 - p)^2
dat <- data.frame(L, p)
ggplot(dat, aes(x = p, y = L)) +
  geom_line() +
  geom_vline(xintercept = 0.6)
```

Calculating the MLE is usually simplified using the loglikelihood function.




<!--chapter:end:01-probability.Rmd-->

# Inference


<!--chapter:end:02-inference.Rmd-->

# Experiments

Some _significant_ applications are demonstrated in this chapter.

## Example one

## Example two

<!--chapter:end:03-experiment.Rmd-->

# Regression

<!--chapter:end:04-regression.Rmd-->

# Generalized Linear Models

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(broom)  # for augment()
library(WVPlots)  # for GainCurvePlot()
library(caret)  # for RMSE()
library(readr)
```


These notes are primarily from PSU Online course [Analysis of Discrete Data](https://online.stat.psu.edu/stat504) which uses Alan Agresti's **Categorical Data Analysis** (which I have not yet purchased) [@Agresti2013].  I also reviewed:

Penn State University, STAT 501, "Lesson 15: Logistic, Poisson & Nonlinear Regression".  [https://newonlinecourses.science.psu.edu/stat501/lesson/15](https://newonlinecourses.science.psu.edu/stat501/lesson/15)

"Generalized Linar Models in R".  DataCamp.  [https://www.datacamp.com/courses/generalized-linear-models-in-r](https://www.datacamp.com/courses/generalized-linear-models-in-r).

"Multiple and Logistic Regression".  DataCamp. [https://www.datacamp.com/courses/multiple-and-logistic-regression](https://www.datacamp.com/courses/multiple-and-logistic-regression).

Molnar, Christoph. "Interpretable machine learning. A Guide for Making Black Box Models Explainable", 2019. [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/).


In generalized linear models (GLMs), the modeled response is *a function* of the mean of $y$.  There are three components to a GLM.  The *random component* is the probability distribution of the response variable (normal, binomial, Poisson, etc.). The *systematic component* is the explanatory variables $X\beta$.  The *link function* $\eta$ specifies the link between random and systematic components. The link function converts the response value from a range $[0,1]$ in logistic and probit and $[0,+\infty]$ for Poisson to a value ranging from $[-\infty, +\infty]$, and creates a linear relationship with the predictor variables.  

For a standard linear regression, the link function is the identity function,

$$f(\mu_Y) = \mu_Y.$$

The standard linear regression is thus a special case of the GLM.  For a logistic regression, the link function is 

$$f(\mu_Y) = \ln(\frac{\pi}{1-\pi})$$

where $\pi$ is the event probability.  For a probit regression, the link function is 

$$f(\mu_Y) = \Phi^{-1}(\pi).$$

The difference between logistic and probit link function is theoretical - [the practical significance is slight](https://www.theanalysisfactor.com/the-difference-between-logistic-and-probit-regression/).  Logistic regression has the advantage that it can be back-transformed from log odds to odds ratios.  For a Poisson regression, the link function is

$$f(\mu_Y) = \ln(\lambda)$$

where $\lambda$ is the expected event rate.  

GLM uses maximum likelihood estimation (MLE) rather than ordinary least squares (OLS) to estimate the parameters, and thus relies on large-sample approximations. 

In R, specify a GLM just like an linear model, but with the `glm()` function, specifying the distribution with the `family` parameter.

* `family = "gaussian"`: linear regression
* `family = "binomial"`: logistic regression
* `family = binomial(link = "probit")`: probit regression
* `family = "poisson"`: Poisson regression


## Logistic Regression

Logistic regression estimates the probability of a particular level of a categorical response variable given a set of predictors. The response levels can be binary, nominal (multiple categories), or ordinal (multiple levels).  

The **binary** logistic regression model is

$$y_i = logit(\pi_i) = \log\left(\frac{\pi_i}{1-\pi_i}\right) = X_i\beta$$

where $\pi_i$ is the "success probability" that observation $i$ is in a specified category of the binary *y* variable.  You can solve for $\pi$ to get

$$\pi = \frac{\exp(X \beta)}{1 + \exp(X \beta)}.$$ 
 
The model predicts the *log odds* of the response variable.  The maximum likelihood estimator maximizes the the likelihood function

$$L(\beta; y, X) = \prod_{i=1}^n \pi_i^{y_i}(1 - \pi_i)^{(1-y_i)} = \prod_{i=1}^n\frac{\exp(y_i X_i \beta)}{1 + \exp(X_i \beta)}.$$

There is no closed-form solution, so GLM estimates coefficients with interatively reweighted least squares.  


### Example {-}

Dataset `leuk` contains response variable `REMISS` indicating whether leukemia remission occurred (1|0) and several explanatory variables.

```{r}
data_dir <- "C:/Users/mpfol/OneDrive/Documents/Data Science/Data/"
leuk <- read_tsv(paste(data_dir, "leukemia_remission.txt", sep = "/"))
str(leuk)
```

Fit a logistic regression in R using `glm(formula, data, family = binomial)` where `family = binomial` specifies a binomial error distribution.

```{r}
m1 <- glm(REMISS ~ ., family = binomial, data = leuk)
m1
summary(m1)
```

The *predicted* value $\hat{y}$ is the estimated **log odds** of the response variable, 

$$\hat{y} = X \hat{\beta} = \ln (\frac{\pi}{1 - \pi}).$$

Suppose each predictor equals its mean value, then the log odds of `REMISS` is $-2.684$.  

```{r}
pred <- predict(m1, newdata = data.frame(CELL = mean(leuk$CELL),
                                         SMEAR = mean(leuk$SMEAR),
                                         INFIL = mean(leuk$INFIL),
                                         LI = mean(leuk$LI),
                                         BLAST = mean(leuk$BLAST),
                                         TEMP = mean(leuk$TEMP)))
```

Log odds are not easy to interpet, but it is convenient for updating prior probabilities in Bayesian analyses.  *See [this article](https://www.statisticshowto.datasciencecentral.com/log-odds/) in Statistics How To.*  Exponentiate the log odds to get the more intuitive **odds**.

$$\exp (\hat{y}) = \exp (X \hat{\beta}) = \frac{\pi}{1 - \pi}.$$

The odds of having achieved remission when each predictor equals its mean value is $\exp(\hat{y}) = 0.068$.  

```{r}
exp(pred)
```

You might express that more commonly as 1 / 0.068 = 15:1.  So a person with average values of the predictors has an odds of "15 to 1" of having achieved remission.

```{r}
1/exp(pred)
```

Or, solve for $\pi$ to get the **probability**.  

$$\pi = \frac{\exp (X \beta)}{1 + \exp (X \beta)}$$

The probability of having achieved remission when each predictor equals its mean value is $\pi = 0.064$.  The `predict()` function for a logistic model returns log-odds, but can also return $\pi$ by specifying parameter `type = "response"`.

```{r}
exp(pred) / (1 + exp(pred))
prob <- predict(m1, newdata = data.frame(CELL = mean(leuk$CELL),
                                         SMEAR = mean(leuk$SMEAR),
                                         INFIL = mean(leuk$INFIL),
                                         LI = mean(leuk$LI),
                                         BLAST = mean(leuk$BLAST),
                                         TEMP = mean(leuk$TEMP)),
                type = "response")
```

It is common to express the results in terms of the **odds ratio**.  The *odds ratio* is the ratio of the odds before and after an increment to the predictors.  It tells you how much the odds would be multiplied after a $X_1 - X_0$ unit increase in $X$.

$$\theta = \frac{\pi / (1 - \pi) |_{X = X_1}}{\pi / (1 - \pi) |_{X = X_0}} = \frac{\exp (X_1 \hat{\beta})}{\exp (X_0 \hat{\beta})} = \exp ((X_1-X_0) \hat{\beta}) = \exp (\delta \hat{\beta})$$

For example, increasing `LI` by .01 increases the odds of remission by a factor of $\exp(0.1 \cdot 4.36) = 1.547$ (from 15:1 to 23:1). 

```{r}
exp(.1 * m1$coefficients)
```

You can calculate an odds ratio using `oddsratio::or_glm()`.

```{r warning=FALSE}
library(oddsratio)
or_glm(data = leuk, 
       model = m1, 
       incr = list(CELL = 0.01, 
                   SMEAR = 0.01, 
                   INFIL = 5, 
                   LI = 0.1, 
                   BLAST = 1.0, 
                   TEMP = 0.3))
```

The predicted values can also be expressed as the probabilities $\pi$.  This produces the familiar signmoidal shape of the binary relationship.

```{r}
augment(m1, type.predict = "response") %>%
  ggplot(aes(x = LI, y = REMISS)) +
  geom_point() +
  geom_line(aes(y = .fitted), color = "red") + 
  labs(x = "LI",
       y = "Probability of Event",
       title = "Binary Fitted Line Plot")
```

Whereas in linear regression the the coefficient p-values use the *t* test (*t* statistic), logistic regression coefficient p-values use the *Wald test* **Z*-statistic).

$$Z = \frac{\hat{\beta_i}}{SE(\hat{\beta}_i)}$$

```{r}
round((z <- m1$coefficients / summary(m1)$coefficients[,"Std. Error"]), 3)
round(pnorm(abs(z), lower.tail = FALSE) * 2, 3)
```

Evaluate a logistic model fit with an analysis of deviance.  Deviance is defined as -2 times the log-likelihood $-2l(\beta)$.  The null deviance is the deviance of the null model and is analagous to SST in ANOVA.  The residual deviance is analagous to SSE in ANOVA.

```{r}
logLik(glm(REMISS ~ ., data = leuk, family = "binomial")) * (-2)
anova(m1)
m1
summary(m1)
```

The deviance of the null model (no regressors) is 34.372.  The deviance of the full model is 26.073.  

```{r}
glance(m1)
```

Use the `GainCurvePlot()` function to plot the gain curve (background on gain curve at [Data Science Central](https://www.datasciencecentral.com/profiles/blogs/understanding-and-interpreting-gain-and-lift-charts) from the model predictions. The x-axis is the fraction of items seen when sorted by the predicted value, and the y-axis is the cumulative summed true outcome. The "wizard" curve is the gain curve when the data is sorted by the true outcome.  If the model's gain curve is close to the wizard gain curve, then the model sorted the response variable well. The grey area is the gain over a random sorting.

```{r}
augment(m1) %>% data.frame() %>% 
  GainCurvePlot(xvar = ".fitted", truthVar = "REMISS", title = "Logistic Model")
```

`REMISS` equals 1 in 9 of the 27 responses. 

* The wizard curve shows that after sorting the responses it encounters all 9 1s (100%) after looking at 9 of the 27 response (33%).  
* The bottom of the grey diagonal shows that after making random predictions and sorting the predictions, it encounters only 3 1s (33%) after looking at 9 of the 27 responses (33%).  It has to look at all 27 responses (100%) to encounter all 9 1s (100%).  
* The gain curve encounters 5 1s (55%) after looking at 9 of the 27 responses (33%).  It has to look at 14 responses to encounter all 9 1s (100%).

Another way to evaluate the predictive model is the ROC curve.  It evaluates all possible thresholds for splitting predicted probabilities into predicted classes.  This is often a much more useful metric than simply ranking models by their accuracy at a set threshold, as different models might require different calibration steps (looking at a confusion matrix at each step) to find the optimal classification threshold for that model.

```{r}
library(caTools)
colAUC(m1$fitted.values, m1$data$REMISS, plotROC = TRUE)
```


## Poisson Regression

Poisson models count data, like "traffic tickets per day", or "website hits per day".  The response is an expected *rate* or intensity.  For count data, specify the generalized model, this time with `family = poisson` or `family = quasipoisson`. 

Recall that the probability of achieving a count $y$ when the expected rate is $\lambda$ is distributed 

$$P(Y = y|\lambda) = \frac{e^{-\lambda} \lambda^y}{y!}.$$


The poisson regression model is

$$\lambda = \exp(X \beta).$$ 
 
You can solve this for $y$ to get

$$y = X\beta = \ln(\lambda).$$

That is, the model predicts the log of the response rate.  For a sample of size *n*, the likelihood function is

$$L(\beta; y, X) = \prod_{i=1}^n \frac{e^{-\exp({X_i\beta})}\exp({X_i\beta})^{y_i}}{y_i!}.$$

The log-likelihood is

$$l(\beta) = \sum_{i=1}^n (y_i X_i \beta - \sum_{i=1}^n\exp(X_i\beta) - \sum_{i=1}^n\log(y_i!).$$

Maximizing the log-likelihood has no closed-form solution, so the coefficient estimates are found through interatively reweighted least squares.  

Poisson processes assume the variance of the response variable equals its mean.  "Equals" means the mean and variance are of a similar order of magnitude.  If that assumption does not hold, use the quasi-poisson.  Use Poisson regression for large datasets.  If the predicted counts are much greater than zero (>30), the linear regression will work fine.  Whereas RMSE is not useful for logistic models, it is a good metric in Poisson.


### Example {-}

Dataset `fire` contains response variable `injuries` counting the number of injuries during the month and one explanatory variable, the month `mo`.

```{r}
fire <- read_csv(file = "C:/Users/mpfol/OneDrive/Documents/Data Science/Data/CivilInjury_0.csv")
fire <- fire %>% 
  mutate(mo = as.POSIXlt(`Injury Date`)$mon + 1) %>%
  rename(dt = `Injury Date`,
         injuries = `Total Injuries`)
str(fire)
```

In a situation like this where there the relationship is bivariate, start with a visualization.

```{r}
ggplot(fire, aes(x = mo, y = injuries)) +
  geom_jitter() +
  geom_smooth(method = "glm", method.args = list(family = "poisson")) +
  labs(title = "Injuries by Month")
```


Fit a poisson regression in R using `glm(formula, data, family = poisson)`.  But first, check whether the mean and variance of `injuries` are the same magnitude?  If not, then use `family = quasipoisson`.

```{r}
mean(fire$injuries)
var(fire$injuries)
```

They are of the same magnitude, so fit the regression with `family = poisson`.

```{r}
m2 <- glm(injuries ~ mo, family = poisson, data = fire)
summary(m2)
```

The *predicted* value $\hat{y}$ is the estimated **log** of the response variable, 

$$\hat{y} = X \hat{\beta} = \ln (\lambda).$$

Suppose `mo` is January (mo = `), then the log of `injuries` is $\hat{y} = 0.323787$. Or, more intuitively, the expected count of injuries is $\exp(0.323787) = 1.38$  

```{r}
predict(m2, newdata = data.frame(mo=1))
predict(m2, newdata = data.frame(mo=1), type = "response")
```

Here is a plot of the predicted counts in red.

```{r}
augment(m2, type.predict = "response") %>%
  ggplot(aes(x = mo, y = injuries)) +
  geom_point() +
  geom_point(aes(y = .fitted), color = "red") + 
  scale_y_continuous(limits = c(0, NA)) +
  labs(x = "Month",
       y = "Injuries",
       title = "Poisson Fitted Line Plot")
```

Evaluate a logistic model fit with an analysis of deviance.  

```{r}
(perf <- glance(m2))
(pseudoR2 <- 1 - perf$deviance / perf$null.deviance)
```

The deviance of the null model (no regressors) is 139.9.  The deviance of the full model is 132.2.  The psuedo-R2 is very low at .05.  How about the RMSE?

```{r}
RMSE(pred = predict(m2, type = "response"), obs = fire$injuries)
```

The average prediction error is about 0.99.  That's almost as much as the variance of `injuries` - i.e., just predicting the mean of `injuries` would be almost as good!  Use the `GainCurvePlot()` function to plot the gain curve.

```{r}
augment(m2, type.predict = "response") %>%
  ggplot(aes(x = injuries, y = .fitted)) +
  geom_point() +
  geom_smooth(method ="lm") +
  labs(x = "Actual",
       y = "Predicted",
       title = "Poisson Fitted vs Actual")
```


```{r}
augment(m2) %>% data.frame() %>% 
  GainCurvePlot(xvar = ".fitted", truthVar = "injuries", title = "Poisson Model")
```

It seems that `mo` was a poor predictor of `injuries`.  


<!--chapter:end:05-glm.Rmd-->

# Classification

<!--chapter:end:06-chisq.Rmd-->

# Classification

<!--chapter:end:07-anova.Rmd-->

# Decision Trees

Decision trees, also known as classification and regression tree (CART) models, are tree-based methods for supervised machine learning.  Simple *classification trees* and *regression trees* are easy to use and interpret, but are not competitive with the best machine learning methods. However, they form the foundation for **bagged trees**, **random forests**, and **boosted trees** models, which although less interpretable, are very accurate.

CART models segment the predictor space into $K$ non-overlapping terminal nodes (leaves), $A_1, A_2, \dots, A_K$.  Each node is described by a set of rules which can be used to predict new responses. The predicted value $\hat{y}$ for each node is the mode (classification), or mean (regression).

CART models define the nodes through a *top-down greedy* process called *recursive binary splitting*. The process is *top-down* because it begins at the top of the tree with all observations in a single region and successively splits the predictor space. It is *greedy* because at each splitting step, the best split is made at that particular step without consideration to subsequent splits.

The best split is the predictor variable and cutpoint that minimizes a cost function. For a regression tree, the most common cost function is the sum of squared residuals, 

$$RSS = \sum_{k=1}^K\sum_{i \in A_k}{\left(y_i - \hat{y}_{A_k} \right)^2}.$$

For a classification tree, the most common cost functions are the Gini index, 

$$G = \sum_{c=1}^C{\hat{p}_{kc}(1 - \hat{p}_{kc})},$$

or the entropy 

$$D = - \sum_{c=1}^C{\hat{p}_{kc} \log \hat{p}_{kc}}$$

where $\hat{p}_{kc}$ is the proportion of training observations in node $k$ node that are class $c$.  A completely *pure* node in a binary tree will have $\hat{p} \in [0, 1]$ and $G = D = 0$. A completely impure node in a binary tree will have $\hat{p} = 0.5$ and $G = 0.5^2 \cdot 2 = 0.25$ and $D = -(0.5 \log(0.5)) \cdot 2 = 0.69$.

CART repeats the splitting process for each of the child nodes until a *stopping criterion* is satisfied, usually when no node size surpasses a predefined maximum, or continued splitting does not improve the model significantly.  CART may also impose a minimum number of observations in each node.

The resulting tree likely over-fits the training data and therefore does not generalize well to test data, so CART *prunes* the tree, minimizing the cross-validated prediction error. Rather than cross-validating every possible subtree to find the one with minimum error, CART uses *cost-complexity pruning*. Cost-complexity is the tradeoff between error (cost) and tree size (complexity) where the tradeoff is quantified with cost-complexity parameter $c_p$.  In the equation below, the cost complexity of the tree $R_{c_p}(T)$ is the sum of its risk (error) plus a "cost complexity" factor $c_p$ multiple of the tree size $|T|$.  

$$R_{c_p}(T) = R(T) + c_p|T|$$

$c_p$ can take on any value from $[0..\infty]$, but it turns out there is an optimal tree for *ranges* of $c_p$ values, so there are only a finite set of *interesting* values for $c_p$ [@James2013] [@Therneau2019] [@Kuhn2016].  A parametric algorithm identifies the interesting $c_p$ values and their associated pruned trees, $T_{c_p}$.

CART uses cross-validation to determine which $c_p$ is optimal.


## Classification Tree

A simple classification tree is rarely performed on its own; the bagged, random forest, and gradient boosting methods build on this logic. However, it is good to start here to build understanding. I'll learn by example. Using the `ISLR::OJ` data set, I will predict which brand of orange juice, Citrus Hill (CH) or Minute Maid = (MM), customers `Purchase` using from the 17 feature variables.  Load the libraries and data.

```{r warning=FALSE, message=FALSE}
library(ISLR)  # OJ dataset
library(rpart)  # classification and regression trees 
library(caret)  # modeling workflow
library(rpart.plot)  # better formatted plots than the ones in rpart
library(plotROC)  # ROC curves
library(ROCR)
library(tidyverse)
library(skimr)  # neat alternative to glance & summary

oj_dat <- OJ
#skim_with(numeric = list(p0 = NULL, p25 = NULL, p50 = NULL, p75 = NULL, 
#                                p100 = NULL, hist = NULL))
#skim(oj_dat)
```

I'll split `oj_dat` (n = 1,070) into `oj_train` (80%, n = 857) and `oj_test` (20%, n = 213).  I'll fit a simple decision tree with `oj_train`, then later a bagged tree, a random forest, and a gradient boosting tree.  I'll compare their predictive performance with `oj_test`. 

```{r}
set.seed(12345)
partition <- createDataPartition(y = oj_dat$Purchase, p = 0.8, list = FALSE)
oj_train <- oj_dat[partition, ]
oj_test <- oj_dat[-partition, ]
```

Function `rpart::rpart()` builds a full tree, minimizing the Gini index $G$ by default (`parms = list(split = "gini")`), until the stopping criterion is satisfied.  The default stopping criterion is 

* only attempt a split if the current node as at least `minsplit = 20` observations,
* only accept a split if each of the two resulting nodes have at least `minbucket = round(minsplit/3)` observations, and 
* only accept a split if the resulting overall fit improves by `cp = 0.01` (i.e., $\Delta G <= 0.01$).

```{r}
set.seed(123)
oj_model_1 <- rpart(
   formula = Purchase ~ .,
   data = oj_train,
   method = "class"  # "class" for classification, "anova" for regression
   )
print(oj_model_1)
```

The output starts with the root node.  The predicted class at the root is `CH` and this prediction produces 334 errors on the 857 observations for a success rate of 0.61026838 and an error rate of 0.38973162.  The child nodes of node "x" are labeled 2x) and 2x+1), so the child nodes of 1) are 2) and 3), and the child nodes of 2) are 4) and 5).  Terminal nodes are labeled with an asterisk (*).  

Surprisingly, only 3 of the 17 features were used the in full tree: `LoyalCH` (Customer brand loyalty for CH), `PriceDiff` (relative price of MM over CH), and `SalePriceMM` (absolute price of MM).  The first split is at `LoyalCH` = 0.48285.  Here is what the full (unpruned) tree looks like.

```{r}
rpart.plot(oj_model_1, yesno = TRUE)
```

The boxes show the node classification (based on mode), the proportion of observations that are *not* `CH`, and the proportion of observations included in the node. 

`rpart()` not only grew the full tree, it identified the set of cost complexity parameters, and measured the model performance of each corresponding tree using cross-validation. `printcp()` displays the candidate $c_p$ values. You can use this table to decide how to prune the tree.

```{r}
printcp(oj_model_1)
```

There are 4 $c_p$ values in this model.  The model with the smallest complexity parameter allows the most splits (`nsplit`).  The highest complexity parameter corresponds to a tree with just a root node.  `rel error` is the error rate relative to the root node.  The root node absolute error is 0.38973162 (the proportion of MM), so its `rel error` is 0.38973162/0.38973162 = 1.0.  That means the absolute error of the full tree (at CP = 0.01) is 0.42814 * 0.38973162 = 0.1669.  You can verify that by calculating the error rate of the predicted values:

```{r}
data.frame(pred = predict(oj_model_1, newdata = oj_train, type = "class")) %>%
   mutate(obs = oj_train$Purchase,
          err = if_else(pred != obs, 1, 0)) %>%
   summarize(mean_err = mean(err))
```

Finishing the CP table tour, `xerror` is the relative cross-validated error rate and `xstd` is its standard error. If you want the lowest possible error, then prune to the tree with the smallest relative CV error (`xerror`) ($c_p$ = 0.01, CV error = `r round(0.46407*0.38973162, 4)`).  If you want to balance predictive power with simplicity, prune to the smallest tree within 1 SE of the one with the smallest relative error.  The CP table is not super-helpful for finding that tree. I'll add a column to find it.

```{r}
oj_model_1$cptable %>%
   data.frame() %>%
   mutate(min_xerror_idx = which.min(oj_model_1$cptable[, "xerror"]),
          rownum = row_number(),
          xerror_cap = oj_model_1$cptable[min_xerror_idx, "xerror"] + 
             oj_model_1$cptable[min_xerror_idx, "xstd"],
          eval = case_when(rownum == min_xerror_idx ~ "min xerror",
                           xerror < xerror_cap ~ "under cap",
                           TRUE ~ "")) %>%
   select(-rownum, -min_xerror_idx) 
```

The simplest tree using the 1-SE rule is $c_p = 0.01347305, CV error = `r round(0.4700599*0.38973162, 4)`).  Fortunately, `plotcp()` presents a nice graphical representation of the relationship between `xerror` and `cp`.

```{r}
plotcp(oj_model_1, upper = "splits")
```

The dashed line is set at the minimum `xerror` + `xstd`.  The top axis shows the number of splits in the tree.  I'm not sure why the CP values are not the same as in the table (they are close, but not the same).  The figure suggests I should prune to 5 or 3 splits.  I see this curve never really hits a minimum - it is still decreasing at 5 splits.  The default tuning parameter value `cp = 0.01` may be too small, so I'll set it to `cp = 0.001` and start over.

```{r}
set.seed(123)
oj_model_1b <- rpart(
   formula = Purchase ~ .,
   data = oj_train,
   method = "class",
   cp = 0.001
   )
print(oj_model_1b)
```

This is a much larger tree.  Did I find a `cp` value that produces a local min?

```{r}
plotcp(oj_model_1b, upper = "splits")
```

Yes, the min is at CP = 0.011 with 5 splits.  The min + 1 SE is at CP = 0.021 with 3 splits.  I'll prune the tree to 3 splits.

```{r}
oj_model_1b_pruned <- prune(
   oj_model_1b,
   cp = oj_model_1b$cptable[oj_model_1b$cptable[, 2] == 3, "CP"]
)
rpart.plot(oj_model_1b_pruned, yesno = TRUE)
```

The most "important" indicator of `Purchase` appears to be `LoyalCH`.  From the **rpart** [vignette](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf) (page 12), 

> "An overall measure of variable importance is the sum of the goodness of split measures for each split for which it was the primary variable, plus goodness (adjusted agreement) for all splits in which it was a surrogate."

Surrogates refer to alternative features for a node to handle missing data. For each split, CART evaluates a variety of alternative "surrogate" splits to use when the feature value for the primary split is NA. Surrogate splits are splits that produce results similar to the original split. 

A variable's importance is the sum of the improvement in the overall Gini (or RMSE) measure produced by the nodes in which it appears. Here is the variable importance for this model.

```{r}
oj_model_1b_pruned$variable.importance
```

```{r}
oj_model_1b_pruned$variable.importance %>% 
   data.frame() %>%
   rownames_to_column(var = "Feature") %>%
   rename(Overall = '.') %>%
   ggplot(aes(x = fct_reorder(Feature, Overall), y = Overall)) +
   geom_pointrange(aes(ymin = 0, ymax = Overall), color = "cadetblue", size = .3) +
   theme_minimal() +
   coord_flip() +
   labs(x = "", y = "", title = "Variable Importance with Simple Classication")
```

`LoyalCH` is by far the most important variable, as expected from its position at the top of the tree, and one level down. 

You can see how the surrogates appear in the model with the `summary()` function.

```{r}
summary(oj_model_1b_pruned)
```

The last step is to make predictions on the validation data set.  For a classification tree, set argument `type = "class"`. 

```{r message=FALSE, warning=FALSE}
oj_model_1b_preds <- predict(oj_model_1b_pruned, oj_test, type = "class")
```

I'll evaluate the predictions and record the accuracy (correct classification percentage) for comparison to other models. Two ways to evaluate the model are the confusion matrix, and the ROC curve. 


### Confusion Matrix

Print the confusion matrix with `caret::confusionMatrix()` to see how well does this model performs against the test data set.

```{r}
oj_model_1b_cm <- confusionMatrix(data = oj_model_1b_preds, reference = oj_test$Purchase)
oj_model_1b_cm
```

The confusion matrix is at the top.  It also includes a lot of statistics.  It's worth getting familiar with the stats.  The model accuracy and 95% CI are calculated from the binomial test.

```{r}
binom.test(x = 113 + 70, n = 213)
```

The "No Information Rate" (NIR) statistic is the class rate for the largest class.  In this case CH is the largest class, so NIR = 130/213 = 0.6103.  "P-Value [Acc > NIR]" is the binomial test that the model accuracy is significantly better than the NIR (i.e., significantly better than just always guessing CH).

```{r}
binom.test(x = 113 + 70, n = 213, p = 130/213, alternative = "greater")
```

The "Accuracy" statistic indicates the model predicts 0.8590 of the observations correctly.  That's good, but less impressive when you consider the prevalence of CH is 0.6103 - you could achieve 61% accuracy just by predicting CH every time. A measure that controls for the prevalence is Cohen's kappa statistic. The kappa statistic is explained [here](https://standardwisdom.com/softwarejournal/2011/12/confusion-matrix-another-single-value-metric-kappa-statistic/). It compares the accuracy to the accuracy of a "random system".  It is defined as

$$\kappa = \frac{Acc - RA}{1-RA}$$

where 

$$RA = \frac{ActFalse \times PredFalse + ActTrue \times PredTrue}{Total \times Total}$$

is the hypotheical probability of a chance agreement.  ActFalse will be the number of "MM" (13 + 70 = 83) and actual true will be the number of "CH" (113 + 17 = 130).  The predicted counts are

```{r}
table(oj_model_1b_preds)
```

So, $RA = (83*87 + 130*126) / 213^2 = 0.5202$ and $\kappa = (0.8592 - 0.5202)/(1 - 0.5202) = 0.7064$.  The kappa statistic varies from 0 to 1 where 0 means accurate predictions occur merely by chance, and 1 means the predictions are in perfect agreement with the observations.  In this case, a kappa statistic of 0.7064 is "substantial".  See chart [here](https://www.statisticshowto.datasciencecentral.com/cohens-kappa-statistic/).

The other measures from the `confusionMatrix()` output are various proportions and you can remind yourself of their definitions in the documentation with `?confusionMatrix`.

Visuals are almost always helpful.  Here is a plot of the confusion matrix.

```{r}
plot(oj_test$Purchase, oj_model_1b_preds, 
     main = "Simple Classification: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")
```

By the way, how does the validation set accuracy ()

```{r}
oj_model_1b_train_preds <- predict(oj_model_1b_pruned, oj_train, type = "class")
oj_model_1b_train_cm <- confusionMatrix(data = oj_model_1b_train_preds, reference = oj_train$Purchase)
oj_model_1b_train_cm$overall
```

The accuracy on the training data set was a little lower than on the test data set.  I though it would be higher, not lower.


### ROC Curve

Another measure of accuracy is the ROC (receiver operating characteristics) curve [@Fawcett2005].  The ROC curve is a plot of the true positive rate (TPR, sensitivity) versus the false positive rate (FPR, 1 - specificity) for a set of thresholds. By default, the threshold for predicting the default classification is 0.50, but it could be any threshold.  The ROC curves varies the thresholds.  (I'll use the `geom_roc` geom from **plotROC**.

```{r}
data.frame(M = predict(oj_model_1b_pruned, oj_test, "prob")[, 1],
           D = if_else(oj_test$Purchase == "CH", 1, 0)) %>%
   ggplot() + 
   geom_roc(aes(m = M, d = D), hjust = -0.4, vjust = 1.5, linealpha = 0.6, labelsize = 3, n.cuts = 10) + 
   geom_abline(intercept = 0, slope = 1, linetype = 2) +
   coord_equal() +
   theme_minimal() +
   labs(x = "FPR", y = "TPR",
        title = "Model 1b ROC Curve",
        subtitle = "Pruned model using rpart",
        caption = "Data: ISLM OJ data set.")

```

You can also use `prediction()` and `plot.prediction()` from the **ROCR** package. 

```{r}
pred <- prediction(predict(oj_model_1b_pruned, newdata = oj_test, type = "prob")[, 2], oj_test$Purchase)
plot(performance(pred, "tpr", "fpr"))
abline(0, 1, lty = 2)
```

Hmm, not quite the same...

A few points on the ROC space are helpful for understanding how to use it.  

* The lower left point (0, 0) is the result of *always* predicting "negative" or in this case "MM" if "CH" is taken as the default class.  Sure, your false positive rate is zero, but since you never predict a positive, your true positive rate is also zero.  
* The upper right point (1, 1) is the results of *always* predicting "positive" (or "CH" here).  You catch all the positives, but you miss all the negatives.
* The upper left point (0, 1) is the result of perfect accuracy.  You catch all the positives and all the negatives.
* The lower right point (1, 0) is the result of perfect imbecility.  You made the exact wrong prediction every time. 
* The 45 degree diagonal is the result of randomly guessing positive (CH) X percent of the time.  If you guess positive 90% of the time and the prevalence is 50%, your TPR will be 90% and your FPR will also be 90%, etc.

From the last bullet, it is evident that any point below and to the right of the 45 degree diagonal represents an instance where the model would have been better off just predicting entirely one way or the other.  The goal is for all nodes to bunch up in the upper left.

Points to the left of the diagonal with a low TPR can be thought of as "conservative" predicters - they only make positive (CH) predictions with strong evidence.  Points to the left of the diagnonal with a high TPR can be thought of as "liberal" predicters - they make positive (CH) predictions with weak evidence.  

### Caret Approach

I can also fit the model with `caret::train()`.  There are two ways to tune hyperparameters in `train()`: 

* set the number of tuning parameter values to consider by setting `tuneLength`, or
* set particular values to consider for each parameter by defining a `tuneGrid`.

I'll build the model using 10-fold cross-validation to optimize the hyperparameter CP. If you don't have any idea what the tuning parameter ought to look like, use `tuneLength` to get close, then fine-tune with `tuneGrid`.  That's what I'll do.  I'll create a training control object that I can re-use in other model builds.  

```{r}
oj_trControl = trainControl(
   method = "cv",  # k-fold cross validation
   number = 10,  # 10 folds
   savePredictions = "final",       # save predictions for the optimal tuning parameter
   classProbs = TRUE  # return class probabilities in addition to predicted values
#   summaryFunction = twoClassSummary  # computes sensitivity, specificity and the area under the ROC curve.
   )
```

Now fit the model.

```{r}
set.seed(1234)
oj_model_2 = train(
   Purchase ~ ., 
   data = oj_train, 
   method = "rpart",
   tuneLength = 5,
   metric = "Accuracy",
   trControl = oj_trControl
   )
```

`caret` built a full tree using `rpart`'s default parameters: gini splitting index, at least 20 observations in a node in order to consider splitting it, and at least 6 observations in each node.  Caret then calculated the accuracy for each candidate value of $\alpha$.  Here is the results.

```{r}
print(oj_model_2)
```

The second `cp` (0.008982036) produced the highest accuracy.  I can drill into the best value of `cp` using a tuning grid.  I'll try that now.

```{r}
set.seed(1234)
oj_model_3 = train(
   Purchase ~ ., 
   data = oj_train, 
   method = "rpart",
   tuneGrid = expand.grid(cp = seq(from = 0.001, to = 0.010, length = 11)),  
   metric='Accuracy',
   trControl = oj_trControl
   )
print(oj_model_3)
```

The beset model is at cp = 0.009.  Here are the rules in the final model.  

```{r}
oj_model_3$finalModel
```

Here is the tree.

```{r}
rpart.plot(oj_model_3$finalModel)
```

Here is the ROC curve.

```{r}
library(plotROC)
ggplot(oj_model_3$pred) + 
    geom_roc(
       aes(
          m = MM, 
          d = factor(obs, levels = c("CH", "MM"))
       ),
       hjust = -0.4, vjust = 1.5
    ) +
   coord_equal()
```

Here are the cross-validated Accuracy for each candidate cp value.

```{r}
plot(oj_model_3)
```
 
Evaluate the model by making predictions with the test data set.  

```{r}
oj_model_3_preds <- predict(oj_model_3, oj_test, type = "raw")
```

The confusion matrix shows the true positives and true negatives.

```{r}
oj_model_3_cm <- confusionMatrix(
   data = oj_model_3_preds, 
   reference = oj_test$Purchase
)
oj_model_3_cm
```

The accuracy metric is the slightly worse than in my previous model.  Here is a graphical representation of the confusion matrix.

```{r}
plot(oj_test$Purchase, oj_model_3_preds, 
     main = "Simple Classification: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")
```

Finally, here is the variable importance plot.

```{r}
plot(varImp(oj_model_3), main="Variable Importance with Simple Classication")
```

Looks like the manual effort faired best.  Here is a summary the accuracy rates of the three models.

```{r}
rbind(data.frame(model = "Manual Class", Acc = round(oj_model_1b_cm$overall["Accuracy"], 5)), 
      data.frame(model = "Caret w/tuneGrid", Acc = round(oj_model_3_cm$overall["Accuracy"], 5))
)
```


## Regression Trees

A simple regression tree is built in a manner similar to a simple classificatioon tree, and like the simple classification tree, it is rarely invoked on its own; the bagged, random forest, and gradient boosting methods build on this logic.  I'll learn by example again. Using the `ISLR::Carseats` data set, I will predict `Sales` using from the 10 feature variables.  Load the data.

```{r}
carseats_dat <- Carseats
#skim_with(numeric = list(p0 = NULL, p25 = NULL, p50 = NULL, p75 = NULL, 
#                                p100 = NULL, hist = NULL))
#skim(carseats_dat)
```

I'll split `careseats_dat` (n = 400) into `carseats_train` (80%, n = 321) and `carseats_test` (20%, n = 79).  I'll fit a simple decision tree with `carseats_train`, then later a bagged tree, a random forest, and a gradient boosting tree.  I'll compare their predictive performance with `carseats_test`.

```{r}
set.seed(12345)
partition <- createDataPartition(y = carseats_dat$Sales, p = 0.8, list = FALSE)
carseats_train <- carseats_dat[partition, ]
carseats_test <- carseats_dat[-partition, ]
```

The first step is to build a full tree, then perform k-fold cross-validation to help select the optimal cost complexity (cp).  The only difference here is the `rpart()` parameter `method = "anova"` to produce a regression tree.

```{r}
set.seed(1234)
carseats_model_1 <- rpart(
   formula = Sales ~ .,
   data = carseats_train,
   method = "anova", 
   xval = 10,
   model = TRUE  # to plot splits with factor variables.
)
print(carseats_model_1)
```

The output starts with the root node.  The predicted `Sales` at the root is the mean `Sales` for the training data set, 7.535950 (values are $000s).  The deviance at the root is the SSE, 2567.768.  The child nodes of node "x" are labeled 2x) and 2x+1), so the child nodes of 1) are 2) and 3), and the child nodes of 2) are 4) and 5).  Terminal nodes are labeled with an asterisk (*).

The first split is at `ShelveLoc` = [Bad, Medium] vs Good.  Here is what the full (unpruned) tree looks like.

```{r}
rpart.plot(carseats_model_1, yesno = TRUE)
```

The boxes show the node predicted value (mean) and the proportion of observations that are in the node (or child nodes). 

`rpart()` not only grew the full tree, it also used cross-validation to test the performance of the possible complexity hyperparameters. `printcp()` displays the candidate cp values. You can use this table to decide how to prune the tree.

```{r}
printcp(carseats_model_1)
```

There are 16 possible cp values in this model.  The model with the smallest complexity parameter allows the most splits (`nsplit`).  The highest complexity parameter corresponds to a tree with just a root node.  `rel error` is the SSE relative to the root node.  The root node SSE is 2567.76800, so its `rel error` is 2567.76800/2567.76800 = 1.0.  That means the absolute error of the full tree (at CP = 0.01) is 0.30963 * 2567.76800 = 795.058. You can verify that by calculating the SSE of the model predicted values:

```{r}
data.frame(pred = predict(carseats_model_1, newdata = carseats_train)) %>%
   mutate(obs = carseats_train$Sales,
          sq_err = (obs - pred)^2) %>%
   summarize(sse = sum(sq_err))
```

Finishing the CP table tour, `xerror` is the cross-validated SSE and `xstd` is its standard error. If you want the lowest possible error, then prune to the tree with the smallest relative SSE (`xerror`).  If you want to balance predictive power with simplicity, prune to the smallest tree within 1 SE of the one with the smallest relative SSE.  The CP table is not super-helpful for finding that tree. I'll add a column to find it.

```{r}
carseats_model_1$cptable %>%
   data.frame() %>%
   mutate(min_xerror_idx = which.min(carseats_model_1$cptable[, "xerror"]),
          rownum = row_number(),
          xerror_cap = carseats_model_1$cptable[min_xerror_idx, "xerror"] + 
             carseats_model_1$cptable[min_xerror_idx, "xstd"],
          eval = case_when(rownum == min_xerror_idx ~ "min xerror",
                           xerror < xerror_cap ~ "under cap",
                           TRUE ~ "")) %>%
   select(-rownum, -min_xerror_idx) 
```

Okay, so the simplest tree is the one with CP = 0.01544139 (8 splits).  Fortunately, `plotcp()` presents a nice graphical representation of the relationship between `xerror` and `cp`.

```{r}
plotcp(carseats_model_1, upper = "splits")
```

The dashed line is set at the minimum `xerror` + `xstd`.  The top axis shows the number of splits in the tree.  I'm not sure why the CP values are not the same as in the table (they are close, but not the same).  The smallest relative error is at `r carseats_model_1$cptable[which.min(carseats_model_1$cptable[, "xerror"]), "CP"]`, but the maximum CP below the dashed line (one standard deviation above the mimimum error) is at CP = .019 (8 splits). Use the `prune()` function to prune the tree by specifying the associated cost-complexity `cp`.  

```{r}
carseats_model_1_pruned <- prune(
   carseats_model_1,
   cp = carseats_model_1$cptable[carseats_model_1$cptable[, 2] == 8, "CP"]
)
rpart.plot(carseats_model_1_pruned, yesno = TRUE)
```

The most "important" indicator of `Sales` is `ShelveLoc`.  Here are the importance values from the model. 

```{r}
carseats_model_1_pruned$variable.importance %>% 
   data.frame() %>%
   rownames_to_column(var = "Feature") %>%
   rename(Overall = '.') %>%
   ggplot(aes(x = fct_reorder(Feature, Overall), y = Overall)) +
   geom_pointrange(aes(ymin = 0, ymax = Overall), color = "cadetblue", size = .3) +
   theme_minimal() +
   coord_flip() +
   labs(x = "", y = "", title = "Variable Importance with Simple Regression")
```

The most important indicator of `Sales` is `ShelveLoc`, then `Price`, then `Age`, all of which appear in the final model.  `CompPrice` was also important.

The last step is to make predictions on the validation data set. The root mean squared error ($RMSE = \sqrt{(1/2) \sum{(actual - pred)^2}})$ and mean absolute error ($MAE = (1/n) \sum{|actual - pred|}$) are the two most common measures of predictive accuracy. The key difference is that RMSE punishes large errors more harshly. For a regression tree, set argument `type = "vector"` (or do not specify at all). 

```{r message=FALSE, warning=FALSE}
carseats_model_1_preds <- predict(
   carseats_model_1_pruned, 
   carseats_test, 
   type = "vector"
)

carseats_model_1_pruned_rmse <- RMSE(
   pred = carseats_model_1_preds,
   obs = carseats_test$Sales
)
carseats_model_1_pruned_rmse
```

The pruning process leads to an average prediction error of `r round(carseats_model_1_pruned_rmse, 3)` in the test data set.  Not too bad considering the standard deviation of `Sales` is `r round(sd(carseats_test$Sales), 3)`. Here is a predicted vs actual plot. 

```{r message=FALSE, warning=FALSE}
plot(carseats_test$Sales, carseats_model_1_preds, 
     main = "Simple Regression: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")
abline(0, 1)
```

The 6 possible predicted values do a decent job of binning the observations.

### Caret Approach

I can also fit the model with `caret::train()`, specifying `method = "rpart"`.

I'll build the model using 10-fold cross-validation to optimize the hyperparameter CP. 

```{r}
carseats_trControl = trainControl(
   method = "cv",  # k-fold cross validation
   number = 10,  # 10 folds
   savePredictions = "final"       # save predictions for the optimal tuning parameter
)
```


I'll let the model look for the best CP tuning parameter with `tuneLength` to get close, then fine-tune with `tuneGrid`.  

```{r}
set.seed(1234)
carseats_model_2 = train(
   Sales ~ ., 
   data = carseats_train, 
   method = "rpart",  # for classification tree
   tuneLength = 5,  # choose up to 5 combinations of tuning parameters (cp)
   metric = "RMSE",  # evaluate hyperparamter combinations with RMSE
   trControl = carseats_trControl
)
print(carseats_model_2)
```

The first `cp` (0.04167149) produced the smallest RMSE.  I can drill into the best value of `cp` using a tuning grid.  I'll try that now.

```{r}
myGrid <-  expand.grid(cp = seq(from = 0, to = 0.1, by = 0.01))
carseats_model_3 = train(
   Sales ~ ., 
   data = carseats_train, 
   method = "rpart",  # for classification tree
   tuneGrid = myGrid,  # choose up to 5 combinations of tuning parameters (cp)
   metric = "RMSE",  # evaluate hyperparamter combinations with RMSE
   trControl = carseats_trControl
)
print(carseats_model_3)
```

It looks like the best performing tree is the unpruned one.

```{r}
plot(carseats_model_3)
```

Lets's see the final model.

```{r}
rpart.plot(carseats_model_3$finalModel)
```

What were the most important variables?
```{r}
plot(varImp(carseats_model_3), main="Variable Importance with Simple Regression")
```

Evaluate the model by making predictions with the test data set.  

```{r}
carseats_model_3_preds <- predict(carseats_model_3, carseats_test, type = "raw")
data.frame(Actual = carseats_test$Sales, Predicted = carseats_model_3_preds) %>%
ggplot(aes(x = Actual, y = Predicted)) +
   geom_point() +
   geom_smooth() +
   geom_abline(slope = 1, intercept = 0) + 
   scale_y_continuous(limits = c(0, 15)) +
   labs(title = "Simple Regression: Predicted vs. Actual")
```

Looks like the model over-estimates at the low end and undestimates at the high end.  Calculate the test data set RMSE.

```{r}
carseats_model_3_pruned_rmse <- RMSE(
   pred = carseats_model_3_preds,
   obs = carseats_test$Sales
)
carseats_model_3_pruned_rmse
```


Caret faired better in this model.  Here is a summary the RMSE values of the two models.

```{r}
rbind(data.frame(model = "Manual ANOVA", 
                 RMSE = round(carseats_model_1_pruned_rmse, 5)), 
      data.frame(model = "Caret", 
                 RMSE = round(carseats_model_3_pruned_rmse, 5))
)
```


## Bagging

Bootstrap aggregation, or *bagging*, is a general-purpose procedure for reducing the variance of a statistical learning method.  The algorithm constructs *B* regression trees using *B* bootstrapped training sets, and averages the resulting predictions. These trees are grown deep, and are not pruned. Hence each individual tree has high variance, but low bias. Averaging these *B* trees reduces the variance.  For classification trees, bagging takes the "majority vote" for the prediction.  Use a value of *B* sufficiently large that the error has settled down.

To test the model accuracy, the out-of-bag observations are predicted from the models that do not use them.  If B/3 of observations are in-bag, there are *B/3* predictions per observation.  These predictions are averaged for the test prediction.  Again, for classification trees, a majority vote is taken.

The downside to bagging is that it improves accuracy at the expense of interpretability.  There is no longer a single tree to interpret, so it is no longer clear which variables are more important than others.  

Bagged trees are a special case of random forests, so see the next section for an example.


## Random Forests

Random forests improve bagged trees by way of a small tweak that de-correlates the trees.  As in bagging, the algorithm builds a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of *mtry* predictors is chosen as split candidates from the full set of *p* predictors.  A fresh sample of *mtry* predictors is taken at each split.  Typically $mtry \sim \sqrt{b}$.  Bagged trees are thus a special case of random forests where *mtry = p*.


#### Bagging Classification Example

Again using the `OJ` data set to predict `Purchase`, this time I'll use the bagging method by specifying `method = "treebag"`.  I'll use `tuneLength = 5` and not worry about `tuneGrid` anymore.  Caret has no hyperparameters to tune with this model. 

```{r}
oj.bag = train(Purchase ~ ., 
               data = oj_train, 
               method = "treebag",  # for bagging
               tuneLength = 5,  # choose up to 5 combinations of tuning parameters
               metric = "ROC",  # evaluate hyperparamter combinations with ROC
               trControl = trainControl(
                 method = "cv",  # k-fold cross validation
                 number = 10,  # k=10 folds
                 savePredictions = "final",       # save predictions for the optimal tuning parameters
                      classProbs = TRUE,  # return class probabilities in addition to predicted values
                      summaryFunction = twoClassSummary  # for binary response variable
                      )
                    )
oj.bag
#plot(oj.bag$)
oj.pred <- predict(oj.bag, oj_test, type = "raw")
plot(oj_test$Purchase, oj.pred, 
     main = "Bagging Classification: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")

(oj.conf <- confusionMatrix(data = oj.pred, 
                            reference = oj_test$Purchase))
oj.bag.acc <- as.numeric(oj.conf$overall[1])
rm(oj.pred)
rm(oj.conf)
#plot(oj.bag$, oj.bag$finalModel$y)
plot(varImp(oj.bag), main="Variable Importance with Simple Classication")
```

#### Random Forest Classification Example

Now I'll try it with the random forest method by specifying `method = "ranger"`.  I'll stick with `tuneLength = 5`.  Caret tunes three hyperparameters: 

* `mtry`: number of randomly selected predictors.  Default is sqrt(p).
* `splitrule`: splitting rule.  For classification, options are "gini" (default) and "extratrees".
* `min.node.size`: minimal node size.  Default is 1 for classification.

```{r}
oj.frst = train(Purchase ~ ., 
               data = oj_train, 
               method = "ranger",  # for random forest
               tuneLength = 5,  # choose up to 5 combinations of tuning parameters
               metric = "ROC",  # evaluate hyperparamter combinations with ROC
               trControl = trainControl(
                 method = "cv",  # k-fold cross validation
                 number = 10,  # 10 folds
                 savePredictions = "final",       # save predictions for the optimal tuning parameter1
                 classProbs = TRUE,  # return class probabilities in addition to predicted values
                 summaryFunction = twoClassSummary  # for binary response variable
                 )
               )
oj.frst
plot(oj.frst)
oj.pred <- predict(oj.frst, oj_test, type = "raw")
plot(oj_test$Purchase, oj.pred, 
     main = "Random Forest Classification: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")

(oj.conf <- confusionMatrix(data = oj.pred, 
                            reference = oj_test$Purchase))
oj.frst.acc <- as.numeric(oj.conf$overall[1])
rm(oj.pred)
rm(oj.conf)
#plot(oj.bag$, oj.bag$finalModel$y)
#plot(varImp(oj.frst), main="Variable Importance with Simple Classication")
```

The model algorithm explains *"ROC was used to select the optimal model using the largest value. The final values used for the model were mtry = 9, splitrule = extratrees and min.node.size = 1."*  You can see the results of tuning grid combinations in the associated plot of ROC AUC vs mtry grouped by splitting rule.

The bagging (accuracy = 0.80751) and random forest (accuracy = 0.81690) models faired pretty well, but the manual classification tree is still in first place.  There's still gradient boosting to investigate!

```{r}
rbind(data.frame(model = "Manual Class", Accuracy = round(oj_model_1b_cm$overall["Accuracy"], 5)), 
      data.frame(model = "Caret w.tuneGrid", Accuracy = round(oj_model_3_cm$overall["Accuracy"], 5)),
      data.frame(model = "Bagging", Accuracy = round(oj.bag.acc, 5)),
      data.frame(model = "Random Forest", Accuracy = round(oj.frst.acc, 5))
) %>% arrange(desc(Accuracy))
```

#### Bagging Regression Example

Again using the `Carseats` data set to predict `Sales`, this time I'll use the bagging method by specifying `method = "treebag"`.  I'll use `tuneLength = 5` and not worry about `tuneGrid` anymore.  Caret has no hyperparameters to tune with this model. 

```{r}
carseats.bag = train(Sales ~ ., 
               data = carseats_train, 
               method = "treebag",  # for bagging
               tuneLength = 5,  # choose up to 5 combinations of tuning parameters
               metric = "RMSE",  # evaluate hyperparamter combinations with RMSE
               trControl = trainControl(
                 method = "cv",  # k-fold cross validation
                 number = 10,  # 10 folds
                 savePredictions = "final"       # save predictions for the optimal tuning parameter1
                 )
               )
carseats.bag
#plot(carseats.bag$finalModel)
carseats.pred <- predict(carseats.bag, carseats_test, type = "raw")
plot(carseats_test$Sales, carseats.pred, 
     main = "Bagging Regression: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")
abline(0, 1)

(carseats.bag.rmse <- RMSE(pred = carseats.pred,
                           obs = carseats_test$Sales))
rm(carseats.pred)
plot(varImp(carseats.bag), main="Variable Importance with Regression Bagging")
```


#### Random Forest Regression Example

Now I'll try it with the random forest method by specifying `method = "ranger"`.  I'll stick with `tuneLength = 5`.  Caret tunes three hyperparameters: 

* `mtry`: number of randomly selected predictors
* `splitrule`: splitting rule.  For regression, options are "variance" (default), "extratrees", and "maxstat". 
* `min.node.size`: minimal node size

```{r}
carseats.frst = train(Sales ~ ., 
               data = carseats_train, 
               method = "ranger",  # for random forest
               tuneLength = 5,  # choose up to 5 combinations of tuning parameters
               metric = "RMSE",  # evaluate hyperparamter combinations with RMSE
               trControl = trainControl(
                 method = "cv",  # k-fold cross validation
                 number = 10,  # 10 folds
                 savePredictions = "final"       # save predictions for the optimal tuning parameter1
                 )
               )
carseats.frst
plot(carseats.frst)
carseats.pred <- predict(carseats.frst, carseats_test, type = "raw")
plot(carseats_test$Sales, carseats.pred, 
     main = "Random Forest Regression: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")
abline(0, 1)
(carseats.frst.rmse <- RMSE(pred = carseats.pred,
                           obs = carseats_test$Sales))
rm(carseats.pred)
#plot(varImp(carseats.frst), main="Variable Importance with Regression Random Forest")
```

The model algorithm explains *"RMSE was used to select the optimal model using the smallest value. The final values used for the model were mtry = 11, splitrule = variance and min.node.size = 5."*  You can see the results of tuning grid combinations in the associated plot of ROC AUC vs mtry grouped by splitting rule.

The bagging and random forest models faired very well - they took over the first and second place!

```{r}
rbind(data.frame(model = "Manual ANOVA", RMSE = round(carseats_model_1_pruned_rmse, 5)), 
      data.frame(model = "ANOVA w.tuneGrid", RMSE = round(carseats_model_3_pruned_rmse, 5)),
      data.frame(model = "Bagging", RMSE = round(carseats.bag.rmse, 5)),
      data.frame(model = "Random Forest", RMSE = round(carseats.frst.rmse, 5))
) %>% arrange(RMSE)
```


## Gradient Boosting

Boosting is a method to improve (boost) the week learners sequentially and increase the model accuracy with a combined model. There are several boosting algorithms.  One of the earliest was AdaBoost (adaptive boost).  A more recent innovation is gradient boosting.

Adaboost creates a single split tree (decision stump) then weights the observations by how well the initial tree performed, putting more weight on the difficult observations.  It then creates a second tree using the weights so that it focuses on the difficult observations.  Observations that are difficult to classify receive increasing larger weights until the algorithm identifies a model that correctly classifies them.  The final model returns predictions that are a majority vode. (*I think Adaboost applies only to classification problems, not regressions*). 
 
Gradient boosting generalizes the AdaBoost method, so that the object is to minimize a loss function.  In the case of classification problems, the loss function is the log-loss; for regression problems, the loss function is mean squared error.  The regression trees are addative, so that the successive models can be added together to correct the residuals in the earlier models.  Gradient boosting constructs its trees in a "greedy" manner, meaning it chooses the best splits based on purity scores like Gini or minimizing the loss.  It is common to constrain the weak learners by setting maximum tree size parameters.  Gradient boosting continues until it reaches maximum number of trees or an acceptible error level.  This can result in overfitting, so it is common to employ regularization methods that penalize aspects of the model.

**Tree Constraints**.  In general the more constrained the tree, the more trees need to be grown.  Parameters to optimize include number of trees, tree depth, number of nodes, minimmum observations per split, and minimum improvement to loss.

**Learning Rate**.  Each successive tree can be weighted to slow down the learning rate.  Decreasing the learning rate increases the number of required trees.  Common growth rates are 0.1 to 0.3.

The gradient boosting algorithm fits a shallow tree $T_1$ to the data, $M_1 = T_1$.  Then it fits a tree $T_2$ to the residuals and adds a weighted sum of the tree to the original tree as $M_2 = M_1 + \gamma T_2$.  For regularized boosting, include a learning rate factor $\eta \in (0..1)$, $M_2 = M_1 + \eta \gamma T_2$.  A larger $\eta$ produces faster learning, but risks overfitting.  The process repeats until the residuals are small enough, or until it reaches the maximum iterations.  Because overfitting is a risk, use cross-validation to select the appropriate number of trees (the number of trees producing the lowest RMSE).


#### Gradient Boosting Classification Example

Again using the `OJ` data set to predict `Purchase`, this time I'll use the gradient boosting method by specifying `method = "gbm"`.  I'll use `tuneLength = 5` and not worry about `tuneGrid` anymore.  Caret tunes the following  hyperparameters (see `modelLookup("gbm")`). 

* `n.trees`: number of boosting iterations
* `interaction.depth`: maximum tree depth
* `shrinkage`: shrinkage
* `n.minobsinnode`: mimimum terminal node size

```{r}
oj.gbm <- train(Purchase ~ ., 
               data = oj_train, 
               method = "gbm",  # for bagged tree
               tuneLength = 5,  # choose up to 5 combinations of tuning parameters
               metric = "ROC",  # evaluate hyperparamter combinations with ROC
               trControl = trainControl(
                 method = "cv",  # k-fold cross validation
                 number = 10,  # 10 folds
                 savePredictions = "final",       # save predictions for the optimal tuning parameter1
                      classProbs = TRUE,  # return class probabilities in addition to predicted values
                      summaryFunction = twoClassSummary  # for binary response variable
                      )
                    )
oj.gbm
plot(oj.gbm)
oj.pred <- predict(oj.gbm, oj_test, type = "raw")
plot(oj_test$Purchase, oj.pred, 
     main = "Gradient Boosing Classification: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")

(oj.conf <- confusionMatrix(data = oj.pred, 
                            reference = oj_test$Purchase))
oj.gbm.acc <- as.numeric(oj.conf$overall[1])
rm(oj.pred)
rm(oj.conf)
#plot(oj.bag$, oj.bag$finalModel$y)
#plot(varImp(oj.gbm), main="Variable Importance with Gradient Boosting")
```


#### Gradient Boosting Regression Example

Again using the `Carseats` data set to predict `Sales`, this time I'll use the gradient boosting method by specifying `method = "gbm"`.  I'll use `tuneLength = 5` and not worry about `tuneGrid` anymore.  Caret tunes the following  hyperparameters. 

* `n.trees`: number of boosting iterations (increasing `n.trees` reduces the error on training set, but may lead to over-fitting)
* `interaction.depth`: maximum tree depth (the default six - node tree appears to do an excellent job)
* `shrinkage`: learning rate (reduces the impact of each additional fitted base-learner (tree) by reducing the size of incremental steps and thus penalizes the importance of each consecutive iteration.  The intuition is that it is better to improve a model by taking many small steps than by taking fewer large steps. If one of the boosting iterations turns out to be erroneous, its negative impact can be easily corrected in subsequent steps.)
* `n.minobsinnode`: mimimum terminal node size

```{r}
carseats.gbm <- train(Sales ~ ., 
                      data = carseats_train, 
                      method = "gbm",  # for bagged tree
                      tuneLength = 5,  # choose up to 5 combinations of tuning parameters
                      metric = "RMSE",  # evaluate hyperparamter combinations with ROC
                      trControl = trainControl(
                        method = "cv",  # k-fold cross validation
                        number = 10,  # 10 folds
                        savePredictions = "final",       # save predictions for the optimal tuning parameter1
                        verboseIter = FALSE,
                        returnData = FALSE
                        )
                      )
carseats.gbm
plot(carseats.gbm)
carseats.pred <- predict(carseats.gbm, carseats_test, type = "raw")
plot(carseats_test$Sales, carseats.pred, 
     main = "Gradient Boosing Regression: Predicted vs. Actual",
     xlab = "Actual",
     ylab = "Predicted")
abline(0,1)

(carseats.gbm.rmse <- RMSE(pred = carseats.pred,
                           obs = carseats_test$Sales))
rm(carseats.pred)

#plot(varImp(carseats.gbm), main="Variable Importance with Gradient Boosting")
```

## Summary

Okay, I'm going to tally up the results!  For the classification division, the winner is the manual classification tree!  Gradient boosting made a valiant run at it, but came up just a little short.

```{r}
rbind(data.frame(model = "Manual Class", Acc = round(oj_model_1b_cm$overall["Accuracy"], 5)), 
      data.frame(model = "Class w.tuneGrid", Acc = round(oj_model_3_cm$overall["Accuracy"], 5)),
      data.frame(model = "Bagging", Acc = round(oj.bag.acc, 5)),
      data.frame(model = "Random Forest", Acc = round(oj.frst.acc, 5)),
      data.frame(model = "Gradient Boosting", Acc = round(oj.gbm.acc, 5))
) %>% arrange(desc(Acc))
```

And now for the regression division, the winnner is... gradient boosting!

```{r}
rbind(data.frame(model = "Manual ANOVA", RMSE = round(carseats_model_1_pruned_rmse, 5)), 
      data.frame(model = "ANOVA w.tuneGrid", RMSE = round(carseats_model_3_pruned_rmse, 5)),
      data.frame(model = "Bagging", RMSE = round(carseats.bag.rmse, 5)),
      data.frame(model = "Random Forest", RMSE = round(carseats.frst.rmse, 5)),
      data.frame(model = "Gradient Boosting", RMSE = round(carseats.gbm.rmse, 5))
) %>% arrange(RMSE)
```


Here are plots of the ROC curves for all the models (one from each chapter) on the same graph. The ROCR package provides the prediction() and performance() functions which generate the data required for plotting the ROC curve, given a set of predictions and actual (true) values. 
The more "up and to the left" the ROC curve of a model is, the better the model. The AUC performance metric is literally the "Area Under the ROC Curve", so the greater the area under this curve, the higher the AUC, and the better-performing the model is.

```{r}
library(ROCR)
# List of predictions
oj.class.pred <- predict(oj_model_3, oj_test, type = "prob")[,2]
oj.bag.pred <- predict(oj.bag, oj_test, type = "prob")[,2]
oj.frst.pred <- predict(oj.frst, oj_test, type = "prob")[,2]
oj.gbm.pred <- predict(oj.gbm, oj_test, type = "prob")[,2]

preds_list <- list(oj.class.pred, oj.bag.pred, oj.frst.pred, oj.gbm.pred)
#preds_list <- list(oj.class.pred)

# List of actual values (same for all)
m <- length(preds_list)
actuals_list <- rep(list(oj_test$Purchase), m)

# Plot the ROC curves
pred <- prediction(preds_list, actuals_list)
#pred <- prediction(oj.class.pred[,2], oj_test$Purchase)
rocs <- performance(pred, "tpr", "fpr")
plot(rocs, col = as.list(1:m), main = "Test Set ROC Curves")
legend(x = "bottomright", 
       legend = c("Decision Tree", "Bagged Trees", "Random Forest", "GBM"),
       fill = 1:m)
```


## Reference

Penn State University, STAT 508: Applied Data Mining and Statistical Learning, "Lesson 11: Tree-based Methods". [https://newonlinecourses.science.psu.edu/stat508/lesson/11](https://newonlinecourses.science.psu.edu/stat508/lesson/11).

Brownlee, Jason. "Classification And Regression Trees for Machine Learning", Machine Learning Mastery.  [https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/](https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/). 

Brownlee, Jason. "A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning", Machine Learning Mastery. [https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/).

[DataCamp: Machine Learning with Tree-Based Models in R](https://campus.datacamp.com/courses/machine-learning-with-tree-based-models-in-r)

[An Introduction to Statistical Learning](http://faculty.marshall.usc.edu/gareth-james/ISL/) by Gareth James, et al.

[SAS Documentation](http://support.sas.com/documentation/cdl/en/stathpug/68163/HTML/default/viewer.htm#stathpug_hpsplit_details01.htm)

[StatMethods: Tree-Based Models](https://www.statmethods.net/advstats/cart.html)

[Machine Learning Plus](https://www.machinelearningplus.com/machine-learning/caret-package/)

[GBM (Boosted Models) Tuning Parameters](https://www.listendata.com/2015/07/gbm-boosted-models-tuning-parameters.html)  from Listen Data

[Harry Southworth](https://github.com/harrysouthworth/gbm/blob/master/demo/bernoulli.R) on GitHub

[Gradient Boosting Classification with GBM in R](https://www.datatechnotes.com/2018/03/classification-with-gradient-boosting.html) in DataTechNotes

Molnar, Christoph. "Interpretable machine learning. A Guide for Making Black Box Models Explainable", 2019. [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/).


<!--chapter:end:08-cart.Rmd-->

# Regularization

<!--chapter:end:09-regularization.Rmd-->

# Non-linear Models

Linear methods can model nonlinear relationships by including polynomial terms, interaction effects, and variable transformations.  However, it is often difficult to identify how to formulate the model. Nonlinear models may be preferable because you do not need to know the the exact form of the nonlinearity prior to model training.

## Splines

A regression spline fits a piecewise polynomial to the range of *X* partitioned by *knots* (*K* knots produce *K + 1* piecewise polynomials) **James et al** [@James2013].  The polynomials can be of any degree *d*, but are usually in the range [0, 3], most commonly 3 (a cubic spline).  To avoid discontinuities in the fit, a degree-*d* spline is constrained to have continuity in derivatives up to degree *d*−1 at each knot.

A cubic spline fit to a data set with *K* knots, performs least squares regression with an intercept and 3 + *K* predictors, of the form 

$$y_i = \beta_0 + \beta_1X + \beta_2X^2 + \beta_3X^3 + \beta_4h(X, \xi_1) + \beta_5h(X, \xi_2) + \dots + \beta_{K+3}h(X, \xi_K)$$

where $\xi_1, \dots, \xi_K$ are the knots are truncated power basis functions $h(X, \xi) = (X - \xi)^3$ if $X > \xi$, else 0.

Splines can have high variance at the outer range of the predictors.  A **natural spline** is a regression spline additionally constrained to be linear at the boundaries.

How many knots should there be, and Where should the knots be placed?  It is common to place knots in a uniform fashion, with equal numbers of points between each knot.  The number of knots is typically chosen by trial and error using cross-validation to minimize the RSS.  The number of knots is usually expressed in terms of degrees of freedom.  A cubic spline will have *K* + 3 + 1 degrees of freedom.  A natural spline has *K* + 3 + 1 - 5 degrees of freedom due to the constraints at the endpoints.

A further constraint can be added to reduce overfitting by enforcing smoothness in the spline.  Instead of minimizing the loss function $\sum{(y - g(x))^2}$ where $g(x)$ is a natural spline, minimize a loss function with an additional penalty for variability:

$$L = \sum{(y_i - g(x_i))^2 + \lambda \int g''(t)^2dt}.$$

The function $g(x)$ that minimizes the loss function is a *natural cubic spline* with knots at each $x_1, \dots, x_n$. This is called a **smoothing spline**.  The larger g is, the greater the penalty on variation in the spline.  In a smoothing spline, you do not optimize the number or location of the knots -- there is a knot at each training observation. Instead, you optimize $\lambda$. One way to optimze $\lambda$ is cross-validation to minimize RSS. Leave-one-out cross-validation (LOOCV) can be computed efficiently for smoothing splines.


## MARS

Multivariate adaptive regression splines (MARS) is a non-parametric algorithm that creates a piecewise linear model to capture nonlinearities and interactions effects. The resulting model is a weighted sum of *basis* functions $B_i(X)$:

$$\hat{y} = \sum_{i=1}^{k}{w_iB_i(x)}$$

The basis functions are either a constant (for the intercept), a *hinge* function of the form $\max(0, x - x_0)$ or $\max(0, x_0 - x)$ (a more concise representation is $[\pm(x - x_0)]_+$), or products of two or more hinge functions (for interactions).  MARS automatically selects which predictors to use and what predictor values to serve as the *knots* of the hinge functions.

MARS builds a model in two phases: the forward pass and the backward pass, similar to growing and pruning of tree models. MARS starts with a model consisting of just the intercept term equaling the mean of the response values.  It then asseses every predictor to find a basis function pair consisting of opposing sides of a mirrored hinge function which produces the maximum improvement in the model error.  MARS repeats the process until either it reaches a predefined limit of terms or the error improvement reaches a predefined limit.  MARS generalizes the model by removing terms according to the generalized cross validation (GCV) criterion.  GCV is a form of regularization: it trades off goodness-of-fit against model complexity. 

The `earth::earth()` function ([documentation](https://www.rdocumentation.org/packages/earth/versions/5.1.2/topics/earth)) performs the MARS algorithm (*the term "MARS" is trademarked, so open-source implementations use "Earth" instead*).  The caret implementation tunes two parameters: `nprune` and `degree`.  `nprune` is the maximum number of terms in the pruned model.  `degree` is the maximum degree of interaction (default is 1 (no interactions)).  However, there are other hyperparameters in the model that may improve performance, including `minspan` which regulates the number of knots in the predictors.

Here is an example using the Ames housing data set (following [this](http://uc-r.github.io/mars) tutorial. 

```{r model_mars, cache=TRUE, message=FALSE}
library(tidyverse)
library(earth)
library(caret)

# set up
ames <- AmesHousing::make_ames()
set.seed(12345)
idx <- createDataPartition(ames$Sale_Price, p = 0.80, list = FALSE)
ames_train <- ames[idx, ] %>% as.data.frame()
ames_test  <- ames[-idx, ]

m <- train(
  x = subset(ames_train, select = -Sale_Price),
  y = ames_train$Sale_Price,
  method = "earth",
  metric = "RMSE",
  minspan = -15,
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = expand.grid(
    degree = 1:3, 
    nprune = seq(2, 100, length.out = 10) %>% floor()
  )
)
```

The model plot shows the best tuning parameter combination.

```{r}
plot(m, main = "MARS Parameter Tuning")
m$bestTune
```

How does this model perform against the holdout data?

```{r}
caret::postResample(
  pred = log(predict(m, newdata = ames_test)),
  obs = log(ames_test$Sale_Price)
)
```


## GAM

Generalized additive models (GAM) allow for non-linear relationships between each feature and the response by replacing each linear component $\beta_j x_{ij}$ with a nonlinear function $f_j(x_{ij})$. The GAM model is of the form

$$y_i = \beta_0 + \sum{f_j(x_{ij})} + \epsilon_i.$$

It is called an additive model because we calculate a separate $f_j$ for each $X_j$, and then add together all of their contributions. 

The advantage of GAMs is that they automatically model non-linear relationships so you do not need to manually try out many diﬀerent transformations on each variable individually.  And because the model is additive, you can still examine the eﬀect of each $X_j$ on $Y$ individually while holding all of the other variables ﬁxed.  The main limitation of GAMs is that the model is restricted to be additive, so important interactions can be missed unless you explicitly add them. 

<!--chapter:end:10-nonlin.Rmd-->

# Support Vector Machines

These notes rely on [@James2013], [@Hastie2017], and [@Kuhn2016]. I also reviewed the material in PSU's Applied Data Mining and Statistical Learning ([STAT 508](https://online.stat.psu.edu/stat508/)), and the *e1071* [Support Vector Machines](https://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf) vignette.

The Support Vector Machines (SVM) algorithm finds the optimal separating hyperplane between members of two classes using an appropriate nonlinear mapping to a sufficiently high dimension. The hyperplane is defined by the observations that lie within a margin optimized by a cost hyperparameter. These observations are called the *support vectors*.

SVM is an extension of the *support vector classifier* which in turn is a generalization of the simple and intuitive *maximal margin classifier*.  


## Maximal Margin Classifier

The maximal margin classifier is the optimal hyperplane defined in the (rare) case where two classes are *linearly separable*.  Given an $n \times p$ data matrix $X$ with binary response variable defined as $y \in [-1, 1]$ it *may* be possible to define a *p*-dimensional hyperplane $h(X) = \beta_0 + \beta_1X_1 + \beta_2X_2 \dots + \beta_pX_p = x_i^T \beta + \beta_0 = 0$ such that all observations of each class fall on opposite sides of the hyperplane. This "separating hyperplane" has the property that if $\beta$ is constrained to be a unit vector, $||\beta|| = \sum\beta^2 = 1$, then the product of the hyperplane and response variables are positive perpendicular distances from the hyperplane, the smallest of which may be termed the hyperplane *margin*, $M$,  

$$y_i (x_i^{'} \beta + \beta_0) \ge M.$$

The maximal margin classifier is the hyperplane with the maximum margin.  That is, $\max \{M\}$ subject to $||\beta|| = 1$.  A separating hyperplane rarely exists.  In fact, even if a separating hyperplane does exist, its resulting margin is probably undesirably narrow. 


## Support Vector Classifier

The maximal margin classifier can be generalized to non-separable cases using a so-called "soft margin".  The generalization is called the *support vector classifier*.  The soft margin allows some misclassification in the interest of greater robustness to individual observations. The support vector classifier optimizes 

$$y_i (x_i^{'} \beta + \beta_0) \ge M(1 - \xi_i)$$

where the $\xi_i$ are positive *slack variables* whose sum is bounded by some constant tuning parameter $\sum{\xi_i} \le constant$.  The slack variable values indicate where the observation lies:  $\xi_i = 0$ observations lie on the correct side of the margin;  $\xi_i > 0$ observation lie on the wrong side of the margin;  $\xi_i > 1$ observations lie on the wrong side of the hyperplane.  The constant sets the tolerance for margin violation.  If $constant = 0$, then all observations must reside on the correct side of the margin, as in the maximal margin classifier.  The $constant$ controls the bias-variance trade-off.  As the $constant$ increases, the margin widens and allows more violations.  The classifier bias increases but its variance decreases.

The support vector classifier is usually defined by dropping the $||\beta|| = 1$ constraint, and defining $M = 1 / ||\beta||$.  The optimization problem then becomes

$$
 \min ||\beta|| \hspace{2mm} s.t. \hspace{2mm}  
  \begin{cases} 
   y_i(x_i^T\beta + \beta_0) \ge 1 - \xi_i, \hspace{2mm} \forall i &  \\
   \xi_i \ge 0, \hspace{2mm} \sum \xi_i \le constant.      
  \end{cases}
$$

This is a quadratic equation with linear inequality constraints, so it is a convex optimization problem which can be solved using Lagrange multipliers. Re-express the optimization problem as

$$
\min_{\beta_0, \beta} \frac{1}{2}||\beta||^2 = C\sum_{i = 1}^N \xi_i \\
s.t. \xi_i \ge 0, \hspace{2mm} y_i(x_i^T\beta + \beta_0) \ge 1 - \xi_i, \hspace{2mm} \forall i
$$

where the "cost" parameter $C$ replaces the constant and penalizes large residuals.  This optimization problem is equivalent to *another* optimization problem, the familiar *loss + penalty* formulation:

$$\min_{\beta_0, \beta} \sum_{i=1}^N{[1 - y_if(x_i)]_+} + \frac{\lambda}{2} ||\beta||^2 $$

where $\lambda = 1 / C$ and $[1 - y_if(x_i)]_+$ is a "hinge" loss function with $f(x_i) = sign[Pr(Y = +1|x) - 1 / 2]$.  
The parameter estimates can be written as functions of a set of unknown parameters $(\alpha_i)$ and data points. The solution to the optimization problem requires only the inner products of the observations, represented as $\langle x_i, x_j \rangle$,

$$f(x) = \beta_0 + \sum_{i = 1}^n {\alpha_i \langle x, x_i \rangle}$$
The solution has the interesting property that only observations on or within the margin affect the hyperplane.  These observations are known as support vectors.  As the constant increases, the number of violating observations increase, and thus the number of support vectors increases.  This property makes the algorithm robust to the extreme observations far away from the hyperplane.

The parameter estimators for $\alpha_i$ are nonzero only for the support vectors in the solution—that is, if a training observation is not a support vector, then its $\alpha_i$ equals zero.
 
The only shortcoming with the algorithm is that it presumes a linear decision boundary.  


## Support Vector Machines

Enlarging the feature space of the support vector classifier accommodates nonlinar relationships.  Support vector machines do this in a specific way, using *kernals*.  The kernal is a generalization of the inner product with form $K(x_i, x_i^{'})$.  So the linear kernal is simply 

$$K(x_i, x_i^{'}) = \langle x, x_i \rangle$$

and the solution is 

$$f(x) = \beta_0 + \sum_{i = 1}^n {\alpha_i K(x_i, x_i^{'})}$$

$K$ can take onother form instead, such as polynomial 

$$K(x, x') = (\gamma \langle x, x' \rangle + c_0)^d$$ 

or radial 

$$K(x, x') = \exp\{-\gamma ||x - x'||^2\}.$$


## Example

Here is a data set of two classes $y \in [-1, 1]$ described by two features $X1$ and $X2$.

```{r message=FALSE, warning=FALSE}
library(tidyverse)
set.seed(1)
x <- matrix(rnorm (20*2), ncol=2)
y <- c(rep(-1, 10), rep(1, 10))
x[y==1, ] <- x[y==1, ] + 1
train_data <- data.frame(x, y)
train_data$y <- as.factor(y)
```

A scatter plot reveals whether the classes are linearly separable.

```{r}
ggplot(train_data, aes(x = X1, y = X2, color = y)) +
  geom_point(size = 2) +
  labs(title = "Binary response with two features") +
  theme(legend.position = "top")
```

No, they are not linearly separable.  Now fit a support vector machine. The **e1071** library implements the SVM algorithm.   `svm(..., kernel="linear")` fits a support vector classifier. Change the kernal to `c("polynomial", "radial")` for SVM.  Try a cost of 10.

```{r}
library(e1071)
m <- svm(
  y ~ ., 
  data = train_data,
  kernel = "linear",
  type = "C-classification",  # (default) for classification
  cost = 10,  # default is 1
  scale = FALSE  # do not standardize features
)
plot(m, train_data)
```

The support vectors are plotted as "x's".  There are seven of them.

```{r}
m$index
```

The summary shows adds additional information, including the distribution of the support vector classes.

```{r}
summary(m)
```

The seven support vectors are comprised of four in one class, three in the other. What if we lower the cost of margin violations?  This will increase bias and lower variance.

```{r}
m <- svm(
  y ~ ., 
  data = train_data,
  kernel = "linear",
  type = "C-classification",  
  cost = 0.1,
  scale = FALSE
)
plot(m, train_data)
```

There are many more support vectors now.  *(In case you hoped to see the linear decision boundary formulation, or at least a graphical representation of the margins, keep hoping. The model is generalized beyond two features, so it evidently does not worry too much about supporting sanitized two-feature demos.)*

Which cost level yields the *best* predictive performance on holdout data?  Use cross validation to find out. SVM defaults to 10-fold CV.  I'll try seven candidate values for `cost`.

```{r}
set.seed(1)
m_tune <- tune(
  svm,
  y ~ .,
  data = train_data,
  kernel ="linear",
  ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100))
)
summary(m_tune)
```

The lowest cross-validation error rate is 0.10 with cost = 0.1.  `tune()` saves the best tuning parameter value.
```{r}
m_best <- m_tune$best.model
summary(m_best)
```

There are 16 support vectors, 8 in each class.  This is a pretty wide margin.

```{r}
plot(m_best, train_data)
```

What if the classes had been linearly separable?  Then we could create a maximal margin classifier.  

```{r}
train_data_2 <- train_data %>% 
  mutate(
    X1 = X1 + ifelse(y==1, 1.0, 0),
    X2 = X2 + ifelse(y==1, 1.0, 0)
  )
ggplot(train_data_2, aes(x = X1, y = X2, color = y)) +
  geom_point(size = 2) +
  labs(title = "Binary response with two features, linearly separable")
```

Specify a huge cost = 1e5 so that no support vectors violate the margin.  

```{r}
m2 <- svm(
  y ~ ., 
  data = train_data_2,
  kernel = "linear",
  cost = 1e5,
  scale = FALSE  # do not standardize features
)
plot(m2, train_data_2)
summary(m2)
```

This model will have very low bias, but very high variance.  To fit an SVM, use a different kernel.  You can use `kernal = c("polynomial", "radial", "sigmoid")`.  For a polynomial model, also specify the polynomial degree. For a radial model, include the gamma value.

```{r}
set.seed(1)
m3_tune <- tune(
  svm,
  y ~ .,
  data = train_data,
  kernel ="polynomial",
  ranges = list(
    cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100),
    degree = c(1, 2, 3)
  )
)
summary(m3_tune)
```


The lowest cross-validation error rate is 0.10 with cost = 1, polynomial degree 1.

```{r}
m3_best <- m3_tune$best.model
summary(m3_best)
```

There are 12 support vectors, 6 in each class.  This is a pretty wide margin.

```{r}
plot(m3_best, train_data)
```

## Using Caret

The model can also be fit using **caret**.  I'll used LOOCV since the data set is so small.  Normalize the variables to make their scale comparable.

```{r message=FALSE, warning=FALSE}
library(caret)
library(kernlab)

train_data_3 <- train_data %>%
  mutate(y = factor(y, labels = c("A", "B")))

m4 <- train(
  y ~ .,
  data = train_data_3,
  method = "svmPoly",
  preProcess = c("center", "scale"),
  trControl = trainControl(
    method = "cv",
    number = 5,
    summaryFunction = twoClassSummary,	# Use AUC to pick the best model
    classProbs=TRUE
  )
)

m4$bestTune
```

```{r}
plot(m4)
```


<!--chapter:end:11-svm.Rmd-->

# Principal Components Analysis

<!--chapter:end:12-pca.Rmd-->

# Clustering

<!--chapter:end:13-cluster.Rmd-->

# Text Mining

<!--chapter:end:14-text-mining.Rmd-->

# Appendix {-}

Here are miscellaneous skills, knowledge, and technologies I should know.

## Publishing to BookDown {-}

The **bookdown** package, written by Yihui Xie, is built on top of R Markdown and the **knitr** package.  Use it to publish a book or long manuscript where each chapter is a separate file.  There are instructions for how to author a book in his [bookdown book](https://bookdown.org/yihui/bookdown/) [@xie2019].  The main advantage of **bookdown** over R Markdown is that you can produce multi-page HTML output with numbered headers, equations, figures, etc., just like in a book.  I'm using **bookdown** to create a compendium of all my data science notes.  

The first step to using **bookdown** is installing the **bookdown* package with `install.packages("bookdown")`.

Next, create an account at [bookdown.org](http://bookdown.org), and connect the account to RStudio.  Follow the instructions at [https://bookdown.org/home/about/](https://bookdown.org/home/about/).

Finally, create a project in R Studio by creating a new project of type *Book Project using Bookdown*.

After creating all of your Markdown pages, knit the book or click the **Build Book** button in the Build panel.


## Shiny Apps {-}

## Packages {-}

**R Packages** [@Wickham2015] by Hadley Wickham is a good manual on packages, but it does not include a full tutorial.  The [Developing R Packages](https://campus.datacamp.com/courses/developing-r-packages/) Data Camp course is also helpful.  I will set up my own exercise and present it here.  I will create a package for my pretend organization, "MF".  The package will include the following:

* R Markdown template.  My template will integrate code, output, and commentary in a single R Markdown. The template will produce a familiar work product containing standard content (summary, data management, exploratory analysis, methods, results, conclusions), and a standard style (colors, typeface, size, logo).

* Functions.  Common I/O functions for database retrieval, writing to Excel.  Common graphing functions for ggplot styling.

I am mostly copying the logic and code from the ggthemes [economist.R](https://rdrr.io/cran/ggthemes/src/R/economist.R) script.


### Create a package {-}

1. In the RStudio IDE, click File > New Project.  Select "New Directory".  Select "R Package".  You can also use `devtools::create("mfstylr")`.  This will create the minimum items for an R package.

![](./images/create_pkg.png)

    + R directory: R scripts with function definitions.
    + man directory: documentation
    + NAMESPACE file: information about imported functions and functions made available (managed by **roxygen2**)
    + DESCRIPTION file: metadata about the package
  
2. Write functions in R scripts in R directory.  Document with tags readable by *roxygen2* package.

3. Select XYZ > Install and Restart.

### Document Functions with roxygen

Add roxygen documentation with `#'` characters.  The first three lines are always the title, Description, and Details.  They don't need any tags, but you need to separate them with blank lines.

```{r eval=FALSE}

```



### Create Data {-}

Add an RData file to your package with `use_data()`

### Create Vignette {-}

Add a directory and template vignette with `use_vignette(name, title)`.

```{r eval=FALSE}
use_vignette("Creating-Plots-with-mfstylr", "Creating Plots with mfstylr")
```


#### Step 2: Create an R Markdown template {-}

I relied on [this blog](http://freerangestats.info/blog/2017/09/09/rmarkdown) at *free range statistics* for a lot what follows.  There is also good information about R Markdown and templates in Yihui Xie's **R Markdown: The Definitive Guide** [@Xie2019b].  

Use `usethis::use_rmarkdown_template()` to create an Rmd template.  I will create a "Kaggle Report" template.  In the Console (or a script), enter

```{r eval=FALSE}
usethis::use_rmarkdown_template(
  template_name = "Kaggle Report",
  template_dir = "kaggle_report",
  template_description = "Template for creating Kaggle reports in RMarkdown.",
  template_create_dir = FALSE
)
```

Since my project directory is `C:\Users\mpfol\OneDrive\Documents\GitHub\mfstylr`, `use_rmarkdown_template()` creates subdirectories `.\inst\rmarkdown\templates\kaggle_report\skeleton` with three files

* `.\inst\rmarkdown\templates\kaggle_report\template.yaml`
* `.\inst\rmarkdown\templates\kaggle_report\skeleton\skeleton.Rmd`

My kaggle report template will include a logo.  Looks like there are two ways to embed an image in your document.  One is a direct image loading reference `!()`, but I don't think you can control the attributes this way.  A second way is adding html tags.

```{r eval=FALSE}
![](logo.png)

# or for more control
<img src="logo.png" style="position:absolute;top:0px;right:0px;" />
```

 

<!--chapter:end:15-appendix.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:16-references.Rmd-->

