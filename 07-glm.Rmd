# Generalized Linear Models

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(broom)  # for augment()
library(WVPlots)  # for GainCurvePlot()
library(caret)  # for RMSE()
library(readr)
```


These notes are primarily from PSU Online course [Analysis of Discrete Data](https://online.stat.psu.edu/stat504) which uses Alan Agresti's **Categorical Data Analysis** (which I have not yet purchased) [@Agresti2013].  I also reviewed:

Penn State University, STAT 501, "Lesson 15: Logistic, Poisson & Nonlinear Regression".  [https://newonlinecourses.science.psu.edu/stat501/lesson/15](https://newonlinecourses.science.psu.edu/stat501/lesson/15)

"Generalized Linar Models in R".  DataCamp.  [https://www.datacamp.com/courses/generalized-linear-models-in-r](https://www.datacamp.com/courses/generalized-linear-models-in-r).

"Multiple and Logistic Regression".  DataCamp. [https://www.datacamp.com/courses/multiple-and-logistic-regression](https://www.datacamp.com/courses/multiple-and-logistic-regression).

Molnar, Christoph. "Interpretable machine learning. A Guide for Making Black Box Models Explainable", 2019. [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/).


In generalized linear models (GLMs), the modeled response is *a function* of the mean of $y$.  There are three components to a GLM.  The *random component* is the probability distribution of the response variable (normal, binomial, Poisson, etc.). The *systematic component* is the explanatory variables $X\beta$.  The *link function* $\eta$ specifies the link between random and systematic components. The link function converts the response value from a range $[0,1]$ in logistic and probit and $[0,+\infty]$ for Poisson to a value ranging from $[-\infty, +\infty]$, and creates a linear relationship with the predictor variables.  

For a standard linear regression, the link function is the identity function,

$$f(\mu_Y) = \mu_Y.$$

The standard linear regression is thus a special case of the GLM.  For a logistic regression, the link function is 

$$f(\mu_Y) = \ln(\frac{\pi}{1-\pi})$$

where $\pi$ is the event probability.  For a probit regression, the link function is 

$$f(\mu_Y) = \Phi^{-1}(\pi).$$

The difference between logistic and probit link function is theoretical - [the practical significance is slight](https://www.theanalysisfactor.com/the-difference-between-logistic-and-probit-regression/).  Logistic regression has the advantage that it can be back-transformed from log odds to odds ratios.  For a Poisson regression, the link function is

$$f(\mu_Y) = \ln(\lambda)$$

where $\lambda$ is the expected event rate.  

GLM uses maximum likelihood estimation (MLE) rather than ordinary least squares (OLS) to estimate the parameters, and thus relies on large-sample approximations. 

In R, specify a GLM just like an linear model, but with the `glm()` function, specifying the distribution with the `family` parameter.

* `family = "gaussian"`: linear regression
* `family = "binomial"`: logistic regression
* `family = binomial(link = "probit")`: probit regression
* `family = "poisson"`: Poisson regression


## Logistic Regression

Logistic regression estimates the probability of a particular level of a categorical response variable given a set of predictors. The response levels can be binary, nominal (multiple categories), or ordinal (multiple levels).  

The **binary** logistic regression model is

$$y_i = logit(\pi_i) = \log\left(\frac{\pi_i}{1-\pi_i}\right) = X_i\beta$$

where $\pi_i$ is the "success probability" that observation $i$ is in a specified category of the binary *y* variable.  You can solve for $\pi$ to get

$$\pi = \frac{\exp(X \beta)}{1 + \exp(X \beta)}.$$ 
 
The model predicts the *log odds* of the response variable.  The maximum likelihood estimator maximizes the the likelihood function

$$L(\beta; y, X) = \prod_{i=1}^n \pi_i^{y_i}(1 - \pi_i)^{(1-y_i)} = \prod_{i=1}^n\frac{\exp(y_i X_i \beta)}{1 + \exp(X_i \beta)}.$$

There is no closed-form solution, so GLM estimates coefficients with interatively reweighted least squares.  


### Example {-}

Dataset `leuk` contains response variable `REMISS` indicating whether leukemia remission occurred (1|0) and several explanatory variables.

```{r}
data_dir <- "C:/Users/mpfol/OneDrive/Documents/Data Science/Data/"
leuk <- read_tsv(paste(data_dir, "leukemia_remission.txt", sep = "/"))
str(leuk)
```

Fit a logistic regression in R using `glm(formula, data, family = binomial)` where `family = binomial` specifies a binomial error distribution.

```{r}
m1 <- glm(REMISS ~ ., family = binomial, data = leuk)
m1
summary(m1)
```

The *predicted* value $\hat{y}$ is the estimated **log odds** of the response variable, 

$$\hat{y} = X \hat{\beta} = \ln (\frac{\pi}{1 - \pi}).$$

Suppose each predictor equals its mean value, then the log odds of `REMISS` is $-2.684$.  

```{r}
pred <- predict(m1, newdata = data.frame(CELL = mean(leuk$CELL),
                                         SMEAR = mean(leuk$SMEAR),
                                         INFIL = mean(leuk$INFIL),
                                         LI = mean(leuk$LI),
                                         BLAST = mean(leuk$BLAST),
                                         TEMP = mean(leuk$TEMP)))
```

Log odds are not easy to interpet, but it is convenient for updating prior probabilities in Bayesian analyses.  *See [this article](https://www.statisticshowto.datasciencecentral.com/log-odds/) in Statistics How To.*  Exponentiate the log odds to get the more intuitive **odds**.

$$\exp (\hat{y}) = \exp (X \hat{\beta}) = \frac{\pi}{1 - \pi}.$$

The odds of having achieved remission when each predictor equals its mean value is $\exp(\hat{y}) = 0.068$.  

```{r}
exp(pred)
```

You might express that more commonly as 1 / 0.068 = 15:1.  So a person with average values of the predictors has an odds of "15 to 1" of having achieved remission.

```{r}
1/exp(pred)
```

Or, solve for $\pi$ to get the **probability**.  

$$\pi = \frac{\exp (X \beta)}{1 + \exp (X \beta)}$$

The probability of having achieved remission when each predictor equals its mean value is $\pi = 0.064$.  The `predict()` function for a logistic model returns log-odds, but can also return $\pi$ by specifying parameter `type = "response"`.

```{r}
exp(pred) / (1 + exp(pred))
prob <- predict(m1, newdata = data.frame(CELL = mean(leuk$CELL),
                                         SMEAR = mean(leuk$SMEAR),
                                         INFIL = mean(leuk$INFIL),
                                         LI = mean(leuk$LI),
                                         BLAST = mean(leuk$BLAST),
                                         TEMP = mean(leuk$TEMP)),
                type = "response")
```

It is common to express the results in terms of the **odds ratio**.  The *odds ratio* is the ratio of the odds before and after an increment to the predictors.  It tells you how much the odds would be multiplied after a $X_1 - X_0$ unit increase in $X$.

$$\theta = \frac{\pi / (1 - \pi) |_{X = X_1}}{\pi / (1 - \pi) |_{X = X_0}} = \frac{\exp (X_1 \hat{\beta})}{\exp (X_0 \hat{\beta})} = \exp ((X_1-X_0) \hat{\beta}) = \exp (\delta \hat{\beta})$$

For example, increasing `LI` by .01 increases the odds of remission by a factor of $\exp(0.1 \cdot 4.36) = 1.547$ (from 15:1 to 23:1). 

```{r}
exp(.1 * m1$coefficients)
```

You can calculate an odds ratio using `oddsratio::or_glm()`.

```{r warning=FALSE}
library(oddsratio)
or_glm(data = leuk, 
       model = m1, 
       incr = list(CELL = 0.01, 
                   SMEAR = 0.01, 
                   INFIL = 5, 
                   LI = 0.1, 
                   BLAST = 1.0, 
                   TEMP = 0.3))
```

The predicted values can also be expressed as the probabilities $\pi$.  This produces the familiar signmoidal shape of the binary relationship.

```{r}
augment(m1, type.predict = "response") %>%
  ggplot(aes(x = LI, y = REMISS)) +
  geom_point() +
  geom_line(aes(y = .fitted), color = "red") + 
  labs(x = "LI",
       y = "Probability of Event",
       title = "Binary Fitted Line Plot")
```

Whereas in linear regression the the coefficient p-values use the *t* test (*t* statistic), logistic regression coefficient p-values use the *Wald test* **Z*-statistic).

$$Z = \frac{\hat{\beta_i}}{SE(\hat{\beta}_i)}$$

```{r}
round((z <- m1$coefficients / summary(m1)$coefficients[,"Std. Error"]), 3)
round(pnorm(abs(z), lower.tail = FALSE) * 2, 3)
```

Evaluate a logistic model fit with an analysis of deviance.  Deviance is defined as -2 times the log-likelihood $-2l(\beta)$.  The null deviance is the deviance of the null model and is analagous to SST in ANOVA.  The residual deviance is analagous to SSE in ANOVA.

```{r}
logLik(glm(REMISS ~ ., data = leuk, family = "binomial")) * (-2)
anova(m1)
m1
summary(m1)
```

The deviance of the null model (no regressors) is 34.372.  The deviance of the full model is 26.073.  

```{r}
glance(m1)
```

Use the `GainCurvePlot()` function to plot the gain curve (background on gain curve at [Data Science Central](https://www.datasciencecentral.com/profiles/blogs/understanding-and-interpreting-gain-and-lift-charts) from the model predictions. The x-axis is the fraction of items seen when sorted by the predicted value, and the y-axis is the cumulative summed true outcome. The "wizard" curve is the gain curve when the data is sorted by the true outcome.  If the model's gain curve is close to the wizard gain curve, then the model sorted the response variable well. The grey area is the gain over a random sorting.

```{r}
augment(m1) %>% data.frame() %>% 
  GainCurvePlot(xvar = ".fitted", truthVar = "REMISS", title = "Logistic Model")
```

`REMISS` equals 1 in 9 of the 27 responses. 

* The wizard curve shows that after sorting the responses it encounters all 9 1s (100%) after looking at 9 of the 27 response (33%).  
* The bottom of the grey diagonal shows that after making random predictions and sorting the predictions, it encounters only 3 1s (33%) after looking at 9 of the 27 responses (33%).  It has to look at all 27 responses (100%) to encounter all 9 1s (100%).  
* The gain curve encounters 5 1s (55%) after looking at 9 of the 27 responses (33%).  It has to look at 14 responses to encounter all 9 1s (100%).

Another way to evaluate the predictive model is the ROC curve.  It evaluates all possible thresholds for splitting predicted probabilities into predicted classes.  This is often a much more useful metric than simply ranking models by their accuracy at a set threshold, as different models might require different calibration steps (looking at a confusion matrix at each step) to find the optimal classification threshold for that model.

```{r}
library(caTools)
colAUC(m1$fitted.values, m1$data$REMISS, plotROC = TRUE)
```


## Ordinal Logistic Regression

The following notes rely heavily upon [this UVA web page](https://data.library.virginia.edu/fitting-and-interpreting-a-proportional-odds-model/).

The ordinal logistic regression model is

$$logit[P(Y \le j)] = \alpha_j - \beta X, \hspace{5mm} j \in [1, J-1]$$
where $j \in [1, J-1]$ are the levels of the ordinal outcome variable $Y$.  The proportional odds model assumes there is a common set of slope parameters $\beta$ for the predictors.  The ordinal outcomes are distinguished by the $J-1$ intercepts $\alpha_j$.  The benchmark level is $J$.  "Logit" means log-odds, so $logit[P(Y \le j)] = \log[P(Y \le j) / P(Y \gt j)]$.

Suppose we want to model the probability a respondent holds a political ideology ["Very Liberal", "Slightly Liberal", "Moderate", "Slightly Conservative", "Very Conservative"] given their party affiliation ["Republican", "Democrat"].

```{r}
ideo <- c(
  "Very Liberal",
  "Slightly Liberal",
  "Moderate",
  "Slightly Conservative",
  "Very Conservative"
)
ideo_cnt_rep <- c(30, 46, 148, 84, 99)
ideo_cnt_dem <- c(80, 81, 171, 41, 55)
dat <- data.frame(
  party = factor(rep(c("Rep", "Dem"), c(407, 428)), levels = c("Rep", "Dem")),
  ideo = factor(
    c(rep(ideo, ideo_cnt_rep), rep(ideo, ideo_cnt_dem)), 
    levels = ideo
  )
)
table(dat)
```

Fit a proportional odds model with the `MASS::polr()` function (polr stands for proportional odds linear regression).

```{r}
pom <- MASS::polr(ideo ~ party, data = dat)

summary(pom)
```

The log-odds a Democrat identifies as “Very Liberal” or lower is 
$$logit[P(Y \lt 1)] = -2.4690 - (-0.9745)(1) = -1.4945.$$
Solve $logit[P(Y \le 1)] = \log[P(Y \le 1) / P(Y \gt 1)]$ for $P(Y \le 1)$ to get the probability a Democrat identifies as “Very Liberal” or lower as $P(Y \le 1) = exp(-1.4945) / (1 + exp(-1.4945)) = 0.183$. 


The log odds a Democrat identifies as “Slightly Liberal” or lower is
$$logit[P(Y \lt 2)] = -1.4745 - (-0.9745)(1) = -0.5.$$
The corresponding probability is $P(Y \le 2) = exp(-0.5) / (1 + exp(-0.5)) = 0.378$. 

To get the probability a Democrat identifies as "Slightly Liberal", just subtract the adjacent cumulative probabilities, $P(Y \le 2) - P(Y \le 1) = 0.378 = 0.183 = 0.194$.  That's what's happening when you use the model to predict the level probabilities.

```{r}
predict(pom, newdata = data.frame(party = "Dem"), type = "probs")
```

The baseline for the model was "Republican", so the log-odds a Republicn identifies as "Very Liberal" or lower is 
$$logit[P(Y \lt 1)] = -2.4690 - (-0.9745)(0) = -2.4690$$
with corresponding probability $P(Y \le 1) = exp(-2.4690) / (1 + exp(-2.4690)) = 0.078$.  And so on.

```{r}
predict(pom, newdata = data.frame(party = "Rep"), type = "probs")
```

Here is a manual calculation of the probabilities.  Note the `zeta` variable instead of `beta` because the model fits $\alpha_j - \beta X$ instead of $\alpha_j + \beta X$, and so it uses a new variable $\zeta = -\beta$.  Technically, the model could be expressed $logit[P(Y \le j)] = \alpha_j + \zeta X$.

```{r}
library(stringr)
# cum log odds, dems an reps
dclo <- c(pom$zeta - pom$coefficients)
rclo <- c(pom$zeta - 0)
# cum probs, dems and reps
dcp <- exp(dclo) / (1 + exp(dclo))
rcp <- exp(rclo) / (1 + exp(rclo))
# fix the names and add 1 for "Very..."
names(dcp) <- str_sub(names(dcp), start = 1, end = str_locate(names(dcp), "\\|")[, 1] - 1)
names(rcp) <- str_sub(names(rcp), start = 1, end = str_locate(names(rcp), "\\|")[, 1] - 1)
dcp <- c(dcp, 1)
rcp <- c(rcp, 1)
names(dcp) <- c(names(dcp)[1:4], "Very Conservative")
names(rcp) <- c(names(rcp)[1:4], "Very Conservative")
# Democrat probs
(dp <- dcp - lag(dcp, 1, 0))
(rp <- rcp - lag(rcp, 1, 0))
```

The "proportional odds" part of the proportional odds model is that the ratios of the $J - 1$ odds-ratios are identical for the different levels of the predictors.  Here we have a single predictor, `party`.  The odds ratios for `party = Dem` are

```{r}
(dcp / (1 - dcp)) / (rcp / (1 - rcp))
```

Which is to say, any level of ideology $j$, the estimated odds that a Democrat’s ideology is more liberal $(\lt j)$ rather than more conservative $(\ge j)$ is about 2.65 times a Republicans odds.

The log of these odds ratios is the coefficient estimator for `party`.
```{r}
log((dcp / (1 - dcp)) / (rcp / (1 - rcp)))
```

Always check the assumption of proportional odds. One way to do this is by comparing the proportional odds model with a multinomial logit model, also called an unconstrained baseline logit model. The multinomial logit model models *unordered* responses and fits a slope to each level of the $J – 1$ responses. The proportional odds model is nested in the multinomial model, so can use a likelihood ratio test to see if the models are statistically different.

```{r}
mlm <- nnet::multinom(ideo ~ party, data = dat)
```

Calculate the deviance test statistic $D = -2 loglik(\beta)$.

```{r}
G <- -2 * (logLik(pom)[1] - logLik(mlm)[1])
pchisq(G, df = length(pom$zeta) - 1, lower.tail = FALSE)
```

The p-value is high, so the proportional odds model fits as well as the more complex multinomial logit model.


## Poisson Regression

Poisson models count data, like "traffic tickets per day", or "website hits per day".  The response is an expected *rate* or intensity.  For count data, specify the generalized model, this time with `family = poisson` or `family = quasipoisson`. 

Recall that the probability of achieving a count $y$ when the expected rate is $\lambda$ is distributed 

$$P(Y = y|\lambda) = \frac{e^{-\lambda} \lambda^y}{y!}.$$


The poisson regression model is

$$\lambda = \exp(X \beta).$$ 
 
You can solve this for $y$ to get

$$y = X\beta = \ln(\lambda).$$

That is, the model predicts the log of the response rate.  For a sample of size *n*, the likelihood function is

$$L(\beta; y, X) = \prod_{i=1}^n \frac{e^{-\exp({X_i\beta})}\exp({X_i\beta})^{y_i}}{y_i!}.$$

The log-likelihood is

$$l(\beta) = \sum_{i=1}^n (y_i X_i \beta - \sum_{i=1}^n\exp(X_i\beta) - \sum_{i=1}^n\log(y_i!).$$

Maximizing the log-likelihood has no closed-form solution, so the coefficient estimates are found through interatively reweighted least squares.  

Poisson processes assume the variance of the response variable equals its mean.  "Equals" means the mean and variance are of a similar order of magnitude.  If that assumption does not hold, use the quasi-poisson.  Use Poisson regression for large datasets.  If the predicted counts are much greater than zero (>30), the linear regression will work fine.  Whereas RMSE is not useful for logistic models, it is a good metric in Poisson.


### Example {-}

Dataset `fire` contains response variable `injuries` counting the number of injuries during the month and one explanatory variable, the month `mo`.

```{r}
fire <- read_csv(file = "C:/Users/mpfol/OneDrive/Documents/Data Science/Data/CivilInjury_0.csv")
fire <- fire %>% 
  mutate(mo = as.POSIXlt(`Injury Date`)$mon + 1) %>%
  rename(dt = `Injury Date`,
         injuries = `Total Injuries`)
str(fire)
```

In a situation like this where there the relationship is bivariate, start with a visualization.

```{r}
ggplot(fire, aes(x = mo, y = injuries)) +
  geom_jitter() +
  geom_smooth(method = "glm", method.args = list(family = "poisson")) +
  labs(title = "Injuries by Month")
```


Fit a poisson regression in R using `glm(formula, data, family = poisson)`.  But first, check whether the mean and variance of `injuries` are the same magnitude?  If not, then use `family = quasipoisson`.

```{r}
mean(fire$injuries)
var(fire$injuries)
```

They are of the same magnitude, so fit the regression with `family = poisson`.

```{r}
m2 <- glm(injuries ~ mo, family = poisson, data = fire)
summary(m2)
```

The *predicted* value $\hat{y}$ is the estimated **log** of the response variable, 

$$\hat{y} = X \hat{\beta} = \ln (\lambda).$$

Suppose `mo` is January (mo = `), then the log of `injuries` is $\hat{y} = 0.323787$. Or, more intuitively, the expected count of injuries is $\exp(0.323787) = 1.38$  

```{r}
predict(m2, newdata = data.frame(mo=1))
predict(m2, newdata = data.frame(mo=1), type = "response")
```

Here is a plot of the predicted counts in red.

```{r}
augment(m2, type.predict = "response") %>%
  ggplot(aes(x = mo, y = injuries)) +
  geom_point() +
  geom_point(aes(y = .fitted), color = "red") + 
  scale_y_continuous(limits = c(0, NA)) +
  labs(x = "Month",
       y = "Injuries",
       title = "Poisson Fitted Line Plot")
```

Evaluate a logistic model fit with an analysis of deviance.  

```{r}
(perf <- glance(m2))
(pseudoR2 <- 1 - perf$deviance / perf$null.deviance)
```

The deviance of the null model (no regressors) is 139.9.  The deviance of the full model is 132.2.  The psuedo-R2 is very low at .05.  How about the RMSE?

```{r}
RMSE(pred = predict(m2, type = "response"), obs = fire$injuries)
```

The average prediction error is about 0.99.  That's almost as much as the variance of `injuries` - i.e., just predicting the mean of `injuries` would be almost as good!  Use the `GainCurvePlot()` function to plot the gain curve.

```{r}
augment(m2, type.predict = "response") %>%
  ggplot(aes(x = injuries, y = .fitted)) +
  geom_point() +
  geom_smooth(method ="lm") +
  labs(x = "Actual",
       y = "Predicted",
       title = "Poisson Fitted vs Actual")
```


```{r}
augment(m2) %>% data.frame() %>% 
  GainCurvePlot(xvar = ".fitted", truthVar = "injuries", title = "Poisson Model")
```

It seems that `mo` was a poor predictor of `injuries`.  

